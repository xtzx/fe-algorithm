# ğŸ¯ 18 - ç»¼åˆå®æˆ˜é¡¹ç›®

> ç«¯åˆ°ç«¯æ•°æ®åˆ†æé¡¹ç›®ï¼ŒæŠŠæ‰€å­¦æŠ€èƒ½ä¸²è”èµ·æ¥

---

## ç›®å½•

1. [é¡¹ç›® 1ï¼šç”µå½±æ•°æ®åˆ†æï¼ˆåˆçº§ï¼‰](#é¡¹ç›®-1ç”µå½±æ•°æ®åˆ†æåˆçº§)
2. [é¡¹ç›® 2ï¼šæ–‡æœ¬ç›¸ä¼¼åº¦æœç´¢ï¼ˆä¸­çº§ï¼‰](#é¡¹ç›®-2æ–‡æœ¬ç›¸ä¼¼åº¦æœç´¢ä¸­çº§)
3. [é¡¹ç›® 3ï¼šè‚¡ç¥¨æ•°æ®åˆ†æï¼ˆè¿›é˜¶ï¼‰](#é¡¹ç›®-3è‚¡ç¥¨æ•°æ®åˆ†æè¿›é˜¶)
4. [é¡¹ç›®æ‰©å±•æ–¹å‘](#é¡¹ç›®æ‰©å±•æ–¹å‘)

---

## é¡¹ç›® 1ï¼šç”µå½±æ•°æ®åˆ†æï¼ˆåˆçº§ï¼‰

### ğŸ“‹ é¡¹ç›®æ¦‚è¿°

| é¡¹ç›®ä¿¡æ¯ | å†…å®¹ |
|---------|------|
| **éš¾åº¦** | â­ åˆçº§ |
| **æ•°æ®é›†** | TMDB 5000 Movies |
| **æŠ€èƒ½ç‚¹** | Pandasã€Matplotlibã€æ•°æ®æ¸…æ´—ã€EDA |
| **é¢„è®¡æ—¶é—´** | 2-3 å°æ—¶ |

### ğŸ¯ å­¦ä¹ ç›®æ ‡

- æŒæ¡æ•°æ®åŠ è½½ä¸æ¢ç´¢
- å­¦ä¼šå¤„ç†ç¼ºå¤±å€¼å’Œå¼‚å¸¸å€¼
- è¿›è¡Œæ•°æ®å¯è§†åŒ–åˆ†æ
- æ„å»ºç®€å•çš„é¢„æµ‹æ¨¡å‹

### ğŸ“¦ ç¯å¢ƒå‡†å¤‡

```python
# å®‰è£…ä¾èµ–
# pip install pandas numpy matplotlib seaborn scikit-learn

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
import warnings
warnings.filterwarnings('ignore')

# è®¾ç½®ä¸­æ–‡æ˜¾ç¤º
plt.rcParams['font.sans-serif'] = ['SimHei', 'Arial Unicode MS']
plt.rcParams['axes.unicode_minus'] = False

print("âœ… ç¯å¢ƒå‡†å¤‡å®Œæˆ")
```

### 1ï¸âƒ£ æ•°æ®åŠ è½½ä¸æ¢ç´¢ï¼ˆEDAï¼‰

```python
# ä¸‹è½½æ•°æ®é›†ï¼ˆä» Kaggleï¼‰
# https://www.kaggle.com/datasets/tmdb/tmdb-movie-metadata

# åŠ è½½æ•°æ®
movies = pd.read_csv('tmdb_5000_movies.csv')
credits = pd.read_csv('tmdb_5000_credits.csv')

# åŸºç¡€ä¿¡æ¯
print("=" * 50)
print("ğŸ“Š æ•°æ®é›†åŸºæœ¬ä¿¡æ¯")
print("=" * 50)
print(f"ç”µå½±æ•°é‡: {len(movies)}")
print(f"ç‰¹å¾æ•°é‡: {len(movies.columns)}")
print(f"\nåˆ—å: {movies.columns.tolist()}")

# æŸ¥çœ‹å‰å‡ è¡Œ
movies.head()
```

```python
# æ•°æ®ç±»å‹å’Œç¼ºå¤±å€¼
print("\nğŸ“‹ æ•°æ®ç±»å‹:")
print(movies.dtypes)

print("\nâ“ ç¼ºå¤±å€¼ç»Ÿè®¡:")
missing = movies.isnull().sum()
missing_pct = (missing / len(movies) * 100).round(2)
missing_df = pd.DataFrame({
    'ç¼ºå¤±æ•°é‡': missing,
    'ç¼ºå¤±æ¯”ä¾‹(%)': missing_pct
})
print(missing_df[missing_df['ç¼ºå¤±æ•°é‡'] > 0])

# ç»Ÿè®¡æè¿°
print("\nğŸ“ˆ æ•°å€¼ç‰¹å¾ç»Ÿè®¡:")
movies.describe()
```

```python
# æ•°æ®åˆ†å¸ƒå¯è§†åŒ–
fig, axes = plt.subplots(2, 2, figsize=(14, 10))

# 1. é¢„ç®—åˆ†å¸ƒ
axes[0, 0].hist(movies['budget'][movies['budget'] > 0], bins=50, edgecolor='black')
axes[0, 0].set_title('ç”µå½±é¢„ç®—åˆ†å¸ƒ')
axes[0, 0].set_xlabel('é¢„ç®— ($)')
axes[0, 0].set_ylabel('ç”µå½±æ•°é‡')

# 2. ç¥¨æˆ¿åˆ†å¸ƒ
axes[0, 1].hist(movies['revenue'][movies['revenue'] > 0], bins=50, edgecolor='black', color='green')
axes[0, 1].set_title('ç”µå½±ç¥¨æˆ¿åˆ†å¸ƒ')
axes[0, 1].set_xlabel('ç¥¨æˆ¿ ($)')
axes[0, 1].set_ylabel('ç”µå½±æ•°é‡')

# 3. è¯„åˆ†åˆ†å¸ƒ
axes[1, 0].hist(movies['vote_average'], bins=20, edgecolor='black', color='orange')
axes[1, 0].set_title('ç”µå½±è¯„åˆ†åˆ†å¸ƒ')
axes[1, 0].set_xlabel('è¯„åˆ†')
axes[1, 0].set_ylabel('ç”µå½±æ•°é‡')

# 4. æ—¶é•¿åˆ†å¸ƒ
axes[1, 1].hist(movies['runtime'].dropna(), bins=30, edgecolor='black', color='purple')
axes[1, 1].set_title('ç”µå½±æ—¶é•¿åˆ†å¸ƒ')
axes[1, 1].set_xlabel('æ—¶é•¿ (åˆ†é’Ÿ)')
axes[1, 1].set_ylabel('ç”µå½±æ•°é‡')

plt.tight_layout()
plt.show()
```

### 2ï¸âƒ£ æ•°æ®æ¸…æ´—

```python
# åˆ›å»ºå·¥ä½œå‰¯æœ¬
df = movies.copy()

# 1. å¤„ç†ç¼ºå¤±å€¼
print("å¤„ç†å‰å½¢çŠ¶:", df.shape)

# åˆ é™¤å…³é”®åˆ—ç¼ºå¤±çš„è¡Œ
df = df.dropna(subset=['release_date', 'runtime'])

# å¡«å……å…¶ä»–ç¼ºå¤±å€¼
df['homepage'] = df['homepage'].fillna('æ— ')
df['tagline'] = df['tagline'].fillna('æ— ')
df['overview'] = df['overview'].fillna('æ— ç®€ä»‹')

print("å¤„ç†åå½¢çŠ¶:", df.shape)

# 2. å¤„ç†å¼‚å¸¸å€¼
# è¿‡æ»¤æ‰é¢„ç®—å’Œç¥¨æˆ¿ä¸º 0 çš„ç”µå½±ï¼ˆé€šå¸¸æ˜¯æ•°æ®ç¼ºå¤±ï¼‰
df_valid = df[(df['budget'] > 0) & (df['revenue'] > 0)]
print(f"æœ‰æ•ˆæ•°æ®ï¼ˆé¢„ç®—å’Œç¥¨æˆ¿ > 0ï¼‰: {len(df_valid)} æ¡")

# 3. è½¬æ¢æ—¥æœŸ
df['release_date'] = pd.to_datetime(df['release_date'])
df['release_year'] = df['release_date'].dt.year
df['release_month'] = df['release_date'].dt.month

# 4. è§£æ JSON æ ¼å¼çš„åˆ—
import ast

def parse_json_column(x):
    """è§£æ JSON å­—ç¬¦ä¸²åˆ—"""
    try:
        return ast.literal_eval(x)
    except:
        return []

# è§£æç±»å‹
df['genres_list'] = df['genres'].apply(parse_json_column)
df['genre_names'] = df['genres_list'].apply(
    lambda x: [g['name'] for g in x] if x else []
)

# è§£æå…³é”®è¯
df['keywords_list'] = df['keywords'].apply(parse_json_column)

print("\nâœ… æ•°æ®æ¸…æ´—å®Œæˆ")
df.head()
```

### 3ï¸âƒ£ ç‰¹å¾å·¥ç¨‹

```python
# åˆ›å»ºæ–°ç‰¹å¾

# 1. ROIï¼ˆæŠ•èµ„å›æŠ¥ç‡ï¼‰
df_valid = df[(df['budget'] > 0) & (df['revenue'] > 0)].copy()
df_valid['roi'] = (df_valid['revenue'] - df_valid['budget']) / df_valid['budget']

# 2. æ˜¯å¦ç›ˆåˆ©
df_valid['is_profitable'] = df_valid['revenue'] > df_valid['budget']

# 3. ç±»å‹æ•°é‡
df_valid['genre_count'] = df_valid['genre_names'].apply(len)

# 4. æ ‡é¢˜é•¿åº¦
df_valid['title_length'] = df_valid['title'].apply(len)

# 5. æ˜¯å¦æœ‰ä¸»é¡µ
df_valid['has_homepage'] = df_valid['homepage'] != 'æ— '

# 6. å‘å¸ƒå­£åº¦
df_valid['release_quarter'] = df_valid['release_date'].dt.quarter

print("æ–°ç‰¹å¾:")
print(df_valid[['title', 'roi', 'is_profitable', 'genre_count', 'title_length']].head())
```

### 4ï¸âƒ£ å¯è§†åŒ–åˆ†æ

```python
# 4.1 ç¥¨æˆ¿ Top 20
fig, ax = plt.subplots(figsize=(12, 8))
top20 = df_valid.nlargest(20, 'revenue')
bars = ax.barh(top20['title'], top20['revenue'] / 1e9)
ax.set_xlabel('ç¥¨æˆ¿ (åäº¿ç¾å…ƒ)')
ax.set_title('ç¥¨æˆ¿ Top 20 ç”µå½±')
ax.invert_yaxis()
plt.tight_layout()
plt.show()
```

```python
# 4.2 å¹´åº¦ç”µå½±æ•°é‡è¶‹åŠ¿
fig, axes = plt.subplots(1, 2, figsize=(14, 5))

# å¹´åº¦æ•°é‡
yearly = df.groupby('release_year').size()
axes[0].plot(yearly.index, yearly.values, marker='o', markersize=3)
axes[0].set_xlabel('å¹´ä»½')
axes[0].set_ylabel('ç”µå½±æ•°é‡')
axes[0].set_title('å¹´åº¦ç”µå½±å‘å¸ƒæ•°é‡è¶‹åŠ¿')
axes[0].grid(True, alpha=0.3)

# å¹´åº¦å¹³å‡è¯„åˆ†
yearly_rating = df.groupby('release_year')['vote_average'].mean()
axes[1].plot(yearly_rating.index, yearly_rating.values, marker='o', markersize=3, color='orange')
axes[1].set_xlabel('å¹´ä»½')
axes[1].set_ylabel('å¹³å‡è¯„åˆ†')
axes[1].set_title('å¹´åº¦å¹³å‡è¯„åˆ†è¶‹åŠ¿')
axes[1].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
```

```python
# 4.3 ç”µå½±ç±»å‹åˆ†æ
from collections import Counter

# ç»Ÿè®¡å„ç±»å‹æ•°é‡
all_genres = []
for genres in df['genre_names']:
    all_genres.extend(genres)

genre_counts = Counter(all_genres)
genre_df = pd.DataFrame(genre_counts.items(), columns=['ç±»å‹', 'æ•°é‡'])
genre_df = genre_df.sort_values('æ•°é‡', ascending=False)

fig, ax = plt.subplots(figsize=(12, 6))
bars = ax.bar(genre_df['ç±»å‹'], genre_df['æ•°é‡'], color='steelblue')
ax.set_xlabel('ç”µå½±ç±»å‹')
ax.set_ylabel('æ•°é‡')
ax.set_title('ç”µå½±ç±»å‹åˆ†å¸ƒ')
plt.xticks(rotation=45, ha='right')
plt.tight_layout()
plt.show()
```

```python
# 4.4 é¢„ç®—ä¸ç¥¨æˆ¿å…³ç³»
fig, ax = plt.subplots(figsize=(10, 8))

scatter = ax.scatter(
    df_valid['budget'] / 1e6,
    df_valid['revenue'] / 1e6,
    c=df_valid['vote_average'],
    cmap='RdYlGn',
    alpha=0.6,
    s=30
)
ax.set_xlabel('é¢„ç®— (ç™¾ä¸‡ç¾å…ƒ)')
ax.set_ylabel('ç¥¨æˆ¿ (ç™¾ä¸‡ç¾å…ƒ)')
ax.set_title('é¢„ç®— vs ç¥¨æˆ¿ï¼ˆé¢œè‰²è¡¨ç¤ºè¯„åˆ†ï¼‰')

# æ·»åŠ å¯¹è§’çº¿ï¼ˆç›ˆäºå¹³è¡¡ï¼‰
max_val = max(df_valid['budget'].max(), df_valid['revenue'].max()) / 1e6
ax.plot([0, max_val], [0, max_val], 'r--', label='ç›ˆäºå¹³è¡¡çº¿')

plt.colorbar(scatter, label='è¯„åˆ†')
ax.legend()
plt.tight_layout()
plt.show()

# ç›¸å…³ç³»æ•°
corr = df_valid[['budget', 'revenue', 'vote_average', 'runtime', 'popularity']].corr()
print("\nç›¸å…³ç³»æ•°çŸ©é˜µ:")
print(corr.round(3))
```

### 5ï¸âƒ£ ç®€å•é¢„æµ‹æ¨¡å‹

```python
# ä½¿ç”¨é¢„ç®—é¢„æµ‹ç¥¨æˆ¿

# å‡†å¤‡æ•°æ®
features = ['budget', 'popularity', 'runtime', 'vote_average', 'genre_count']
X = df_valid[features].dropna()
y = df_valid.loc[X.index, 'revenue']

# åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

print(f"è®­ç»ƒé›†å¤§å°: {len(X_train)}")
print(f"æµ‹è¯•é›†å¤§å°: {len(X_test)}")

# è®­ç»ƒçº¿æ€§å›å½’æ¨¡å‹
model = LinearRegression()
model.fit(X_train, y_train)

# é¢„æµ‹
y_pred = model.predict(X_test)

# è¯„ä¼°
mse = mean_squared_error(y_test, y_pred)
rmse = np.sqrt(mse)
r2 = r2_score(y_test, y_pred)

print(f"\nğŸ“Š æ¨¡å‹è¯„ä¼°:")
print(f"RMSE: ${rmse/1e6:.2f} ç™¾ä¸‡")
print(f"RÂ² Score: {r2:.4f}")

# ç‰¹å¾é‡è¦æ€§
print(f"\nğŸ“ˆ ç‰¹å¾ç³»æ•°:")
for feat, coef in zip(features, model.coef_):
    print(f"  {feat}: {coef:.2f}")
```

```python
# å¯è§†åŒ–é¢„æµ‹ç»“æœ
fig, ax = plt.subplots(figsize=(10, 8))

ax.scatter(y_test / 1e6, y_pred / 1e6, alpha=0.5)
ax.plot([0, y_test.max() / 1e6], [0, y_test.max() / 1e6], 'r--', label='å®Œç¾é¢„æµ‹')
ax.set_xlabel('å®é™…ç¥¨æˆ¿ (ç™¾ä¸‡ç¾å…ƒ)')
ax.set_ylabel('é¢„æµ‹ç¥¨æˆ¿ (ç™¾ä¸‡ç¾å…ƒ)')
ax.set_title(f'ç¥¨æˆ¿é¢„æµ‹ (RÂ² = {r2:.3f})')
ax.legend()
plt.tight_layout()
plt.show()
```

### 6ï¸âƒ£ ç»“è®ºæŠ¥å‘Š

```python
print("=" * 60)
print("ğŸ“ ç”µå½±æ•°æ®åˆ†ææŠ¥å‘Š")
print("=" * 60)

print("""
ğŸ” æ•°æ®æ¦‚è§ˆï¼š
- åˆ†æäº† {total} éƒ¨ç”µå½±
- æ—¶é—´èŒƒå›´ï¼š{min_year} - {max_year}
- æœ‰æ•ˆæ•°æ®ï¼ˆæœ‰é¢„ç®—å’Œç¥¨æˆ¿ï¼‰ï¼š{valid} éƒ¨

ğŸ“Š å…³é”®å‘ç°ï¼š

1. ç¥¨æˆ¿åˆ†å¸ƒ
   - ç¥¨æˆ¿å‘ˆç°ä¸¥é‡å³ååˆ†å¸ƒ
   - å°‘æ•°å¤§ç‰‡è´¡çŒ®äº†å¤§éƒ¨åˆ†ç¥¨æˆ¿
   - å¹³å‡ç¥¨æˆ¿ï¼š${avg_revenue:.0f} ä¸‡ç¾å…ƒ

2. ç±»å‹åˆ†æ
   - æœ€å¸¸è§ç±»å‹ï¼š{top_genre}
   - ç±»å‹æ•°é‡ä¸ç¥¨æˆ¿å¼±æ­£ç›¸å…³

3. é¢„ç®—ä¸ç¥¨æˆ¿
   - é¢„ç®—å’Œç¥¨æˆ¿å¼ºæ­£ç›¸å…³ (r = {budget_corr:.2f})
   - é«˜é¢„ç®—ç”µå½±æ›´å®¹æ˜“è·å¾—é«˜ç¥¨æˆ¿
   - ä½†å­˜åœ¨å¤§é‡ä½é¢„ç®—é«˜ç¥¨æˆ¿æ¡ˆä¾‹

4. è¯„åˆ†è¶‹åŠ¿
   - å¹³å‡è¯„åˆ†ï¼š{avg_rating:.1f}
   - è¯„åˆ†ä¸ç¥¨æˆ¿å¼±æ­£ç›¸å…³

ğŸ¯ é¢„æµ‹æ¨¡å‹ï¼š
- ä½¿ç”¨çº¿æ€§å›å½’é¢„æµ‹ç¥¨æˆ¿
- RÂ² = {r2:.3f}
- ä¸»è¦å½±å“å› ç´ ï¼šé¢„ç®—ã€äººæ°”æŒ‡æ•°

ğŸ’¡ å»ºè®®ï¼š
- é«˜é¢„ç®—ä¸ä¿è¯é«˜ç¥¨æˆ¿
- å…³æ³¨ç»†åˆ†å¸‚åœºå’Œå£ç¢‘è¥é”€
- è€ƒè™‘æ›´å¤šç‰¹å¾ï¼ˆå¯¼æ¼”ã€æ¼”å‘˜ç­‰ï¼‰
""".format(
    total=len(df),
    min_year=df['release_year'].min(),
    max_year=df['release_year'].max(),
    valid=len(df_valid),
    avg_revenue=df_valid['revenue'].mean() / 1e4,
    top_genre=genre_df.iloc[0]['ç±»å‹'],
    budget_corr=corr.loc['budget', 'revenue'],
    avg_rating=df['vote_average'].mean(),
    r2=r2
))
```

### ğŸ“ æŠ€èƒ½ç‚¹æ€»ç»“

| æŠ€èƒ½ | ç”¨åˆ°çš„çŸ¥è¯† |
|------|-----------|
| **æ•°æ®åŠ è½½** | `pd.read_csv()` |
| **æ•°æ®æ¢ç´¢** | `.info()`, `.describe()`, `.head()` |
| **ç¼ºå¤±å€¼å¤„ç†** | `.isnull()`, `.dropna()`, `.fillna()` |
| **æ•°æ®è½¬æ¢** | `pd.to_datetime()`, `.apply()` |
| **JSON è§£æ** | `ast.literal_eval()` |
| **ç‰¹å¾å·¥ç¨‹** | åˆ›å»ºæ´¾ç”Ÿç‰¹å¾ |
| **æ•°æ®å¯è§†åŒ–** | Matplotlib, Seaborn |
| **æœºå™¨å­¦ä¹ ** | sklearn LinearRegression |

---

## é¡¹ç›® 2ï¼šæ–‡æœ¬ç›¸ä¼¼åº¦æœç´¢ï¼ˆä¸­çº§ï¼‰

### ğŸ“‹ é¡¹ç›®æ¦‚è¿°

| é¡¹ç›®ä¿¡æ¯ | å†…å®¹ |
|---------|------|
| **éš¾åº¦** | â­â­ ä¸­çº§ |
| **ä»»åŠ¡** | æ„å»ºç®€æ˜“è¯­ä¹‰æœç´¢å¼•æ“ |
| **æŠ€èƒ½ç‚¹** | NumPy å‘é‡è¿ç®—ã€æ–‡æœ¬å¤„ç†ã€ä½™å¼¦ç›¸ä¼¼åº¦ |
| **é¢„è®¡æ—¶é—´** | 3-4 å°æ—¶ |

### ğŸ¯ å­¦ä¹ ç›®æ ‡

- ç†è§£æ–‡æœ¬å‘é‡åŒ–åŸç†
- æŒæ¡ TF-IDF ç®—æ³•
- å®ç°ä½™å¼¦ç›¸ä¼¼åº¦è®¡ç®—
- æ„å»ºç®€å•çš„æœç´¢ç³»ç»Ÿ

### ğŸ“¦ ç¯å¢ƒå‡†å¤‡

```python
import numpy as np
import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import re
from collections import Counter

print("âœ… ç¯å¢ƒå‡†å¤‡å®Œæˆ")
```

### 1ï¸âƒ£ å‡†å¤‡æ–‡æœ¬æ•°æ®

```python
# ç¤ºä¾‹æ–‡æ¡£åº“ï¼ˆä¹Ÿå¯ä»¥ç”¨çœŸå®æ–°é—»æ•°æ®ï¼‰
documents = [
    {"id": 1, "title": "Python å…¥é—¨æ•™ç¨‹",
     "content": "Python æ˜¯ä¸€ç§ç®€å•æ˜“å­¦çš„ç¼–ç¨‹è¯­è¨€ï¼Œå¹¿æ³›ç”¨äºæ•°æ®ç§‘å­¦å’Œäººå·¥æ™ºèƒ½é¢†åŸŸã€‚"},
    {"id": 2, "title": "æœºå™¨å­¦ä¹ åŸºç¡€",
     "content": "æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œé€šè¿‡æ•°æ®è®­ç»ƒæ¨¡å‹æ¥è¿›è¡Œé¢„æµ‹ã€‚"},
    {"id": 3, "title": "æ·±åº¦å­¦ä¹ å…¥é—¨",
     "content": "æ·±åº¦å­¦ä¹ ä½¿ç”¨ç¥ç»ç½‘ç»œå¤„ç†å¤æ‚é—®é¢˜ï¼Œæ˜¯æœºå™¨å­¦ä¹ çš„ä¸€ä¸ªå­é¢†åŸŸã€‚"},
    {"id": 4, "title": "æ•°æ®ç§‘å­¦å·¥å…·",
     "content": "NumPy å’Œ Pandas æ˜¯ Python æ•°æ®ç§‘å­¦çš„æ ¸å¿ƒå·¥å…·åº“ã€‚"},
    {"id": 5, "title": "Web å¼€å‘æ¡†æ¶",
     "content": "Flask å’Œ Django æ˜¯æµè¡Œçš„ Python Web å¼€å‘æ¡†æ¶ã€‚"},
    {"id": 6, "title": "è‡ªç„¶è¯­è¨€å¤„ç†",
     "content": "NLP æ˜¯äººå·¥æ™ºèƒ½å¤„ç†äººç±»è¯­è¨€çš„æŠ€æœ¯ï¼ŒåŒ…æ‹¬æ–‡æœ¬åˆ†ç±»å’Œæƒ…æ„Ÿåˆ†æã€‚"},
    {"id": 7, "title": "è®¡ç®—æœºè§†è§‰",
     "content": "è®¡ç®—æœºè§†è§‰è®©æœºå™¨èƒ½å¤Ÿç†è§£å›¾åƒå’Œè§†é¢‘ï¼Œåº”ç”¨äºäººè„¸è¯†åˆ«å’Œè‡ªåŠ¨é©¾é©¶ã€‚"},
    {"id": 8, "title": "æ¨èç³»ç»Ÿ",
     "content": "æ¨èç³»ç»Ÿä½¿ç”¨æœºå™¨å­¦ä¹ ç®—æ³•ä¸ºç”¨æˆ·æ¨èä¸ªæ€§åŒ–å†…å®¹ã€‚"},
    {"id": 9, "title": "å¼ºåŒ–å­¦ä¹ ",
     "content": "å¼ºåŒ–å­¦ä¹ é€šè¿‡å¥–åŠ±æœºåˆ¶è®­ç»ƒæ™ºèƒ½ä½“åœ¨ç¯å¢ƒä¸­åšå‡ºå†³ç­–ã€‚"},
    {"id": 10, "title": "å¤§æ•°æ®å¤„ç†",
     "content": "Spark å’Œ Hadoop æ˜¯å¤„ç†å¤§è§„æ¨¡æ•°æ®çš„åˆ†å¸ƒå¼è®¡ç®—æ¡†æ¶ã€‚"},
]

df = pd.DataFrame(documents)
df['text'] = df['title'] + ' ' + df['content']
print(f"æ–‡æ¡£æ•°é‡: {len(df)}")
df.head()
```

### 2ï¸âƒ£ æ–‡æœ¬é¢„å¤„ç†

```python
import jieba  # ä¸­æ–‡åˆ†è¯ï¼ˆpip install jiebaï¼‰

def preprocess_text(text):
    """æ–‡æœ¬é¢„å¤„ç†"""
    # 1. è½¬å°å†™ï¼ˆè‹±æ–‡ï¼‰
    text = text.lower()

    # 2. ä¸­æ–‡åˆ†è¯
    words = jieba.cut(text)

    # 3. å»é™¤æ ‡ç‚¹å’Œç©ºç™½
    words = [w.strip() for w in words if w.strip() and len(w.strip()) > 1]

    return ' '.join(words)

# é¢„å¤„ç†æ‰€æœ‰æ–‡æ¡£
df['processed'] = df['text'].apply(preprocess_text)

print("é¢„å¤„ç†ç¤ºä¾‹:")
for i in range(3):
    print(f"\nåŸæ–‡: {df.iloc[i]['text'][:50]}...")
    print(f"å¤„ç†å: {df.iloc[i]['processed'][:50]}...")
```

### 3ï¸âƒ£ TF-IDF å‘é‡åŒ–

```python
# TF-IDF åŸç†è¯´æ˜
print("""
ğŸ“š TF-IDF åŸç†ï¼š

TF (Term Frequency) = è¯åœ¨æ–‡æ¡£ä¸­å‡ºç°çš„æ¬¡æ•° / æ–‡æ¡£æ€»è¯æ•°
IDF (Inverse Document Frequency) = log(æ€»æ–‡æ¡£æ•° / åŒ…å«è¯¥è¯çš„æ–‡æ¡£æ•°)
TF-IDF = TF Ã— IDF

æ„ä¹‰ï¼š
- æŸè¯åœ¨å½“å‰æ–‡æ¡£å‡ºç°è¶Šå¤šï¼ŒTF è¶Šé«˜
- æŸè¯åœ¨æ‰€æœ‰æ–‡æ¡£ä¸­è¶Šç½•è§ï¼ŒIDF è¶Šé«˜
- TF-IDF é«˜ â†’ è¯¥è¯å¯¹å½“å‰æ–‡æ¡£å¾ˆé‡è¦ä¸”æœ‰åŒºåˆ†åº¦
""")

# ä½¿ç”¨ sklearn çš„ TfidfVectorizer
vectorizer = TfidfVectorizer(
    max_features=1000,  # æœ€å¤šä¿ç•™ 1000 ä¸ªè¯
    min_df=1,           # æœ€å°‘å‡ºç° 1 æ¬¡
    max_df=0.9,         # æœ€å¤šå‡ºç°åœ¨ 90% çš„æ–‡æ¡£ä¸­
)

# è®­ç»ƒå¹¶è½¬æ¢
tfidf_matrix = vectorizer.fit_transform(df['processed'])

print(f"\nTF-IDF çŸ©é˜µå½¢çŠ¶: {tfidf_matrix.shape}")
print(f"  - {tfidf_matrix.shape[0]} ä¸ªæ–‡æ¡£")
print(f"  - {tfidf_matrix.shape[1]} ä¸ªç‰¹å¾è¯")

# æŸ¥çœ‹è¯æ±‡è¡¨
vocab = vectorizer.get_feature_names_out()
print(f"\néƒ¨åˆ†è¯æ±‡: {vocab[:20].tolist()}")
```

### 4ï¸âƒ£ ä½™å¼¦ç›¸ä¼¼åº¦è®¡ç®—

```python
# ä½™å¼¦ç›¸ä¼¼åº¦åŸç†
print("""
ğŸ“ ä½™å¼¦ç›¸ä¼¼åº¦ï¼š

cos(Î¸) = (A Â· B) / (||A|| Ã— ||B||)

- å€¼åŸŸï¼š[-1, 1]ï¼Œé€šå¸¸ TF-IDF å‘é‡æ˜¯éè´Ÿçš„ï¼Œæ‰€ä»¥æ˜¯ [0, 1]
- 1 è¡¨ç¤ºå®Œå…¨ç›¸åŒ
- 0 è¡¨ç¤ºå®Œå…¨æ— å…³
""")

# æ‰‹åŠ¨å®ç°ä½™å¼¦ç›¸ä¼¼åº¦
def cosine_similarity_manual(vec1, vec2):
    """æ‰‹åŠ¨å®ç°ä½™å¼¦ç›¸ä¼¼åº¦"""
    dot_product = np.dot(vec1, vec2)
    norm1 = np.linalg.norm(vec1)
    norm2 = np.linalg.norm(vec2)
    if norm1 == 0 or norm2 == 0:
        return 0
    return dot_product / (norm1 * norm2)

# è®¡ç®—æ–‡æ¡£é—´ç›¸ä¼¼åº¦çŸ©é˜µ
similarity_matrix = cosine_similarity(tfidf_matrix)
print(f"\nç›¸ä¼¼åº¦çŸ©é˜µå½¢çŠ¶: {similarity_matrix.shape}")

# æ˜¾ç¤ºç›¸ä¼¼åº¦çŸ©é˜µï¼ˆéƒ¨åˆ†ï¼‰
sim_df = pd.DataFrame(
    similarity_matrix[:5, :5],
    index=df['title'][:5],
    columns=df['title'][:5]
)
print("\næ–‡æ¡£ç›¸ä¼¼åº¦çŸ©é˜µï¼ˆå‰ 5 ä¸ªæ–‡æ¡£ï¼‰:")
print(sim_df.round(3))
```

### 5ï¸âƒ£ æ„å»ºæœç´¢æ¥å£

```python
class SimpleSearchEngine:
    """ç®€æ˜“æœç´¢å¼•æ“"""

    def __init__(self, documents, vectorizer, tfidf_matrix):
        self.documents = documents
        self.vectorizer = vectorizer
        self.tfidf_matrix = tfidf_matrix

    def search(self, query, top_k=5):
        """
        æœç´¢ç›¸ä¼¼æ–‡æ¡£

        å‚æ•°:
            query: æœç´¢æŸ¥è¯¢
            top_k: è¿”å›å‰ k ä¸ªç»“æœ

        è¿”å›:
            ç›¸ä¼¼æ–‡æ¡£åˆ—è¡¨
        """
        # 1. é¢„å¤„ç†æŸ¥è¯¢
        processed_query = preprocess_text(query)

        # 2. å‘é‡åŒ–æŸ¥è¯¢
        query_vector = self.vectorizer.transform([processed_query])

        # 3. è®¡ç®—ä¸æ‰€æœ‰æ–‡æ¡£çš„ç›¸ä¼¼åº¦
        similarities = cosine_similarity(query_vector, self.tfidf_matrix)[0]

        # 4. æ’åºå¹¶è¿”å› top_k
        top_indices = similarities.argsort()[::-1][:top_k]

        results = []
        for idx in top_indices:
            results.append({
                'id': self.documents.iloc[idx]['id'],
                'title': self.documents.iloc[idx]['title'],
                'content': self.documents.iloc[idx]['content'],
                'score': similarities[idx]
            })

        return results

    def find_similar(self, doc_id, top_k=5):
        """æŸ¥æ‰¾ä¸æŒ‡å®šæ–‡æ¡£ç›¸ä¼¼çš„æ–‡æ¡£"""
        doc_idx = self.documents[self.documents['id'] == doc_id].index[0]
        similarities = similarity_matrix[doc_idx]

        # æ’é™¤è‡ªå·±
        top_indices = similarities.argsort()[::-1][1:top_k+1]

        results = []
        for idx in top_indices:
            results.append({
                'id': self.documents.iloc[idx]['id'],
                'title': self.documents.iloc[idx]['title'],
                'score': similarities[idx]
            })

        return results

# åˆ›å»ºæœç´¢å¼•æ“å®ä¾‹
search_engine = SimpleSearchEngine(df, vectorizer, tfidf_matrix)
```

### 6ï¸âƒ£ æµ‹è¯•æœç´¢

```python
# æµ‹è¯•æœç´¢åŠŸèƒ½
print("=" * 60)
print("ğŸ” æœç´¢æµ‹è¯•")
print("=" * 60)

queries = [
    "Python æ•°æ®åˆ†æ",
    "äººå·¥æ™ºèƒ½ æ·±åº¦å­¦ä¹ ",
    "Web å¼€å‘",
]

for query in queries:
    print(f"\næŸ¥è¯¢: '{query}'")
    print("-" * 40)
    results = search_engine.search(query, top_k=3)
    for i, r in enumerate(results, 1):
        print(f"{i}. [{r['score']:.3f}] {r['title']}")
        print(f"   {r['content'][:50]}...")
```

```python
# æµ‹è¯•ç›¸ä¼¼æ–‡æ¡£æ¨è
print("\n" + "=" * 60)
print("ğŸ“š ç›¸ä¼¼æ–‡æ¡£æ¨è")
print("=" * 60)

doc_id = 2  # æœºå™¨å­¦ä¹ åŸºç¡€
print(f"\nä¸ '{df[df['id']==doc_id]['title'].values[0]}' ç›¸ä¼¼çš„æ–‡æ¡£:")
print("-" * 40)

similar_docs = search_engine.find_similar(doc_id, top_k=5)
for i, doc in enumerate(similar_docs, 1):
    print(f"{i}. [{doc['score']:.3f}] {doc['title']}")
```

### 7ï¸âƒ£ å¯è§†åŒ–å±•ç¤º

```python
import matplotlib.pyplot as plt
from sklearn.manifold import TSNE

# ä½¿ç”¨ t-SNE é™ç»´å¯è§†åŒ–
tsne = TSNE(n_components=2, random_state=42, perplexity=min(5, len(df)-1))
doc_embeddings_2d = tsne.fit_transform(tfidf_matrix.toarray())

# å¯è§†åŒ–
fig, ax = plt.subplots(figsize=(12, 8))

scatter = ax.scatter(
    doc_embeddings_2d[:, 0],
    doc_embeddings_2d[:, 1],
    c=range(len(df)),
    cmap='tab10',
    s=100
)

# æ·»åŠ æ ‡ç­¾
for i, title in enumerate(df['title']):
    ax.annotate(title, (doc_embeddings_2d[i, 0], doc_embeddings_2d[i, 1]),
                fontsize=9, ha='center', va='bottom')

ax.set_title('æ–‡æ¡£å‘é‡ t-SNE å¯è§†åŒ–')
ax.set_xlabel('ç»´åº¦ 1')
ax.set_ylabel('ç»´åº¦ 2')
plt.tight_layout()
plt.show()
```

### ğŸ“ æŠ€èƒ½ç‚¹æ€»ç»“

| æŠ€èƒ½ | ç”¨åˆ°çš„çŸ¥è¯† |
|------|-----------|
| **æ–‡æœ¬é¢„å¤„ç†** | jieba åˆ†è¯ã€æ­£åˆ™è¡¨è¾¾å¼ |
| **TF-IDF** | `TfidfVectorizer` |
| **ä½™å¼¦ç›¸ä¼¼åº¦** | `cosine_similarity` |
| **å‘é‡è¿ç®—** | NumPy ç‚¹ç§¯ã€èŒƒæ•° |
| **é™ç»´å¯è§†åŒ–** | t-SNE |
| **é¢å‘å¯¹è±¡** | å°è£…æœç´¢å¼•æ“ç±» |

---

## é¡¹ç›® 3ï¼šè‚¡ç¥¨æ•°æ®åˆ†æï¼ˆè¿›é˜¶ï¼‰

### ğŸ“‹ é¡¹ç›®æ¦‚è¿°

| é¡¹ç›®ä¿¡æ¯ | å†…å®¹ |
|---------|------|
| **éš¾åº¦** | â­â­â­ è¿›é˜¶ |
| **æ•°æ®æº** | Yahoo Finance API |
| **æŠ€èƒ½ç‚¹** | æ—¶é—´åºåˆ—ã€æŠ€æœ¯æŒ‡æ ‡ã€Plotly å¯è§†åŒ– |
| **é¢„è®¡æ—¶é—´** | 4-5 å°æ—¶ |

### ğŸ“¦ ç¯å¢ƒå‡†å¤‡

```python
# pip install yfinance pandas numpy matplotlib plotly

import yfinance as yf
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import plotly.graph_objects as go
from plotly.subplots import make_subplots
from datetime import datetime, timedelta

print("âœ… ç¯å¢ƒå‡†å¤‡å®Œæˆ")
```

### 1ï¸âƒ£ æ•°æ®è·å–

```python
# ä¸‹è½½è‚¡ç¥¨æ•°æ®
def download_stock_data(symbol, period='2y'):
    """
    ä¸‹è½½è‚¡ç¥¨æ•°æ®

    å‚æ•°:
        symbol: è‚¡ç¥¨ä»£ç ï¼ˆå¦‚ 'AAPL', 'GOOGL', '600519.SS'ï¼‰
        period: æ—¶é—´èŒƒå›´ï¼ˆ1d, 5d, 1mo, 3mo, 6mo, 1y, 2y, 5y, 10y, ytd, maxï¼‰
    """
    stock = yf.Ticker(symbol)
    df = stock.history(period=period)
    return df

# ä¸‹è½½è‹¹æœè‚¡ç¥¨æ•°æ®
symbol = 'AAPL'
df = download_stock_data(symbol)

print(f"ğŸ“ˆ {symbol} è‚¡ç¥¨æ•°æ®")
print(f"æ—¶é—´èŒƒå›´: {df.index[0].date()} ~ {df.index[-1].date()}")
print(f"æ•°æ®ç‚¹æ•°: {len(df)}")
print(f"\nåˆ—å: {df.columns.tolist()}")
df.tail()
```

### 2ï¸âƒ£ æ•°æ®é¢„å¤„ç†

```python
# æ•°æ®æ¸…æ´—å’Œç‰¹å¾å·¥ç¨‹
def prepare_stock_data(df):
    """å‡†å¤‡è‚¡ç¥¨æ•°æ®"""
    data = df.copy()

    # ç¡®ä¿ç´¢å¼•æ˜¯ datetime
    data.index = pd.to_datetime(data.index)

    # è®¡ç®—æ—¥æ”¶ç›Šç‡
    data['Daily_Return'] = data['Close'].pct_change()

    # è®¡ç®—ç´¯è®¡æ”¶ç›Šç‡
    data['Cumulative_Return'] = (1 + data['Daily_Return']).cumprod() - 1

    # è®¡ç®—æ³¢åŠ¨ç‡ï¼ˆ20 æ—¥æ»šåŠ¨æ ‡å‡†å·®ï¼‰
    data['Volatility'] = data['Daily_Return'].rolling(window=20).std() * np.sqrt(252)

    return data

df = prepare_stock_data(df)
print("âœ… æ•°æ®é¢„å¤„ç†å®Œæˆ")
df.tail()
```

### 3ï¸âƒ£ æŠ€æœ¯æŒ‡æ ‡è®¡ç®—

```python
def add_technical_indicators(df):
    """æ·»åŠ æŠ€æœ¯æŒ‡æ ‡"""
    data = df.copy()

    # 1. ç§»åŠ¨å¹³å‡çº¿ (MA)
    data['MA5'] = data['Close'].rolling(window=5).mean()
    data['MA20'] = data['Close'].rolling(window=20).mean()
    data['MA60'] = data['Close'].rolling(window=60).mean()

    # 2. æŒ‡æ•°ç§»åŠ¨å¹³å‡ (EMA)
    data['EMA12'] = data['Close'].ewm(span=12, adjust=False).mean()
    data['EMA26'] = data['Close'].ewm(span=26, adjust=False).mean()

    # 3. MACD
    data['MACD'] = data['EMA12'] - data['EMA26']
    data['Signal'] = data['MACD'].ewm(span=9, adjust=False).mean()
    data['MACD_Hist'] = data['MACD'] - data['Signal']

    # 4. RSI (Relative Strength Index)
    delta = data['Close'].diff()
    gain = delta.where(delta > 0, 0)
    loss = -delta.where(delta < 0, 0)
    avg_gain = gain.rolling(window=14).mean()
    avg_loss = loss.rolling(window=14).mean()
    rs = avg_gain / avg_loss
    data['RSI'] = 100 - (100 / (1 + rs))

    # 5. å¸ƒæ—å¸¦ (Bollinger Bands)
    data['BB_Middle'] = data['Close'].rolling(window=20).mean()
    data['BB_Std'] = data['Close'].rolling(window=20).std()
    data['BB_Upper'] = data['BB_Middle'] + 2 * data['BB_Std']
    data['BB_Lower'] = data['BB_Middle'] - 2 * data['BB_Std']

    return data

df = add_technical_indicators(df)
print("âœ… æŠ€æœ¯æŒ‡æ ‡è®¡ç®—å®Œæˆ")
print(f"\næ–°å¢åˆ—: {['MA5', 'MA20', 'MA60', 'MACD', 'RSI', 'BB_Upper', 'BB_Lower']}")
```

### 4ï¸âƒ£ Matplotlib å¯è§†åŒ–

```python
# è‚¡ä»·å’Œç§»åŠ¨å¹³å‡çº¿
fig, axes = plt.subplots(3, 1, figsize=(14, 12), sharex=True)

# 1. ä»·æ ¼å’Œç§»åŠ¨å¹³å‡
ax1 = axes[0]
ax1.plot(df.index, df['Close'], label='æ”¶ç›˜ä»·', linewidth=1)
ax1.plot(df.index, df['MA5'], label='MA5', linewidth=0.8)
ax1.plot(df.index, df['MA20'], label='MA20', linewidth=0.8)
ax1.plot(df.index, df['MA60'], label='MA60', linewidth=0.8)
ax1.fill_between(df.index, df['BB_Lower'], df['BB_Upper'], alpha=0.2, label='å¸ƒæ—å¸¦')
ax1.set_ylabel('ä»·æ ¼ ($)')
ax1.set_title(f'{symbol} è‚¡ç¥¨åˆ†æ')
ax1.legend(loc='upper left')
ax1.grid(True, alpha=0.3)

# 2. æˆäº¤é‡
ax2 = axes[1]
colors = ['green' if c >= o else 'red' for c, o in zip(df['Close'], df['Open'])]
ax2.bar(df.index, df['Volume'], color=colors, alpha=0.7)
ax2.set_ylabel('æˆäº¤é‡')
ax2.grid(True, alpha=0.3)

# 3. RSI
ax3 = axes[2]
ax3.plot(df.index, df['RSI'], color='purple', linewidth=1)
ax3.axhline(y=70, color='r', linestyle='--', alpha=0.5, label='è¶…ä¹° (70)')
ax3.axhline(y=30, color='g', linestyle='--', alpha=0.5, label='è¶…å– (30)')
ax3.fill_between(df.index, 70, df['RSI'], where=df['RSI'] >= 70, alpha=0.3, color='red')
ax3.fill_between(df.index, 30, df['RSI'], where=df['RSI'] <= 30, alpha=0.3, color='green')
ax3.set_ylabel('RSI')
ax3.set_ylim(0, 100)
ax3.legend(loc='upper left')
ax3.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
```

### 5ï¸âƒ£ Plotly äº¤äº’å¼å¯è§†åŒ–

```python
# åˆ›å»ºäº¤äº’å¼ K çº¿å›¾
fig = make_subplots(
    rows=3, cols=1,
    shared_xaxes=True,
    vertical_spacing=0.05,
    row_heights=[0.6, 0.2, 0.2],
    subplot_titles=['Kçº¿å›¾', 'æˆäº¤é‡', 'MACD']
)

# 1. K çº¿å›¾
fig.add_trace(
    go.Candlestick(
        x=df.index,
        open=df['Open'],
        high=df['High'],
        low=df['Low'],
        close=df['Close'],
        name='Kçº¿'
    ),
    row=1, col=1
)

# æ·»åŠ ç§»åŠ¨å¹³å‡çº¿
fig.add_trace(
    go.Scatter(x=df.index, y=df['MA20'], name='MA20', line=dict(width=1)),
    row=1, col=1
)
fig.add_trace(
    go.Scatter(x=df.index, y=df['MA60'], name='MA60', line=dict(width=1)),
    row=1, col=1
)

# æ·»åŠ å¸ƒæ—å¸¦
fig.add_trace(
    go.Scatter(x=df.index, y=df['BB_Upper'], name='BB Upper',
               line=dict(width=1, dash='dash'), opacity=0.5),
    row=1, col=1
)
fig.add_trace(
    go.Scatter(x=df.index, y=df['BB_Lower'], name='BB Lower',
               line=dict(width=1, dash='dash'), opacity=0.5,
               fill='tonexty', fillcolor='rgba(0,100,255,0.1)'),
    row=1, col=1
)

# 2. æˆäº¤é‡
colors = ['red' if c < o else 'green' for c, o in zip(df['Close'], df['Open'])]
fig.add_trace(
    go.Bar(x=df.index, y=df['Volume'], name='æˆäº¤é‡', marker_color=colors),
    row=2, col=1
)

# 3. MACD
fig.add_trace(
    go.Scatter(x=df.index, y=df['MACD'], name='MACD', line=dict(width=1)),
    row=3, col=1
)
fig.add_trace(
    go.Scatter(x=df.index, y=df['Signal'], name='Signal', line=dict(width=1)),
    row=3, col=1
)
colors_macd = ['green' if v >= 0 else 'red' for v in df['MACD_Hist']]
fig.add_trace(
    go.Bar(x=df.index, y=df['MACD_Hist'], name='MACD Hist', marker_color=colors_macd),
    row=3, col=1
)

# æ›´æ–°å¸ƒå±€
fig.update_layout(
    title=f'{symbol} è‚¡ç¥¨æŠ€æœ¯åˆ†æ',
    xaxis_rangeslider_visible=False,
    height=800,
    showlegend=True
)

fig.show()
```

### 6ï¸âƒ£ ç®€å•é¢„æµ‹æ¨¡å‹

```python
from sklearn.preprocessing import MinMaxScaler
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_absolute_error

# ä½¿ç”¨å†å²æ•°æ®é¢„æµ‹ä¸‹ä¸€æ—¥æ”¶ç›˜ä»·
def create_features(df, lookback=5):
    """åˆ›å»ºç‰¹å¾"""
    data = df.copy()

    # ä½¿ç”¨è¿‡å» N å¤©çš„æ”¶ç›˜ä»·ä½œä¸ºç‰¹å¾
    for i in range(1, lookback + 1):
        data[f'Close_Lag{i}'] = data['Close'].shift(i)

    # æŠ€æœ¯æŒ‡æ ‡ä½œä¸ºç‰¹å¾
    data['MA5_Signal'] = (data['Close'] > data['MA5']).astype(int)
    data['MA20_Signal'] = (data['Close'] > data['MA20']).astype(int)
    data['RSI_Signal'] = data['RSI'] / 100

    # ç›®æ ‡ï¼šä¸‹ä¸€æ—¥æ”¶ç›˜ä»·
    data['Target'] = data['Close'].shift(-1)

    return data.dropna()

# å‡†å¤‡æ•°æ®
df_ml = create_features(df, lookback=5)
feature_cols = [f'Close_Lag{i}' for i in range(1, 6)] + ['MA5_Signal', 'MA20_Signal', 'RSI_Signal']

X = df_ml[feature_cols]
y = df_ml['Target']

# åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†ï¼ˆæ—¶é—´åºåˆ—ä¸èƒ½éšæœºåˆ’åˆ†ï¼‰
train_size = int(len(X) * 0.8)
X_train, X_test = X[:train_size], X[train_size:]
y_train, y_test = y[:train_size], y[train_size:]

print(f"è®­ç»ƒé›†: {len(X_train)} æ ·æœ¬")
print(f"æµ‹è¯•é›†: {len(X_test)} æ ·æœ¬")

# è®­ç»ƒæ¨¡å‹
model = LinearRegression()
model.fit(X_train, y_train)

# é¢„æµ‹
y_pred = model.predict(X_test)

# è¯„ä¼°
mae = mean_absolute_error(y_test, y_pred)
print(f"\nğŸ“Š æ¨¡å‹è¯„ä¼°:")
print(f"MAE: ${mae:.2f}")

# å¯è§†åŒ–é¢„æµ‹ç»“æœ
fig, ax = plt.subplots(figsize=(14, 6))
ax.plot(y_test.index, y_test.values, label='å®é™…ä»·æ ¼', linewidth=1)
ax.plot(y_test.index, y_pred, label='é¢„æµ‹ä»·æ ¼', linewidth=1, linestyle='--')
ax.set_xlabel('æ—¥æœŸ')
ax.set_ylabel('ä»·æ ¼ ($)')
ax.set_title(f'{symbol} è‚¡ä»·é¢„æµ‹ (MAE: ${mae:.2f})')
ax.legend()
ax.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()
```

### ğŸ“ æŠ€èƒ½ç‚¹æ€»ç»“

| æŠ€èƒ½ | ç”¨åˆ°çš„çŸ¥è¯† |
|------|-----------|
| **æ•°æ®è·å–** | yfinance API |
| **æ—¶é—´åºåˆ—å¤„ç†** | pandas datetime, rolling |
| **æŠ€æœ¯æŒ‡æ ‡** | MA, EMA, MACD, RSI, å¸ƒæ—å¸¦ |
| **é™æ€å¯è§†åŒ–** | Matplotlib |
| **äº¤äº’å¼å¯è§†åŒ–** | Plotly |
| **æœºå™¨å­¦ä¹ ** | æ—¶é—´åºåˆ—é¢„æµ‹ |

---

## é¡¹ç›®æ‰©å±•æ–¹å‘

### é¡¹ç›® 1 æ‰©å±•
- æ·»åŠ å¯¼æ¼”/æ¼”å‘˜åˆ†æï¼ˆè§£æ credits æ•°æ®ï¼‰
- ä½¿ç”¨éšæœºæ£®æ—/XGBoost æå‡é¢„æµ‹
- æ„å»ºç”µå½±æ¨èç³»ç»Ÿ

### é¡¹ç›® 2 æ‰©å±•
- ä½¿ç”¨ Word2Vec æˆ– BERT å‘é‡åŒ–
- æ·»åŠ  Web ç•Œé¢ï¼ˆFlask/FastAPIï¼‰
- æ”¯æŒå¢é‡ç´¢å¼•

### é¡¹ç›® 3 æ‰©å±•
- å¤šè‚¡ç¥¨å¯¹æ¯”åˆ†æ
- å›æµ‹äº¤æ˜“ç­–ç•¥
- ä½¿ç”¨ LSTM è¿›è¡Œé¢„æµ‹

---

## â¡ï¸ ä¸‹ä¸€æ­¥

å­¦å®Œæœ¬èŠ‚åï¼Œç»§ç»­å­¦ä¹  [19-FAQä¸å¸¸è§å‘.md](./19-FAQä¸å¸¸è§å‘.md)

