# ğŸ”¢ 08 - æ•°å­¦ç›´è§‰

> ä¸å·æ•°å­¦æ¨å¯¼ï¼Œä½†å»ºç«‹ç›´è§‰ï¼Œçœ‹åˆ°å…¬å¼ä¸å®³æ€•

---

## ç›®å½•

1. [çº¿æ€§ä»£æ•°ç›´è§‰](#1-çº¿æ€§ä»£æ•°ç›´è§‰)
2. [å¾®ç§¯åˆ†ç›´è§‰](#2-å¾®ç§¯åˆ†ç›´è§‰)
3. [æ¦‚ç‡ä¸ä¿¡æ¯è®º](#3-æ¦‚ç‡ä¸ä¿¡æ¯è®º)
4. [ç»ƒä¹ é¢˜](#4-ç»ƒä¹ é¢˜)

---

## 1. çº¿æ€§ä»£æ•°ç›´è§‰

### 1.1 å‘é‡

**ç›´è§‰**ï¼šå‘é‡å°±æ˜¯ä¸€ä¸ªç®­å¤´ï¼Œæœ‰æ–¹å‘å’Œé•¿åº¦

```python
import numpy as np
import matplotlib.pyplot as plt

# å‘é‡å°±æ˜¯ä¸€ç»„æœ‰åºçš„æ•°
v = np.array([3, 2])

# å¯è§†åŒ–
plt.figure(figsize=(8, 6))
plt.quiver(0, 0, v[0], v[1], angles='xy', scale_units='xy', scale=1, color='blue')
plt.xlim(-1, 5)
plt.ylim(-1, 4)
plt.grid(True)
plt.xlabel('x')
plt.ylabel('y')
plt.title(f'Vector v = {v}')
plt.axhline(y=0, color='k', linestyle='-', linewidth=0.5)
plt.axvline(x=0, color='k', linestyle='-', linewidth=0.5)
plt.show()

# å‘é‡çš„é•¿åº¦ï¼ˆèŒƒæ•°ï¼‰
length = np.linalg.norm(v)
print(f"å‘é‡é•¿åº¦: {length:.2f}")  # sqrt(3Â² + 2Â²) = sqrt(13) â‰ˆ 3.61
```

### 1.2 ç‚¹ç§¯ï¼ˆDot Productï¼‰

**ç›´è§‰**ï¼šè¡¡é‡ä¸¤ä¸ªå‘é‡çš„ç›¸ä¼¼ç¨‹åº¦

```python
a = np.array([1, 0])  # æŒ‡å‘å³
b = np.array([0, 1])  # æŒ‡å‘ä¸Š
c = np.array([1, 1])  # æŒ‡å‘å³ä¸Š

# ç‚¹ç§¯å…¬å¼: a Â· b = |a||b|cos(Î¸)
# å¦‚æœä¸¤ä¸ªå‘é‡æ–¹å‘ç›¸åŒï¼Œç‚¹ç§¯æœ€å¤§
# å¦‚æœä¸¤ä¸ªå‘é‡å‚ç›´ï¼Œç‚¹ç§¯ä¸º 0
# å¦‚æœä¸¤ä¸ªå‘é‡æ–¹å‘ç›¸åï¼Œç‚¹ç§¯ä¸ºè´Ÿ

print(f"a Â· b = {np.dot(a, b)}")  # 0ï¼ˆå‚ç›´ï¼‰
print(f"a Â· c = {np.dot(a, c)}")  # 1ï¼ˆæœ‰å…±åŒæˆåˆ†ï¼‰

# å¯è§†åŒ–
fig, ax = plt.subplots(figsize=(8, 6))
origin = [0, 0]
ax.quiver(*origin, *a, angles='xy', scale_units='xy', scale=1, color='blue', label='a=[1,0]')
ax.quiver(*origin, *b, angles='xy', scale_units='xy', scale=1, color='red', label='b=[0,1]')
ax.quiver(*origin, *c, angles='xy', scale_units='xy', scale=1, color='green', label='c=[1,1]')
ax.set_xlim(-0.5, 2)
ax.set_ylim(-0.5, 2)
ax.grid(True)
ax.legend()
ax.set_aspect('equal')
plt.title('Dot Product Intuition')
plt.show()
```

**AI ä¸­çš„åº”ç”¨**ï¼š
- **ç›¸ä¼¼åº¦è®¡ç®—**ï¼šä¸¤ä¸ªè¯å‘é‡çš„ç‚¹ç§¯è¶Šå¤§ï¼Œè¯­ä¹‰è¶Šç›¸ä¼¼
- **æ³¨æ„åŠ›æœºåˆ¶**ï¼šQuery å’Œ Key çš„ç‚¹ç§¯å†³å®šæ³¨æ„åŠ›æƒé‡

### 1.3 çŸ©é˜µ

**ç›´è§‰**ï¼šçŸ©é˜µæ˜¯ä¸€ç§å˜æ¢ï¼Œå¯ä»¥æ—‹è½¬ã€ç¼©æ”¾ã€æŠ•å½±å‘é‡

```python
# çŸ©é˜µå°±æ˜¯ä¸€ç»„å‘é‡ï¼ˆæŒ‰åˆ—æ’åˆ—ï¼‰
A = np.array([[2, 0],
              [0, 1]])

# çŸ©é˜µä¹˜å‘é‡ = å˜æ¢
v = np.array([1, 1])
v_transformed = A @ v  # [2, 1]

print(f"åŸå‘é‡: {v}")
print(f"å˜æ¢å: {v_transformed}")

# è¿™ä¸ªçŸ©é˜µçš„æ•ˆæœæ˜¯ï¼šx æ–¹å‘æ”¾å¤§ 2 å€ï¼Œy æ–¹å‘ä¸å˜
```

### 1.4 çŸ©é˜µä¹˜æ³•

**ç›´è§‰**ï¼šçŸ©é˜µä¹˜æ³• = å‡½æ•°å¤åˆï¼ˆå…ˆåšä¸€ä¸ªå˜æ¢ï¼Œå†åšå¦ä¸€ä¸ªï¼‰

```python
# çŸ©é˜µ A: æ”¾å¤§
A = np.array([[2, 0],
              [0, 2]])

# çŸ©é˜µ B: æ—‹è½¬ 90 åº¦
theta = np.pi / 2  # 90 åº¦
B = np.array([[np.cos(theta), -np.sin(theta)],
              [np.sin(theta), np.cos(theta)]])

# A @ B: å…ˆæ—‹è½¬ï¼Œå†æ”¾å¤§
# B @ A: å…ˆæ”¾å¤§ï¼Œå†æ—‹è½¬
# æ³¨æ„ï¼šçŸ©é˜µä¹˜æ³•ä¸æ»¡è¶³äº¤æ¢å¾‹ï¼

v = np.array([1, 0])
print(f"åŸå‘é‡: {v}")
print(f"A @ vï¼ˆæ”¾å¤§ï¼‰: {A @ v}")
print(f"B @ vï¼ˆæ—‹è½¬ï¼‰: {B @ v}")
print(f"A @ B @ v: {A @ B @ v}")  # å…ˆæ—‹è½¬å†æ”¾å¤§
```

**AI ä¸­çš„åº”ç”¨**ï¼š
- **ç¥ç»ç½‘ç»œçš„å…¨è¿æ¥å±‚**å°±æ˜¯çŸ©é˜µä¹˜æ³•ï¼š`output = input @ weight + bias`

### 1.5 å¼ é‡ï¼ˆTensorï¼‰

**ç›´è§‰**ï¼šå¼ é‡æ˜¯é«˜ç»´æ•°ç»„

```python
# æ ‡é‡ï¼ˆ0ç»´ï¼‰ï¼šä¸€ä¸ªæ•°
scalar = 5

# å‘é‡ï¼ˆ1ç»´ï¼‰ï¼šä¸€åˆ—æ•°
vector = np.array([1, 2, 3])

# çŸ©é˜µï¼ˆ2ç»´ï¼‰ï¼šä¸€å¼ è¡¨
matrix = np.array([[1, 2], [3, 4], [5, 6]])

# å¼ é‡ï¼ˆ3ç»´+ï¼‰ï¼šå¤šå¼ è¡¨
tensor_3d = np.random.randn(2, 3, 4)  # 2 ä¸ª 3x4 çš„çŸ©é˜µ

# å›¾åƒé€šå¸¸æ˜¯ 3D å¼ é‡ï¼š(é«˜åº¦, å®½åº¦, é€šé“)
image = np.random.randint(0, 256, (224, 224, 3))  # RGB å›¾åƒ

# ä¸€æ‰¹å›¾åƒæ˜¯ 4D å¼ é‡ï¼š(æ‰¹é‡å¤§å°, é«˜åº¦, å®½åº¦, é€šé“)
batch_images = np.random.randint(0, 256, (32, 224, 224, 3))

print(f"æ ‡é‡å½¢çŠ¶: {np.array(scalar).shape}")    # ()
print(f"å‘é‡å½¢çŠ¶: {vector.shape}")               # (3,)
print(f"çŸ©é˜µå½¢çŠ¶: {matrix.shape}")               # (3, 2)
print(f"3Då¼ é‡å½¢çŠ¶: {tensor_3d.shape}")          # (2, 3, 4)
print(f"å›¾åƒå½¢çŠ¶: {image.shape}")                # (224, 224, 3)
print(f"æ‰¹é‡å›¾åƒå½¢çŠ¶: {batch_images.shape}")     # (32, 224, 224, 3)
```

---

## 2. å¾®ç§¯åˆ†ç›´è§‰

### 2.1 å¯¼æ•°

**ç›´è§‰**ï¼šå¯¼æ•° = å˜åŒ–ç‡ = æ›²çº¿åœ¨æŸç‚¹çš„æ–œç‡

```python
import numpy as np
import matplotlib.pyplot as plt

# å‡½æ•° f(x) = xÂ²
def f(x):
    return x ** 2

# å¯¼æ•° f'(x) = 2xï¼ˆæ–œç‡ï¼‰
def df(x):
    return 2 * x

x = np.linspace(-3, 3, 100)

fig, axes = plt.subplots(1, 2, figsize=(14, 5))

# å·¦å›¾ï¼šå‡½æ•°å’Œåˆ‡çº¿
axes[0].plot(x, f(x), 'b-', label='f(x) = xÂ²')
x0 = 1  # åœ¨ x=1 å¤„çš„åˆ‡çº¿
slope = df(x0)
tangent = f(x0) + slope * (x - x0)
axes[0].plot(x, tangent, 'r--', label=f'Tangent at x={x0}, slope={slope}')
axes[0].scatter([x0], [f(x0)], color='red', s=100, zorder=5)
axes[0].set_xlim(-3, 3)
axes[0].set_ylim(-1, 9)
axes[0].legend()
axes[0].grid(True, alpha=0.3)
axes[0].set_title('Function and Tangent Line')

# å³å›¾ï¼šå¯¼æ•°
axes[1].plot(x, df(x), 'g-', label="f'(x) = 2x")
axes[1].axhline(y=0, color='k', linestyle='-', linewidth=0.5)
axes[1].legend()
axes[1].grid(True, alpha=0.3)
axes[1].set_title('Derivative')

plt.tight_layout()
plt.show()

print(f"åœ¨ x=1 å¤„ï¼Œæ–œç‡ = {df(1)}")
print(f"åœ¨ x=-2 å¤„ï¼Œæ–œç‡ = {df(-2)}")
print(f"åœ¨ x=0 å¤„ï¼Œæ–œç‡ = {df(0)}ï¼ˆå‡½æ•°æœ€å°å€¼ç‚¹ï¼Œæ–œç‡ä¸º0ï¼‰")
```

### 2.2 æ¢¯åº¦

**ç›´è§‰**ï¼šæ¢¯åº¦ = å¤šç»´ç©ºé—´ä¸­å˜åŒ–æœ€å¿«çš„æ–¹å‘

```python
# å¯¹äºå¤šå˜é‡å‡½æ•° f(x, y)
# æ¢¯åº¦æ˜¯ä¸€ä¸ªå‘é‡ï¼š(âˆ‚f/âˆ‚x, âˆ‚f/âˆ‚y)
# æŒ‡å‘å‡½æ•°å¢é•¿æœ€å¿«çš„æ–¹å‘

# ä¾‹å¦‚ f(x, y) = xÂ² + yÂ²
# æ¢¯åº¦ = (2x, 2y)

def f(x, y):
    return x**2 + y**2

# åœ¨ç‚¹ (1, 2) å¤„
x0, y0 = 1, 2
gradient = np.array([2*x0, 2*y0])  # [2, 4]
print(f"ç‚¹ ({x0}, {y0}) å¤„çš„æ¢¯åº¦: {gradient}")
print(f"æ¢¯åº¦æŒ‡å‘å‡½æ•°å€¼å¢åŠ æœ€å¿«çš„æ–¹å‘")

# å¯è§†åŒ–
from mpl_toolkits.mplot3d import Axes3D

x = np.linspace(-3, 3, 50)
y = np.linspace(-3, 3, 50)
X, Y = np.meshgrid(x, y)
Z = f(X, Y)

fig = plt.figure(figsize=(10, 8))
ax = fig.add_subplot(111, projection='3d')
ax.plot_surface(X, Y, Z, cmap='viridis', alpha=0.7)
ax.set_xlabel('x')
ax.set_ylabel('y')
ax.set_zlabel('f(x,y)')
ax.set_title('f(x,y) = xÂ² + yÂ²')
plt.show()
```

### 2.3 æ¢¯åº¦ä¸‹é™

**ç›´è§‰**ï¼šæƒ³è±¡ä½ åœ¨å±±ä¸Šï¼Œè’™ç€çœ¼ç›æƒ³ä¸‹å±±ï¼Œæ€ä¹ˆåŠï¼Ÿæ²¿ç€è„šä¸‹æœ€é™¡çš„æ–¹å‘èµ°ï¼

```python
import numpy as np
import matplotlib.pyplot as plt

# ç›®æ ‡ï¼šæ‰¾åˆ° f(x) = (x-3)Â² çš„æœ€å°å€¼ç‚¹
def f(x):
    return (x - 3) ** 2

def gradient(x):
    return 2 * (x - 3)

# æ¢¯åº¦ä¸‹é™
x = 0  # èµ·å§‹ç‚¹
learning_rate = 0.1  # å­¦ä¹ ç‡ï¼ˆæ­¥é•¿ï¼‰
history = [x]

for i in range(20):
    grad = gradient(x)
    x = x - learning_rate * grad  # æœæ¢¯åº¦çš„åæ–¹å‘ç§»åŠ¨
    history.append(x)
    print(f"Step {i+1}: x = {x:.4f}, f(x) = {f(x):.4f}")

# å¯è§†åŒ–
x_range = np.linspace(-1, 7, 100)
plt.figure(figsize=(10, 6))
plt.plot(x_range, f(x_range), 'b-', label='f(x) = (x-3)Â²')
plt.scatter(history, [f(h) for h in history], c=range(len(history)), cmap='Reds', s=50, zorder=5)
plt.xlabel('x')
plt.ylabel('f(x)')
plt.title('Gradient Descent: Finding Minimum')
plt.legend()
plt.grid(True, alpha=0.3)
plt.colorbar(label='Step')
plt.show()
```

**å…³é”®ç‚¹**ï¼š
- **å­¦ä¹ ç‡å¤ªå¤§**ï¼šè·³è¿‡æœ€å°å€¼ï¼Œéœ‡è¡
- **å­¦ä¹ ç‡å¤ªå°**ï¼šæ”¶æ•›å¤ªæ…¢
- **å±€éƒ¨æœ€å°å€¼**ï¼šå¯èƒ½é™·å…¥å±€éƒ¨æœ€ä¼˜

### 2.4 é“¾å¼æ³•åˆ™

**ç›´è§‰**ï¼šå¤åˆå‡½æ•°çš„å¯¼æ•° = å¤–å±‚å¯¼æ•° Ã— å†…å±‚å¯¼æ•°

```python
# å¦‚æœ y = f(g(x))
# é‚£ä¹ˆ dy/dx = df/dg * dg/dx

# ä¾‹å¦‚ï¼šy = (x + 1)Â²
# ä»¤ u = x + 1, y = uÂ²
# dy/dx = dy/du * du/dx = 2u * 1 = 2(x+1)

# åœ¨ç¥ç»ç½‘ç»œä¸­ï¼Œè¿™å°±æ˜¯åå‘ä¼ æ’­çš„æ•°å­¦åŸºç¡€ï¼
# è¯¯å·®ä»è¾“å‡ºå±‚ä¸€å±‚å±‚ä¼ å›è¾“å…¥å±‚
```

---

## 3. æ¦‚ç‡ä¸ä¿¡æ¯è®º

### 3.1 æ¦‚ç‡åŸºç¡€

```python
import numpy as np

# æ¦‚ç‡ = äº‹ä»¶å‘ç”Ÿçš„å¯èƒ½æ€§ï¼ŒèŒƒå›´ [0, 1]
# P(A) = äº‹ä»¶ A å‘ç”Ÿçš„æ¦‚ç‡

# æ¡ä»¶æ¦‚ç‡
# P(A|B) = åœ¨ B å‘ç”Ÿçš„æ¡ä»¶ä¸‹ï¼ŒA å‘ç”Ÿçš„æ¦‚ç‡
# P(A|B) = P(A âˆ© B) / P(B)

# è´å¶æ–¯å…¬å¼ï¼ˆè¶…çº§é‡è¦ï¼ï¼‰
# P(A|B) = P(B|A) * P(A) / P(B)

# ä¾‹å­ï¼šåƒåœ¾é‚®ä»¶æ£€æµ‹
# P(åƒåœ¾|å«"å…è´¹") = P(å«"å…è´¹"|åƒåœ¾) * P(åƒåœ¾) / P(å«"å…è´¹")

# å‡è®¾æ•°æ®
p_spam = 0.3  # 30% çš„é‚®ä»¶æ˜¯åƒåœ¾é‚®ä»¶
p_free_given_spam = 0.8  # åƒåœ¾é‚®ä»¶ä¸­ 80% å«"å…è´¹"
p_free_given_ham = 0.1  # æ­£å¸¸é‚®ä»¶ä¸­ 10% å«"å…è´¹"

# P(å«"å…è´¹") = P(å«"å…è´¹"|åƒåœ¾)*P(åƒåœ¾) + P(å«"å…è´¹"|æ­£å¸¸)*P(æ­£å¸¸)
p_free = p_free_given_spam * p_spam + p_free_given_ham * (1 - p_spam)

# è´å¶æ–¯å…¬å¼
p_spam_given_free = (p_free_given_spam * p_spam) / p_free

print(f"ä¸€å°å«'å…è´¹'çš„é‚®ä»¶æ˜¯åƒåœ¾é‚®ä»¶çš„æ¦‚ç‡: {p_spam_given_free:.2%}")
```

### 3.2 å¸¸è§åˆ†å¸ƒ

```python
import numpy as np
import matplotlib.pyplot as plt
from scipy import stats

fig, axes = plt.subplots(2, 2, figsize=(12, 10))

# 1. å‡åŒ€åˆ†å¸ƒ
x_uniform = np.linspace(0, 1, 100)
axes[0, 0].plot(x_uniform, stats.uniform.pdf(x_uniform))
axes[0, 0].fill_between(x_uniform, stats.uniform.pdf(x_uniform), alpha=0.3)
axes[0, 0].set_title('Uniform Distribution')
axes[0, 0].set_xlabel('x')
axes[0, 0].set_ylabel('Probability Density')

# 2. æ­£æ€åˆ†å¸ƒï¼ˆé«˜æ–¯åˆ†å¸ƒï¼‰
x_normal = np.linspace(-4, 4, 100)
for mu, sigma in [(0, 1), (0, 2), (1, 1)]:
    axes[0, 1].plot(x_normal, stats.norm.pdf(x_normal, mu, sigma),
                    label=f'Î¼={mu}, Ïƒ={sigma}')
axes[0, 1].set_title('Normal Distribution')
axes[0, 1].legend()

# 3. ä¼¯åŠªåˆ©åˆ†å¸ƒï¼ˆäºŒé¡¹åˆ†å¸ƒçš„ç‰¹ä¾‹ï¼‰
x_binomial = np.arange(0, 11)
for p in [0.3, 0.5, 0.7]:
    axes[1, 0].bar(x_binomial + (p-0.5)*0.2, stats.binom.pmf(x_binomial, 10, p),
                   width=0.2, alpha=0.7, label=f'p={p}')
axes[1, 0].set_title('Binomial Distribution (n=10)')
axes[1, 0].legend()

# 4. æ³Šæ¾åˆ†å¸ƒ
x_poisson = np.arange(0, 20)
for lam in [1, 4, 8]:
    axes[1, 1].bar(x_poisson + (lam-4)*0.1, stats.poisson.pmf(x_poisson, lam),
                   width=0.3, alpha=0.7, label=f'Î»={lam}')
axes[1, 1].set_title('Poisson Distribution')
axes[1, 1].legend()

plt.tight_layout()
plt.show()
```

### 3.3 Softmax å‡½æ•°

**ç›´è§‰**ï¼šæŠŠä»»æ„å®æ•°è½¬æ¢æˆæ¦‚ç‡åˆ†å¸ƒï¼ˆæ‰€æœ‰å€¼åŠ èµ·æ¥ç­‰äº 1ï¼‰

```python
import numpy as np

def softmax(x):
    exp_x = np.exp(x - np.max(x))  # å‡å»æœ€å¤§å€¼ï¼Œé˜²æ­¢æº¢å‡º
    return exp_x / np.sum(exp_x)

# ç¥ç»ç½‘ç»œè¾“å‡ºçš„ logitsï¼ˆå¯ä»¥æ˜¯ä»»æ„å®æ•°ï¼‰
logits = np.array([2.0, 1.0, 0.1])

# è½¬æ¢æˆæ¦‚ç‡
probs = softmax(logits)
print(f"Logits: {logits}")
print(f"Softmax: {probs}")
print(f"Sum: {probs.sum():.4f}")  # 1.0

# å¯è§†åŒ–
fig, axes = plt.subplots(1, 2, figsize=(12, 5))

axes[0].bar(['A', 'B', 'C'], logits)
axes[0].set_title('Logits (Raw Scores)')
axes[0].set_ylabel('Value')

axes[1].bar(['A', 'B', 'C'], probs)
axes[1].set_title('After Softmax (Probabilities)')
axes[1].set_ylabel('Probability')

plt.tight_layout()
plt.show()
```

**AI ä¸­çš„åº”ç”¨**ï¼š
- åˆ†ç±»ä»»åŠ¡çš„è¾“å‡ºå±‚
- æ³¨æ„åŠ›æœºåˆ¶ä¸­çš„æƒé‡å½’ä¸€åŒ–

### 3.4 ç†µï¼ˆEntropyï¼‰

**ç›´è§‰**ï¼šç†µ = ä¸ç¡®å®šæ€§çš„åº¦é‡

```python
import numpy as np

def entropy(p):
    """è®¡ç®—ç†µï¼ˆä»¥ 2 ä¸ºåº•ï¼‰"""
    p = np.array(p)
    # è¿‡æ»¤æ‰ 0ï¼ˆlog(0) æœªå®šä¹‰ï¼‰
    p = p[p > 0]
    return -np.sum(p * np.log2(p))

# æŠ›ç¡¬å¸çš„ä¾‹å­
# å…¬å¹³ç¡¬å¸ï¼šP(æ­£) = P(å) = 0.5 â†’ æœ€å¤§ä¸ç¡®å®šæ€§
# ä¸å…¬å¹³ç¡¬å¸ï¼šP(æ­£) = 0.9, P(å) = 0.1 â†’ è¾ƒä½ä¸ç¡®å®šæ€§
# å®Œå…¨ç¡®å®šï¼šP(æ­£) = 1.0, P(å) = 0.0 â†’ ç†µ = 0

print(f"å…¬å¹³ç¡¬å¸çš„ç†µ: {entropy([0.5, 0.5]):.4f} bits")
print(f"ä¸å…¬å¹³ç¡¬å¸çš„ç†µ: {entropy([0.9, 0.1]):.4f} bits")
print(f"å®Œå…¨ç¡®å®šçš„ç†µ: {entropy([1.0, 0.0]):.4f} bits")

# ç†µè¶Šé«˜ï¼Œä¿¡æ¯é‡è¶Šå¤§ï¼Œä¸ç¡®å®šæ€§è¶Šé«˜
```

### 3.5 äº¤å‰ç†µï¼ˆCross-Entropyï¼‰

**ç›´è§‰**ï¼šè¡¡é‡é¢„æµ‹åˆ†å¸ƒå’ŒçœŸå®åˆ†å¸ƒçš„å·®å¼‚

```python
def cross_entropy(y_true, y_pred):
    """è®¡ç®—äº¤å‰ç†µ"""
    y_pred = np.clip(y_pred, 1e-15, 1 - 1e-15)  # é˜²æ­¢ log(0)
    return -np.sum(y_true * np.log(y_pred))

# ä¸‰åˆ†ç±»é—®é¢˜
y_true = np.array([1, 0, 0])  # çœŸå®æ ‡ç­¾ï¼šç±»åˆ« 0

# å¥½çš„é¢„æµ‹
y_pred_good = np.array([0.9, 0.05, 0.05])
print(f"å¥½çš„é¢„æµ‹çš„äº¤å‰ç†µ: {cross_entropy(y_true, y_pred_good):.4f}")

# å·®çš„é¢„æµ‹
y_pred_bad = np.array([0.1, 0.5, 0.4])
print(f"å·®çš„é¢„æµ‹çš„äº¤å‰ç†µ: {cross_entropy(y_true, y_pred_bad):.4f}")

# äº¤å‰ç†µè¶Šå°ï¼Œé¢„æµ‹è¶Šå‡†ç¡®
# è¿™å°±æ˜¯åˆ†ç±»ä»»åŠ¡æœ€å¸¸ç”¨çš„æŸå¤±å‡½æ•°ï¼
```

---

## 4. ç»ƒä¹ é¢˜

### åŸºç¡€ç»ƒä¹ 

1. è®¡ç®—å‘é‡ [3, 4] çš„é•¿åº¦
2. è®¡ç®—å‘é‡ [1, 2, 3] å’Œ [4, 5, 6] çš„ç‚¹ç§¯
3. å†™ä¸€ä¸ªæ¢¯åº¦ä¸‹é™å‡½æ•°ï¼Œæ‰¾åˆ° f(x) = xÂ² + 2x + 1 çš„æœ€å°å€¼
4. è®¡ç®— softmax([1, 2, 3])

### å‚è€ƒç­”æ¡ˆ

<details>
<summary>ç‚¹å‡»æŸ¥çœ‹ç­”æ¡ˆ</summary>

```python
import numpy as np

# 1. å‘é‡é•¿åº¦
v = np.array([3, 4])
length = np.linalg.norm(v)  # æˆ– np.sqrt(3**2 + 4**2)
print(f"å‘é‡é•¿åº¦: {length}")  # 5.0

# 2. ç‚¹ç§¯
a = np.array([1, 2, 3])
b = np.array([4, 5, 6])
dot = np.dot(a, b)  # æˆ– a @ b
print(f"ç‚¹ç§¯: {dot}")  # 32

# 3. æ¢¯åº¦ä¸‹é™
def gradient_descent(start, lr, iterations):
    x = start
    # f(x) = xÂ² + 2x + 1, f'(x) = 2x + 2
    for i in range(iterations):
        grad = 2 * x + 2
        x = x - lr * grad
    return x

result = gradient_descent(start=5, lr=0.1, iterations=100)
print(f"æœ€å°å€¼ç‚¹: x = {result:.4f}")  # çº¦ -1

# 4. Softmax
def softmax(x):
    exp_x = np.exp(x - np.max(x))
    return exp_x / np.sum(exp_x)

result = softmax(np.array([1, 2, 3]))
print(f"Softmax: {result}")  # [0.0900, 0.2447, 0.6652]
print(f"Sum: {result.sum()}")  # 1.0
```

</details>

---

## â¡ï¸ ä¸‹ä¸€æ­¥

å­¦å®Œæœ¬èŠ‚åï¼Œç»§ç»­å­¦ä¹  [09-AIè¾…åŠ©å­¦ä¹ .md](./09-AIè¾…åŠ©å­¦ä¹ .md)

