# ğŸ”¢ 08 - æ•°å­¦ç›´è§‰

> ä¸å·æ•°å­¦æ¨å¯¼ï¼Œä½†å»ºç«‹ç›´è§‰ï¼Œçœ‹åˆ°å…¬å¼ä¸å®³æ€•

---

## ç›®å½•

1. [çº¿æ€§ä»£æ•°ç›´è§‰](#1-çº¿æ€§ä»£æ•°ç›´è§‰)
2. [å¾®ç§¯åˆ†ç›´è§‰](#2-å¾®ç§¯åˆ†ç›´è§‰)
3. [æ¦‚ç‡ä¸ä¿¡æ¯è®º](#3-æ¦‚ç‡ä¸ä¿¡æ¯è®º)
4. [AI ä¸­çš„æ•°å­¦åº”ç”¨](#4-ai-ä¸­çš„æ•°å­¦åº”ç”¨)
5. [äº¤äº’å¼å¯è§†åŒ–](#5-äº¤äº’å¼å¯è§†åŒ–)
6. [å¸¸è§å…¬å¼é€ŸæŸ¥è¡¨](#6-å¸¸è§å…¬å¼é€ŸæŸ¥è¡¨)
7. [ç»ƒä¹ é¢˜](#7-ç»ƒä¹ é¢˜)
8. [æ¨èå­¦ä¹ èµ„æº](#8-æ¨èå­¦ä¹ èµ„æº)

---

## 1. çº¿æ€§ä»£æ•°ç›´è§‰

### 1.1 å‘é‡

**ç›´è§‰**ï¼šå‘é‡å°±æ˜¯ä¸€ä¸ªç®­å¤´ï¼Œæœ‰æ–¹å‘å’Œé•¿åº¦

```python
import numpy as np
import matplotlib.pyplot as plt

# å‘é‡å°±æ˜¯ä¸€ç»„æœ‰åºçš„æ•°
v = np.array([3, 2])

# å¯è§†åŒ–
plt.figure(figsize=(8, 6))
plt.quiver(0, 0, v[0], v[1], angles='xy', scale_units='xy', scale=1, color='blue')
plt.xlim(-1, 5)
plt.ylim(-1, 4)
plt.grid(True)
plt.xlabel('x')
plt.ylabel('y')
plt.title(f'Vector v = {v}')
plt.axhline(y=0, color='k', linestyle='-', linewidth=0.5)
plt.axvline(x=0, color='k', linestyle='-', linewidth=0.5)
plt.show()

# å‘é‡çš„é•¿åº¦ï¼ˆèŒƒæ•°ï¼‰
length = np.linalg.norm(v)
print(f"å‘é‡é•¿åº¦: {length:.2f}")  # sqrt(3Â² + 2Â²) = sqrt(13) â‰ˆ 3.61
```

**ğŸ¤– AI ä¸­çš„å‘é‡**ï¼š
- **è¯å‘é‡**ï¼šæ¯ä¸ªè¯æ˜¯ä¸€ä¸ªé«˜ç»´å‘é‡ï¼ˆå¦‚ 300 ç»´ï¼‰ï¼Œè¯­ä¹‰ç›¸è¿‘çš„è¯å‘é‡è·ç¦»è¿‘
- **å›¾åƒåµŒå…¥**ï¼šå›¾ç‰‡ç»è¿‡ CNN åå˜æˆä¸€ä¸ªå‘é‡ï¼ˆå¦‚ 2048 ç»´ï¼‰
- **ç”¨æˆ·ç”»åƒ**ï¼šç”¨æˆ·çš„è¡Œä¸ºç‰¹å¾å¯ä»¥è¡¨ç¤ºä¸ºå‘é‡

### 1.2 ç‚¹ç§¯ï¼ˆDot Productï¼‰

**ç›´è§‰**ï¼šè¡¡é‡ä¸¤ä¸ªå‘é‡çš„ç›¸ä¼¼ç¨‹åº¦

```python
a = np.array([1, 0])  # æŒ‡å‘å³
b = np.array([0, 1])  # æŒ‡å‘ä¸Š
c = np.array([1, 1])  # æŒ‡å‘å³ä¸Š

# ç‚¹ç§¯å…¬å¼: a Â· b = |a||b|cos(Î¸)
# å¦‚æœä¸¤ä¸ªå‘é‡æ–¹å‘ç›¸åŒï¼Œç‚¹ç§¯æœ€å¤§
# å¦‚æœä¸¤ä¸ªå‘é‡å‚ç›´ï¼Œç‚¹ç§¯ä¸º 0
# å¦‚æœä¸¤ä¸ªå‘é‡æ–¹å‘ç›¸åï¼Œç‚¹ç§¯ä¸ºè´Ÿ

print(f"a Â· b = {np.dot(a, b)}")  # 0ï¼ˆå‚ç›´ï¼‰
print(f"a Â· c = {np.dot(a, c)}")  # 1ï¼ˆæœ‰å…±åŒæˆåˆ†ï¼‰

# å¯è§†åŒ–
fig, ax = plt.subplots(figsize=(8, 6))
origin = [0, 0]
ax.quiver(*origin, *a, angles='xy', scale_units='xy', scale=1, color='blue', label='a=[1,0]')
ax.quiver(*origin, *b, angles='xy', scale_units='xy', scale=1, color='red', label='b=[0,1]')
ax.quiver(*origin, *c, angles='xy', scale_units='xy', scale=1, color='green', label='c=[1,1]')
ax.set_xlim(-0.5, 2)
ax.set_ylim(-0.5, 2)
ax.grid(True)
ax.legend()
ax.set_aspect('equal')
plt.title('Dot Product Intuition')
plt.show()
```

**ğŸ¤– AI ä¸­çš„ç‚¹ç§¯**ï¼š
| åº”ç”¨åœºæ™¯ | ç‚¹ç§¯çš„ä½œç”¨ |
|---------|-----------|
| **ä½™å¼¦ç›¸ä¼¼åº¦** | `cos(Î¸) = (aÂ·b)/(â€–aâ€–â€–bâ€–)` è¡¡é‡ä¸¤ä¸ªå‘é‡çš„ç›¸ä¼¼åº¦ |
| **æ³¨æ„åŠ›æœºåˆ¶** | Query å’Œ Key çš„ç‚¹ç§¯å†³å®šæ³¨æ„åŠ›æƒé‡ |
| **æ¨èç³»ç»Ÿ** | ç”¨æˆ·å‘é‡å’Œç‰©å“å‘é‡çš„ç‚¹ç§¯ = åå¥½ç¨‹åº¦ |
| **å…¨è¿æ¥å±‚** | `output = input Â· weight + bias` |

```python
# ä½™å¼¦ç›¸ä¼¼åº¦ç¤ºä¾‹
def cosine_similarity(a, b):
    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))

# å‡è®¾è¿™æ˜¯ä¸¤ä¸ªè¯çš„å‘é‡
word1 = np.array([0.8, 0.2, 0.5])  # "å›½ç‹"
word2 = np.array([0.7, 0.3, 0.4])  # "çš‡å¸"
word3 = np.array([-0.5, 0.8, 0.1]) # "è‹¹æœ"

print(f"å›½ç‹-çš‡å¸ ç›¸ä¼¼åº¦: {cosine_similarity(word1, word2):.3f}")  # é«˜
print(f"å›½ç‹-è‹¹æœ ç›¸ä¼¼åº¦: {cosine_similarity(word1, word3):.3f}")  # ä½
```

### 1.3 çŸ©é˜µ

**ç›´è§‰**ï¼šçŸ©é˜µæ˜¯ä¸€ç§å˜æ¢ï¼Œå¯ä»¥æ—‹è½¬ã€ç¼©æ”¾ã€æŠ•å½±å‘é‡

```python
# çŸ©é˜µå°±æ˜¯ä¸€ç»„å‘é‡ï¼ˆæŒ‰åˆ—æ’åˆ—ï¼‰
A = np.array([[2, 0],
              [0, 1]])

# çŸ©é˜µä¹˜å‘é‡ = å˜æ¢
v = np.array([1, 1])
v_transformed = A @ v  # [2, 1]

print(f"åŸå‘é‡: {v}")
print(f"å˜æ¢å: {v_transformed}")

# è¿™ä¸ªçŸ©é˜µçš„æ•ˆæœæ˜¯ï¼šx æ–¹å‘æ”¾å¤§ 2 å€ï¼Œy æ–¹å‘ä¸å˜
```

**ğŸ¤– AI ä¸­çš„çŸ©é˜µ**ï¼š
- **æƒé‡çŸ©é˜µ**ï¼šç¥ç»ç½‘ç»œçš„æ¯ä¸€å±‚éƒ½æ˜¯ä¸€ä¸ªçŸ©é˜µå˜æ¢
- **æ³¨æ„åŠ›çŸ©é˜µ**ï¼šTransformer ä¸­çš„ Qã€Kã€V å˜æ¢
- **åæ–¹å·®çŸ©é˜µ**ï¼šæè¿°æ•°æ®ç‰¹å¾ä¹‹é—´çš„å…³è”

### 1.4 çŸ©é˜µä¹˜æ³•

**ç›´è§‰**ï¼šçŸ©é˜µä¹˜æ³• = å‡½æ•°å¤åˆï¼ˆå…ˆåšä¸€ä¸ªå˜æ¢ï¼Œå†åšå¦ä¸€ä¸ªï¼‰

```python
# çŸ©é˜µ A: æ”¾å¤§
A = np.array([[2, 0],
              [0, 2]])

# çŸ©é˜µ B: æ—‹è½¬ 90 åº¦
theta = np.pi / 2  # 90 åº¦
B = np.array([[np.cos(theta), -np.sin(theta)],
              [np.sin(theta), np.cos(theta)]])

# A @ B: å…ˆæ—‹è½¬ï¼Œå†æ”¾å¤§
# B @ A: å…ˆæ”¾å¤§ï¼Œå†æ—‹è½¬
# æ³¨æ„ï¼šçŸ©é˜µä¹˜æ³•ä¸æ»¡è¶³äº¤æ¢å¾‹ï¼

v = np.array([1, 0])
print(f"åŸå‘é‡: {v}")
print(f"A @ vï¼ˆæ”¾å¤§ï¼‰: {A @ v}")
print(f"B @ vï¼ˆæ—‹è½¬ï¼‰: {B @ v}")
print(f"A @ B @ v: {A @ B @ v}")  # å…ˆæ—‹è½¬å†æ”¾å¤§
```

**ğŸ¤– ç¥ç»ç½‘ç»œä¸­çš„çŸ©é˜µä¹˜æ³•**ï¼š

```python
# å…¨è¿æ¥å±‚å°±æ˜¯çŸ©é˜µä¹˜æ³•
def linear_layer(x, W, b):
    """
    x: è¾“å…¥ (batch_size, input_dim)
    W: æƒé‡ (input_dim, output_dim)
    b: åç½® (output_dim,)
    """
    return x @ W + b

# ç¤ºä¾‹ï¼šä¸€ä¸ª 3â†’2 çš„å…¨è¿æ¥å±‚
x = np.array([[1, 2, 3],      # æ ·æœ¬ 1
              [4, 5, 6]])      # æ ·æœ¬ 2
W = np.random.randn(3, 2)      # æƒé‡
b = np.random.randn(2)         # åç½®

output = linear_layer(x, W, b)
print(f"è¾“å…¥å½¢çŠ¶: {x.shape}")      # (2, 3)
print(f"æƒé‡å½¢çŠ¶: {W.shape}")      # (3, 2)
print(f"è¾“å‡ºå½¢çŠ¶: {output.shape}") # (2, 2)
```

### 1.5 å¼ é‡ï¼ˆTensorï¼‰

**ç›´è§‰**ï¼šå¼ é‡æ˜¯é«˜ç»´æ•°ç»„

```python
# æ ‡é‡ï¼ˆ0ç»´ï¼‰ï¼šä¸€ä¸ªæ•°
scalar = 5

# å‘é‡ï¼ˆ1ç»´ï¼‰ï¼šä¸€åˆ—æ•°
vector = np.array([1, 2, 3])

# çŸ©é˜µï¼ˆ2ç»´ï¼‰ï¼šä¸€å¼ è¡¨
matrix = np.array([[1, 2], [3, 4], [5, 6]])

# å¼ é‡ï¼ˆ3ç»´+ï¼‰ï¼šå¤šå¼ è¡¨
tensor_3d = np.random.randn(2, 3, 4)  # 2 ä¸ª 3x4 çš„çŸ©é˜µ

# å›¾åƒé€šå¸¸æ˜¯ 3D å¼ é‡ï¼š(é«˜åº¦, å®½åº¦, é€šé“)
image = np.random.randint(0, 256, (224, 224, 3))  # RGB å›¾åƒ

# ä¸€æ‰¹å›¾åƒæ˜¯ 4D å¼ é‡ï¼š(æ‰¹é‡å¤§å°, é«˜åº¦, å®½åº¦, é€šé“)
batch_images = np.random.randint(0, 256, (32, 224, 224, 3))

print(f"æ ‡é‡å½¢çŠ¶: {np.array(scalar).shape}")    # ()
print(f"å‘é‡å½¢çŠ¶: {vector.shape}")               # (3,)
print(f"çŸ©é˜µå½¢çŠ¶: {matrix.shape}")               # (3, 2)
print(f"3Då¼ é‡å½¢çŠ¶: {tensor_3d.shape}")          # (2, 3, 4)
print(f"å›¾åƒå½¢çŠ¶: {image.shape}")                # (224, 224, 3)
print(f"æ‰¹é‡å›¾åƒå½¢çŠ¶: {batch_images.shape}")     # (32, 224, 224, 3)
```

**ğŸ¤– AI ä¸­å¸¸è§çš„å¼ é‡å½¢çŠ¶**ï¼š

| æ•°æ®ç±»å‹ | å½¢çŠ¶ | è¯´æ˜ |
|---------|------|------|
| **æ‰¹é‡å‘é‡** | `(batch, features)` | å¦‚è¡¨æ ¼æ•°æ® |
| **æ‰¹é‡å›¾åƒ** | `(batch, H, W, C)` | TensorFlow æ ¼å¼ |
| **æ‰¹é‡å›¾åƒ** | `(batch, C, H, W)` | PyTorch æ ¼å¼ |
| **æ‰¹é‡åºåˆ—** | `(batch, seq_len, features)` | å¦‚æ–‡æœ¬ã€æ—¶é—´åºåˆ— |
| **Attention** | `(batch, heads, seq, seq)` | æ³¨æ„åŠ›æƒé‡ |

---

## 2. å¾®ç§¯åˆ†ç›´è§‰

### 2.1 å¯¼æ•°

**ç›´è§‰**ï¼šå¯¼æ•° = å˜åŒ–ç‡ = æ›²çº¿åœ¨æŸç‚¹çš„æ–œç‡

```python
import numpy as np
import matplotlib.pyplot as plt

# å‡½æ•° f(x) = xÂ²
def f(x):
    return x ** 2

# å¯¼æ•° f'(x) = 2xï¼ˆæ–œç‡ï¼‰
def df(x):
    return 2 * x

x = np.linspace(-3, 3, 100)

fig, axes = plt.subplots(1, 2, figsize=(14, 5))

# å·¦å›¾ï¼šå‡½æ•°å’Œåˆ‡çº¿
axes[0].plot(x, f(x), 'b-', label='f(x) = xÂ²')
x0 = 1  # åœ¨ x=1 å¤„çš„åˆ‡çº¿
slope = df(x0)
tangent = f(x0) + slope * (x - x0)
axes[0].plot(x, tangent, 'r--', label=f'Tangent at x={x0}, slope={slope}')
axes[0].scatter([x0], [f(x0)], color='red', s=100, zorder=5)
axes[0].set_xlim(-3, 3)
axes[0].set_ylim(-1, 9)
axes[0].legend()
axes[0].grid(True, alpha=0.3)
axes[0].set_title('Function and Tangent Line')

# å³å›¾ï¼šå¯¼æ•°
axes[1].plot(x, df(x), 'g-', label="f'(x) = 2x")
axes[1].axhline(y=0, color='k', linestyle='-', linewidth=0.5)
axes[1].legend()
axes[1].grid(True, alpha=0.3)
axes[1].set_title('Derivative')

plt.tight_layout()
plt.show()

print(f"åœ¨ x=1 å¤„ï¼Œæ–œç‡ = {df(1)}")
print(f"åœ¨ x=-2 å¤„ï¼Œæ–œç‡ = {df(-2)}")
print(f"åœ¨ x=0 å¤„ï¼Œæ–œç‡ = {df(0)}ï¼ˆå‡½æ•°æœ€å°å€¼ç‚¹ï¼Œæ–œç‡ä¸º0ï¼‰")
```

**ğŸ¤– AI ä¸­å¯¼æ•°çš„æ„ä¹‰**ï¼š
- **æŸå¤±å¯¹å‚æ•°çš„å¯¼æ•°** = å‚æ•°æ”¹å˜æ—¶æŸå¤±å¦‚ä½•å˜åŒ–
- **å¯¼æ•°ä¸º 0 çš„ç‚¹** = å¯èƒ½æ˜¯æœ€ä¼˜è§£
- **å¯¼æ•°çš„ç¬¦å·** = å†³å®šå‚æ•°åº”è¯¥å¢å¤§è¿˜æ˜¯å‡å°

### 2.2 æ¢¯åº¦

**ç›´è§‰**ï¼šæ¢¯åº¦ = å¤šç»´ç©ºé—´ä¸­å˜åŒ–æœ€å¿«çš„æ–¹å‘

```python
# å¯¹äºå¤šå˜é‡å‡½æ•° f(x, y)
# æ¢¯åº¦æ˜¯ä¸€ä¸ªå‘é‡ï¼š(âˆ‚f/âˆ‚x, âˆ‚f/âˆ‚y)
# æŒ‡å‘å‡½æ•°å¢é•¿æœ€å¿«çš„æ–¹å‘

# ä¾‹å¦‚ f(x, y) = xÂ² + yÂ²
# æ¢¯åº¦ = (2x, 2y)

def f(x, y):
    return x**2 + y**2

# åœ¨ç‚¹ (1, 2) å¤„
x0, y0 = 1, 2
gradient = np.array([2*x0, 2*y0])  # [2, 4]
print(f"ç‚¹ ({x0}, {y0}) å¤„çš„æ¢¯åº¦: {gradient}")
print(f"æ¢¯åº¦æŒ‡å‘å‡½æ•°å€¼å¢åŠ æœ€å¿«çš„æ–¹å‘")

# å¯è§†åŒ–
from mpl_toolkits.mplot3d import Axes3D

x = np.linspace(-3, 3, 50)
y = np.linspace(-3, 3, 50)
X, Y = np.meshgrid(x, y)
Z = f(X, Y)

fig = plt.figure(figsize=(10, 8))
ax = fig.add_subplot(111, projection='3d')
ax.plot_surface(X, Y, Z, cmap='viridis', alpha=0.7)
ax.set_xlabel('x')
ax.set_ylabel('y')
ax.set_zlabel('f(x,y)')
ax.set_title('f(x,y) = xÂ² + yÂ²')
plt.show()
```

**ğŸ¤– æ¢¯åº¦åœ¨ç¥ç»ç½‘ç»œä¸­çš„è§’è‰²**ï¼š

```
æŸå¤±å‡½æ•° L(Î¸)
    â†“
è®¡ç®—æ¢¯åº¦ âˆ‡L(Î¸)
    â†“
æ›´æ–°å‚æ•° Î¸ = Î¸ - Î±Â·âˆ‡L(Î¸)
    â†“
é‡å¤ç›´åˆ°æ”¶æ•›
```

### 2.3 æ¢¯åº¦ä¸‹é™

**ç›´è§‰**ï¼šæƒ³è±¡ä½ åœ¨å±±ä¸Šï¼Œè’™ç€çœ¼ç›æƒ³ä¸‹å±±ï¼Œæ€ä¹ˆåŠï¼Ÿæ²¿ç€è„šä¸‹æœ€é™¡çš„æ–¹å‘èµ°ï¼

```python
import numpy as np
import matplotlib.pyplot as plt

# ç›®æ ‡ï¼šæ‰¾åˆ° f(x) = (x-3)Â² çš„æœ€å°å€¼ç‚¹
def f(x):
    return (x - 3) ** 2

def gradient(x):
    return 2 * (x - 3)

# æ¢¯åº¦ä¸‹é™
x = 0  # èµ·å§‹ç‚¹
learning_rate = 0.1  # å­¦ä¹ ç‡ï¼ˆæ­¥é•¿ï¼‰
history = [x]

for i in range(20):
    grad = gradient(x)
    x = x - learning_rate * grad  # æœæ¢¯åº¦çš„åæ–¹å‘ç§»åŠ¨
    history.append(x)
    print(f"Step {i+1}: x = {x:.4f}, f(x) = {f(x):.4f}")

# å¯è§†åŒ–
x_range = np.linspace(-1, 7, 100)
plt.figure(figsize=(10, 6))
plt.plot(x_range, f(x_range), 'b-', label='f(x) = (x-3)Â²')
plt.scatter(history, [f(h) for h in history], c=range(len(history)), cmap='Reds', s=50, zorder=5)
plt.xlabel('x')
plt.ylabel('f(x)')
plt.title('Gradient Descent: Finding Minimum')
plt.legend()
plt.grid(True, alpha=0.3)
plt.colorbar(label='Step')
plt.show()
```

**ğŸ¤– å­¦ä¹ ç‡çš„å½±å“**ï¼š

| å­¦ä¹ ç‡ | æ•ˆæœ | é—®é¢˜ |
|-------|------|------|
| **å¤ªå¤§** | è·³è¿‡æœ€å°å€¼ | éœ‡è¡ã€å‘æ•£ |
| **å¤ªå°** | å®‰å…¨ä½†ç¼“æ…¢ | è®­ç»ƒæ—¶é—´è¿‡é•¿ |
| **åˆé€‚** | å¿«é€Ÿæ”¶æ•› | éœ€è¦è°ƒå‚æ‰¾åˆ° |

```python
# ä¸åŒå­¦ä¹ ç‡çš„å¯¹æ¯”
def visualize_learning_rates():
    fig, axes = plt.subplots(1, 3, figsize=(15, 4))

    learning_rates = [0.01, 0.1, 0.9]
    titles = ['å¤ªå° (lr=0.01)', 'åˆé€‚ (lr=0.1)', 'å¤ªå¤§ (lr=0.9)']

    for ax, lr, title in zip(axes, learning_rates, titles):
        x = 0
        history = [x]
        for _ in range(20):
            grad = 2 * (x - 3)
            x = x - lr * grad
            history.append(x)

        x_range = np.linspace(-2, 8, 100)
        ax.plot(x_range, (x_range - 3)**2, 'b-')
        ax.scatter(history, [(h-3)**2 for h in history],
                   c=range(len(history)), cmap='Reds', s=30)
        ax.set_title(title)
        ax.set_xlabel('x')
        ax.set_ylabel('f(x)')
        ax.grid(True, alpha=0.3)

    plt.tight_layout()
    plt.show()

visualize_learning_rates()
```

### 2.4 é“¾å¼æ³•åˆ™

**ç›´è§‰**ï¼šå¤åˆå‡½æ•°çš„å¯¼æ•° = å¤–å±‚å¯¼æ•° Ã— å†…å±‚å¯¼æ•°

```python
# å¦‚æœ y = f(g(x))
# é‚£ä¹ˆ dy/dx = df/dg * dg/dx

# ä¾‹å¦‚ï¼šy = (x + 1)Â²
# ä»¤ u = x + 1, y = uÂ²
# dy/dx = dy/du * du/dx = 2u * 1 = 2(x+1)

# åœ¨ç¥ç»ç½‘ç»œä¸­ï¼Œè¿™å°±æ˜¯åå‘ä¼ æ’­çš„æ•°å­¦åŸºç¡€ï¼
# è¯¯å·®ä»è¾“å‡ºå±‚ä¸€å±‚å±‚ä¼ å›è¾“å…¥å±‚
```

**ğŸ¤– åå‘ä¼ æ’­ç¤ºä¾‹**ï¼š

```python
# ç®€å•çš„ä¸¤å±‚ç½‘ç»œçš„åå‘ä¼ æ’­
def simple_backprop():
    """
    ç½‘ç»œç»“æ„: x -> [Linear] -> z -> [ReLU] -> a -> [Linear] -> y
    """
    # å‰å‘ä¼ æ’­
    x = 2.0       # è¾“å…¥
    w1 = 0.5      # ç¬¬ä¸€å±‚æƒé‡
    w2 = -0.3     # ç¬¬äºŒå±‚æƒé‡

    # å‰å‘
    z = x * w1           # z = 1.0
    a = max(0, z)        # ReLU, a = 1.0
    y = a * w2           # y = -0.3

    # å‡è®¾ç›®æ ‡æ˜¯ 1.0ï¼ŒæŸå¤± = (y - target)Â²
    target = 1.0
    loss = (y - target) ** 2  # loss = 1.69

    # åå‘ä¼ æ’­ï¼ˆé“¾å¼æ³•åˆ™ï¼‰
    dL_dy = 2 * (y - target)      # âˆ‚L/âˆ‚y = -2.6
    dL_da = dL_dy * w2            # âˆ‚L/âˆ‚a = 0.78
    dL_dz = dL_da * (1 if z > 0 else 0)  # ReLU å¯¼æ•°
    dL_dw1 = dL_dz * x            # âˆ‚L/âˆ‚w1
    dL_dw2 = dL_dy * a            # âˆ‚L/âˆ‚w2

    print(f"æŸå¤±: {loss:.4f}")
    print(f"âˆ‚L/âˆ‚w1 = {dL_dw1:.4f}")
    print(f"âˆ‚L/âˆ‚w2 = {dL_dw2:.4f}")

simple_backprop()
```

---

## 3. æ¦‚ç‡ä¸ä¿¡æ¯è®º

### 3.1 æ¦‚ç‡åŸºç¡€

```python
import numpy as np

# æ¦‚ç‡ = äº‹ä»¶å‘ç”Ÿçš„å¯èƒ½æ€§ï¼ŒèŒƒå›´ [0, 1]
# P(A) = äº‹ä»¶ A å‘ç”Ÿçš„æ¦‚ç‡

# æ¡ä»¶æ¦‚ç‡
# P(A|B) = åœ¨ B å‘ç”Ÿçš„æ¡ä»¶ä¸‹ï¼ŒA å‘ç”Ÿçš„æ¦‚ç‡
# P(A|B) = P(A âˆ© B) / P(B)

# è´å¶æ–¯å…¬å¼ï¼ˆè¶…çº§é‡è¦ï¼ï¼‰
# P(A|B) = P(B|A) * P(A) / P(B)

# ä¾‹å­ï¼šåƒåœ¾é‚®ä»¶æ£€æµ‹
# P(åƒåœ¾|å«"å…è´¹") = P(å«"å…è´¹"|åƒåœ¾) * P(åƒåœ¾) / P(å«"å…è´¹")

# å‡è®¾æ•°æ®
p_spam = 0.3  # 30% çš„é‚®ä»¶æ˜¯åƒåœ¾é‚®ä»¶
p_free_given_spam = 0.8  # åƒåœ¾é‚®ä»¶ä¸­ 80% å«"å…è´¹"
p_free_given_ham = 0.1  # æ­£å¸¸é‚®ä»¶ä¸­ 10% å«"å…è´¹"

# P(å«"å…è´¹") = P(å«"å…è´¹"|åƒåœ¾)*P(åƒåœ¾) + P(å«"å…è´¹"|æ­£å¸¸)*P(æ­£å¸¸)
p_free = p_free_given_spam * p_spam + p_free_given_ham * (1 - p_spam)

# è´å¶æ–¯å…¬å¼
p_spam_given_free = (p_free_given_spam * p_spam) / p_free

print(f"ä¸€å°å«'å…è´¹'çš„é‚®ä»¶æ˜¯åƒåœ¾é‚®ä»¶çš„æ¦‚ç‡: {p_spam_given_free:.2%}")
```

**ğŸ¤– è´å¶æ–¯åœ¨ AI ä¸­çš„åº”ç”¨**ï¼š

| åœºæ™¯ | åº”ç”¨ |
|------|------|
| **æœ´ç´ è´å¶æ–¯åˆ†ç±»** | æ–‡æœ¬åˆ†ç±»ã€åƒåœ¾é‚®ä»¶è¿‡æ»¤ |
| **è´å¶æ–¯ä¼˜åŒ–** | è¶…å‚æ•°è°ƒä¼˜ |
| **è´å¶æ–¯ç¥ç»ç½‘ç»œ** | ä¸ç¡®å®šæ€§ä¼°è®¡ |
| **éšé©¬å°”å¯å¤«æ¨¡å‹** | è¯­éŸ³è¯†åˆ«ã€åºåˆ—æ ‡æ³¨ |

### 3.2 å¸¸è§åˆ†å¸ƒ

```python
import numpy as np
import matplotlib.pyplot as plt
from scipy import stats

fig, axes = plt.subplots(2, 2, figsize=(12, 10))

# 1. å‡åŒ€åˆ†å¸ƒ
x_uniform = np.linspace(0, 1, 100)
axes[0, 0].plot(x_uniform, stats.uniform.pdf(x_uniform))
axes[0, 0].fill_between(x_uniform, stats.uniform.pdf(x_uniform), alpha=0.3)
axes[0, 0].set_title('Uniform Distribution')
axes[0, 0].set_xlabel('x')
axes[0, 0].set_ylabel('Probability Density')

# 2. æ­£æ€åˆ†å¸ƒï¼ˆé«˜æ–¯åˆ†å¸ƒï¼‰
x_normal = np.linspace(-4, 4, 100)
for mu, sigma in [(0, 1), (0, 2), (1, 1)]:
    axes[0, 1].plot(x_normal, stats.norm.pdf(x_normal, mu, sigma),
                    label=f'Î¼={mu}, Ïƒ={sigma}')
axes[0, 1].set_title('Normal Distribution')
axes[0, 1].legend()

# 3. ä¼¯åŠªåˆ©åˆ†å¸ƒï¼ˆäºŒé¡¹åˆ†å¸ƒçš„ç‰¹ä¾‹ï¼‰
x_binomial = np.arange(0, 11)
for p in [0.3, 0.5, 0.7]:
    axes[1, 0].bar(x_binomial + (p-0.5)*0.2, stats.binom.pmf(x_binomial, 10, p),
                   width=0.2, alpha=0.7, label=f'p={p}')
axes[1, 0].set_title('Binomial Distribution (n=10)')
axes[1, 0].legend()

# 4. æ³Šæ¾åˆ†å¸ƒ
x_poisson = np.arange(0, 20)
for lam in [1, 4, 8]:
    axes[1, 1].bar(x_poisson + (lam-4)*0.1, stats.poisson.pmf(x_poisson, lam),
                   width=0.3, alpha=0.7, label=f'Î»={lam}')
axes[1, 1].set_title('Poisson Distribution')
axes[1, 1].legend()

plt.tight_layout()
plt.show()
```

**ğŸ¤– åˆ†å¸ƒåœ¨ AI ä¸­çš„åº”ç”¨**ï¼š

| åˆ†å¸ƒ | AI åº”ç”¨ |
|------|--------|
| **æ­£æ€åˆ†å¸ƒ** | æƒé‡åˆå§‹åŒ–ã€å™ªå£°æ³¨å…¥ã€VAE |
| **ä¼¯åŠªåˆ©åˆ†å¸ƒ** | äºŒåˆ†ç±»ã€Dropout |
| **ç±»åˆ«åˆ†å¸ƒ** | å¤šåˆ†ç±»ã€è¯­è¨€æ¨¡å‹è¾“å‡º |
| **æ³Šæ¾åˆ†å¸ƒ** | äº‹ä»¶è®¡æ•°ã€å¼‚å¸¸æ£€æµ‹ |

### 3.3 Softmax å‡½æ•°

**ç›´è§‰**ï¼šæŠŠä»»æ„å®æ•°è½¬æ¢æˆæ¦‚ç‡åˆ†å¸ƒï¼ˆæ‰€æœ‰å€¼åŠ èµ·æ¥ç­‰äº 1ï¼‰

```python
import numpy as np

def softmax(x):
    exp_x = np.exp(x - np.max(x))  # å‡å»æœ€å¤§å€¼ï¼Œé˜²æ­¢æº¢å‡º
    return exp_x / np.sum(exp_x)

# ç¥ç»ç½‘ç»œè¾“å‡ºçš„ logitsï¼ˆå¯ä»¥æ˜¯ä»»æ„å®æ•°ï¼‰
logits = np.array([2.0, 1.0, 0.1])

# è½¬æ¢æˆæ¦‚ç‡
probs = softmax(logits)
print(f"Logits: {logits}")
print(f"Softmax: {probs}")
print(f"Sum: {probs.sum():.4f}")  # 1.0

# å¯è§†åŒ–
fig, axes = plt.subplots(1, 2, figsize=(12, 5))

axes[0].bar(['A', 'B', 'C'], logits)
axes[0].set_title('Logits (Raw Scores)')
axes[0].set_ylabel('Value')

axes[1].bar(['A', 'B', 'C'], probs)
axes[1].set_title('After Softmax (Probabilities)')
axes[1].set_ylabel('Probability')

plt.tight_layout()
plt.show()
```

**ğŸ¤– Softmax æ¸©åº¦å‚æ•°**ï¼š

```python
def softmax_with_temperature(x, T=1.0):
    """
    T=1.0: æ ‡å‡† softmax
    T<1.0: æ›´å°–é”ï¼ˆæ›´ç¡®å®šï¼‰
    T>1.0: æ›´å¹³æ»‘ï¼ˆæ›´éšæœºï¼‰
    """
    exp_x = np.exp((x - np.max(x)) / T)
    return exp_x / np.sum(exp_x)

logits = np.array([2.0, 1.0, 0.5])

print("æ¸©åº¦å¯¹åˆ†å¸ƒçš„å½±å“:")
for T in [0.5, 1.0, 2.0, 5.0]:
    probs = softmax_with_temperature(logits, T)
    print(f"T={T}: {probs.round(3)}")
```

### 3.4 ç†µï¼ˆEntropyï¼‰

**ç›´è§‰**ï¼šç†µ = ä¸ç¡®å®šæ€§çš„åº¦é‡

```python
import numpy as np

def entropy(p):
    """è®¡ç®—ç†µï¼ˆä»¥ 2 ä¸ºåº•ï¼‰"""
    p = np.array(p)
    # è¿‡æ»¤æ‰ 0ï¼ˆlog(0) æœªå®šä¹‰ï¼‰
    p = p[p > 0]
    return -np.sum(p * np.log2(p))

# æŠ›ç¡¬å¸çš„ä¾‹å­
# å…¬å¹³ç¡¬å¸ï¼šP(æ­£) = P(å) = 0.5 â†’ æœ€å¤§ä¸ç¡®å®šæ€§
# ä¸å…¬å¹³ç¡¬å¸ï¼šP(æ­£) = 0.9, P(å) = 0.1 â†’ è¾ƒä½ä¸ç¡®å®šæ€§
# å®Œå…¨ç¡®å®šï¼šP(æ­£) = 1.0, P(å) = 0.0 â†’ ç†µ = 0

print(f"å…¬å¹³ç¡¬å¸çš„ç†µ: {entropy([0.5, 0.5]):.4f} bits")
print(f"ä¸å…¬å¹³ç¡¬å¸çš„ç†µ: {entropy([0.9, 0.1]):.4f} bits")
print(f"å®Œå…¨ç¡®å®šçš„ç†µ: {entropy([1.0, 0.0]):.4f} bits")

# ç†µè¶Šé«˜ï¼Œä¿¡æ¯é‡è¶Šå¤§ï¼Œä¸ç¡®å®šæ€§è¶Šé«˜
```

### 3.5 äº¤å‰ç†µï¼ˆCross-Entropyï¼‰

**ç›´è§‰**ï¼šè¡¡é‡é¢„æµ‹åˆ†å¸ƒå’ŒçœŸå®åˆ†å¸ƒçš„å·®å¼‚

```python
def cross_entropy(y_true, y_pred):
    """è®¡ç®—äº¤å‰ç†µ"""
    y_pred = np.clip(y_pred, 1e-15, 1 - 1e-15)  # é˜²æ­¢ log(0)
    return -np.sum(y_true * np.log(y_pred))

# ä¸‰åˆ†ç±»é—®é¢˜
y_true = np.array([1, 0, 0])  # çœŸå®æ ‡ç­¾ï¼šç±»åˆ« 0

# å¥½çš„é¢„æµ‹
y_pred_good = np.array([0.9, 0.05, 0.05])
print(f"å¥½çš„é¢„æµ‹çš„äº¤å‰ç†µ: {cross_entropy(y_true, y_pred_good):.4f}")

# å·®çš„é¢„æµ‹
y_pred_bad = np.array([0.1, 0.5, 0.4])
print(f"å·®çš„é¢„æµ‹çš„äº¤å‰ç†µ: {cross_entropy(y_true, y_pred_bad):.4f}")

# äº¤å‰ç†µè¶Šå°ï¼Œé¢„æµ‹è¶Šå‡†ç¡®
# è¿™å°±æ˜¯åˆ†ç±»ä»»åŠ¡æœ€å¸¸ç”¨çš„æŸå¤±å‡½æ•°ï¼
```

**ğŸ¤– äº¤å‰ç†µ vs MSE**ï¼š

| æŸå¤±å‡½æ•° | é€‚ç”¨åœºæ™¯ | ç‰¹ç‚¹ |
|---------|---------|------|
| **äº¤å‰ç†µ** | åˆ†ç±»ä»»åŠ¡ | æƒ©ç½šé”™è¯¯é¢„æµ‹æ›´å¼º |
| **MSE** | å›å½’ä»»åŠ¡ | å¹³æ–¹è¯¯å·® |
| **äºŒå…ƒäº¤å‰ç†µ** | äºŒåˆ†ç±» | `-yÂ·log(Å·) - (1-y)Â·log(1-Å·)` |

---

## 4. AI ä¸­çš„æ•°å­¦åº”ç”¨

### 4.1 ç¥ç»ç½‘ç»œå‰å‘ä¼ æ’­

```python
import numpy as np

def relu(x):
    return np.maximum(0, x)

def softmax(x):
    exp_x = np.exp(x - np.max(x, axis=-1, keepdims=True))
    return exp_x / np.sum(exp_x, axis=-1, keepdims=True)

# ç®€å•çš„ä¸¤å±‚ç¥ç»ç½‘ç»œ
class SimpleNN:
    def __init__(self, input_dim, hidden_dim, output_dim):
        # éšæœºåˆå§‹åŒ–æƒé‡
        self.W1 = np.random.randn(input_dim, hidden_dim) * 0.01
        self.b1 = np.zeros(hidden_dim)
        self.W2 = np.random.randn(hidden_dim, output_dim) * 0.01
        self.b2 = np.zeros(output_dim)

    def forward(self, X):
        """å‰å‘ä¼ æ’­"""
        # ç¬¬ä¸€å±‚: z1 = X @ W1 + b1
        self.z1 = X @ self.W1 + self.b1
        # æ¿€æ´»å‡½æ•°
        self.a1 = relu(self.z1)
        # ç¬¬äºŒå±‚
        self.z2 = self.a1 @ self.W2 + self.b2
        # Softmax è¾“å‡º
        self.probs = softmax(self.z2)
        return self.probs

# åˆ›å»ºç½‘ç»œ
nn = SimpleNN(input_dim=784, hidden_dim=128, output_dim=10)

# æ¨¡æ‹Ÿä¸€å¼  28x28 çš„å›¾åƒ
X = np.random.randn(1, 784)
probs = nn.forward(X)

print(f"è¾“å…¥å½¢çŠ¶: {X.shape}")
print(f"è¾“å‡ºæ¦‚ç‡: {probs.round(3)}")
print(f"é¢„æµ‹ç±»åˆ«: {np.argmax(probs)}")
```

### 4.2 æ³¨æ„åŠ›æœºåˆ¶çš„æ•°å­¦

```python
import numpy as np

def attention(Q, K, V):
    """
    ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›
    Q: Query (seq_len, d_k)
    K: Key (seq_len, d_k)
    V: Value (seq_len, d_v)
    """
    d_k = K.shape[-1]

    # è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°: Q @ K^T / sqrt(d_k)
    scores = Q @ K.T / np.sqrt(d_k)

    # Softmax å½’ä¸€åŒ–
    weights = softmax(scores)

    # åŠ æƒæ±‚å’Œ
    output = weights @ V

    return output, weights

# ç¤ºä¾‹
seq_len, d_k, d_v = 4, 8, 16
Q = np.random.randn(seq_len, d_k)
K = np.random.randn(seq_len, d_k)
V = np.random.randn(seq_len, d_v)

output, weights = attention(Q, K, V)

print(f"Q å½¢çŠ¶: {Q.shape}")
print(f"æ³¨æ„åŠ›æƒé‡å½¢çŠ¶: {weights.shape}")
print(f"è¾“å‡ºå½¢çŠ¶: {output.shape}")
print(f"\næ³¨æ„åŠ›æƒé‡ï¼ˆæ¯è¡Œå’Œä¸º1ï¼‰:\n{weights.round(3)}")
```

### 4.3 è¯å‘é‡çš„æ•°å­¦

```python
import numpy as np

# æ¨¡æ‹Ÿè¯å‘é‡
vocab = {
    "king": np.array([0.9, 0.1, 0.8, 0.2]),
    "queen": np.array([0.85, 0.15, 0.7, 0.8]),
    "man": np.array([0.8, 0.1, 0.9, 0.1]),
    "woman": np.array([0.75, 0.15, 0.8, 0.9]),
}

def cosine_sim(a, b):
    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))

# è‘—åçš„è¯å‘é‡ç­‰å¼: king - man + woman â‰ˆ queen
result = vocab["king"] - vocab["man"] + vocab["woman"]

# æ‰¾æœ€ç›¸ä¼¼çš„è¯
similarities = {word: cosine_sim(result, vec) for word, vec in vocab.items()}
print("king - man + woman æœ€ç›¸ä¼¼çš„è¯:")
for word, sim in sorted(similarities.items(), key=lambda x: -x[1]):
    print(f"  {word}: {sim:.3f}")
```

---

## 5. äº¤äº’å¼å¯è§†åŒ–

### 5.1 æ¢¯åº¦ä¸‹é™å¯è§†åŒ–

```python
import numpy as np
import matplotlib.pyplot as plt
from matplotlib.animation import FuncAnimation
from IPython.display import HTML

def gradient_descent_animation():
    """åˆ›å»ºæ¢¯åº¦ä¸‹é™åŠ¨ç”»"""
    # æŸå¤±å‡½æ•°: f(x, y) = xÂ² + yÂ²
    def loss(x, y):
        return x**2 + y**2

    # æ¢¯åº¦
    def grad(x, y):
        return np.array([2*x, 2*y])

    # åˆå§‹ç‚¹
    x, y = 3.0, 4.0
    lr = 0.1

    # è®°å½•è½¨è¿¹
    path = [(x, y)]
    for _ in range(30):
        g = grad(x, y)
        x, y = x - lr * g[0], y - lr * g[1]
        path.append((x, y))
    path = np.array(path)

    # åˆ›å»ºç­‰é«˜çº¿å›¾
    fig, ax = plt.subplots(figsize=(10, 8))

    X = np.linspace(-4, 4, 100)
    Y = np.linspace(-5, 5, 100)
    X, Y = np.meshgrid(X, Y)
    Z = loss(X, Y)

    ax.contour(X, Y, Z, levels=20, cmap='viridis', alpha=0.7)
    ax.set_xlabel('x')
    ax.set_ylabel('y')
    ax.set_title('Gradient Descent Visualization')

    # ç”»è½¨è¿¹
    line, = ax.plot([], [], 'r-o', markersize=4, linewidth=1)
    point, = ax.plot([], [], 'ro', markersize=10)

    def init():
        line.set_data([], [])
        point.set_data([], [])
        return line, point

    def animate(i):
        line.set_data(path[:i+1, 0], path[:i+1, 1])
        point.set_data([path[i, 0]], [path[i, 1]])
        return line, point

    ani = FuncAnimation(fig, animate, init_func=init,
                        frames=len(path), interval=200, blit=True)
    plt.close()
    return ani

# åœ¨ Jupyter ä¸­è¿è¡Œä»¥ä¸‹ä»£ç æŸ¥çœ‹åŠ¨ç”»
# ani = gradient_descent_animation()
# HTML(ani.to_jshtml())
```

### 5.2 æ¿€æ´»å‡½æ•°å¯è§†åŒ–

```python
import numpy as np
import matplotlib.pyplot as plt

def plot_activation_functions():
    """å¯è§†åŒ–å¸¸è§æ¿€æ´»å‡½æ•°"""
    x = np.linspace(-5, 5, 200)

    # æ¿€æ´»å‡½æ•°
    activations = {
        'Sigmoid': lambda x: 1 / (1 + np.exp(-x)),
        'Tanh': lambda x: np.tanh(x),
        'ReLU': lambda x: np.maximum(0, x),
        'Leaky ReLU': lambda x: np.where(x > 0, x, 0.1 * x),
        'ELU': lambda x: np.where(x > 0, x, np.exp(x) - 1),
        'GELU': lambda x: x * 0.5 * (1 + np.tanh(np.sqrt(2/np.pi) * (x + 0.044715 * x**3)))
    }

    fig, axes = plt.subplots(2, 3, figsize=(15, 10))
    axes = axes.flatten()

    for ax, (name, func) in zip(axes, activations.items()):
        y = func(x)
        ax.plot(x, y, 'b-', linewidth=2)
        ax.axhline(y=0, color='k', linestyle='-', linewidth=0.5)
        ax.axvline(x=0, color='k', linestyle='-', linewidth=0.5)
        ax.grid(True, alpha=0.3)
        ax.set_title(name, fontsize=14)
        ax.set_xlabel('x')
        ax.set_ylabel('y')
        ax.set_xlim(-5, 5)

    plt.tight_layout()
    plt.show()

plot_activation_functions()
```

### 5.3 æŸå¤±æ›²é¢å¯è§†åŒ–

```python
from mpl_toolkits.mplot3d import Axes3D
import numpy as np
import matplotlib.pyplot as plt

def plot_loss_surface():
    """å¯è§†åŒ–æŸå¤±æ›²é¢"""
    # æ¨¡æ‹Ÿä¸€ä¸ªæœ‰å¤šä¸ªå±€éƒ¨æœ€å°å€¼çš„æŸå¤±å‡½æ•°
    def loss(x, y):
        return np.sin(x) * np.cos(y) + 0.1 * (x**2 + y**2)

    x = np.linspace(-3, 3, 100)
    y = np.linspace(-3, 3, 100)
    X, Y = np.meshgrid(x, y)
    Z = loss(X, Y)

    fig = plt.figure(figsize=(14, 5))

    # 3D è§†å›¾
    ax1 = fig.add_subplot(121, projection='3d')
    ax1.plot_surface(X, Y, Z, cmap='viridis', alpha=0.8)
    ax1.set_xlabel('Parameter 1')
    ax1.set_ylabel('Parameter 2')
    ax1.set_zlabel('Loss')
    ax1.set_title('Loss Surface (3D View)')

    # ç­‰é«˜çº¿è§†å›¾
    ax2 = fig.add_subplot(122)
    contour = ax2.contour(X, Y, Z, levels=20, cmap='viridis')
    ax2.clabel(contour, inline=True, fontsize=8)
    ax2.set_xlabel('Parameter 1')
    ax2.set_ylabel('Parameter 2')
    ax2.set_title('Loss Surface (Contour View)')

    plt.tight_layout()
    plt.show()

plot_loss_surface()
```

---

## 6. å¸¸è§å…¬å¼é€ŸæŸ¥è¡¨

### 6.1 çº¿æ€§ä»£æ•°

| å…¬å¼ | åç§° | Python |
|------|------|--------|
| `aÂ·b = Î£aáµ¢báµ¢` | ç‚¹ç§¯ | `np.dot(a, b)` æˆ– `a @ b` |
| `â€–vâ€– = âˆš(Î£váµ¢Â²)` | L2 èŒƒæ•° | `np.linalg.norm(v)` |
| `AâŠ¤` | è½¬ç½® | `A.T` |
| `Aâ»Â¹` | é€†çŸ©é˜µ | `np.linalg.inv(A)` |
| `det(A)` | è¡Œåˆ—å¼ | `np.linalg.det(A)` |
| `Av = Î»v` | ç‰¹å¾æ–¹ç¨‹ | `np.linalg.eig(A)` |

### 6.2 å¾®ç§¯åˆ†

| å…¬å¼ | åç§° | è¯´æ˜ |
|------|------|------|
| `f'(x) = lim[hâ†’0] (f(x+h)-f(x))/h` | å¯¼æ•°å®šä¹‰ | å˜åŒ–ç‡ |
| `âˆ‡f = (âˆ‚f/âˆ‚xâ‚, ..., âˆ‚f/âˆ‚xâ‚™)` | æ¢¯åº¦ | æœ€é™¡æ–¹å‘ |
| `Î¸ = Î¸ - Î±âˆ‡L` | æ¢¯åº¦ä¸‹é™ | å‚æ•°æ›´æ–° |
| `d/dx[f(g(x))] = f'(g(x))Â·g'(x)` | é“¾å¼æ³•åˆ™ | åå‘ä¼ æ’­åŸºç¡€ |

### 6.3 æ¦‚ç‡ç»Ÿè®¡

| å…¬å¼ | åç§° | Python |
|------|------|--------|
| `E[X] = Î£xáµ¢p(xáµ¢)` | æœŸæœ› | `np.mean(x)` |
| `Var(X) = E[(X-Î¼)Â²]` | æ–¹å·® | `np.var(x)` |
| `P(A\|B) = P(B\|A)P(A)/P(B)` | è´å¶æ–¯å…¬å¼ | - |
| `H(p) = -Î£páµ¢log(páµ¢)` | ç†µ | `scipy.stats.entropy(p)` |

### 6.4 ç¥ç»ç½‘ç»œ

| å…¬å¼ | åç§° | è¯´æ˜ |
|------|------|------|
| `Ïƒ(x) = 1/(1+eâ»Ë£)` | Sigmoid | è¾“å‡º (0, 1) |
| `ReLU(x) = max(0, x)` | ReLU | æœ€å¸¸ç”¨æ¿€æ´» |
| `softmax(xáµ¢) = eË£â±/Î£eË£Ê²` | Softmax | å¤šåˆ†ç±»è¾“å‡º |
| `CE = -Î£yáµ¢log(Å·áµ¢)` | äº¤å‰ç†µ | åˆ†ç±»æŸå¤± |
| `MSE = (1/n)Î£(y-Å·)Â²` | å‡æ–¹è¯¯å·® | å›å½’æŸå¤± |

### 6.5 æ³¨æ„åŠ›æœºåˆ¶

| å…¬å¼ | è¯´æ˜ |
|------|------|
| `Attention(Q,K,V) = softmax(QKâŠ¤/âˆšdâ‚–)V` | ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ› |
| `MultiHead = Concat(headâ‚,...,headâ‚•)WÂ° ` | å¤šå¤´æ³¨æ„åŠ› |

---

## 7. ç»ƒä¹ é¢˜

### åŸºç¡€ç»ƒä¹ 

1. è®¡ç®—å‘é‡ [3, 4] çš„é•¿åº¦
2. è®¡ç®—å‘é‡ [1, 2, 3] å’Œ [4, 5, 6] çš„ç‚¹ç§¯
3. å†™ä¸€ä¸ªæ¢¯åº¦ä¸‹é™å‡½æ•°ï¼Œæ‰¾åˆ° f(x) = xÂ² + 2x + 1 çš„æœ€å°å€¼
4. è®¡ç®— softmax([1, 2, 3])
5. è®¡ç®—åˆ†å¸ƒ [0.25, 0.25, 0.25, 0.25] çš„ç†µ

### è¿›é˜¶ç»ƒä¹ 

6. å®ç°ä¸€ä¸ªç®€å•çš„çº¿æ€§å›å½’ï¼ˆç”¨æ¢¯åº¦ä¸‹é™ï¼‰
7. å®ç°äºŒåˆ†ç±»çš„äº¤å‰ç†µæŸå¤±
8. ç”¨ NumPy å®ç°æ³¨æ„åŠ›æœºåˆ¶
9. å¯è§†åŒ–ä¸åŒå­¦ä¹ ç‡å¯¹æ¢¯åº¦ä¸‹é™çš„å½±å“
10. å®ç° Batch Normalization çš„å‰å‘ä¼ æ’­

### å‚è€ƒç­”æ¡ˆ

<details>
<summary>ç‚¹å‡»æŸ¥çœ‹ç­”æ¡ˆ</summary>

```python
import numpy as np

# 1. å‘é‡é•¿åº¦
v = np.array([3, 4])
length = np.linalg.norm(v)  # æˆ– np.sqrt(3**2 + 4**2)
print(f"å‘é‡é•¿åº¦: {length}")  # 5.0

# 2. ç‚¹ç§¯
a = np.array([1, 2, 3])
b = np.array([4, 5, 6])
dot = np.dot(a, b)  # æˆ– a @ b
print(f"ç‚¹ç§¯: {dot}")  # 32

# 3. æ¢¯åº¦ä¸‹é™
def gradient_descent(start, lr, iterations):
    x = start
    # f(x) = xÂ² + 2x + 1, f'(x) = 2x + 2
    for i in range(iterations):
        grad = 2 * x + 2
        x = x - lr * grad
    return x

result = gradient_descent(start=5, lr=0.1, iterations=100)
print(f"æœ€å°å€¼ç‚¹: x = {result:.4f}")  # çº¦ -1

# 4. Softmax
def softmax(x):
    exp_x = np.exp(x - np.max(x))
    return exp_x / np.sum(exp_x)

result = softmax(np.array([1, 2, 3]))
print(f"Softmax: {result}")  # [0.0900, 0.2447, 0.6652]
print(f"Sum: {result.sum()}")  # 1.0

# 5. ç†µ
p = np.array([0.25, 0.25, 0.25, 0.25])
entropy = -np.sum(p * np.log2(p))
print(f"ç†µ: {entropy}")  # 2.0 bits (æœ€å¤§ç†µ)

# 6. çº¿æ€§å›å½’
def linear_regression_gd(X, y, lr=0.01, iterations=1000):
    n, d = X.shape
    w = np.zeros(d)
    b = 0

    for _ in range(iterations):
        y_pred = X @ w + b
        error = y_pred - y

        # æ¢¯åº¦
        dw = (2/n) * X.T @ error
        db = (2/n) * np.sum(error)

        # æ›´æ–°
        w = w - lr * dw
        b = b - lr * db

    return w, b

# 7. äºŒåˆ†ç±»äº¤å‰ç†µ
def binary_cross_entropy(y_true, y_pred):
    y_pred = np.clip(y_pred, 1e-15, 1 - 1e-15)
    return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))

# 8. æ³¨æ„åŠ›æœºåˆ¶
def attention(Q, K, V):
    d_k = K.shape[-1]
    scores = Q @ K.T / np.sqrt(d_k)
    weights = softmax(scores)
    return weights @ V

# 9. è§ä¸Šæ–‡çš„ visualize_learning_rates å‡½æ•°

# 10. Batch Normalization
def batch_norm_forward(x, gamma, beta, eps=1e-5):
    """
    x: è¾“å…¥ (batch, features)
    gamma, beta: å¯å­¦ä¹ å‚æ•°
    """
    mean = np.mean(x, axis=0)
    var = np.var(x, axis=0)
    x_norm = (x - mean) / np.sqrt(var + eps)
    return gamma * x_norm + beta
```

</details>

---

## 8. æ¨èå­¦ä¹ èµ„æº

### 8.1 è§†é¢‘è¯¾ç¨‹

| èµ„æº | å†…å®¹ | é“¾æ¥ |
|------|------|------|
| **3Blue1Brown çº¿æ€§ä»£æ•°** | æœ€ä½³å¯è§†åŒ–çº¿ä»£è¯¾ç¨‹ | [Bç«™](https://www.bilibili.com/video/BV1ys411472E) |
| **3Blue1Brown å¾®ç§¯åˆ†** | ç›´è§‰ç†è§£å¾®ç§¯åˆ† | [Bç«™](https://www.bilibili.com/video/BV1qW411N7FU) |
| **3Blue1Brown ç¥ç»ç½‘ç»œ** | æ·±åº¦å­¦ä¹ å¯è§†åŒ– | [Bç«™](https://www.bilibili.com/video/BV1bx411M7Zx) |
| **StatQuest** | ç»Ÿè®¡å’Œæœºå™¨å­¦ä¹  | [YouTube](https://www.youtube.com/@statquest) |

### 8.2 äº¤äº’å¼ç½‘ç«™

| ç½‘ç«™ | å†…å®¹ |
|------|------|
| [Seeing Theory](https://seeing-theory.brown.edu/) | æ¦‚ç‡ç»Ÿè®¡å¯è§†åŒ– |
| [Immersive Math](http://immersivemath.com/ila/) | äº¤äº’å¼çº¿æ€§ä»£æ•° |
| [Better Explained](https://betterexplained.com/) | ç›´è§‰å¼æ•°å­¦è§£é‡Š |
| [Distill.pub](https://distill.pub/) | å¯è§†åŒ–æœºå™¨å­¦ä¹  |

### 8.3 ä¹¦ç±æ¨è

| ä¹¦ç± | é€‚åˆäººç¾¤ |
|------|---------|
| ã€Šæ·±åº¦å­¦ä¹ çš„æ•°å­¦ã€‹| å…¥é—¨ï¼Œä»é«˜ä¸­æ•°å­¦è®²èµ· |
| ã€Šçº¿æ€§ä»£æ•°åº”è¯¥è¿™æ ·å­¦ã€‹| æƒ³æ·±å…¥ç†è§£çº¿æ€§ä»£æ•° |
| ã€Šæ¦‚ç‡è®ºä¸æ•°ç†ç»Ÿè®¡ã€‹| ç³»ç»Ÿå­¦ä¹ æ¦‚ç‡ |
| ã€ŠDeep Learningã€‹| æ·±åº¦å­¦ä¹ åœ£ç»ï¼Œå…è´¹åœ¨çº¿ |

### 8.4 Python åº“æ–‡æ¡£

- [NumPy å®˜æ–¹æ•™ç¨‹](https://numpy.org/doc/stable/user/absolute_beginners.html)
- [SciPy çº¿æ€§ä»£æ•°](https://docs.scipy.org/doc/scipy/tutorial/linalg.html)
- [SymPy ç¬¦å·è®¡ç®—](https://docs.sympy.org/latest/tutorials/intro-tutorial/)

---

## â¡ï¸ ä¸‹ä¸€æ­¥

å­¦å®Œæœ¬èŠ‚åï¼Œç»§ç»­å­¦ä¹  [09-AIè¾…åŠ©å­¦ä¹ .md](./09-AIè¾…åŠ©å­¦ä¹ .md)

å¦‚éœ€æ›´æ·±å…¥çš„çº¿æ€§ä»£æ•°çŸ¥è¯†ï¼Œè¯·å‚è€ƒ [14-çº¿æ€§ä»£æ•°è¿›é˜¶.md](./14-çº¿æ€§ä»£æ•°è¿›é˜¶.md)
