# ğŸ“Š 14 - çº¿æ€§ä»£æ•°è¿›é˜¶

> ç‰¹å¾å€¼ã€SVDã€PCA â€”â€” ç”¨ç›´è§‰ç†è§£ AI çš„æ•°å­¦æ ¸å¿ƒ

---

## ç›®å½•

1. [ç‰¹å¾å€¼ä¸ç‰¹å¾å‘é‡](#1-ç‰¹å¾å€¼ä¸ç‰¹å¾å‘é‡)
2. [å¥‡å¼‚å€¼åˆ†è§£ï¼ˆSVDï¼‰](#2-å¥‡å¼‚å€¼åˆ†è§£svd)
3. [ä¸»æˆåˆ†åˆ†æï¼ˆPCAï¼‰](#3-ä¸»æˆåˆ†åˆ†æpca)
4. [çŸ©é˜µåˆ†è§£çš„ AI åº”ç”¨](#4-çŸ©é˜µåˆ†è§£çš„-ai-åº”ç”¨)
5. [å®æˆ˜ç»ƒä¹ ](#5-å®æˆ˜ç»ƒä¹ )

---

## 1. ç‰¹å¾å€¼ä¸ç‰¹å¾å‘é‡

### 1.1 ç›´è§‰ç†è§£

**æ ¸å¿ƒæ€æƒ³**ï¼šçŸ©é˜µæ˜¯ä¸€ç§å˜æ¢ï¼Œç‰¹å¾å‘é‡æ˜¯å˜æ¢ä¸­ã€Œæ–¹å‘ä¸å˜ã€çš„å‘é‡ã€‚

```
æƒ³è±¡ä½ åœ¨æ‹‰ä¸€å—æ©¡çš®æ³¥ï¼š
- å¤§å¤šæ•°æ–¹å‘éƒ½ä¼šè¢«æ‹‰æ­ª
- ä½†æœ‰äº›ç‰¹æ®Šæ–¹å‘åªä¼šè¢«æ‹‰é•¿æˆ–å‹ç¼©ï¼Œæ–¹å‘ä¿æŒä¸å˜
- è¿™äº›ç‰¹æ®Šæ–¹å‘å°±æ˜¯ã€Œç‰¹å¾å‘é‡ã€
- æ‹‰é•¿/å‹ç¼©çš„å€æ•°å°±æ˜¯ã€Œç‰¹å¾å€¼ã€
```

### 1.2 æ•°å­¦å®šä¹‰

å¯¹äºçŸ©é˜µ **A**ï¼Œå¦‚æœå­˜åœ¨å‘é‡ **v** å’Œæ ‡é‡ Î»ï¼Œä½¿å¾—ï¼š

```
Av = Î»v
```

åˆ™ï¼š
- **v** æ˜¯ **A** çš„**ç‰¹å¾å‘é‡**ï¼ˆeigenvectorï¼‰
- Î» æ˜¯å¯¹åº”çš„**ç‰¹å¾å€¼**ï¼ˆeigenvalueï¼‰

### 1.3 å¯è§†åŒ–ç†è§£

```python
import numpy as np
import matplotlib.pyplot as plt

# å®šä¹‰ä¸€ä¸ª 2x2 çŸ©é˜µ
A = np.array([[3, 1],
              [0, 2]])

# è®¡ç®—ç‰¹å¾å€¼å’Œç‰¹å¾å‘é‡
eigenvalues, eigenvectors = np.linalg.eig(A)
print(f"ç‰¹å¾å€¼: {eigenvalues}")      # [3. 2.]
print(f"ç‰¹å¾å‘é‡:\n{eigenvectors}")

# å¯è§†åŒ–ï¼šçŸ©é˜µå˜æ¢å¯¹ä¸åŒå‘é‡çš„å½±å“
fig, axes = plt.subplots(1, 2, figsize=(14, 6))

# ç”Ÿæˆå•ä½åœ†ä¸Šçš„ç‚¹
theta = np.linspace(0, 2*np.pi, 100)
circle = np.array([np.cos(theta), np.sin(theta)])

# å·¦å›¾ï¼šåŸå§‹å‘é‡
axes[0].plot(circle[0], circle[1], 'b-', alpha=0.5, label='å•ä½åœ†')
# ç”»ç‰¹å¾å‘é‡
for i, (val, vec) in enumerate(zip(eigenvalues, eigenvectors.T)):
    axes[0].arrow(0, 0, vec[0], vec[1], head_width=0.1, color=['red', 'green'][i],
                  label=f'ç‰¹å¾å‘é‡ {i+1} (Î»={val:.1f})')
axes[0].set_xlim(-2, 2)
axes[0].set_ylim(-2, 2)
axes[0].set_aspect('equal')
axes[0].grid(True, alpha=0.3)
axes[0].legend()
axes[0].set_title('å˜æ¢å‰')

# å³å›¾ï¼šå˜æ¢å
transformed = A @ circle
axes[1].plot(transformed[0], transformed[1], 'b-', alpha=0.5, label='å˜æ¢å')
# å˜æ¢åçš„ç‰¹å¾å‘é‡ï¼ˆåªæ˜¯æ‹‰ä¼¸ï¼Œæ–¹å‘ä¸å˜ï¼ï¼‰
for i, (val, vec) in enumerate(zip(eigenvalues, eigenvectors.T)):
    new_vec = A @ vec  # = Î» * vec
    axes[1].arrow(0, 0, new_vec[0], new_vec[1], head_width=0.1,
                  color=['red', 'green'][i],
                  label=f'å˜æ¢åç‰¹å¾å‘é‡ {i+1}')
axes[1].set_xlim(-4, 4)
axes[1].set_ylim(-4, 4)
axes[1].set_aspect('equal')
axes[1].grid(True, alpha=0.3)
axes[1].legend()
axes[1].set_title('A å˜æ¢åï¼ˆç‰¹å¾å‘é‡æ–¹å‘ä¸å˜ï¼Œåªæ‹‰ä¼¸ï¼‰')

plt.tight_layout()
plt.show()
```

### 1.4 Python å®ç°

```python
import numpy as np

# æ–¹æ³• 1ï¼šä½¿ç”¨ np.linalg.eig
A = np.array([[4, -2],
              [1,  1]])

eigenvalues, eigenvectors = np.linalg.eig(A)
print("ç‰¹å¾å€¼:", eigenvalues)       # [3. 2.]
print("ç‰¹å¾å‘é‡:\n", eigenvectors)

# éªŒè¯ Av = Î»v
for i in range(len(eigenvalues)):
    v = eigenvectors[:, i]
    Î» = eigenvalues[i]
    print(f"\nÎ»{i+1} = {Î»}")
    print(f"Av  = {A @ v}")
    print(f"Î»v  = {Î» * v}")
    print(f"ç›¸ç­‰ï¼Ÿ{np.allclose(A @ v, Î» * v)}")

# æ–¹æ³• 2ï¼šå¯¹ç§°çŸ©é˜µç”¨ eighï¼ˆæ›´ç¨³å®šï¼Œç‰¹å¾å€¼ä¿è¯æ˜¯å®æ•°ï¼‰
S = np.array([[4, 2],
              [2, 3]])  # å¯¹ç§°çŸ©é˜µ
eigenvalues, eigenvectors = np.linalg.eigh(S)
print("\nå¯¹ç§°çŸ©é˜µç‰¹å¾å€¼:", eigenvalues)  # [1.76... 5.23...]
```

### 1.5 ç‰¹å¾å€¼çš„æ€§è´¨

```python
import numpy as np

A = np.array([[3, 1],
              [0, 2]])

eigenvalues = np.linalg.eigvals(A)

# æ€§è´¨ 1ï¼šç‰¹å¾å€¼ä¹‹å’Œ = è¿¹ï¼ˆå¯¹è§’çº¿å…ƒç´ ä¹‹å’Œï¼‰
print(f"ç‰¹å¾å€¼ä¹‹å’Œ: {sum(eigenvalues)}")  # 5
print(f"è¿¹ tr(A): {np.trace(A)}")          # 5

# æ€§è´¨ 2ï¼šç‰¹å¾å€¼ä¹‹ç§¯ = è¡Œåˆ—å¼
print(f"ç‰¹å¾å€¼ä¹‹ç§¯: {np.prod(eigenvalues)}")  # 6
print(f"è¡Œåˆ—å¼ det(A): {np.linalg.det(A)}")   # 6

# æ€§è´¨ 3ï¼šå¯¹ç§°çŸ©é˜µçš„ç‰¹å¾å‘é‡æ­£äº¤
S = np.array([[4, 2],
              [2, 3]])
_, eigvecs = np.linalg.eigh(S)
print(f"\næ­£äº¤æ€§éªŒè¯ v1Â·v2 = {eigvecs[:, 0] @ eigvecs[:, 1]:.10f}")  # â‰ˆ 0
```

### 1.6 AI ä¸­çš„åº”ç”¨

```python
# åº”ç”¨ 1ï¼šPageRank ç®—æ³•ï¼ˆç®€åŒ–ç‰ˆï¼‰
# ç½‘é¡µé‡è¦æ€§ = è½¬ç§»çŸ©é˜µçš„ä¸»ç‰¹å¾å‘é‡

# å‡è®¾æœ‰ 4 ä¸ªç½‘é¡µï¼Œè½¬ç§»æ¦‚ç‡çŸ©é˜µ
M = np.array([
    [0,   1/2, 1/2, 0  ],
    [1/3, 0,   0,   1/2],
    [1/3, 0,   0,   1/2],
    [1/3, 1/2, 1/2, 0  ]
])

eigenvalues, eigenvectors = np.linalg.eig(M.T)
# æ‰¾ç‰¹å¾å€¼ä¸º 1 çš„ç‰¹å¾å‘é‡
idx = np.argmax(np.abs(eigenvalues))
pagerank = np.abs(eigenvectors[:, idx])
pagerank = pagerank / pagerank.sum()  # å½’ä¸€åŒ–
print("PageRank:", pagerank)

# åº”ç”¨ 2ï¼šåæ–¹å·®çŸ©é˜µçš„ç‰¹å¾åˆ†è§£ â†’ PCA
# è§ä¸‹æ–‡ PCA éƒ¨åˆ†
```

---

## 2. å¥‡å¼‚å€¼åˆ†è§£ï¼ˆSVDï¼‰

### 2.1 ç›´è§‰ç†è§£

**æ ¸å¿ƒæ€æƒ³**ï¼šä»»ä½•çŸ©é˜µéƒ½å¯ä»¥åˆ†è§£ä¸ºã€Œæ—‹è½¬ Ã— ç¼©æ”¾ Ã— æ—‹è½¬ã€

```
SVD çš„å‡ ä½•å«ä¹‰ï¼š
1. VâŠ¤ï¼šå…ˆæ—‹è½¬åˆ°æŸä¸ªç‰¹æ®Šåæ ‡ç³»
2. Î£ï¼šåœ¨è¿™ä¸ªåæ ‡ç³»ä¸‹è¿›è¡Œç¼©æ”¾
3. Uï¼šå†æ—‹è½¬åˆ°æœ€ç»ˆä½ç½®

A = UÎ£VâŠ¤
```

### 2.2 æ•°å­¦å®šä¹‰

å¯¹äºä»»æ„ mÃ—n çŸ©é˜µ **A**ï¼Œå­˜åœ¨åˆ†è§£ï¼š

```
A = UÎ£VâŠ¤

å…¶ä¸­ï¼š
- U: mÃ—m æ­£äº¤çŸ©é˜µï¼ˆå·¦å¥‡å¼‚å‘é‡ï¼‰
- Î£: mÃ—n å¯¹è§’çŸ©é˜µï¼ˆå¥‡å¼‚å€¼ï¼Œéè´Ÿä¸”é™åºï¼‰
- V: nÃ—n æ­£äº¤çŸ©é˜µï¼ˆå³å¥‡å¼‚å‘é‡ï¼‰
```

### 2.3 Python å®ç°

```python
import numpy as np

# åˆ›å»ºä¸€ä¸ªçŸ©é˜µ
A = np.array([[1, 2, 3],
              [4, 5, 6],
              [7, 8, 9]])

# SVD åˆ†è§£
U, s, Vt = np.linalg.svd(A)

print("U (å·¦å¥‡å¼‚å‘é‡):\n", U)
print("\nå¥‡å¼‚å€¼ Ïƒ:", s)
print("\nVâŠ¤ (å³å¥‡å¼‚å‘é‡):\n", Vt)

# é‡æ„åŸçŸ©é˜µ
S = np.zeros_like(A, dtype=float)
S[:len(s), :len(s)] = np.diag(s)
A_reconstructed = U @ S @ Vt
print("\né‡æ„çŸ©é˜µ:\n", A_reconstructed)
print("é‡æ„è¯¯å·®:", np.linalg.norm(A - A_reconstructed))
```

### 2.4 ä½ç§©è¿‘ä¼¼ï¼ˆå›¾åƒå‹ç¼©ï¼‰

**æ ¸å¿ƒæ€æƒ³**ï¼šåªä¿ç•™æœ€å¤§çš„ k ä¸ªå¥‡å¼‚å€¼ï¼Œå¯ä»¥è¿‘ä¼¼åŸçŸ©é˜µ

```python
import numpy as np
import matplotlib.pyplot as plt

# åˆ›å»ºä¸€ä¸ªç¤ºä¾‹å›¾åƒï¼ˆæˆ–ä½¿ç”¨çœŸå®å›¾ç‰‡ï¼‰
from sklearn.datasets import load_sample_image

# åŠ è½½ç¤ºä¾‹å›¾åƒå¹¶è½¬ä¸ºç°åº¦
try:
    img = load_sample_image('china.jpg')
    img_gray = np.mean(img, axis=2)
except:
    # å¦‚æœåŠ è½½å¤±è´¥ï¼Œåˆ›å»ºä¸€ä¸ªæµ‹è¯•å›¾åƒ
    x = np.linspace(0, 10, 200)
    y = np.linspace(0, 10, 200)
    X, Y = np.meshgrid(x, y)
    img_gray = np.sin(X) * np.cos(Y) * 100 + 128

print(f"åŸå§‹å›¾åƒå¤§å°: {img_gray.shape}")

# SVD åˆ†è§£
U, s, Vt = np.linalg.svd(img_gray, full_matrices=False)

# ç”¨ä¸åŒæ•°é‡çš„å¥‡å¼‚å€¼é‡æ„
fig, axes = plt.subplots(2, 3, figsize=(15, 10))

ks = [1, 5, 20, 50, 100, len(s)]
for ax, k in zip(axes.flat, ks):
    # ä½ç§©è¿‘ä¼¼
    img_approx = U[:, :k] @ np.diag(s[:k]) @ Vt[:k, :]

    # è®¡ç®—å‹ç¼©æ¯”
    original_size = img_gray.shape[0] * img_gray.shape[1]
    compressed_size = k * (img_gray.shape[0] + img_gray.shape[1] + 1)
    compression_ratio = original_size / compressed_size

    ax.imshow(img_approx, cmap='gray')
    ax.set_title(f'k={k}, å‹ç¼©æ¯”={compression_ratio:.1f}x')
    ax.axis('off')

plt.suptitle('SVD å›¾åƒå‹ç¼©æ•ˆæœ', fontsize=14)
plt.tight_layout()
plt.show()

# åˆ†æå¥‡å¼‚å€¼è¡°å‡
plt.figure(figsize=(10, 5))
plt.subplot(1, 2, 1)
plt.plot(s[:50], 'b-o', markersize=4)
plt.xlabel('å¥‡å¼‚å€¼åºå·')
plt.ylabel('å¥‡å¼‚å€¼å¤§å°')
plt.title('å‰ 50 ä¸ªå¥‡å¼‚å€¼')
plt.grid(True, alpha=0.3)

plt.subplot(1, 2, 2)
cumsum = np.cumsum(s) / np.sum(s)
plt.plot(cumsum[:100], 'r-')
plt.axhline(y=0.9, color='g', linestyle='--', label='90%')
plt.axhline(y=0.95, color='orange', linestyle='--', label='95%')
plt.xlabel('ä½¿ç”¨çš„å¥‡å¼‚å€¼æ•°é‡')
plt.ylabel('ç´¯è®¡æ–¹å·®è§£é‡Šæ¯”ä¾‹')
plt.title('å¥‡å¼‚å€¼ç´¯è®¡è´¡çŒ®')
plt.legend()
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
```

### 2.5 SVD vs ç‰¹å¾åˆ†è§£

| ç‰¹æ€§ | ç‰¹å¾åˆ†è§£ | SVD |
|------|---------|-----|
| é€‚ç”¨çŸ©é˜µ | æ–¹é˜µ | ä»»æ„çŸ©é˜µ |
| åˆ†è§£å½¢å¼ | A = PDPâ»Â¹ | A = UÎ£VâŠ¤ |
| ç»“æœçŸ©é˜µ | å¯èƒ½ä¸æ­£äº¤ | U, V æ­£äº¤ |
| å€¼çš„æ€§è´¨ | ç‰¹å¾å€¼å¯ä¸ºè´Ÿ/å¤æ•° | å¥‡å¼‚å€¼éè´Ÿå®æ•° |

```python
# å¯¹äºå¯¹ç§°çŸ©é˜µï¼ŒSVD å’Œç‰¹å¾åˆ†è§£æœ‰å¯†åˆ‡å…³ç³»
import numpy as np

A = np.array([[4, 2],
              [2, 3]])

# ç‰¹å¾åˆ†è§£
eigenvalues, eigenvectors = np.linalg.eigh(A)
print("ç‰¹å¾å€¼:", eigenvalues)

# SVD
U, s, Vt = np.linalg.svd(A)
print("å¥‡å¼‚å€¼:", s)

# å¯¹äºå¯¹ç§°åŠæ­£å®šçŸ©é˜µï¼Œå¥‡å¼‚å€¼ = ç‰¹å¾å€¼ï¼ˆéƒ½æ˜¯æ­£çš„ï¼‰
# AâŠ¤A çš„ç‰¹å¾å€¼ = A çš„å¥‡å¼‚å€¼çš„å¹³æ–¹
print("\nAâŠ¤A çš„ç‰¹å¾å€¼:", np.linalg.eigvalsh(A.T @ A))
print("å¥‡å¼‚å€¼çš„å¹³æ–¹:", s ** 2)
```

---

## 3. ä¸»æˆåˆ†åˆ†æï¼ˆPCAï¼‰

### 3.1 ç›´è§‰ç†è§£

**æ ¸å¿ƒæ€æƒ³**ï¼šæ‰¾åˆ°æ•°æ®ä¸­ã€Œæœ€é‡è¦çš„æ–¹å‘ã€ï¼Œç”¨æ›´å°‘çš„ç»´åº¦æè¿°æ•°æ®

```
æƒ³è±¡ä½ åœ¨çœ‹ä¸€ç¾¤äººç«™åœ¨å¹¿åœºä¸Šï¼š
- ä»æ­£é¢çœ‹ï¼Œäººç¾¤åˆ†æ•£å¼€æ¥ï¼ˆä¿¡æ¯é‡å¤§ï¼‰
- ä»ä¸Šé¢çœ‹ï¼Œå¯èƒ½éƒ½æŒ¤åœ¨ä¸€æ¡çº¿ä¸Šï¼ˆä¿¡æ¯é‡å°ï¼‰
- PCA å°±æ˜¯æ‰¾åˆ°ã€Œä»æ­£é¢çœ‹ã€çš„æœ€ä½³è§’åº¦
```

### 3.2 æ•°å­¦åŸç†

```
PCA æ­¥éª¤ï¼š
1. æ•°æ®ä¸­å¿ƒåŒ–ï¼ˆå‡å»å‡å€¼ï¼‰
2. è®¡ç®—åæ–¹å·®çŸ©é˜µ C = (1/n)XâŠ¤X
3. å¯¹ C è¿›è¡Œç‰¹å¾åˆ†è§£
4. é€‰æ‹©æœ€å¤§çš„ k ä¸ªç‰¹å¾å€¼å¯¹åº”çš„ç‰¹å¾å‘é‡
5. å°†æ•°æ®æŠ•å½±åˆ°è¿™äº›ç‰¹å¾å‘é‡ä¸Š
```

### 3.3 ä»é›¶å®ç° PCA

```python
import numpy as np
import matplotlib.pyplot as plt

def pca_from_scratch(X, n_components):
    """
    ä»é›¶å®ç° PCA

    å‚æ•°:
        X: æ•°æ®çŸ©é˜µ (n_samples, n_features)
        n_components: ä¿ç•™çš„ä¸»æˆåˆ†æ•°é‡

    è¿”å›:
        X_transformed: é™ç»´åçš„æ•°æ®
        components: ä¸»æˆåˆ†ï¼ˆç‰¹å¾å‘é‡ï¼‰
        explained_variance: è§£é‡Šçš„æ–¹å·®
    """
    # 1. ä¸­å¿ƒåŒ–
    mean = np.mean(X, axis=0)
    X_centered = X - mean

    # 2. è®¡ç®—åæ–¹å·®çŸ©é˜µ
    n_samples = X.shape[0]
    cov_matrix = (X_centered.T @ X_centered) / (n_samples - 1)

    # 3. ç‰¹å¾åˆ†è§£
    eigenvalues, eigenvectors = np.linalg.eigh(cov_matrix)

    # 4. æŒ‰ç‰¹å¾å€¼é™åºæ’åˆ—
    idx = np.argsort(eigenvalues)[::-1]
    eigenvalues = eigenvalues[idx]
    eigenvectors = eigenvectors[:, idx]

    # 5. é€‰æ‹©å‰ k ä¸ªä¸»æˆåˆ†
    components = eigenvectors[:, :n_components]

    # 6. æŠ•å½±
    X_transformed = X_centered @ components

    # è§£é‡Šçš„æ–¹å·®
    explained_variance = eigenvalues[:n_components]
    explained_variance_ratio = explained_variance / eigenvalues.sum()

    return X_transformed, components, explained_variance_ratio

# åˆ›å»ºç¤ºä¾‹æ•°æ®
np.random.seed(42)
n_samples = 300

# ç”Ÿæˆç›¸å…³çš„äºŒç»´æ•°æ®
mean = [0, 0]
cov = [[3, 2], [2, 2]]  # åæ–¹å·®çŸ©é˜µ
X = np.random.multivariate_normal(mean, cov, n_samples)

# åº”ç”¨ PCA
X_pca, components, var_ratio = pca_from_scratch(X, n_components=2)

# å¯è§†åŒ–
fig, axes = plt.subplots(1, 2, figsize=(14, 6))

# åŸå§‹æ•°æ® + ä¸»æˆåˆ†æ–¹å‘
axes[0].scatter(X[:, 0], X[:, 1], alpha=0.5)
origin = X.mean(axis=0)

# ç”»ä¸»æˆåˆ†æ–¹å‘
for i, (comp, var) in enumerate(zip(components.T, var_ratio)):
    # æŒ‰æ–¹å·®æ¯”ä¾‹ç¼©æ”¾ç®­å¤´é•¿åº¦
    scale = 3 * np.sqrt(var)
    axes[0].arrow(origin[0], origin[1], comp[0]*scale, comp[1]*scale,
                  head_width=0.2, color=['red', 'blue'][i], linewidth=2,
                  label=f'PC{i+1} ({var:.1%})')

axes[0].set_xlabel('X1')
axes[0].set_ylabel('X2')
axes[0].set_title('åŸå§‹æ•°æ®ä¸ä¸»æˆåˆ†æ–¹å‘')
axes[0].legend()
axes[0].axis('equal')
axes[0].grid(True, alpha=0.3)

# æŠ•å½±åˆ°ç¬¬ä¸€ä¸»æˆåˆ†
axes[1].scatter(X_pca[:, 0], np.zeros(n_samples), alpha=0.5)
axes[1].set_xlabel('PC1')
axes[1].set_title(f'æŠ•å½±åˆ°ç¬¬ä¸€ä¸»æˆåˆ† (è§£é‡Šæ–¹å·®: {var_ratio[0]:.1%})')
axes[1].set_yticks([])
axes[1].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

print(f"ç¬¬ä¸€ä¸»æˆåˆ†è§£é‡Šæ–¹å·®: {var_ratio[0]:.2%}")
print(f"ç¬¬äºŒä¸»æˆåˆ†è§£é‡Šæ–¹å·®: {var_ratio[1]:.2%}")
```

### 3.4 ä½¿ç”¨ sklearn å®ç°

```python
from sklearn.decomposition import PCA
from sklearn.datasets import load_iris
import matplotlib.pyplot as plt
import numpy as np

# åŠ è½½é¸¢å°¾èŠ±æ•°æ®é›†ï¼ˆ4 ç»´ï¼‰
iris = load_iris()
X = iris.data
y = iris.target

print(f"åŸå§‹æ•°æ®å½¢çŠ¶: {X.shape}")  # (150, 4)

# PCA é™ç»´åˆ° 2 ç»´
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X)

print(f"é™ç»´åå½¢çŠ¶: {X_pca.shape}")  # (150, 2)
print(f"è§£é‡Šæ–¹å·®æ¯”ä¾‹: {pca.explained_variance_ratio_}")
print(f"ç´¯è®¡è§£é‡Šæ–¹å·®: {pca.explained_variance_ratio_.sum():.2%}")

# å¯è§†åŒ–
plt.figure(figsize=(10, 8))
colors = ['red', 'green', 'blue']
labels = iris.target_names

for i, (color, label) in enumerate(zip(colors, labels)):
    mask = y == i
    plt.scatter(X_pca[mask, 0], X_pca[mask, 1],
                c=color, label=label, alpha=0.7)

plt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%})')
plt.ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%})')
plt.title('Iris æ•°æ®é›† PCA é™ç»´å¯è§†åŒ–')
plt.legend()
plt.grid(True, alpha=0.3)
plt.show()

# æŸ¥çœ‹ä¸»æˆåˆ†çš„å«ä¹‰
print("\nä¸»æˆåˆ†æƒé‡ï¼ˆå“ªäº›ç‰¹å¾æœ€é‡è¦ï¼‰:")
for i, comp in enumerate(pca.components_):
    print(f"PC{i+1}: {dict(zip(iris.feature_names, comp.round(3)))}")
```

### 3.5 é€‰æ‹©ä¸»æˆåˆ†æ•°é‡

```python
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt
import numpy as np

# åˆ›å»ºé«˜ç»´æ•°æ®
np.random.seed(42)
X = np.random.randn(1000, 50)  # 50 ç»´

# å®Œæ•´ PCA
pca = PCA()
pca.fit(X)

# ç´¯è®¡è§£é‡Šæ–¹å·®
cumsum = np.cumsum(pca.explained_variance_ratio_)

# å¯è§†åŒ–
fig, axes = plt.subplots(1, 2, figsize=(14, 5))

# å·¦å›¾ï¼šæ¯ä¸ªä¸»æˆåˆ†çš„æ–¹å·®è´¡çŒ®
axes[0].bar(range(1, 21), pca.explained_variance_ratio_[:20])
axes[0].set_xlabel('ä¸»æˆåˆ†')
axes[0].set_ylabel('è§£é‡Šæ–¹å·®æ¯”ä¾‹')
axes[0].set_title('å‰ 20 ä¸ªä¸»æˆåˆ†çš„æ–¹å·®è´¡çŒ®')

# å³å›¾ï¼šç´¯è®¡æ–¹å·®
axes[1].plot(range(1, len(cumsum)+1), cumsum, 'b-')
axes[1].axhline(y=0.9, color='r', linestyle='--', label='90%')
axes[1].axhline(y=0.95, color='orange', linestyle='--', label='95%')

# æ‰¾åˆ°è¾¾åˆ° 90% å’Œ 95% æ‰€éœ€çš„ä¸»æˆåˆ†æ•°
n_90 = np.argmax(cumsum >= 0.9) + 1
n_95 = np.argmax(cumsum >= 0.95) + 1
axes[1].axvline(x=n_90, color='r', linestyle=':', alpha=0.5)
axes[1].axvline(x=n_95, color='orange', linestyle=':', alpha=0.5)

axes[1].set_xlabel('ä¸»æˆåˆ†æ•°é‡')
axes[1].set_ylabel('ç´¯è®¡è§£é‡Šæ–¹å·®')
axes[1].set_title('ç´¯è®¡è§£é‡Šæ–¹å·®æ›²çº¿')
axes[1].legend()
axes[1].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

print(f"è¾¾åˆ° 90% è§£é‡Šæ–¹å·®éœ€è¦ {n_90} ä¸ªä¸»æˆåˆ†")
print(f"è¾¾åˆ° 95% è§£é‡Šæ–¹å·®éœ€è¦ {n_95} ä¸ªä¸»æˆåˆ†")
```

### 3.6 PCA vs SVD çš„å…³ç³»

```
PCA å’Œ SVD å¯†åˆ‡ç›¸å…³ï¼š

è®¾ X æ˜¯ä¸­å¿ƒåŒ–çš„æ•°æ®çŸ©é˜µï¼ˆnÃ—dï¼‰
åæ–¹å·®çŸ©é˜µ: C = (1/n)XâŠ¤X

å¯¹ X åš SVD: X = UÎ£VâŠ¤
åˆ™: XâŠ¤X = VÎ£Â²VâŠ¤

å› æ­¤:
- V çš„åˆ—å°±æ˜¯ PCA çš„ä¸»æˆåˆ†
- Î£Â² / n å°±æ˜¯ PCA çš„ç‰¹å¾å€¼ï¼ˆæ–¹å·®ï¼‰
- XV = UÎ£ å°±æ˜¯é™ç»´åçš„æ•°æ®
```

```python
import numpy as np
from sklearn.preprocessing import StandardScaler

# åˆ›å»ºæ•°æ®
np.random.seed(42)
X = np.random.randn(100, 5)

# ä¸­å¿ƒåŒ–
X_centered = X - X.mean(axis=0)

# æ–¹æ³• 1ï¼šé€šè¿‡åæ–¹å·®çŸ©é˜µçš„ç‰¹å¾åˆ†è§£
cov = np.cov(X_centered.T)
eigenvalues, eigenvectors = np.linalg.eigh(cov)
idx = np.argsort(eigenvalues)[::-1]
pca_components = eigenvectors[:, idx]

# æ–¹æ³• 2ï¼šé€šè¿‡ SVD
U, s, Vt = np.linalg.svd(X_centered, full_matrices=False)
svd_components = Vt.T

# æ¯”è¾ƒï¼ˆå¯èƒ½ç¬¦å·ç›¸åï¼Œä½†æ–¹å‘ç›¸åŒï¼‰
print("PCA ä¸»æˆåˆ†:\n", pca_components[:, 0])
print("SVD å³å¥‡å¼‚å‘é‡:\n", svd_components[:, 0])
print("ç¬¦å·è°ƒæ•´åæ˜¯å¦ç›¸åŒ:", np.allclose(np.abs(pca_components), np.abs(svd_components)))
```

---

## 4. çŸ©é˜µåˆ†è§£çš„ AI åº”ç”¨

### 4.1 ååŒè¿‡æ»¤æ¨è

```python
import numpy as np

# ç”¨æˆ·-ç‰©å“è¯„åˆ†çŸ©é˜µï¼ˆ5 ä¸ªç”¨æˆ·ï¼Œ4 ä¸ªç‰©å“ï¼‰
# 0 è¡¨ç¤ºæœªè¯„åˆ†
R = np.array([
    [5, 3, 0, 1],
    [4, 0, 0, 1],
    [1, 1, 0, 5],
    [1, 0, 0, 4],
    [0, 1, 5, 4],
])

print("åŸå§‹è¯„åˆ†çŸ©é˜µ:")
print(R)

# ä½¿ç”¨ SVD åˆ†è§£æ¥é¢„æµ‹ç¼ºå¤±è¯„åˆ†
# å…ˆç”¨å‡å€¼å¡«å……ç¼ºå¤±å€¼
R_filled = R.copy().astype(float)
R_filled[R_filled == 0] = np.nan
col_means = np.nanmean(R_filled, axis=0)
for i in range(R.shape[1]):
    R_filled[np.isnan(R_filled[:, i]), i] = col_means[i]

# SVD åˆ†è§£
U, s, Vt = np.linalg.svd(R_filled, full_matrices=False)

# ä½¿ç”¨å‰ 2 ä¸ªå¥‡å¼‚å€¼é‡æ„ï¼ˆä½ç§©è¿‘ä¼¼ï¼‰
k = 2
R_approx = U[:, :k] @ np.diag(s[:k]) @ Vt[:k, :]

print("\né¢„æµ‹è¯„åˆ†çŸ©é˜µï¼ˆä½ç§©è¿‘ä¼¼ï¼‰:")
print(R_approx.round(1))

# ä¸ºç”¨æˆ· 0 æ¨èï¼ˆç‰©å“ 2 çš„é¢„æµ‹è¯„åˆ†ï¼‰
user_id = 0
item_id = 2
print(f"\nç”¨æˆ· {user_id} å¯¹ç‰©å“ {item_id} çš„é¢„æµ‹è¯„åˆ†: {R_approx[user_id, item_id]:.2f}")
```

### 4.2 æ–‡æœ¬ä¸»é¢˜æ¨¡å‹ï¼ˆLSAï¼‰

```python
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.decomposition import TruncatedSVD

# ç¤ºä¾‹æ–‡æ¡£
documents = [
    "æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªåˆ†æ”¯",
    "æ·±åº¦å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„å­é›†",
    "ç¥ç»ç½‘ç»œæ˜¯æ·±åº¦å­¦ä¹ çš„åŸºç¡€",
    "Python æ˜¯æ•°æ®ç§‘å­¦çš„é¦–é€‰è¯­è¨€",
    "NumPy å’Œ Pandas æ˜¯ Python æ•°æ®åˆ†æå·¥å…·",
    "TensorFlow å’Œ PyTorch æ˜¯æ·±åº¦å­¦ä¹ æ¡†æ¶",
]

# TF-IDF å‘é‡åŒ–
vectorizer = TfidfVectorizer()
X = vectorizer.fit_transform(documents)
print(f"æ–‡æ¡£-è¯çŸ©é˜µå½¢çŠ¶: {X.shape}")  # (6, è¯æ±‡é‡)

# LSAï¼ˆä½¿ç”¨ SVD é™ç»´ï¼‰
n_topics = 2
svd = TruncatedSVD(n_components=n_topics)
X_topics = svd.fit_transform(X)

print(f"\né™ç»´åå½¢çŠ¶: {X_topics.shape}")

# æ¯ä¸ªä¸»é¢˜çš„å…³é”®è¯
feature_names = vectorizer.get_feature_names_out()
for i, topic in enumerate(svd.components_):
    top_words_idx = topic.argsort()[-5:][::-1]
    top_words = [feature_names[j] for j in top_words_idx]
    print(f"ä¸»é¢˜ {i+1}: {', '.join(top_words)}")

# æ–‡æ¡£åœ¨ä¸»é¢˜ç©ºé—´çš„ä½ç½®
import matplotlib.pyplot as plt

plt.figure(figsize=(10, 8))
plt.scatter(X_topics[:, 0], X_topics[:, 1])
for i, doc in enumerate(documents):
    plt.annotate(f"Doc{i}", (X_topics[i, 0], X_topics[i, 1]))
plt.xlabel('ä¸»é¢˜ 1')
plt.ylabel('ä¸»é¢˜ 2')
plt.title('æ–‡æ¡£åœ¨ä¸»é¢˜ç©ºé—´çš„åˆ†å¸ƒ')
plt.grid(True, alpha=0.3)
plt.show()
```

### 4.3 å›¾åƒé™å™ª

```python
import numpy as np
import matplotlib.pyplot as plt

# åˆ›å»ºä¸€ä¸ªç®€å•çš„å›¾åƒ
np.random.seed(42)
true_image = np.zeros((50, 50))
true_image[10:40, 10:40] = 1  # ç™½è‰²æ–¹å—

# æ·»åŠ å™ªå£°
noise_level = 0.5
noisy_image = true_image + noise_level * np.random.randn(*true_image.shape)

# ä½¿ç”¨ SVD é™å™ª
U, s, Vt = np.linalg.svd(noisy_image, full_matrices=False)

# ä¸åŒçš„ k å€¼è¿›è¡Œé‡æ„
fig, axes = plt.subplots(2, 3, figsize=(15, 10))

axes[0, 0].imshow(true_image, cmap='gray')
axes[0, 0].set_title('åŸå§‹å›¾åƒ')
axes[0, 0].axis('off')

axes[0, 1].imshow(noisy_image, cmap='gray')
axes[0, 1].set_title(f'å¸¦å™ªå£°å›¾åƒ\nMSE={np.mean((noisy_image-true_image)**2):.4f}')
axes[0, 1].axis('off')

# ç”¨ä¸åŒçš„ k é‡æ„
for idx, k in enumerate([1, 2, 5, 10]):
    denoised = U[:, :k] @ np.diag(s[:k]) @ Vt[:k, :]
    mse = np.mean((denoised - true_image) ** 2)

    row, col = (idx + 2) // 3, (idx + 2) % 3
    axes[row, col].imshow(denoised, cmap='gray')
    axes[row, col].set_title(f'k={k}\nMSE={mse:.4f}')
    axes[row, col].axis('off')

plt.tight_layout()
plt.show()
```

---

## 5. å®æˆ˜ç»ƒä¹ 

### ç»ƒä¹  1ï¼šSVD å›¾åƒå‹ç¼©

ç”¨ SVD å‹ç¼©ä¸€å¼ çœŸå®å›¾ç‰‡ï¼Œåˆ†æä¸åŒå‹ç¼©æ¯”ä¸‹çš„æ•ˆæœã€‚

```python
import numpy as np
import matplotlib.pyplot as plt
from PIL import Image
import requests
from io import BytesIO

# ä¸‹è½½æˆ–åˆ›å»ºæµ‹è¯•å›¾åƒ
def get_test_image():
    """è·å–æµ‹è¯•å›¾åƒ"""
    try:
        # å°è¯•ä¸‹è½½ä¸€å¼ å›¾ç‰‡
        url = "https://upload.wikimedia.org/wikipedia/commons/thumb/a/a7/Camponotus_flavomarginatus_ant.jpg/320px-Camponotus_flavomarginatus_ant.jpg"
        response = requests.get(url, timeout=5)
        img = Image.open(BytesIO(response.content)).convert('L')
        return np.array(img)
    except:
        # åˆ›å»ºä¸€ä¸ªæµ‹è¯•å›¾åƒ
        x = np.linspace(0, 4*np.pi, 200)
        y = np.linspace(0, 4*np.pi, 200)
        X, Y = np.meshgrid(x, y)
        return (np.sin(X) * np.cos(Y) * 127 + 128).astype(np.uint8)

# ä½ çš„ä»£ç ï¼šå®ç° SVD å‹ç¼©å‡½æ•°
def svd_compress(image, k):
    """
    ä½¿ç”¨ SVD å‹ç¼©å›¾åƒ

    å‚æ•°:
        image: ç°åº¦å›¾åƒ (numpy array)
        k: ä¿ç•™çš„å¥‡å¼‚å€¼æ•°é‡

    è¿”å›:
        compressed: å‹ç¼©åçš„å›¾åƒ
        compression_ratio: å‹ç¼©æ¯”
    """
    # TODO: å®ç° SVD åˆ†è§£å’Œé‡æ„
    pass

# æµ‹è¯•ä½ çš„å®ç°
# img = get_test_image()
# for k in [5, 20, 50]:
#     compressed, ratio = svd_compress(img, k)
#     print(f"k={k}, å‹ç¼©æ¯”={ratio:.1f}x")
```

<details>
<summary>å‚è€ƒç­”æ¡ˆ</summary>

```python
def svd_compress(image, k):
    U, s, Vt = np.linalg.svd(image, full_matrices=False)

    # é‡æ„
    compressed = U[:, :k] @ np.diag(s[:k]) @ Vt[:k, :]

    # è®¡ç®—å‹ç¼©æ¯”
    original_size = image.shape[0] * image.shape[1]
    compressed_size = k * (image.shape[0] + image.shape[1] + 1)
    compression_ratio = original_size / compressed_size

    return compressed, compression_ratio
```

</details>

### ç»ƒä¹  2ï¼šPCA æ‰‹å†™æ•°å­—å¯è§†åŒ–

ä½¿ç”¨ PCA å°†æ‰‹å†™æ•°å­—æ•°æ®é™åˆ° 2D å¹¶å¯è§†åŒ–ã€‚

```python
from sklearn.datasets import load_digits
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt
import numpy as np

# åŠ è½½æ‰‹å†™æ•°å­—æ•°æ®
digits = load_digits()
X = digits.data  # (1797, 64) æ¯ä¸ªå›¾åƒæ˜¯ 8x8 = 64 ç»´
y = digits.target

print(f"æ•°æ®å½¢çŠ¶: {X.shape}")
print(f"ç±»åˆ«: {np.unique(y)}")

# TODO:
# 1. ç”¨ PCA é™åˆ° 2 ç»´
# 2. ç”¨æ•£ç‚¹å›¾å¯è§†åŒ–ï¼Œä¸åŒæ•°å­—ç”¨ä¸åŒé¢œè‰²
# 3. æ‰“å°è§£é‡Šæ–¹å·®æ¯”ä¾‹
```

<details>
<summary>å‚è€ƒç­”æ¡ˆ</summary>

```python
# PCA é™ç»´
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X)

print(f"è§£é‡Šæ–¹å·®: {pca.explained_variance_ratio_.sum():.2%}")

# å¯è§†åŒ–
plt.figure(figsize=(12, 10))
scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap='tab10', alpha=0.6)
plt.colorbar(scatter, label='æ•°å­—')
plt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%})')
plt.ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%})')
plt.title('æ‰‹å†™æ•°å­— PCA å¯è§†åŒ–')
plt.grid(True, alpha=0.3)
plt.show()
```

</details>

### ç»ƒä¹  3ï¼šåˆ†æç‰¹å¾å€¼è¡°å‡

åˆ†æä¸€ä¸ªæ•°æ®é›†çš„ç‰¹å¾å€¼è¡°å‡æƒ…å†µï¼Œç¡®å®šæœ€ä½³çš„é™ç»´ç»´æ•°ã€‚

```python
from sklearn.datasets import fetch_california_housing
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
import numpy as np

# åŠ è½½åŠ å·æˆ¿ä»·æ•°æ®
housing = fetch_california_housing()
X = housing.data

# æ ‡å‡†åŒ–
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

print(f"åŸå§‹ç‰¹å¾æ•°: {X.shape[1]}")
print(f"ç‰¹å¾å: {housing.feature_names}")

# TODO:
# 1. è®¡ç®—æ‰€æœ‰ä¸»æˆåˆ†
# 2. ç”»å‡ºç´¯è®¡è§£é‡Šæ–¹å·®æ›²çº¿
# 3. ç¡®å®šä¿ç•™ 95% æ–¹å·®éœ€è¦å¤šå°‘ç»´
```

<details>
<summary>å‚è€ƒç­”æ¡ˆ</summary>

```python
# å®Œæ•´ PCA
pca = PCA()
pca.fit(X_scaled)

# ç´¯è®¡æ–¹å·®
cumsum = np.cumsum(pca.explained_variance_ratio_)

# å¯è§†åŒ–
plt.figure(figsize=(10, 5))
plt.plot(range(1, len(cumsum)+1), cumsum, 'b-o')
plt.axhline(y=0.95, color='r', linestyle='--', label='95%')
plt.xlabel('ä¸»æˆåˆ†æ•°é‡')
plt.ylabel('ç´¯è®¡è§£é‡Šæ–¹å·®')
plt.title('PCA ç´¯è®¡è§£é‡Šæ–¹å·®')
plt.legend()
plt.grid(True, alpha=0.3)
plt.show()

# æ‰¾åˆ° 95% é˜ˆå€¼
n_95 = np.argmax(cumsum >= 0.95) + 1
print(f"ä¿ç•™ 95% æ–¹å·®éœ€è¦ {n_95} ä¸ªä¸»æˆåˆ†")
print(f"ä» {X.shape[1]} ç»´é™åˆ° {n_95} ç»´")
```

</details>

---

## ğŸ“š æ€»ç»“

### å…³é”®æ¦‚å¿µå¯¹ç…§

| æ¦‚å¿µ | ç›´è§‰ | Python |
|------|------|--------|
| **ç‰¹å¾å€¼/å‘é‡** | å˜æ¢ä¸­æ–¹å‘ä¸å˜çš„å‘é‡ | `np.linalg.eig()` |
| **SVD** | æ—‹è½¬ Ã— ç¼©æ”¾ Ã— æ—‹è½¬ | `np.linalg.svd()` |
| **PCA** | æ‰¾æœ€é‡è¦çš„æ–¹å‘ | `sklearn.decomposition.PCA` |

### åº”ç”¨åœºæ™¯

| æŠ€æœ¯ | åº”ç”¨ |
|------|------|
| **SVD** | å›¾åƒå‹ç¼©ã€æ¨èç³»ç»Ÿã€é™å™ª |
| **PCA** | é™ç»´ã€æ•°æ®å¯è§†åŒ–ã€ç‰¹å¾æå– |
| **ç‰¹å¾åˆ†è§£** | PageRankã€å›¾åˆ†æã€é¢‘è°±åˆ†æ |

---

## ğŸ“– æ¨èèµ„æº

- [3Blue1Brown ç‰¹å¾å€¼](https://www.bilibili.com/video/BV1ys411472E?p=14)
- [SVD å¯è§†åŒ–è®²è§£](https://www.youtube.com/watch?v=vSczTbgc8Rc)
- [StatQuest PCA](https://www.youtube.com/watch?v=FgakZw6K1QQ)

---

## â¡ï¸ ä¸‹ä¸€æ­¥

å­¦å®Œæœ¬èŠ‚åï¼Œç»§ç»­å­¦ä¹  [15-Pythonè°ƒè¯•æŠ€å·§.md](./15-Pythonè°ƒè¯•æŠ€å·§.md)

