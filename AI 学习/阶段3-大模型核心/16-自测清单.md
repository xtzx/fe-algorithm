# âœ… 16 - é˜¶æ®µ3 è‡ªæµ‹æ¸…å•

> æ£€éªŒ LLM æ ¸å¿ƒæŠ€æœ¯çš„ç†è§£ç¨‹åº¦

---

## æ¦‚å¿µç†è§£ï¼ˆ10 é¢˜ï¼‰

### 1. Transformer æ¶æ„

```
Q1: Self-Attention çš„è®¡ç®—å¤æ‚åº¦æ˜¯å¤šå°‘ï¼Ÿä¸ºä»€ä¹ˆï¼Ÿ

ç­”æ¡ˆï¼šO(nÂ²d)ï¼Œå…¶ä¸­ n æ˜¯åºåˆ—é•¿åº¦ï¼Œd æ˜¯ç»´åº¦
åŸå› ï¼šæ¯ä¸ª token éœ€è¦ä¸æ‰€æœ‰å…¶ä»– token è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°
```

```
Q2: ä¸ºä»€ä¹ˆ Attention éœ€è¦é™¤ä»¥ âˆšd_kï¼Ÿ

ç­”æ¡ˆï¼šé˜²æ­¢ç‚¹ç§¯å€¼è¿‡å¤§å¯¼è‡´ softmax æ¢¯åº¦æ¶ˆå¤±
å½“ d_k è¾ƒå¤§æ—¶ï¼Œç‚¹ç§¯çš„æ–¹å·®ä¼šéšä¹‹å¢å¤§
é™¤ä»¥ âˆšd_k å¯ä»¥ç¨³å®šåˆ†å¸ƒ
```

### 2. ä½ç½®ç¼–ç 

```
Q3: RoPE ç›¸æ¯”ç»å¯¹ä½ç½®ç¼–ç çš„ä¼˜åŠ¿æ˜¯ä»€ä¹ˆï¼Ÿ

ç­”æ¡ˆï¼š
1. ç›¸å¯¹ä½ç½®ä¿¡æ¯ï¼šæ³¨æ„åŠ›åˆ†æ•°åªä¾èµ–ç›¸å¯¹ä½ç½®
2. é•¿åº¦å¤–æ¨ï¼šç†è®ºä¸Šå¯ä»¥å¤„ç†æ›´é•¿çš„åºåˆ—
3. å®ç°ç®€å•ï¼šé€šè¿‡æ—‹è½¬çŸ©é˜µå®ç°
```

```
Q4: ALiBi çš„æ ¸å¿ƒæ€æƒ³æ˜¯ä»€ä¹ˆï¼Ÿ

ç­”æ¡ˆï¼š
ä¸ä½¿ç”¨ä½ç½® embeddingï¼Œè€Œæ˜¯åœ¨æ³¨æ„åŠ›åˆ†æ•°ä¸Š
æ·»åŠ ä¸€ä¸ªä¸è·ç¦»æˆæ­£æ¯”çš„æƒ©ç½šé¡¹ï¼š
attn[i,j] = attn[i,j] - m * |i-j|
m æ˜¯æ¯ä¸ªå¤´ä¸åŒçš„æ–œç‡å‚æ•°
```

### 3. æ¨¡å‹æ¶æ„

```
Q5: Encoder-onlyã€Decoder-onlyã€Encoder-Decoder
å„è‡ªé€‚åˆä»€ä¹ˆä»»åŠ¡ï¼Ÿ

ç­”æ¡ˆï¼š
- Encoder-only (BERT): ç†è§£ä»»åŠ¡ï¼ˆåˆ†ç±»ã€NERã€é—®ç­”ï¼‰
- Decoder-only (GPT): ç”Ÿæˆä»»åŠ¡ï¼ˆæ–‡æœ¬ç”Ÿæˆã€å¯¹è¯ï¼‰
- Encoder-Decoder (T5): åºåˆ—åˆ°åºåˆ—ï¼ˆç¿»è¯‘ã€æ‘˜è¦ï¼‰
```

```
Q6: ä¸ºä»€ä¹ˆç°åœ¨å¤§æ¨¡å‹éƒ½æ˜¯ Decoder-onlyï¼Ÿ

ç­”æ¡ˆï¼š
1. æ›´é€‚åˆç»Ÿä¸€çš„ç”ŸæˆèŒƒå¼ï¼ˆä»»ä½•ä»»åŠ¡éƒ½å¯ä»¥è½¬ä¸ºç”Ÿæˆï¼‰
2. KV Cache å®ç°ç®€å•ï¼Œæ¨ç†é«˜æ•ˆ
3. è‡ªå›å½’è®­ç»ƒæ›´ç¨³å®šï¼Œæ•°æ®åˆ©ç”¨æ•ˆç‡é«˜
4. æ›´å¥½çš„ scaling ç‰¹æ€§
```

### 4. LLM ä¼˜åŒ–

```
Q7: KV Cache å¦‚ä½•åŠ é€Ÿæ¨ç†ï¼Ÿ

ç­”æ¡ˆï¼š
è‡ªå›å½’ç”Ÿæˆæ—¶ï¼Œæ–° token åªéœ€è¦è®¡ç®—è‡ªå·±çš„ Q
è€Œ Kã€V å¯ä»¥å¤ç”¨ä¹‹å‰ç¼“å­˜çš„å€¼
é¿å…é‡å¤è®¡ç®—ï¼Œæ¨ç†å¤æ‚åº¦ä» O(nÂ²) é™åˆ° O(n)
```

```
Q8: Flash Attention ä¸ºä»€ä¹ˆå¿«ï¼Ÿ

ç­”æ¡ˆï¼š
1. åˆ†å—è®¡ç®—ï¼Œå‡å°‘ HBM è®¿é—®
2. ä¸å­˜å‚¨å®Œæ•´çš„ attention matrix
3. åˆ©ç”¨ GPU SRAM çš„é«˜å¸¦å®½
4. èåˆè®¡ç®—æ ¸ï¼Œå‡å°‘ kernel å¯åŠ¨å¼€é”€
```

### 5. Tokenization

```
Q9: BPE ç®—æ³•çš„åŸºæœ¬æ€æƒ³ï¼Ÿ

ç­”æ¡ˆï¼š
1. ä»å­—ç¬¦çº§å¼€å§‹
2. ç»Ÿè®¡ç›¸é‚» token å¯¹çš„é¢‘ç‡
3. åˆå¹¶æœ€é¢‘ç¹çš„ token å¯¹
4. é‡å¤ç›´åˆ°è¾¾åˆ°è¯è¡¨å¤§å°
```

```
Q10: ä¸ºä»€ä¹ˆéœ€è¦ç‰¹æ®Š tokenï¼Ÿ

ç­”æ¡ˆï¼š
- [PAD]: å¡«å……åˆ°ç›¸åŒé•¿åº¦
- [UNK]: æœªçŸ¥è¯
- [CLS]: å¥å­çº§è¡¨ç¤º
- [SEP]: åˆ†éš”ä¸åŒå¥å­
- [BOS]/[EOS]: åºåˆ—å¼€å§‹/ç»“æŸ
- [MASK]: MLM è®­ç»ƒ
```

---

## ç¼–ç¨‹ä»»åŠ¡ï¼ˆ3 é¢˜ï¼‰

### ä»»åŠ¡1ï¼šå®ç° Scaled Dot-Product Attention

```python
import torch
import torch.nn.functional as F
import math

def scaled_dot_product_attention(Q, K, V, mask=None):
    """
    å®ç°ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›

    Args:
        Q: (batch, heads, seq_len, d_k)
        K: (batch, heads, seq_len, d_k)
        V: (batch, heads, seq_len, d_v)
        mask: (batch, 1, 1, seq_len) æˆ– (batch, 1, seq_len, seq_len)

    Returns:
        output: (batch, heads, seq_len, d_v)
        attention_weights: (batch, heads, seq_len, seq_len)
    """
    # TODO: å®ç°è¿™ä¸ªå‡½æ•°
    pass

# æµ‹è¯•
B, H, T, D = 2, 4, 8, 64
Q = torch.randn(B, H, T, D)
K = torch.randn(B, H, T, D)
V = torch.randn(B, H, T, D)

output, weights = scaled_dot_product_attention(Q, K, V)
print(f"è¾“å‡ºå½¢çŠ¶: {output.shape}")  # (2, 4, 8, 64)
print(f"æƒé‡å½¢çŠ¶: {weights.shape}")  # (2, 4, 8, 8)
```

**å‚è€ƒç­”æ¡ˆï¼š**

```python
def scaled_dot_product_attention(Q, K, V, mask=None):
    d_k = Q.size(-1)

    # è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°: (B, H, T, T)
    scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(d_k)

    # åº”ç”¨ mask
    if mask is not None:
        scores = scores.masked_fill(mask == 0, float('-inf'))

    # Softmax
    attention_weights = F.softmax(scores, dim=-1)

    # åŠ æƒæ±‚å’Œ
    output = torch.matmul(attention_weights, V)

    return output, attention_weights
```

---

### ä»»åŠ¡2ï¼šå®ç° RoPE ä½ç½®ç¼–ç 

```python
import torch

def apply_rotary_embedding(x, seq_len, dim):
    """
    åº”ç”¨æ—‹è½¬ä½ç½®ç¼–ç 

    Args:
        x: (batch, heads, seq_len, head_dim)
        seq_len: åºåˆ—é•¿åº¦
        dim: head_dim

    Returns:
        x: åº”ç”¨ RoPE åçš„å¼ é‡
    """
    # TODO: å®ç°è¿™ä¸ªå‡½æ•°
    pass

# æµ‹è¯•
B, H, T, D = 2, 4, 8, 64
x = torch.randn(B, H, T, D)
x_rope = apply_rotary_embedding(x, T, D)
print(f"è¾“å‡ºå½¢çŠ¶: {x_rope.shape}")  # (2, 4, 8, 64)
```

**å‚è€ƒç­”æ¡ˆï¼š**

```python
def apply_rotary_embedding(x, seq_len, dim):
    # ç”Ÿæˆä½ç½®ç¼–ç 
    inv_freq = 1.0 / (10000 ** (torch.arange(0, dim, 2).float() / dim))
    position = torch.arange(seq_len).float()
    sincos = torch.einsum('i,j->ij', position, inv_freq)  # (T, D/2)
    sin = sincos.sin()
    cos = sincos.cos()

    # æ‰©å±•ç»´åº¦ä»¥åŒ¹é… x
    sin = sin[None, None, :, :].expand(x.size(0), x.size(1), -1, -1)  # (B, H, T, D/2)
    cos = cos[None, None, :, :].expand(x.size(0), x.size(1), -1, -1)

    # åˆ†æˆä¸¤åŠ
    x1 = x[..., ::2]   # å¶æ•°ä½ç½®
    x2 = x[..., 1::2]  # å¥‡æ•°ä½ç½®

    # æ—‹è½¬
    x_rotated = torch.stack([
        x1 * cos - x2 * sin,
        x1 * sin + x2 * cos
    ], dim=-1).flatten(-2)

    return x_rotated
```

---

### ä»»åŠ¡3ï¼šä½¿ç”¨ Hugging Face è¿›è¡Œæ–‡æœ¬ç”Ÿæˆ

```python
from transformers import AutoModelForCausalLM, AutoTokenizer

def generate_text(prompt, model_name="gpt2", max_length=100,
                  temperature=0.7, top_k=50, top_p=0.9):
    """
    ä½¿ç”¨ Hugging Face æ¨¡å‹ç”Ÿæˆæ–‡æœ¬

    Args:
        prompt: è¾“å…¥æç¤º
        model_name: æ¨¡å‹åç§°
        max_length: æœ€å¤§ç”Ÿæˆé•¿åº¦
        temperature: é‡‡æ ·æ¸©åº¦
        top_k: Top-K é‡‡æ ·
        top_p: Top-P (nucleus) é‡‡æ ·

    Returns:
        ç”Ÿæˆçš„æ–‡æœ¬
    """
    # TODO: å®ç°è¿™ä¸ªå‡½æ•°
    pass

# æµ‹è¯•
prompts = [
    "The future of artificial intelligence is",
    "Once upon a time in a land far away",
    "Python is a programming language that"
]

for prompt in prompts:
    result = generate_text(prompt)
    print(f"Prompt: {prompt}")
    print(f"Generated: {result}")
    print("-" * 50)
```

**å‚è€ƒç­”æ¡ˆï¼š**

```python
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

def generate_text(prompt, model_name="gpt2", max_length=100,
                  temperature=0.7, top_k=50, top_p=0.9):
    # åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModelForCausalLM.from_pretrained(model_name)

    # è®¾ç½® pad token
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token

    # ç¼–ç è¾“å…¥
    inputs = tokenizer(prompt, return_tensors="pt")

    # ç”Ÿæˆ
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_length=max_length,
            temperature=temperature,
            top_k=top_k,
            top_p=top_p,
            do_sample=True,
            pad_token_id=tokenizer.pad_token_id
        )

    # è§£ç è¾“å‡º
    generated = tokenizer.decode(outputs[0], skip_special_tokens=True)

    return generated
```

---

## è¿›é˜¶æ€è€ƒé¢˜

### 1. æ¶æ„è®¾è®¡

```
Q: å¦‚æœè®©ä½ è®¾è®¡ä¸€ä¸ªå¤„ç† 100K ä¸Šä¸‹æ–‡é•¿åº¦çš„ LLMï¼Œ
ä½ ä¼šè€ƒè™‘å“ªäº›æŠ€æœ¯ï¼Ÿ

å‚è€ƒæ€è·¯ï¼š
1. ä½ç½®ç¼–ç ï¼šRoPE æˆ– ALiBiï¼ˆæ”¯æŒé•¿åº¦å¤–æ¨ï¼‰
2. æ³¨æ„åŠ›ä¼˜åŒ–ï¼šFlash Attentionï¼ˆå†…å­˜é«˜æ•ˆï¼‰
3. ç¨€ç–æ³¨æ„åŠ›ï¼šæ»‘åŠ¨çª—å£ + å…¨å±€ token
4. å†…å­˜ç®¡ç†ï¼šGradient Checkpointingã€PagedAttention
5. åˆ†å¸ƒå¼ï¼šSequence Parallel
```

### 2. æ•ˆç‡ä¼˜åŒ–

```
Q: æ¨ç†æ—¶å¦‚ä½•åœ¨ä¸é™ä½å¤ªå¤šè´¨é‡çš„å‰æä¸‹åŠ é€Ÿï¼Ÿ

å‚è€ƒæ€è·¯ï¼š
1. KV Cache
2. Flash Attention
3. é‡åŒ–ï¼ˆINT8/INT4ï¼‰
4. Speculative Decoding
5. Continuous Batching
6. æ¨¡å‹å‰ªæ/è’¸é¦
```

### 3. å¤šæ¨¡æ€

```
Q: å¦‚ä½•å°† LLM æ‰©å±•ä¸ºå¤šæ¨¡æ€æ¨¡å‹ï¼Ÿ

å‚è€ƒæ€è·¯ï¼š
1. è§†è§‰ç¼–ç å™¨ï¼ˆCLIP ViTï¼‰æå–å›¾åƒç‰¹å¾
2. æŠ•å½±å±‚å¯¹é½æ¨¡æ€
3. å›¾åƒ token ä¸æ–‡æœ¬ token æ‹¼æ¥è¾“å…¥ LLM
4. ä¸¤é˜¶æ®µè®­ç»ƒï¼šé¢„è®­ç»ƒå¯¹é½ + æŒ‡ä»¤å¾®è°ƒ
```

---

## è¾¾æ ‡æ ‡å‡†

```
â–¡ èƒ½å¤Ÿæ¸…æ™°è§£é‡Š Transformer å„ç»„ä»¶çš„ä½œç”¨
â–¡ ç†è§£ Attention çš„è®¡ç®—è¿‡ç¨‹å’Œå¤æ‚åº¦
â–¡ èƒ½è¯´å‡ºè‡³å°‘ 3 ç§ä½ç½®ç¼–ç æ–¹æ¡ˆåŠå…¶ç‰¹ç‚¹
â–¡ ç†è§£ Encoder-only/Decoder-only/Encoder-Decoder åŒºåˆ«
â–¡ èƒ½è§£é‡Š KV Cacheã€Flash Attention çš„åŸç†
â–¡ äº†è§£ MoE çš„åŸºæœ¬æ¦‚å¿µ
â–¡ èƒ½ä½¿ç”¨ Hugging Face è¿›è¡Œæ¨¡å‹æ¨ç†
â–¡ ç†è§£ Embedding å’Œå‘é‡æ£€ç´¢çš„åŸºæœ¬æµç¨‹
â–¡ å®Œæˆ 3 ä¸ªç¼–ç¨‹ä»»åŠ¡
â–¡ å¯¹å¤šæ¨¡æ€æœ‰åŸºæœ¬è®¤çŸ¥
```

---

## ğŸ‰ æ­å–œå®Œæˆé˜¶æ®µ3ï¼

ä½ å·²ç»æŒæ¡äº† LLM çš„æ ¸å¿ƒæŠ€æœ¯ï¼Œæ¥ä¸‹æ¥è¿›å…¥ï¼š

**é˜¶æ®µ4ï¼šLLM åº”ç”¨å¼€å‘ï¼ˆLLM Engineeringï¼‰**
- Prompt Engineering
- RAG ç³»ç»Ÿ
- AI Agent
- æ¨¡å‹å¾®è°ƒ

ç»§ç»­åŠ æ²¹ï¼ğŸš€

