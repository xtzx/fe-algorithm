# ğŸ”¨ é¡¹ç›®ï¼šæ‰‹å†™æç®€ GPT

> ä»é›¶å®ç°ä¸€ä¸ªç®€åŒ–ç‰ˆçš„ GPT æ¨¡å‹ï¼Œç†è§£ Transformer æ ¸å¿ƒåŸç†

---

## é¡¹ç›®ç›®æ ‡

```
å®ç°ä¸€ä¸ªç®€åŒ–ç‰ˆ GPTï¼š
- æ‰‹å†™å®Œæ•´çš„ Transformer Decoder
- åœ¨å°æ•°æ®é›†ä¸Šè®­ç»ƒå­—ç¬¦çº§è¯­è¨€æ¨¡å‹
- èƒ½å¤Ÿç”Ÿæˆè¿è´¯çš„æ–‡æœ¬

æŠ€æœ¯è¦ç‚¹ï¼š
- Multi-Head Self-Attention
- Causal Mask
- Positional Encoding
- Layer Normalization
- è‡ªå›å½’ç”Ÿæˆ
```

---

## å®Œæ•´å®ç°

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
import math
from typing import Optional

# ========== é…ç½® ==========
class GPTConfig:
    """æ¨¡å‹é…ç½®"""
    vocab_size: int = 256       # å­—ç¬¦çº§è¯è¡¨å¤§å°
    block_size: int = 128       # ä¸Šä¸‹æ–‡é•¿åº¦
    n_embd: int = 256           # embedding ç»´åº¦
    n_head: int = 8             # æ³¨æ„åŠ›å¤´æ•°
    n_layer: int = 6            # Transformer å±‚æ•°
    dropout: float = 0.1

    def __init__(self, **kwargs):
        for k, v in kwargs.items():
            setattr(self, k, v)


# ========== ç»„ä»¶å®ç° ==========

class CausalSelfAttention(nn.Module):
    """å› æœè‡ªæ³¨æ„åŠ›ï¼ˆå¸¦ mask çš„ Multi-Head Attentionï¼‰"""

    def __init__(self, config: GPTConfig):
        super().__init__()
        assert config.n_embd % config.n_head == 0

        self.n_head = config.n_head
        self.head_dim = config.n_embd // config.n_head

        # Q, K, V æŠ•å½±ï¼ˆä¸€æ¬¡æ€§è®¡ç®—ï¼‰
        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd)
        # è¾“å‡ºæŠ•å½±
        self.c_proj = nn.Linear(config.n_embd, config.n_embd)

        self.attn_dropout = nn.Dropout(config.dropout)
        self.resid_dropout = nn.Dropout(config.dropout)

        # å› æœ maskï¼ˆä¸‹ä¸‰è§’çŸ©é˜µï¼‰
        self.register_buffer(
            "mask",
            torch.tril(torch.ones(config.block_size, config.block_size))
                 .view(1, 1, config.block_size, config.block_size)
        )

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        B, T, C = x.size()  # batch, sequence length, embedding dim

        # è®¡ç®— Q, K, V
        qkv = self.c_attn(x)
        q, k, v = qkv.split(C, dim=2)

        # é‡å¡‘ä¸ºå¤šå¤´å½¢å¼: (B, T, n_head, head_dim) -> (B, n_head, T, head_dim)
        q = q.view(B, T, self.n_head, self.head_dim).transpose(1, 2)
        k = k.view(B, T, self.n_head, self.head_dim).transpose(1, 2)
        v = v.view(B, T, self.n_head, self.head_dim).transpose(1, 2)

        # è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°
        # (B, n_head, T, head_dim) @ (B, n_head, head_dim, T) -> (B, n_head, T, T)
        attn = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(self.head_dim))

        # åº”ç”¨å› æœ mask
        attn = attn.masked_fill(self.mask[:, :, :T, :T] == 0, float('-inf'))

        # Softmax + Dropout
        attn = F.softmax(attn, dim=-1)
        attn = self.attn_dropout(attn)

        # åŠ æƒæ±‚å’Œ
        # (B, n_head, T, T) @ (B, n_head, T, head_dim) -> (B, n_head, T, head_dim)
        y = attn @ v

        # åˆå¹¶å¤šå¤´: (B, n_head, T, head_dim) -> (B, T, n_head * head_dim)
        y = y.transpose(1, 2).contiguous().view(B, T, C)

        # è¾“å‡ºæŠ•å½±
        y = self.resid_dropout(self.c_proj(y))

        return y


class MLP(nn.Module):
    """å‰é¦ˆç½‘ç»œ"""

    def __init__(self, config: GPTConfig):
        super().__init__()
        self.c_fc = nn.Linear(config.n_embd, 4 * config.n_embd)
        self.gelu = nn.GELU()
        self.c_proj = nn.Linear(4 * config.n_embd, config.n_embd)
        self.dropout = nn.Dropout(config.dropout)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        x = self.c_fc(x)
        x = self.gelu(x)
        x = self.c_proj(x)
        x = self.dropout(x)
        return x


class Block(nn.Module):
    """Transformer Block = Attention + MLP (Pre-LN ç»“æ„)"""

    def __init__(self, config: GPTConfig):
        super().__init__()
        self.ln_1 = nn.LayerNorm(config.n_embd)
        self.attn = CausalSelfAttention(config)
        self.ln_2 = nn.LayerNorm(config.n_embd)
        self.mlp = MLP(config)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        # Pre-LN: LN -> Attn -> Residual -> LN -> MLP -> Residual
        x = x + self.attn(self.ln_1(x))
        x = x + self.mlp(self.ln_2(x))
        return x


class MiniGPT(nn.Module):
    """æç®€ GPT æ¨¡å‹"""

    def __init__(self, config: GPTConfig):
        super().__init__()
        self.config = config

        # Token Embedding
        self.wte = nn.Embedding(config.vocab_size, config.n_embd)
        # Position Embedding
        self.wpe = nn.Embedding(config.block_size, config.n_embd)

        self.drop = nn.Dropout(config.dropout)

        # Transformer Blocks
        self.blocks = nn.ModuleList([Block(config) for _ in range(config.n_layer)])

        # Final LayerNorm
        self.ln_f = nn.LayerNorm(config.n_embd)

        # Language Model Head
        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)

        # æƒé‡ç»‘å®šï¼ˆEmbedding å’Œ LM Head å…±äº«æƒé‡ï¼‰
        self.wte.weight = self.lm_head.weight

        # åˆå§‹åŒ–æƒé‡
        self.apply(self._init_weights)

        # ç»Ÿè®¡å‚æ•°é‡
        n_params = sum(p.numel() for p in self.parameters())
        print(f"æ¨¡å‹å‚æ•°é‡: {n_params/1e6:.2f}M")

    def _init_weights(self, module):
        if isinstance(module, nn.Linear):
            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)
            if module.bias is not None:
                torch.nn.init.zeros_(module.bias)
        elif isinstance(module, nn.Embedding):
            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)

    def forward(self, idx: torch.Tensor, targets: Optional[torch.Tensor] = None):
        """
        Args:
            idx: (B, T) token indices
            targets: (B, T) target indices for loss computation
        Returns:
            logits: (B, T, vocab_size)
            loss: scalar loss (if targets provided)
        """
        B, T = idx.size()
        assert T <= self.config.block_size, f"åºåˆ—é•¿åº¦ {T} è¶…è¿‡æœ€å¤§ {self.config.block_size}"

        # Token + Position Embedding
        pos = torch.arange(0, T, dtype=torch.long, device=idx.device)  # (T,)
        tok_emb = self.wte(idx)      # (B, T, n_embd)
        pos_emb = self.wpe(pos)      # (T, n_embd)
        x = self.drop(tok_emb + pos_emb)

        # Transformer Blocks
        for block in self.blocks:
            x = block(x)

        x = self.ln_f(x)
        logits = self.lm_head(x)  # (B, T, vocab_size)

        # è®¡ç®—æŸå¤±
        loss = None
        if targets is not None:
            loss = F.cross_entropy(
                logits.view(-1, logits.size(-1)),
                targets.view(-1),
                ignore_index=-1
            )

        return logits, loss

    @torch.no_grad()
    def generate(self, idx: torch.Tensor, max_new_tokens: int,
                 temperature: float = 1.0, top_k: Optional[int] = None):
        """
        è‡ªå›å½’ç”Ÿæˆ

        Args:
            idx: (B, T) èµ·å§‹ token
            max_new_tokens: ç”Ÿæˆçš„æœ€å¤§ token æ•°
            temperature: é‡‡æ ·æ¸©åº¦
            top_k: Top-K é‡‡æ ·
        """
        for _ in range(max_new_tokens):
            # æˆªæ–­åˆ° block_size
            idx_cond = idx if idx.size(1) <= self.config.block_size else idx[:, -self.config.block_size:]

            # å‰å‘ä¼ æ’­
            logits, _ = self(idx_cond)
            logits = logits[:, -1, :] / temperature  # (B, vocab_size)

            # Top-K é‡‡æ ·
            if top_k is not None:
                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))
                logits[logits < v[:, [-1]]] = float('-inf')

            # é‡‡æ ·
            probs = F.softmax(logits, dim=-1)
            idx_next = torch.multinomial(probs, num_samples=1)  # (B, 1)

            # æ‹¼æ¥
            idx = torch.cat((idx, idx_next), dim=1)

        return idx


# ========== æ•°æ®å‡†å¤‡ ==========
class CharDataset(torch.utils.data.Dataset):
    """å­—ç¬¦çº§æ•°æ®é›†"""

    def __init__(self, text: str, block_size: int):
        # å­—ç¬¦åˆ°ç´¢å¼•çš„æ˜ å°„
        chars = sorted(list(set(text)))
        self.stoi = {ch: i for i, ch in enumerate(chars)}
        self.itos = {i: ch for i, ch in enumerate(chars)}
        self.vocab_size = len(chars)

        # ç¼–ç æ–‡æœ¬
        self.data = [self.stoi[ch] for ch in text]
        self.block_size = block_size

    def __len__(self):
        return len(self.data) - self.block_size

    def __getitem__(self, idx):
        chunk = self.data[idx:idx + self.block_size + 1]
        x = torch.tensor(chunk[:-1], dtype=torch.long)
        y = torch.tensor(chunk[1:], dtype=torch.long)
        return x, y

    def encode(self, text: str) -> list:
        return [self.stoi[ch] for ch in text]

    def decode(self, indices: list) -> str:
        return ''.join([self.itos[i] for i in indices])


# ========== è®­ç»ƒ ==========
def train(model, dataset, config, num_epochs=10, batch_size=32, lr=3e-4):
    """è®­ç»ƒå‡½æ•°"""
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model = model.to(device)

    dataloader = torch.utils.data.DataLoader(
        dataset, batch_size=batch_size, shuffle=True
    )

    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)

    model.train()
    for epoch in range(num_epochs):
        total_loss = 0
        for batch_idx, (x, y) in enumerate(dataloader):
            x, y = x.to(device), y.to(device)

            logits, loss = model(x, y)

            optimizer.zero_grad()
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
            optimizer.step()

            total_loss += loss.item()

            if batch_idx % 100 == 0:
                print(f"Epoch {epoch+1}, Batch {batch_idx}, Loss: {loss.item():.4f}")

        avg_loss = total_loss / len(dataloader)
        print(f"Epoch {epoch+1} å®Œæˆ, å¹³å‡ Loss: {avg_loss:.4f}")

        # ç”Ÿæˆç¤ºä¾‹
        generate_sample(model, dataset, device)

    return model


def generate_sample(model, dataset, device, prompt="The ", max_tokens=200):
    """ç”Ÿæˆç¤ºä¾‹æ–‡æœ¬"""
    model.eval()

    # ç¼–ç  prompt
    idx = torch.tensor([dataset.encode(prompt)], dtype=torch.long, device=device)

    # ç”Ÿæˆ
    output = model.generate(idx, max_new_tokens=max_tokens, temperature=0.8, top_k=40)

    # è§£ç 
    generated = dataset.decode(output[0].tolist())
    print(f"\nç”Ÿæˆæ–‡æœ¬:\n{generated}\n{'='*50}")

    model.train()


# ========== ä¸»ç¨‹åº ==========
if __name__ == "__main__":
    # åŠ è½½æ–‡æœ¬æ•°æ®ï¼ˆä½¿ç”¨èå£«æ¯”äºšæ•°æ®é›†ï¼‰
    import urllib.request

    # ä¸‹è½½æ•°æ®
    url = "https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
    try:
        with open('shakespeare.txt', 'r') as f:
            text = f.read()
    except FileNotFoundError:
        print("ä¸‹è½½æ•°æ®...")
        urllib.request.urlretrieve(url, 'shakespeare.txt')
        with open('shakespeare.txt', 'r') as f:
            text = f.read()

    print(f"æ–‡æœ¬é•¿åº¦: {len(text)}")
    print(f"å‰ 500 å­—ç¬¦:\n{text[:500]}")

    # åˆ›å»ºæ•°æ®é›†
    block_size = 128
    dataset = CharDataset(text, block_size)
    print(f"è¯è¡¨å¤§å°: {dataset.vocab_size}")
    print(f"è®­ç»ƒæ ·æœ¬æ•°: {len(dataset)}")

    # åˆ›å»ºæ¨¡å‹
    config = GPTConfig(
        vocab_size=dataset.vocab_size,
        block_size=block_size,
        n_embd=256,
        n_head=8,
        n_layer=6,
        dropout=0.1
    )

    model = MiniGPT(config)

    # è®­ç»ƒ
    model = train(
        model, dataset, config,
        num_epochs=5,
        batch_size=64,
        lr=3e-4
    )

    # ä¿å­˜æ¨¡å‹
    torch.save({
        'model_state_dict': model.state_dict(),
        'config': config.__dict__,
        'vocab': dataset.stoi
    }, 'mini_gpt.pth')

    print("è®­ç»ƒå®Œæˆï¼æ¨¡å‹å·²ä¿å­˜")
```

---

## æ ¸å¿ƒç»„ä»¶è¯¦è§£

### 1. Causal Self-Attention

```python
"""
å› æœæ³¨æ„åŠ›çš„å…³é”®ï¼šä¸‹ä¸‰è§’ mask

å‡è®¾åºåˆ—é•¿åº¦ T=4:
mask = [
    [1, 0, 0, 0],   # token 0 åªèƒ½çœ‹è‡ªå·±
    [1, 1, 0, 0],   # token 1 å¯ä»¥çœ‹ 0, 1
    [1, 1, 1, 0],   # token 2 å¯ä»¥çœ‹ 0, 1, 2
    [1, 1, 1, 1],   # token 3 å¯ä»¥çœ‹æ‰€æœ‰
]

attn_scores = Q @ K^T  # (T, T)
attn_scores = attn_scores.masked_fill(mask == 0, -inf)
attn_probs = softmax(attn_scores)  # -inf å˜æˆ 0
"""

# å¯è§†åŒ–æ³¨æ„åŠ›
def visualize_attention(model, text, dataset, device):
    """å¯è§†åŒ–æ³¨æ„åŠ›æƒé‡"""
    import matplotlib.pyplot as plt

    idx = torch.tensor([dataset.encode(text)], device=device)

    # è·å–æ³¨æ„åŠ›æƒé‡ï¼ˆéœ€è¦ä¿®æ”¹ forward è¿”å›ï¼‰
    # è¿™é‡Œç®€åŒ–å¤„ç†

    with torch.no_grad():
        B, T, C = idx.size()[0], idx.size()[1], model.config.n_embd
        # ... æå–æ³¨æ„åŠ›æƒé‡ ...
```

### 2. Pre-LN vs Post-LN

```python
# Post-LN (åŸå§‹ Transformer)
x = x + attn(x)
x = ln(x)
x = x + mlp(x)
x = ln(x)

# Pre-LN (GPT-2/3 ä½¿ç”¨ï¼Œæ›´ç¨³å®š)
x = x + attn(ln(x))
x = x + mlp(ln(x))
```

### 3. ç”Ÿæˆç­–ç•¥

```python
# Greedyï¼ˆè´ªå©ªï¼‰
next_token = logits.argmax(dim=-1)

# Top-K
top_k_logits, top_k_indices = logits.topk(k)
probs = softmax(top_k_logits)
next_token = top_k_indices[multinomial(probs, 1)]

# Top-P (Nucleus)
sorted_logits, sorted_indices = logits.sort(descending=True)
cumsum_probs = softmax(sorted_logits).cumsum(dim=-1)
mask = cumsum_probs > p
sorted_logits[mask] = -inf
probs = softmax(sorted_logits)
next_token = sorted_indices[multinomial(probs, 1)]

# Temperature
logits = logits / temperature  # T > 1 æ›´éšæœºï¼ŒT < 1 æ›´ç¡®å®š
```

---

## æ‰©å±•é˜…è¯»

```
å‚è€ƒé¡¹ç›®ï¼š
1. nanoGPT: https://github.com/karpathy/nanoGPT
2. minGPT: https://github.com/karpathy/minGPT
3. llm.c: https://github.com/karpathy/llm.c

è§†é¢‘æ•™ç¨‹ï¼š
- "Let's build GPT" by Andrej Karpathy
  https://www.youtube.com/watch?v=kCc8FmEb1nY
```

---

## ç»ƒä¹ 

1. **æ·»åŠ  RoPE ä½ç½®ç¼–ç **ï¼šæ›¿æ¢å›ºå®šä½ç½®ç¼–ç 
2. **å®ç° GQA**ï¼šå‡å°‘ KV å¤´æ•°é‡
3. **æ·»åŠ  KV Cache**ï¼šåŠ é€Ÿæ¨ç†
4. **å®ç° Flash Attention**ï¼šä¼˜åŒ–å†…å­˜

---

## â¡ï¸ ä¸‹ä¸€æ­¥

ç»§ç»­ [13-è‡ªæµ‹æ¸…å•.md](./13-è‡ªæµ‹æ¸…å•.md)

