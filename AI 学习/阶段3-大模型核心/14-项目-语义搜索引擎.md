# ğŸ” 14 - é¡¹ç›®ï¼šè¯­ä¹‰æœç´¢å¼•æ“

> æ„å»ºä¸€ä¸ªåŸºäº Embedding çš„è¯­ä¹‰æœç´¢å¼•æ“

---

## é¡¹ç›®æ¦‚è¿°

### ç›®æ ‡

```
æ„å»ºä¸€ä¸ªç®€å•çš„è¯­ä¹‰æœç´¢å¼•æ“ï¼š
1. æ”¯æŒæ·»åŠ æ–‡æ¡£
2. æ ¹æ®è‡ªç„¶è¯­è¨€æŸ¥è¯¢æ£€ç´¢ç›¸å…³æ–‡æ¡£
3. è¿”å›æ’åºåçš„ç»“æœ

æŠ€æœ¯æ ˆï¼š
- Sentence Transformersï¼ˆEmbeddingï¼‰
- Faissï¼ˆå‘é‡æ£€ç´¢ï¼‰
- FastAPIï¼ˆWeb æœåŠ¡ï¼‰
```

---

## å®Œæ•´å®ç°

### æ ¸å¿ƒæœç´¢å¼•æ“

```python
import numpy as np
import faiss
from sentence_transformers import SentenceTransformer
from typing import List, Dict, Optional
import json
import os

class SemanticSearchEngine:
    """è¯­ä¹‰æœç´¢å¼•æ“"""

    def __init__(self, model_name: str = 'all-MiniLM-L6-v2'):
        """
        åˆå§‹åŒ–æœç´¢å¼•æ“

        Args:
            model_name: Sentence Transformer æ¨¡å‹åç§°
        """
        print(f"åŠ è½½æ¨¡å‹: {model_name}")
        self.model = SentenceTransformer(model_name)
        self.dimension = self.model.get_sentence_embedding_dimension()

        # åˆå§‹åŒ– Faiss ç´¢å¼•ï¼ˆä½¿ç”¨å†…ç§¯ï¼Œé…åˆå½’ä¸€åŒ–å‘é‡ç­‰ä»·äºä½™å¼¦ç›¸ä¼¼åº¦ï¼‰
        self.index = faiss.IndexFlatIP(self.dimension)

        # å­˜å‚¨æ–‡æ¡£å…ƒæ•°æ®
        self.documents: List[Dict] = []
        self.doc_id_counter = 0

        print(f"åˆå§‹åŒ–å®Œæˆï¼Œå‘é‡ç»´åº¦: {self.dimension}")

    def add_documents(self, texts: List[str], metadatas: Optional[List[Dict]] = None) -> List[int]:
        """
        æ·»åŠ æ–‡æ¡£åˆ°ç´¢å¼•

        Args:
            texts: æ–‡æ¡£æ–‡æœ¬åˆ—è¡¨
            metadatas: æ–‡æ¡£å…ƒæ•°æ®åˆ—è¡¨

        Returns:
            æ·»åŠ çš„æ–‡æ¡£ ID åˆ—è¡¨
        """
        if metadatas is None:
            metadatas = [{} for _ in texts]

        if len(texts) != len(metadatas):
            raise ValueError("texts å’Œ metadatas é•¿åº¦å¿…é¡»ç›¸åŒ")

        # ç”Ÿæˆ embedding
        embeddings = self.model.encode(texts, show_progress_bar=len(texts) > 100)
        embeddings = embeddings.astype('float32')

        # å½’ä¸€åŒ–
        faiss.normalize_L2(embeddings)

        # æ·»åŠ åˆ°ç´¢å¼•
        self.index.add(embeddings)

        # å­˜å‚¨æ–‡æ¡£ä¿¡æ¯
        doc_ids = []
        for text, metadata in zip(texts, metadatas):
            doc_id = self.doc_id_counter
            self.documents.append({
                'id': doc_id,
                'text': text,
                'metadata': metadata
            })
            doc_ids.append(doc_id)
            self.doc_id_counter += 1

        print(f"æ·»åŠ äº† {len(texts)} ä¸ªæ–‡æ¡£ï¼Œæ€»è®¡ {len(self.documents)} ä¸ª")
        return doc_ids

    def search(self, query: str, top_k: int = 5, threshold: float = 0.0) -> List[Dict]:
        """
        æœç´¢ç›¸å…³æ–‡æ¡£

        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            top_k: è¿”å›ç»“æœæ•°é‡
            threshold: ç›¸ä¼¼åº¦é˜ˆå€¼

        Returns:
            æœç´¢ç»“æœåˆ—è¡¨
        """
        if len(self.documents) == 0:
            return []

        # ç”ŸæˆæŸ¥è¯¢å‘é‡
        query_embedding = self.model.encode([query]).astype('float32')
        faiss.normalize_L2(query_embedding)

        # æœç´¢
        k = min(top_k, len(self.documents))
        scores, indices = self.index.search(query_embedding, k)

        # æ„å»ºç»“æœ
        results = []
        for score, idx in zip(scores[0], indices[0]):
            if idx < 0 or idx >= len(self.documents):
                continue
            if score < threshold:
                continue

            doc = self.documents[idx]
            results.append({
                'id': doc['id'],
                'text': doc['text'],
                'metadata': doc['metadata'],
                'score': float(score)
            })

        return results

    def delete_document(self, doc_id: int) -> bool:
        """åˆ é™¤æ–‡æ¡£ï¼ˆé‡å»ºç´¢å¼•ï¼‰"""
        # æ‰¾åˆ°è¦åˆ é™¤çš„æ–‡æ¡£
        idx_to_remove = None
        for i, doc in enumerate(self.documents):
            if doc['id'] == doc_id:
                idx_to_remove = i
                break

        if idx_to_remove is None:
            return False

        # åˆ é™¤æ–‡æ¡£
        self.documents.pop(idx_to_remove)

        # é‡å»ºç´¢å¼•
        self._rebuild_index()

        return True

    def _rebuild_index(self):
        """é‡å»ºç´¢å¼•"""
        self.index = faiss.IndexFlatIP(self.dimension)

        if len(self.documents) > 0:
            texts = [doc['text'] for doc in self.documents]
            embeddings = self.model.encode(texts).astype('float32')
            faiss.normalize_L2(embeddings)
            self.index.add(embeddings)

    def save(self, path: str):
        """ä¿å­˜ç´¢å¼•å’Œæ–‡æ¡£"""
        os.makedirs(path, exist_ok=True)

        # ä¿å­˜ Faiss ç´¢å¼•
        faiss.write_index(self.index, os.path.join(path, 'index.faiss'))

        # ä¿å­˜æ–‡æ¡£
        with open(os.path.join(path, 'documents.json'), 'w', encoding='utf-8') as f:
            json.dump({
                'documents': self.documents,
                'doc_id_counter': self.doc_id_counter
            }, f, ensure_ascii=False, indent=2)

        print(f"ä¿å­˜åˆ° {path}")

    def load(self, path: str):
        """åŠ è½½ç´¢å¼•å’Œæ–‡æ¡£"""
        # åŠ è½½ Faiss ç´¢å¼•
        self.index = faiss.read_index(os.path.join(path, 'index.faiss'))

        # åŠ è½½æ–‡æ¡£
        with open(os.path.join(path, 'documents.json'), 'r', encoding='utf-8') as f:
            data = json.load(f)
            self.documents = data['documents']
            self.doc_id_counter = data['doc_id_counter']

        print(f"ä» {path} åŠ è½½äº† {len(self.documents)} ä¸ªæ–‡æ¡£")

    def get_stats(self) -> Dict:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        return {
            'total_documents': len(self.documents),
            'index_size': self.index.ntotal,
            'dimension': self.dimension
        }


# ========== æµ‹è¯• ==========
if __name__ == "__main__":
    # åˆ›å»ºå¼•æ“
    engine = SemanticSearchEngine()

    # æ·»åŠ æ–‡æ¡£
    documents = [
        "Python is a popular programming language for machine learning and data science.",
        "TensorFlow is an open-source machine learning framework developed by Google.",
        "PyTorch is a deep learning library developed by Facebook's AI Research lab.",
        "Natural language processing (NLP) is a field of AI focused on text and speech.",
        "Computer vision enables computers to interpret and understand visual information.",
        "Reinforcement learning is about training agents through trial and error.",
        "Neural networks are inspired by the structure of the human brain.",
        "Transfer learning allows models to reuse knowledge from pre-trained models.",
        "Transformers have revolutionized NLP with their attention mechanism.",
        "GPT and BERT are popular transformer-based language models."
    ]

    metadatas = [{"category": "programming"}, {"category": "framework"}] + [{}] * 8

    engine.add_documents(documents, metadatas)

    # æœç´¢æµ‹è¯•
    print("\n" + "="*60)
    queries = [
        "What programming language is good for AI?",
        "Which deep learning framework should I use?",
        "How do transformers work?"
    ]

    for query in queries:
        print(f"\næŸ¥è¯¢: {query}")
        print("-" * 40)
        results = engine.search(query, top_k=3)
        for i, r in enumerate(results, 1):
            print(f"{i}. [Score: {r['score']:.4f}] {r['text'][:60]}...")

    # ä¿å­˜å’ŒåŠ è½½æµ‹è¯•
    engine.save("./search_index")

    new_engine = SemanticSearchEngine()
    new_engine.load("./search_index")
    print(f"\nåŠ è½½åç»Ÿè®¡: {new_engine.get_stats()}")
```

### FastAPI Web æœåŠ¡

```python
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from typing import List, Optional, Dict
import uvicorn

# åˆ›å»ºåº”ç”¨
app = FastAPI(title="è¯­ä¹‰æœç´¢å¼•æ“ API")

# å…¨å±€æœç´¢å¼•æ“å®ä¾‹
engine = SemanticSearchEngine()

# ========== æ•°æ®æ¨¡å‹ ==========
class Document(BaseModel):
    text: str
    metadata: Optional[Dict] = None

class AddDocumentsRequest(BaseModel):
    documents: List[Document]

class SearchRequest(BaseModel):
    query: str
    top_k: int = 5
    threshold: float = 0.0

class SearchResult(BaseModel):
    id: int
    text: str
    metadata: Dict
    score: float

# ========== API ç«¯ç‚¹ ==========
@app.get("/")
def root():
    return {"message": "è¯­ä¹‰æœç´¢å¼•æ“ API", "stats": engine.get_stats()}

@app.post("/documents")
def add_documents(request: AddDocumentsRequest):
    """æ·»åŠ æ–‡æ¡£"""
    texts = [doc.text for doc in request.documents]
    metadatas = [doc.metadata or {} for doc in request.documents]

    doc_ids = engine.add_documents(texts, metadatas)

    return {
        "message": f"æˆåŠŸæ·»åŠ  {len(doc_ids)} ä¸ªæ–‡æ¡£",
        "doc_ids": doc_ids
    }

@app.post("/search", response_model=List[SearchResult])
def search(request: SearchRequest):
    """æœç´¢æ–‡æ¡£"""
    results = engine.search(
        query=request.query,
        top_k=request.top_k,
        threshold=request.threshold
    )
    return results

@app.delete("/documents/{doc_id}")
def delete_document(doc_id: int):
    """åˆ é™¤æ–‡æ¡£"""
    success = engine.delete_document(doc_id)
    if not success:
        raise HTTPException(status_code=404, detail="æ–‡æ¡£ä¸å­˜åœ¨")
    return {"message": f"æˆåŠŸåˆ é™¤æ–‡æ¡£ {doc_id}"}

@app.get("/stats")
def get_stats():
    """è·å–ç»Ÿè®¡ä¿¡æ¯"""
    return engine.get_stats()

@app.post("/save")
def save_index(path: str = "./search_index"):
    """ä¿å­˜ç´¢å¼•"""
    engine.save(path)
    return {"message": f"ç´¢å¼•å·²ä¿å­˜åˆ° {path}"}

@app.post("/load")
def load_index(path: str = "./search_index"):
    """åŠ è½½ç´¢å¼•"""
    try:
        engine.load(path)
        return {"message": f"ç´¢å¼•å·²ä» {path} åŠ è½½", "stats": engine.get_stats()}
    except Exception as e:
        raise HTTPException(status_code=400, detail=str(e))

# ========== å¯åŠ¨æœåŠ¡ ==========
if __name__ == "__main__":
    # æ·»åŠ ä¸€äº›ç¤ºä¾‹æ•°æ®
    sample_docs = [
        "Python is great for machine learning",
        "JavaScript is used for web development",
        "Docker helps with containerization",
    ]
    engine.add_documents(sample_docs)

    # å¯åŠ¨æœåŠ¡
    uvicorn.run(app, host="0.0.0.0", port=8000)
```

### å®¢æˆ·ç«¯ä½¿ç”¨

```python
import requests

BASE_URL = "http://localhost:8000"

# æ·»åŠ æ–‡æ¡£
response = requests.post(f"{BASE_URL}/documents", json={
    "documents": [
        {"text": "Python is a programming language", "metadata": {"lang": "en"}},
        {"text": "æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„å­é¢†åŸŸ", "metadata": {"lang": "zh"}}
    ]
})
print(response.json())

# æœç´¢
response = requests.post(f"{BASE_URL}/search", json={
    "query": "What is machine learning?",
    "top_k": 3
})
print(response.json())

# è·å–ç»Ÿè®¡
response = requests.get(f"{BASE_URL}/stats")
print(response.json())
```

---

## è¿›é˜¶åŠŸèƒ½

### æ··åˆæœç´¢

```python
from rank_bm25 import BM25Okapi

class HybridSearchEngine(SemanticSearchEngine):
    """æ··åˆæœç´¢ï¼šBM25 + è¯­ä¹‰"""

    def __init__(self, model_name='all-MiniLM-L6-v2', alpha=0.5):
        super().__init__(model_name)
        self.alpha = alpha  # è¯­ä¹‰æœç´¢æƒé‡
        self.bm25 = None
        self.tokenized_docs = []

    def add_documents(self, texts, metadatas=None):
        doc_ids = super().add_documents(texts, metadatas)

        # æ›´æ–° BM25 ç´¢å¼•
        self.tokenized_docs.extend([text.lower().split() for text in texts])
        self.bm25 = BM25Okapi(self.tokenized_docs)

        return doc_ids

    def search(self, query, top_k=5, threshold=0.0):
        if len(self.documents) == 0:
            return []

        # BM25 åˆ†æ•°
        bm25_scores = self.bm25.get_scores(query.lower().split())
        bm25_scores = (bm25_scores - bm25_scores.min()) / (bm25_scores.max() - bm25_scores.min() + 1e-9)

        # è¯­ä¹‰åˆ†æ•°
        query_emb = self.model.encode([query]).astype('float32')
        faiss.normalize_L2(query_emb)
        semantic_scores, _ = self.index.search(query_emb, len(self.documents))
        semantic_scores = (semantic_scores[0] + 1) / 2  # å½’ä¸€åŒ–åˆ° [0, 1]

        # æ··åˆåˆ†æ•°
        hybrid_scores = self.alpha * semantic_scores + (1 - self.alpha) * bm25_scores

        # æ’åº
        indices = np.argsort(hybrid_scores)[::-1][:top_k]

        results = []
        for idx in indices:
            if hybrid_scores[idx] < threshold:
                continue
            doc = self.documents[idx]
            results.append({
                'id': doc['id'],
                'text': doc['text'],
                'metadata': doc['metadata'],
                'score': float(hybrid_scores[idx]),
                'bm25_score': float(bm25_scores[idx]),
                'semantic_score': float(semantic_scores[idx])
            })

        return results
```

### Reranker

```python
from sentence_transformers import CrossEncoder

class SearchEngineWithReranker(SemanticSearchEngine):
    """å¸¦ Reranker çš„æœç´¢å¼•æ“"""

    def __init__(self, bi_encoder='all-MiniLM-L6-v2',
                 cross_encoder='cross-encoder/ms-marco-MiniLM-L-6-v2'):
        super().__init__(bi_encoder)
        self.reranker = CrossEncoder(cross_encoder)

    def search(self, query, top_k=5, rerank_top_k=20, threshold=0.0):
        # ç¬¬ä¸€é˜¶æ®µï¼šBi-Encoder å¬å›
        candidates = super().search(query, top_k=rerank_top_k, threshold=0)

        if len(candidates) == 0:
            return []

        # ç¬¬äºŒé˜¶æ®µï¼šCross-Encoder é‡æ’
        pairs = [[query, c['text']] for c in candidates]
        rerank_scores = self.reranker.predict(pairs)

        # æ›´æ–°åˆ†æ•°
        for i, score in enumerate(rerank_scores):
            candidates[i]['rerank_score'] = float(score)

        # æŒ‰é‡æ’åˆ†æ•°æ’åº
        candidates.sort(key=lambda x: x['rerank_score'], reverse=True)

        # è¿‡æ»¤å’Œæˆªæ–­
        results = [c for c in candidates if c['rerank_score'] >= threshold][:top_k]

        return results
```

---

## éƒ¨ç½²å»ºè®®

```bash
# requirements.txt
sentence-transformers>=2.2.0
faiss-cpu>=1.7.0
fastapi>=0.100.0
uvicorn>=0.23.0
pydantic>=2.0.0
rank-bm25>=0.2.2

# Dockerfile
FROM python:3.11-slim
WORKDIR /app
COPY requirements.txt .
RUN pip install -r requirements.txt
COPY . .
CMD ["uvicorn", "api:app", "--host", "0.0.0.0", "--port", "8000"]

# è¿è¡Œ
docker build -t semantic-search .
docker run -p 8000:8000 semantic-search
```

---

## â¡ï¸ ä¸‹ä¸€æ­¥

ç»§ç»­ [15-é¡¹ç›®-æ‰‹å†™æç®€GPT.md](./15-é¡¹ç›®-æ‰‹å†™æç®€GPT.md)

