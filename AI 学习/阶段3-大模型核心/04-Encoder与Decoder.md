# ğŸ”„ 04 - Encoder ä¸ Decoder

> ç†è§£ BERTã€GPTã€T5 ä¸‰ç§æ¶æ„çš„åŒºåˆ«å’Œé€‚ç”¨åœºæ™¯

---

## ç›®å½•

1. [ä¸‰ç§æ¶æ„æ¦‚è¿°](#1-ä¸‰ç§æ¶æ„æ¦‚è¿°)
2. [Encoder-only (BERT)](#2-encoder-only-bert)
3. [Decoder-only (GPT)](#3-decoder-only-gpt)
4. [Encoder-Decoder (T5)](#4-encoder-decoder-t5)
5. [æ¶æ„é€‰æ‹©æŒ‡å—](#5-æ¶æ„é€‰æ‹©æŒ‡å—)
6. [ç»ƒä¹ é¢˜](#6-ç»ƒä¹ é¢˜)

---

## 1. ä¸‰ç§æ¶æ„æ¦‚è¿°

### 1.1 æ¶æ„å¯¹æ¯”

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Transformer æ¶æ„å˜ä½“                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                     â”‚
â”‚  Encoder-only (BERT)     Decoder-only (GPT)    Encoder-Decoder (T5) â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚               â”‚       â”‚               â”‚     â”‚     â”‚         â”‚   â”‚
â”‚  â”‚   Encoder     â”‚       â”‚   Decoder     â”‚     â”‚ Enc â”‚   Dec   â”‚   â”‚
â”‚  â”‚   (åŒå‘)      â”‚       â”‚   (å•å‘)      â”‚     â”‚     â”‚         â”‚   â”‚
â”‚  â”‚               â”‚       â”‚               â”‚     â””â”€â”€â”¬â”€â”€â”´â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚       â”‚        â”‚
â”‚                                                   â””â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â”‚  æ³¨æ„åŠ›ï¼šå…¨å±€åŒå‘         æ³¨æ„åŠ›ï¼šå› æœï¼ˆåªçœ‹å‰é¢ï¼‰  EncoderåŒå‘+Decoderå•å‘â”‚
â”‚                                                                     â”‚
â”‚  ä»£è¡¨ï¼šBERT, RoBERTa     ä»£è¡¨ï¼šGPT, LLaMA        ä»£è¡¨ï¼šT5, BART      â”‚
â”‚        DeBERTa                 Qwen, Mistral          mT5           â”‚
â”‚                                                                     â”‚
â”‚  ç”¨é€”ï¼šç†è§£ä»»åŠ¡           ç”¨é€”ï¼šç”Ÿæˆä»»åŠ¡          ç”¨é€”ï¼šåºåˆ—åˆ°åºåˆ—    â”‚
â”‚       åˆ†ç±»ã€NERã€é—®ç­”           æ–‡æœ¬ç”Ÿæˆã€å¯¹è¯          ç¿»è¯‘ã€æ‘˜è¦    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 1.2 æ³¨æ„åŠ›æ¨¡å¼å¯¹æ¯”

```python
import torch
import matplotlib.pyplot as plt

def visualize_attention_patterns():
    seq_len = 6

    fig, axes = plt.subplots(1, 3, figsize=(15, 4))

    # Encoder (BERT): å…¨å±€åŒå‘æ³¨æ„åŠ›
    encoder_mask = torch.ones(seq_len, seq_len)
    axes[0].imshow(encoder_mask, cmap='Blues')
    axes[0].set_title('Encoder-only (BERT)\nåŒå‘ï¼šæ¯ä¸ªä½ç½®çœ‹æ‰€æœ‰ä½ç½®')
    axes[0].set_xlabel('Key Position')
    axes[0].set_ylabel('Query Position')

    # Decoder (GPT): å› æœæ³¨æ„åŠ›
    decoder_mask = torch.tril(torch.ones(seq_len, seq_len))
    axes[1].imshow(decoder_mask, cmap='Blues')
    axes[1].set_title('Decoder-only (GPT)\nå•å‘ï¼šåªèƒ½çœ‹å‰é¢çš„ä½ç½®')

    # Encoder-Decoder: æ··åˆ
    # ç®€åŒ–æ˜¾ç¤º
    enc_dec_mask = torch.ones(seq_len, seq_len)
    enc_dec_mask[3:, :3] = 0.5  # è¡¨ç¤º cross-attention
    axes[2].imshow(enc_dec_mask, cmap='Blues')
    axes[2].set_title('Encoder-Decoder (T5)\nEncoderåŒå‘ + Decoderå•å‘ + Cross')

    plt.tight_layout()
    plt.show()

visualize_attention_patterns()
```

---

## 2. Encoder-only (BERT)

### 2.1 BERT æ¶æ„

```
BERT = Bidirectional Encoder Representations from Transformers

è¾“å…¥ï¼š[CLS] token1 token2 ... tokenN [SEP]
     â†“
Embedding (token + position + segment)
     â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Transformer Encoder Block Ã— 12     â”‚
â”‚  (Self-Attention + FFN)             â”‚
â”‚  (åŒå‘æ³¨æ„åŠ›ï¼šæ¯ä¸ªä½ç½®çœ‹æ‰€æœ‰ä½ç½®)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â†“
è¾“å‡ºï¼šæ¯ä¸ªä½ç½®çš„ä¸Šä¸‹æ–‡è¡¨ç¤º

[CLS] ä½ç½®çš„è¾“å‡ºç”¨äºå¥å­çº§ä»»åŠ¡
å…¶ä»–ä½ç½®çš„è¾“å‡ºç”¨äº token çº§ä»»åŠ¡
```

### 2.2 é¢„è®­ç»ƒä»»åŠ¡

```python
# BERT çš„é¢„è®­ç»ƒä»»åŠ¡

# 1. Masked Language Modeling (MLM)
# éšæœºé®ç›– 15% çš„ tokenï¼Œè®©æ¨¡å‹é¢„æµ‹
"""
è¾“å…¥: The [MASK] sat on the mat.
ç›®æ ‡: cat

ä¸ºä»€ä¹ˆä¸èƒ½ç”¨ GPT çš„æ–¹å¼ï¼Ÿ
å› ä¸º BERT éœ€è¦åŒå‘ä¸Šä¸‹æ–‡ï¼Œä¸èƒ½åªçœ‹å‰é¢
"""

# 2. Next Sentence Prediction (NSP)
# é¢„æµ‹ä¸¤ä¸ªå¥å­æ˜¯å¦è¿ç»­
"""
è¾“å…¥: [CLS] Sentence A [SEP] Sentence B [SEP]
è¾“å‡º: IsNext / NotNext

åæ¥å‘ç° NSP ç”¨å¤„ä¸å¤§ï¼ŒRoBERTa å»æ‰äº†å®ƒ
"""
```

### 2.3 BERT ä»£ç ç¤ºä¾‹

```python
import torch
import torch.nn as nn

class BertEmbedding(nn.Module):
    def __init__(self, vocab_size, d_model, max_len, n_segments=2):
        super().__init__()
        self.token_embedding = nn.Embedding(vocab_size, d_model)
        self.position_embedding = nn.Embedding(max_len, d_model)
        self.segment_embedding = nn.Embedding(n_segments, d_model)
        self.norm = nn.LayerNorm(d_model)
        self.dropout = nn.Dropout(0.1)

    def forward(self, input_ids, segment_ids=None):
        seq_len = input_ids.size(1)
        positions = torch.arange(seq_len, device=input_ids.device)

        x = self.token_embedding(input_ids)
        x = x + self.position_embedding(positions)

        if segment_ids is not None:
            x = x + self.segment_embedding(segment_ids)

        return self.dropout(self.norm(x))

class BertModel(nn.Module):
    def __init__(self, vocab_size, d_model=768, num_heads=12, num_layers=12, d_ff=3072):
        super().__init__()
        self.embedding = BertEmbedding(vocab_size, d_model, max_len=512)

        encoder_layer = nn.TransformerEncoderLayer(
            d_model=d_model,
            nhead=num_heads,
            dim_feedforward=d_ff,
            dropout=0.1,
            activation='gelu',
            batch_first=True
        )
        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers)
        self.pooler = nn.Linear(d_model, d_model)

    def forward(self, input_ids, attention_mask=None, segment_ids=None):
        x = self.embedding(input_ids, segment_ids)

        # attention_mask: 1 è¡¨ç¤ºæœ‰æ•ˆï¼Œ0 è¡¨ç¤º padding
        if attention_mask is not None:
            attention_mask = (attention_mask == 0)  # PyTorch ç”¨ True è¡¨ç¤ºå¿½ç•¥

        x = self.encoder(x, src_key_padding_mask=attention_mask)

        # [CLS] token çš„è¾“å‡º
        cls_output = x[:, 0]
        pooled = torch.tanh(self.pooler(cls_output))

        return x, pooled  # sequence_output, pooled_output

# åˆ†ç±»ä»»åŠ¡
class BertForClassification(nn.Module):
    def __init__(self, bert, num_classes):
        super().__init__()
        self.bert = bert
        self.classifier = nn.Linear(768, num_classes)
        self.dropout = nn.Dropout(0.1)

    def forward(self, input_ids, attention_mask=None):
        _, pooled = self.bert(input_ids, attention_mask)
        return self.classifier(self.dropout(pooled))
```

### 2.4 BERT çš„é€‚ç”¨åœºæ™¯

```
âœ… æ–‡æœ¬åˆ†ç±»ï¼ˆæƒ…æ„Ÿåˆ†æã€ä¸»é¢˜åˆ†ç±»ï¼‰
âœ… å‘½åå®ä½“è¯†åˆ«ï¼ˆNERï¼‰
âœ… é—®ç­”ï¼ˆæŠ½å–å¼ï¼‰
âœ… å¥å­ç›¸ä¼¼åº¦
âœ… æ–‡æœ¬è•´å«

âŒ æ–‡æœ¬ç”Ÿæˆï¼ˆä¸é€‚åˆï¼Œå› ä¸ºæ˜¯åŒå‘çš„ï¼‰
âŒ å¼€æ”¾å¼å¯¹è¯
```

---

## 3. Decoder-only (GPT)

### 3.1 GPT æ¶æ„

```
GPT = Generative Pre-trained Transformer

è¾“å…¥ï¼štoken1 token2 ... tokenN
     â†“
Embedding (token + position)
     â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Transformer Decoder Block Ã— N      â”‚
â”‚  (Causal Self-Attention + FFN)      â”‚
â”‚  (å•å‘æ³¨æ„åŠ›ï¼šåªèƒ½çœ‹å‰é¢çš„ token)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â†“
è¾“å‡ºï¼šä¸‹ä¸€ä¸ª token çš„é¢„æµ‹

è‡ªå›å½’ç”Ÿæˆï¼š
é¢„æµ‹ token1 â†’ é¢„æµ‹ token2ï¼ˆåŸºäº token1ï¼‰â†’ é¢„æµ‹ token3ï¼ˆåŸºäº 1,2ï¼‰â†’ ...
```

### 3.2 é¢„è®­ç»ƒä»»åŠ¡

```python
# GPT çš„é¢„è®­ç»ƒä»»åŠ¡ï¼šNext Token Prediction

"""
è¾“å…¥: The cat sat on
ç›®æ ‡: the (é¢„æµ‹ä¸‹ä¸€ä¸ª token)

è¾“å…¥: The cat sat on the
ç›®æ ‡: mat

è¿™å°±æ˜¯ä¸ºä»€ä¹ˆå«"è¯­è¨€æ¨¡å‹"
P(sentence) = P(t1) Ã— P(t2|t1) Ã— P(t3|t1,t2) Ã— ...
"""

# æŸå¤±å‡½æ•°ï¼šCross Entropy Loss
# é¢„æµ‹æ¯ä¸ªä½ç½®çš„ä¸‹ä¸€ä¸ª token
```

### 3.3 GPT ä»£ç ç¤ºä¾‹

```python
class GPTModel(nn.Module):
    def __init__(self, vocab_size, d_model=768, num_heads=12, num_layers=12, d_ff=3072, max_len=1024):
        super().__init__()

        self.token_embedding = nn.Embedding(vocab_size, d_model)
        self.position_embedding = nn.Embedding(max_len, d_model)

        decoder_layer = nn.TransformerEncoderLayer(
            d_model=d_model,
            nhead=num_heads,
            dim_feedforward=d_ff,
            dropout=0.1,
            activation='gelu',
            batch_first=True
        )
        self.transformer = nn.TransformerEncoder(decoder_layer, num_layers)

        self.ln_f = nn.LayerNorm(d_model)
        self.lm_head = nn.Linear(d_model, vocab_size, bias=False)

        # å› æœæ©ç 
        self.register_buffer('causal_mask', None)

    def _get_causal_mask(self, seq_len, device):
        # ä¸Šä¸‰è§’ä¸º Trueï¼ˆè¢«å¿½ç•¥ï¼‰ï¼Œä¸‹ä¸‰è§’ä¸º Falseï¼ˆä¿ç•™ï¼‰
        mask = torch.triu(torch.ones(seq_len, seq_len, device=device), diagonal=1).bool()
        return mask

    def forward(self, input_ids):
        batch_size, seq_len = input_ids.shape
        device = input_ids.device

        # Embedding
        positions = torch.arange(seq_len, device=device)
        x = self.token_embedding(input_ids) + self.position_embedding(positions)

        # å› æœæ©ç 
        causal_mask = self._get_causal_mask(seq_len, device)

        # Transformer
        x = self.transformer(x, mask=causal_mask)
        x = self.ln_f(x)

        # é¢„æµ‹ä¸‹ä¸€ä¸ª token
        logits = self.lm_head(x)

        return logits

    def generate(self, input_ids, max_new_tokens=50, temperature=1.0):
        """è‡ªå›å½’ç”Ÿæˆ"""
        for _ in range(max_new_tokens):
            # å‰å‘ä¼ æ’­
            logits = self.forward(input_ids)

            # å–æœ€åä¸€ä¸ªä½ç½®çš„é¢„æµ‹
            next_token_logits = logits[:, -1, :] / temperature

            # é‡‡æ ·
            probs = torch.softmax(next_token_logits, dim=-1)
            next_token = torch.multinomial(probs, num_samples=1)

            # æ‹¼æ¥
            input_ids = torch.cat([input_ids, next_token], dim=1)

        return input_ids

# è®­ç»ƒ
def train_gpt(model, dataloader, optimizer, device):
    model.train()

    for input_ids in dataloader:
        input_ids = input_ids.to(device)

        # è¾“å…¥å’Œç›®æ ‡é”™ä½ä¸€ä¸ªä½ç½®
        x = input_ids[:, :-1]  # è¾“å…¥
        y = input_ids[:, 1:]   # ç›®æ ‡ï¼ˆä¸‹ä¸€ä¸ª tokenï¼‰

        logits = model(x)
        loss = nn.functional.cross_entropy(
            logits.view(-1, logits.size(-1)),
            y.view(-1)
        )

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
```

### 3.4 ç°ä»£ LLM æ¶æ„ï¼ˆLLaMA é£æ ¼ï¼‰

```python
class LLaMABlock(nn.Module):
    """LLaMA é£æ ¼çš„ Transformer Block"""
    def __init__(self, d_model, num_heads, d_ff, dropout=0.0):
        super().__init__()

        # Pre-LN
        self.norm1 = RMSNorm(d_model)
        self.norm2 = RMSNorm(d_model)

        # æ³¨æ„åŠ›ï¼ˆå¸¦ RoPEï¼‰
        self.attention = CausalSelfAttention(d_model, num_heads, dropout)

        # SwiGLU FFN
        self.ffn = SwiGLU(d_model, d_ff, dropout)

    def forward(self, x, freqs_cis=None):
        # æ®‹å·® + Pre-LN
        x = x + self.attention(self.norm1(x), freqs_cis)
        x = x + self.ffn(self.norm2(x))
        return x

class RMSNorm(nn.Module):
    """RMS Normalizationï¼ˆLLaMA ä½¿ç”¨ï¼Œæ¯” LayerNorm æ›´é«˜æ•ˆï¼‰"""
    def __init__(self, dim, eps=1e-6):
        super().__init__()
        self.eps = eps
        self.weight = nn.Parameter(torch.ones(dim))

    def forward(self, x):
        rms = torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
        return x * rms * self.weight
```

---

## 4. Encoder-Decoder (T5)

### 4.1 T5 æ¶æ„

```
T5 = Text-to-Text Transfer Transformer

æŠŠæ‰€æœ‰ä»»åŠ¡ç»Ÿä¸€ä¸º text-to-text æ ¼å¼ï¼š

ç¿»è¯‘ï¼š   "translate English to German: The cat sat on the mat."
         â†’ "Die Katze saÃŸ auf der Matte."

æ‘˜è¦ï¼š   "summarize: [é•¿æ–‡æœ¬]"
         â†’ "çŸ­æ‘˜è¦"

åˆ†ç±»ï¼š   "sentiment: This movie is great!"
         â†’ "positive"

é—®ç­”ï¼š   "question: What is AI? context: AI is..."
         â†’ "AI is artificial intelligence"
```

### 4.2 Encoder-Decoder ç»“æ„

```python
class EncoderDecoderModel(nn.Module):
    def __init__(self, vocab_size, d_model=512, num_heads=8,
                 num_encoder_layers=6, num_decoder_layers=6, d_ff=2048):
        super().__init__()

        # å…±äº«è¯åµŒå…¥
        self.embedding = nn.Embedding(vocab_size, d_model)

        # Encoder
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=d_model, nhead=num_heads, dim_feedforward=d_ff,
            batch_first=True
        )
        self.encoder = nn.TransformerEncoder(encoder_layer, num_encoder_layers)

        # Decoder
        decoder_layer = nn.TransformerDecoderLayer(
            d_model=d_model, nhead=num_heads, dim_feedforward=d_ff,
            batch_first=True
        )
        self.decoder = nn.TransformerDecoder(decoder_layer, num_decoder_layers)

        # è¾“å‡º
        self.lm_head = nn.Linear(d_model, vocab_size)

    def forward(self, src_ids, tgt_ids, src_mask=None, tgt_mask=None):
        # Encoder
        src_emb = self.embedding(src_ids)
        encoder_output = self.encoder(src_emb, src_key_padding_mask=src_mask)

        # Decoder
        tgt_emb = self.embedding(tgt_ids)
        # Decoder éœ€è¦å› æœæ©ç 
        tgt_causal_mask = self._generate_causal_mask(tgt_ids.size(1), tgt_ids.device)

        decoder_output = self.decoder(
            tgt_emb,
            encoder_output,
            tgt_mask=tgt_causal_mask,
            tgt_key_padding_mask=tgt_mask,
            memory_key_padding_mask=src_mask
        )

        logits = self.lm_head(decoder_output)
        return logits

    def _generate_causal_mask(self, seq_len, device):
        return torch.triu(torch.ones(seq_len, seq_len, device=device), diagonal=1).bool()
```

### 4.3 T5 çš„é€‚ç”¨åœºæ™¯

```
âœ… æœºå™¨ç¿»è¯‘
âœ… æ–‡æœ¬æ‘˜è¦
âœ… é—®ç­”ï¼ˆç”Ÿæˆå¼ï¼‰
âœ… æ–‡æœ¬æ”¹å†™
âœ… æ•°æ®å¢å¼º

ç‰¹ç‚¹ï¼š
- çµæ´»ï¼šå¯ä»¥å¤„ç†å„ç§ seq2seq ä»»åŠ¡
- ä½†å‚æ•°é‡é€šå¸¸æ¯” Decoder-only å¤§ï¼ˆå› ä¸ºæœ‰ä¸¤å¥—ï¼‰
```

---

## 5. æ¶æ„é€‰æ‹©æŒ‡å—

### 5.1 å†³ç­–æ ‘

```
ä½ çš„ä»»åŠ¡æ˜¯ä»€ä¹ˆï¼Ÿ
â”‚
â”œâ”€â†’ ç†è§£/åˆ†ç±»ä»»åŠ¡ï¼Ÿ
â”‚   â””â”€â†’ ç”¨ Encoder-only (BERT)
â”‚
â”œâ”€â†’ ç”Ÿæˆä»»åŠ¡ï¼Ÿ
â”‚   â”œâ”€â†’ å¼€æ”¾ç”Ÿæˆï¼ˆèŠå¤©ã€å†™ä½œï¼‰ï¼Ÿ
â”‚   â”‚   â””â”€â†’ ç”¨ Decoder-only (GPT/LLaMA)
â”‚   â”‚
â”‚   â””â”€â†’ æ¡ä»¶ç”Ÿæˆï¼ˆç¿»è¯‘ã€æ‘˜è¦ï¼‰ï¼Ÿ
â”‚       â”œâ”€â†’ è¾“å…¥è¾“å‡ºé•¿åº¦å·®å¼‚å¤§ï¼Ÿ
â”‚       â”‚   â””â”€â†’ ç”¨ Encoder-Decoder (T5)
â”‚       â”‚
â”‚       â””â”€â†’ é•¿åº¦ç›¸è¿‘/é€šç”¨éœ€æ±‚ï¼Ÿ
â”‚           â””â”€â†’ Decoder-only ä¹Ÿå¯ä»¥ï¼ˆæŒ‡ä»¤å¾®è°ƒï¼‰
â”‚
â””â”€â†’ ä¸ç¡®å®šï¼Ÿ
    â””â”€â†’ ç°ä»£è¶‹åŠ¿ï¼šDecoder-only + æŒ‡ä»¤å¾®è°ƒ
        ï¼ˆä¸€ä¸ªæ¨¡å‹æå®šæ‰€æœ‰ä»»åŠ¡ï¼‰
```

### 5.2 ç°ä»£è¶‹åŠ¿

```
2023-2024 çš„è¶‹åŠ¿ï¼š

1. Decoder-only æˆä¸ºä¸»æµ
   - GPT-4, Claude, LLaMA, Qwen éƒ½æ˜¯ Decoder-only
   - é€šè¿‡æŒ‡ä»¤å¾®è°ƒï¼Œä¸€ä¸ªæ¨¡å‹å¯ä»¥åšæ‰€æœ‰ä»»åŠ¡

2. è§„æ¨¡æ•ˆåº”
   - è¶³å¤Ÿå¤§çš„ Decoder-only æ¨¡å‹å¯ä»¥æ¶Œç°å„ç§èƒ½åŠ›
   - ä¸éœ€è¦ä¸“é—¨è®¾è®¡æ¶æ„

3. ç‰¹å®šä»»åŠ¡ä»æœ‰ä»·å€¼
   - å°è§„æ¨¡åœºæ™¯ï¼šBERT ä»ç„¶é«˜æ•ˆ
   - ç¿»è¯‘/æ‘˜è¦ï¼šT5 ç³»åˆ—ä»ç„¶å¼º

4. å¤šæ¨¡æ€
   - è§†è§‰ç¼–ç å™¨ + LLM è§£ç å™¨ï¼ˆç±»ä¼¼ Encoder-Decoderï¼‰
```

---

## 6. ç»ƒä¹ é¢˜

### åŸºç¡€ç»ƒä¹ 

1. ç”¨ Hugging Face åŠ è½½ BERT å’Œ GPT-2ï¼Œè§‚å¯Ÿå®ƒä»¬çš„æ³¨æ„åŠ›æ¨¡å¼
2. å®ç°ä¸€ä¸ªç®€å•çš„ seq2seq æ¨¡å‹ï¼ˆEncoder-Decoderï¼‰
3. æ¯”è¾ƒ BERT å’Œ GPT åœ¨æ–‡æœ¬åˆ†ç±»ä»»åŠ¡ä¸Šçš„æ•ˆæœ

### å‚è€ƒç­”æ¡ˆ

<details>
<summary>ç‚¹å‡»æŸ¥çœ‹ç­”æ¡ˆ</summary>

```python
from transformers import BertModel, GPT2Model, BertTokenizer, GPT2Tokenizer

# 1. åŠ è½½æ¨¡å‹å¹¶è§‚å¯Ÿæ³¨æ„åŠ›
tokenizer_bert = BertTokenizer.from_pretrained('bert-base-uncased')
model_bert = BertModel.from_pretrained('bert-base-uncased', output_attentions=True)

tokenizer_gpt = GPT2Tokenizer.from_pretrained('gpt2')
model_gpt = GPT2Model.from_pretrained('gpt2', output_attentions=True)

text = "The quick brown fox jumps over the lazy dog."

# BERT
inputs_bert = tokenizer_bert(text, return_tensors='pt')
outputs_bert = model_bert(**inputs_bert)
attn_bert = outputs_bert.attentions[0][0]  # ç¬¬ä¸€å±‚ï¼Œç¬¬ä¸€ä¸ªæ ·æœ¬

# GPT-2
inputs_gpt = tokenizer_gpt(text, return_tensors='pt')
outputs_gpt = model_gpt(**inputs_gpt)
attn_gpt = outputs_gpt.attentions[0][0]

print(f"BERT æ³¨æ„åŠ›å½¢çŠ¶: {attn_bert.shape}")  # [num_heads, seq_len, seq_len]
print(f"GPT-2 æ³¨æ„åŠ›å½¢çŠ¶: {attn_gpt.shape}")

# è§‚å¯Ÿï¼šGPT-2 çš„æ³¨æ„åŠ›çŸ©é˜µæ˜¯ä¸‹ä¸‰è§’ï¼ˆå› æœæ©ç ï¼‰
# BERT çš„æ³¨æ„åŠ›çŸ©é˜µæ˜¯å…¨çš„ï¼ˆåŒå‘ï¼‰


# 2. ç®€å• seq2seq
class SimpleSeq2Seq(nn.Module):
    def __init__(self, vocab_size, d_model=256, num_layers=2):
        super().__init__()
        self.embedding = nn.Embedding(vocab_size, d_model)
        self.encoder = nn.LSTM(d_model, d_model, num_layers, batch_first=True)
        self.decoder = nn.LSTM(d_model, d_model, num_layers, batch_first=True)
        self.fc = nn.Linear(d_model, vocab_size)

    def forward(self, src, tgt):
        # Encode
        src_emb = self.embedding(src)
        _, (hidden, cell) = self.encoder(src_emb)

        # Decode
        tgt_emb = self.embedding(tgt)
        outputs, _ = self.decoder(tgt_emb, (hidden, cell))

        return self.fc(outputs)

# æµ‹è¯•
model = SimpleSeq2Seq(vocab_size=1000)
src = torch.randint(0, 1000, (2, 10))
tgt = torch.randint(0, 1000, (2, 8))
out = model(src, tgt)
print(f"Seq2Seq è¾“å‡º: {out.shape}")  # [2, 8, 1000]


# 3. BERT vs GPT åˆ†ç±»å¯¹æ¯”ï¼ˆä¼ªä»£ç ï¼‰
"""
from transformers import BertForSequenceClassification, GPT2ForSequenceClassification

# BERT åˆ†ç±»ï¼šå¤©ç„¶é€‚åˆï¼Œä½¿ç”¨ [CLS] token
bert_clf = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)

# GPT åˆ†ç±»ï¼šéœ€è¦ä½¿ç”¨æœ€åä¸€ä¸ª token
gpt_clf = GPT2ForSequenceClassification.from_pretrained('gpt2')
gpt_clf.config.pad_token_id = tokenizer_gpt.eos_token_id

# é€šå¸¸ BERT åœ¨å°æ•°æ®é›†åˆ†ç±»ä»»åŠ¡ä¸Šæ•ˆæœæ›´å¥½
# å› ä¸ºå®ƒæ˜¯åŒå‘çš„ï¼Œèƒ½æ›´å¥½åœ°ç†è§£ä¸Šä¸‹æ–‡
"""
```

</details>

---

## â¡ï¸ ä¸‹ä¸€æ­¥

å­¦å®Œæœ¬èŠ‚åï¼Œç»§ç»­å­¦ä¹  [05-LLMä¼˜åŒ–æŠ€æœ¯.md](./05-LLMä¼˜åŒ–æŠ€æœ¯.md)

