# ğŸ› 01 - Transformer æ¶æ„

> Transformer æ˜¯ç°ä»£ NLP å’Œ LLM çš„åŸºçŸ³ï¼Œç†è§£å®ƒæ˜¯å­¦ä¹ å¤§æ¨¡å‹çš„ç¬¬ä¸€æ­¥

---

## ç›®å½•

1. [Transformer å…¨å±€è§†å›¾](#1-transformer-å…¨å±€è§†å›¾)
2. [Self-Attention](#2-self-attention)
3. [Multi-Head Attention](#3-multi-head-attention)
4. [Feed Forward Network](#4-feed-forward-network)
5. [æ®‹å·®è¿æ¥ä¸ LayerNorm](#5-æ®‹å·®è¿æ¥ä¸-layernorm)
6. [Transformer Block](#6-transformer-block)
7. [ä»£ç å®ç°](#7-ä»£ç å®ç°)
8. [ç»ƒä¹ é¢˜](#8-ç»ƒä¹ é¢˜)

---

## 1. Transformer å…¨å±€è§†å›¾

### 1.1 ä¸ºä»€ä¹ˆéœ€è¦ Transformerï¼Ÿ

```
RNN çš„é—®é¢˜ï¼š
1. é¡ºåºå¤„ç†ï¼Œæ— æ³•å¹¶è¡Œ â†’ è®­ç»ƒæ…¢
2. é•¿è·ç¦»ä¾èµ–éš¾ä»¥å­¦ä¹  â†’ æ¢¯åº¦æ¶ˆå¤±
3. ä¿¡æ¯éœ€è¦"ä¼ é€’"å¾ˆå¤šæ­¥ â†’ ä¿¡æ¯ä¸¢å¤±

Transformer çš„è§£å†³æ–¹æ¡ˆï¼š
1. Self-Attention å¹¶è¡Œè®¡ç®—æ‰€æœ‰ä½ç½®
2. æ¯ä¸ªä½ç½®ç›´æ¥"çœ‹"åˆ°æ‰€æœ‰å…¶ä»–ä½ç½®
3. ä¸éœ€è¦ä¿¡æ¯ä¼ é€’ï¼Œç›´æ¥å…¨å±€äº¤äº’
```

### 1.2 åŸå§‹ Transformer æ¶æ„

```
åŸå§‹ Transformerï¼ˆ2017ï¼‰æ˜¯ Encoder-Decoder ç»“æ„ï¼š

        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
è¾“å…¥ â†’  â”‚    Encoder      â”‚  â†’ ç¼–ç è¡¨ç¤º
        â”‚  (N ä¸ª Block)   â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â†“
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
è¾“å‡º â†  â”‚    Decoder      â”‚  â† ç›®æ ‡åºåˆ—
        â”‚  (N ä¸ª Block)   â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ç”¨é€”ï¼šæœºå™¨ç¿»è¯‘ï¼ˆè¾“å…¥è‹±æ–‡ â†’ è¾“å‡ºä¸­æ–‡ï¼‰
```

### 1.3 ç°ä»£å˜ä½“

```
Encoder-only (BERT)ï¼š
- åªæœ‰ Encoder
- åŒå‘æ³¨æ„åŠ›ï¼ˆçœ‹å‰çœ‹åï¼‰
- ç”¨é€”ï¼šæ–‡æœ¬ç†è§£ã€åˆ†ç±»ã€NER

Decoder-only (GPT/LLaMA)ï¼š
- åªæœ‰ Decoder
- å•å‘æ³¨æ„åŠ›ï¼ˆåªçœ‹å‰é¢ï¼‰+ å› æœæ©ç 
- ç”¨é€”ï¼šæ–‡æœ¬ç”Ÿæˆ
- ç°ä»£ LLM çš„ä¸»æµé€‰æ‹©ï¼

Encoder-Decoder (T5)ï¼š
- å®Œæ•´ç»“æ„
- ç”¨é€”ï¼šç¿»è¯‘ã€æ‘˜è¦ã€é—®ç­”
```

---

## 2. Self-Attention

### 2.1 æ ¸å¿ƒç›´è§‰

```
Self-Attention çš„æœ¬è´¨ï¼š
è®©åºåˆ—ä¸­çš„æ¯ä¸ªä½ç½®"å…³æ³¨"å…¶ä»–æ‰€æœ‰ä½ç½®ï¼Œå­¦ä¹ å®ƒä»¬ä¹‹é—´çš„å…³ç³»

è¾“å…¥åºåˆ—ï¼š"The cat sat on the mat"
         [0]  [1] [2] [3] [4]  [5]

å¯¹äº "sat"ï¼ˆä½ç½® 2ï¼‰ï¼š
- å®ƒéœ€è¦çŸ¥é“"è°åœ¨å"â†’ å…³æ³¨ "cat"
- å®ƒéœ€è¦çŸ¥é“"ååœ¨å“ª"â†’ å…³æ³¨ "mat"
- è¿™ç§å…³ç³»é€šè¿‡æ³¨æ„åŠ›æƒé‡å­¦ä¹ 

è¾“å‡ºï¼šæ¯ä¸ªä½ç½®çš„è¡¨ç¤ºéƒ½èåˆäº†ç›¸å…³ä¸Šä¸‹æ–‡
```

### 2.2 Qã€Kã€V çš„ç›´è§‰

```
ä¸‰ä¸ªå‘é‡çš„è§’è‰²ï¼š

Query (Q) = "æˆ‘åœ¨æ‰¾ä»€ä¹ˆï¼Ÿ"
Key (K)   = "æˆ‘æœ‰ä»€ä¹ˆå¯ä»¥è¢«æ‰¾åˆ°ï¼Ÿ"
Value (V) = "å¦‚æœè¢«æ‰¾åˆ°ï¼Œè¿”å›ä»€ä¹ˆå†…å®¹ï¼Ÿ"

ç±»æ¯”ï¼šå›¾ä¹¦é¦†æŸ¥è¯¢
- Queryï¼šä½ æƒ³æ‰¾çš„ä¹¦çš„ç‰¹å¾ï¼ˆç§‘å¹»ã€æœ€æ–°çš„ï¼‰
- Keyï¼šæ¯æœ¬ä¹¦çš„æ ‡ç­¾ï¼ˆåˆ†ç±»ã€å‡ºç‰ˆæ—¥æœŸï¼‰
- Valueï¼šä¹¦çš„å®é™…å†…å®¹

è¿‡ç¨‹ï¼š
1. ç”¨ Query å’Œæ‰€æœ‰ Key è®¡ç®—ç›¸ä¼¼åº¦ï¼ˆæ³¨æ„åŠ›åˆ†æ•°ï¼‰
2. ç›¸ä¼¼åº¦é«˜çš„ Key å¯¹åº”çš„ Value æƒé‡å¤§
3. åŠ æƒæ±‚å’Œå¾—åˆ°è¾“å‡º
```

### 2.3 æ•°å­¦å…¬å¼

```
Attention(Q, K, V) = softmax(QK^T / âˆšd_k) Ã— V

åˆ†æ­¥è§£é‡Šï¼š
1. QK^Tï¼šè®¡ç®— Query å’Œæ‰€æœ‰ Key çš„ç›¸ä¼¼åº¦
   - å½¢çŠ¶ï¼š[seq_len, d_k] Ã— [d_k, seq_len] = [seq_len, seq_len]

2. / âˆšd_kï¼šç¼©æ”¾å› å­ï¼Œé˜²æ­¢ç‚¹ç§¯è¿‡å¤§å¯¼è‡´ softmax é¥±å’Œ
   - d_k æ˜¯ Key çš„ç»´åº¦

3. softmaxï¼šè½¬æ¢ä¸ºæ¦‚ç‡åˆ†å¸ƒï¼ˆæƒé‡å’Œä¸º 1ï¼‰

4. Ã— Vï¼šç”¨æ³¨æ„åŠ›æƒé‡åŠ æƒ Value
   - å½¢çŠ¶ï¼š[seq_len, seq_len] Ã— [seq_len, d_v] = [seq_len, d_v]
```

### 2.4 ä»£ç å®ç°

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
import math

def scaled_dot_product_attention(Q, K, V, mask=None):
    """
    ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›

    Args:
        Q: [batch, seq_len, d_k]
        K: [batch, seq_len, d_k]
        V: [batch, seq_len, d_v]
        mask: [batch, 1, seq_len] æˆ– [batch, seq_len, seq_len]

    Returns:
        output: [batch, seq_len, d_v]
        attention_weights: [batch, seq_len, seq_len]
    """
    d_k = Q.size(-1)

    # 1. è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°
    scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(d_k)
    # scores: [batch, seq_len, seq_len]

    # 2. åº”ç”¨æ©ç ï¼ˆå¯é€‰ï¼‰
    if mask is not None:
        scores = scores.masked_fill(mask == 0, float('-inf'))

    # 3. Softmax å½’ä¸€åŒ–
    attention_weights = F.softmax(scores, dim=-1)

    # 4. åŠ æƒæ±‚å’Œ
    output = torch.matmul(attention_weights, V)

    return output, attention_weights

# æµ‹è¯•
batch_size = 2
seq_len = 4
d_k = d_v = 8

Q = torch.randn(batch_size, seq_len, d_k)
K = torch.randn(batch_size, seq_len, d_k)
V = torch.randn(batch_size, seq_len, d_v)

output, attn = scaled_dot_product_attention(Q, K, V)
print(f"è¾“å…¥ Q: {Q.shape}")
print(f"è¾“å‡º: {output.shape}")
print(f"æ³¨æ„åŠ›æƒé‡: {attn.shape}")
print(f"æ³¨æ„åŠ›æƒé‡ï¼ˆç¬¬ä¸€ä¸ªæ ·æœ¬ï¼‰:\n{attn[0]}")
```

### 2.5 å¯è§†åŒ–æ³¨æ„åŠ›

```python
import matplotlib.pyplot as plt
import seaborn as sns

def visualize_attention(attention_weights, tokens):
    """å¯è§†åŒ–æ³¨æ„åŠ›æƒé‡"""
    plt.figure(figsize=(10, 8))
    sns.heatmap(
        attention_weights.detach().numpy(),
        xticklabels=tokens,
        yticklabels=tokens,
        cmap='Blues',
        annot=True,
        fmt='.2f'
    )
    plt.xlabel('Key')
    plt.ylabel('Query')
    plt.title('Self-Attention Weights')
    plt.show()

# ç¤ºä¾‹
tokens = ["The", "cat", "sat", "on"]
attn_example = torch.softmax(torch.randn(4, 4), dim=-1)
visualize_attention(attn_example, tokens)
```

---

## 3. Multi-Head Attention

### 3.1 ä¸ºä»€ä¹ˆéœ€è¦å¤šå¤´ï¼Ÿ

```
å•å¤´æ³¨æ„åŠ›çš„å±€é™ï¼š
- åªèƒ½å­¦ä¹ ä¸€ç§æ³¨æ„åŠ›æ¨¡å¼
- æ¯”å¦‚åªèƒ½å…³æ³¨"è¯­æ³•å…³ç³»"æˆ–"è¯­ä¹‰å…³ç³»"

å¤šå¤´æ³¨æ„åŠ›ï¼š
- å¹¶è¡Œè¿è¡Œå¤šä¸ªæ³¨æ„åŠ›å¤´
- æ¯ä¸ªå¤´å­¦ä¹ ä¸åŒçš„æ¨¡å¼
- æ‹¼æ¥åæŠ•å½±ï¼Œèåˆå¤šç§ä¿¡æ¯

ç±»æ¯”ï¼šå¤šä¸ªä¸“å®¶
- Head 1ï¼šä¸“æ³¨è¯­æ³•ä¾èµ–ï¼ˆä¸»è°“å…³ç³»ï¼‰
- Head 2ï¼šä¸“æ³¨æŒ‡ä»£å…³ç³»ï¼ˆä»£è¯æŒ‡å‘è°ï¼‰
- Head 3ï¼šä¸“æ³¨è¯­ä¹‰ç›¸ä¼¼æ€§
```

### 3.2 å¤šå¤´æ³¨æ„åŠ›ç»“æ„

```
è¾“å…¥ X
    â”‚
    â”œâ”€â†’ Linear(W_Q^1) â”€â†’ Q^1 â”€â”
    â”œâ”€â†’ Linear(W_K^1) â”€â†’ K^1 â”€â”¼â”€â†’ Head^1 â”€â”
    â”œâ”€â†’ Linear(W_V^1) â”€â†’ V^1 â”€â”˜           â”‚
    â”‚                                      â”‚
    â”œâ”€â†’ Linear(W_Q^2) â”€â†’ Q^2 â”€â”            â”‚
    â”œâ”€â†’ Linear(W_K^2) â”€â†’ K^2 â”€â”¼â”€â†’ Head^2 â”€â”¼â”€â†’ Concat â”€â†’ Linear(W_O) â”€â†’ è¾“å‡º
    â”œâ”€â†’ Linear(W_V^2) â”€â†’ V^2 â”€â”˜            â”‚
    â”‚                                      â”‚
    â””â”€â†’ ... (æ›´å¤šå¤´) ...                   â”˜

ç»´åº¦å˜åŒ–ï¼ˆå‡è®¾ d_model=512, num_heads=8ï¼‰ï¼š
- è¾“å…¥ï¼š[batch, seq_len, 512]
- æ¯ä¸ªå¤´ï¼šd_k = d_v = 512 / 8 = 64
- æ¯ä¸ªå¤´è¾“å‡ºï¼š[batch, seq_len, 64]
- æ‹¼æ¥åï¼š[batch, seq_len, 512]
- çº¿æ€§å˜æ¢åï¼š[batch, seq_len, 512]
```

### 3.3 ä»£ç å®ç°

```python
class MultiHeadAttention(nn.Module):
    def __init__(self, d_model, num_heads, dropout=0.1):
        super().__init__()
        assert d_model % num_heads == 0, "d_model å¿…é¡»èƒ½è¢« num_heads æ•´é™¤"

        self.d_model = d_model
        self.num_heads = num_heads
        self.d_k = d_model // num_heads  # æ¯ä¸ªå¤´çš„ç»´åº¦

        # Q, K, V æŠ•å½±çŸ©é˜µ
        self.W_q = nn.Linear(d_model, d_model)
        self.W_k = nn.Linear(d_model, d_model)
        self.W_v = nn.Linear(d_model, d_model)

        # è¾“å‡ºæŠ•å½±
        self.W_o = nn.Linear(d_model, d_model)

        self.dropout = nn.Dropout(dropout)

    def forward(self, query, key, value, mask=None):
        batch_size = query.size(0)

        # 1. çº¿æ€§æŠ•å½±
        Q = self.W_q(query)  # [batch, seq_len, d_model]
        K = self.W_k(key)
        V = self.W_v(value)

        # 2. æ‹†åˆ†æˆå¤šå¤´
        # [batch, seq_len, d_model] â†’ [batch, seq_len, num_heads, d_k]
        # â†’ [batch, num_heads, seq_len, d_k]
        Q = Q.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)
        K = K.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)
        V = V.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)

        # 3. è®¡ç®—æ³¨æ„åŠ›
        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)

        if mask is not None:
            # æ‰©å±• mask ç»´åº¦ä»¥åŒ¹é…å¤šå¤´
            mask = mask.unsqueeze(1)  # [batch, 1, ...]
            scores = scores.masked_fill(mask == 0, float('-inf'))

        attention_weights = F.softmax(scores, dim=-1)
        attention_weights = self.dropout(attention_weights)

        # 4. åŠ æƒæ±‚å’Œ
        context = torch.matmul(attention_weights, V)
        # [batch, num_heads, seq_len, d_k]

        # 5. åˆå¹¶å¤šå¤´
        # [batch, num_heads, seq_len, d_k] â†’ [batch, seq_len, num_heads, d_k]
        # â†’ [batch, seq_len, d_model]
        context = context.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)

        # 6. è¾“å‡ºæŠ•å½±
        output = self.W_o(context)

        return output, attention_weights

# æµ‹è¯•
d_model = 512
num_heads = 8
seq_len = 10
batch_size = 2

mha = MultiHeadAttention(d_model, num_heads)
x = torch.randn(batch_size, seq_len, d_model)

output, attn = mha(x, x, x)  # Self-Attention: Q=K=V=X
print(f"è¾“å…¥: {x.shape}")
print(f"è¾“å‡º: {output.shape}")
print(f"æ³¨æ„åŠ›æƒé‡: {attn.shape}")  # [batch, num_heads, seq_len, seq_len]
```

---

## 4. Feed Forward Network

### 4.1 FFN çš„ä½œç”¨

```
æ³¨æ„åŠ›å±‚ï¼šå­¦ä¹ ä½ç½®ä¹‹é—´çš„å…³ç³»ï¼ˆäº¤äº’ï¼‰
FFN å±‚ï¼šå¯¹æ¯ä¸ªä½ç½®ç‹¬ç«‹è¿›è¡Œéçº¿æ€§å˜æ¢ï¼ˆç‰¹å¾æå–ï¼‰

ç»“æ„ï¼šä¸¤å±‚ MLP + æ¿€æ´»å‡½æ•°
FFN(x) = Linear_2(Activation(Linear_1(x)))

ç»´åº¦å˜åŒ–ï¼š
d_model â†’ d_ff â†’ d_model
512 â†’ 2048 â†’ 512ï¼ˆé€šå¸¸ d_ff = 4 * d_modelï¼‰
```

### 4.2 ä»£ç å®ç°

```python
class FeedForward(nn.Module):
    def __init__(self, d_model, d_ff, dropout=0.1):
        super().__init__()
        self.linear1 = nn.Linear(d_model, d_ff)
        self.linear2 = nn.Linear(d_ff, d_model)
        self.dropout = nn.Dropout(dropout)

    def forward(self, x):
        # x: [batch, seq_len, d_model]
        x = self.linear1(x)      # [batch, seq_len, d_ff]
        x = F.gelu(x)            # ç°ä»£ Transformer å¸¸ç”¨ GELU
        x = self.dropout(x)
        x = self.linear2(x)      # [batch, seq_len, d_model]
        return x

# æµ‹è¯•
ffn = FeedForward(d_model=512, d_ff=2048)
x = torch.randn(2, 10, 512)
y = ffn(x)
print(f"FFN: {x.shape} â†’ {y.shape}")
```

### 4.3 ç°ä»£å˜ä½“ï¼šSwiGLU

```python
class SwiGLU(nn.Module):
    """LLaMA ç­‰ç°ä»£æ¨¡å‹ä½¿ç”¨çš„ FFN å˜ä½“"""
    def __init__(self, d_model, d_ff, dropout=0.1):
        super().__init__()
        # SwiGLU éœ€è¦ä¸¤ä¸ªä¸ŠæŠ•å½±
        self.w1 = nn.Linear(d_model, d_ff, bias=False)
        self.w2 = nn.Linear(d_ff, d_model, bias=False)
        self.w3 = nn.Linear(d_model, d_ff, bias=False)  # é—¨æ§
        self.dropout = nn.Dropout(dropout)

    def forward(self, x):
        # SwiGLU(x) = (Swish(xW1) âŠ™ xW3) W2
        return self.dropout(self.w2(F.silu(self.w1(x)) * self.w3(x)))
```

---

## 5. æ®‹å·®è¿æ¥ä¸ LayerNorm

### 5.1 æ®‹å·®è¿æ¥

```
ä½œç”¨ï¼š
1. ç¼“è§£æ¢¯åº¦æ¶ˆå¤±
2. è®©ç½‘ç»œå¯ä»¥æ›´æ·±
3. è®©æ¨¡å‹å¯ä»¥å­¦ä¹ "ä»€ä¹ˆéƒ½ä¸åš"

output = x + Sublayer(x)
```

### 5.2 LayerNorm

```python
# LayerNormï¼šåœ¨ç‰¹å¾ç»´åº¦ä¸Šå½’ä¸€åŒ–
class LayerNorm(nn.Module):
    def __init__(self, d_model, eps=1e-6):
        super().__init__()
        self.gamma = nn.Parameter(torch.ones(d_model))
        self.beta = nn.Parameter(torch.zeros(d_model))
        self.eps = eps

    def forward(self, x):
        mean = x.mean(dim=-1, keepdim=True)
        std = x.std(dim=-1, keepdim=True)
        return self.gamma * (x - mean) / (std + self.eps) + self.beta

# å®é™…ä½¿ç”¨ PyTorch å†…ç½®çš„
layer_norm = nn.LayerNorm(512)
```

### 5.3 Pre-LN vs Post-LN

```
Post-LNï¼ˆåŸå§‹ Transformerï¼‰ï¼š
x â†’ Attention â†’ Add â†’ LayerNorm â†’ FFN â†’ Add â†’ LayerNorm

Pre-LNï¼ˆç°ä»£å¸¸ç”¨ï¼‰ï¼š
x â†’ LayerNorm â†’ Attention â†’ Add â†’ LayerNorm â†’ FFN â†’ Add

Pre-LN çš„ä¼˜åŠ¿ï¼š
- è®­ç»ƒæ›´ç¨³å®š
- ä¸éœ€è¦ Warmup
- ç°ä»£ LLMï¼ˆGPT-3ã€LLaMAï¼‰éƒ½ç”¨ Pre-LN
```

```python
# Pre-LN å®ç°
class PreNormBlock(nn.Module):
    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):
        super().__init__()
        self.norm1 = nn.LayerNorm(d_model)
        self.norm2 = nn.LayerNorm(d_model)
        self.attention = MultiHeadAttention(d_model, num_heads, dropout)
        self.ffn = FeedForward(d_model, d_ff, dropout)

    def forward(self, x, mask=None):
        # Pre-LN: Norm before sublayer
        attn_out, _ = self.attention(
            self.norm1(x), self.norm1(x), self.norm1(x), mask
        )
        x = x + attn_out  # æ®‹å·®è¿æ¥

        ffn_out = self.ffn(self.norm2(x))
        x = x + ffn_out  # æ®‹å·®è¿æ¥

        return x
```

---

## 6. Transformer Block

### 6.1 å®Œæ•´çš„ Encoder Block

```python
class TransformerEncoderBlock(nn.Module):
    """Transformer Encoder Block (Pre-LN)"""
    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):
        super().__init__()
        self.norm1 = nn.LayerNorm(d_model)
        self.norm2 = nn.LayerNorm(d_model)
        self.self_attention = MultiHeadAttention(d_model, num_heads, dropout)
        self.ffn = FeedForward(d_model, d_ff, dropout)
        self.dropout = nn.Dropout(dropout)

    def forward(self, x, mask=None):
        # Self-Attention with residual
        normed_x = self.norm1(x)
        attn_out, attn_weights = self.self_attention(normed_x, normed_x, normed_x, mask)
        x = x + self.dropout(attn_out)

        # FFN with residual
        normed_x = self.norm2(x)
        ffn_out = self.ffn(normed_x)
        x = x + self.dropout(ffn_out)

        return x, attn_weights
```

### 6.2 å®Œæ•´çš„ Decoder Block

```python
class TransformerDecoderBlock(nn.Module):
    """Transformer Decoder Block (Pre-LN)"""
    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):
        super().__init__()
        self.norm1 = nn.LayerNorm(d_model)
        self.norm2 = nn.LayerNorm(d_model)
        self.norm3 = nn.LayerNorm(d_model)

        # Masked Self-Attention
        self.self_attention = MultiHeadAttention(d_model, num_heads, dropout)
        # Cross-Attentionï¼ˆç”¨äº Encoder-Decoder ç»“æ„ï¼‰
        self.cross_attention = MultiHeadAttention(d_model, num_heads, dropout)
        self.ffn = FeedForward(d_model, d_ff, dropout)
        self.dropout = nn.Dropout(dropout)

    def forward(self, x, encoder_output=None, src_mask=None, tgt_mask=None):
        # Masked Self-Attention
        normed_x = self.norm1(x)
        attn_out, _ = self.self_attention(normed_x, normed_x, normed_x, tgt_mask)
        x = x + self.dropout(attn_out)

        # Cross-Attentionï¼ˆå¦‚æœæœ‰ encoder è¾“å‡ºï¼‰
        if encoder_output is not None:
            normed_x = self.norm2(x)
            cross_out, _ = self.cross_attention(
                normed_x, encoder_output, encoder_output, src_mask
            )
            x = x + self.dropout(cross_out)

        # FFN
        normed_x = self.norm3(x)
        ffn_out = self.ffn(normed_x)
        x = x + self.dropout(ffn_out)

        return x
```

---

## 7. ä»£ç å®ç°

### 7.1 å®Œæ•´çš„å°å‹ Transformer

```python
import torch
import torch.nn as nn
import math

class Transformer(nn.Module):
    """ç®€åŒ–ç‰ˆ Transformerï¼ˆEncoder-onlyï¼Œç±»ä¼¼ BERTï¼‰"""
    def __init__(
        self,
        vocab_size,
        d_model=512,
        num_heads=8,
        num_layers=6,
        d_ff=2048,
        max_seq_len=512,
        dropout=0.1,
        num_classes=2  # åˆ†ç±»ä»»åŠ¡
    ):
        super().__init__()

        # è¯åµŒå…¥
        self.embedding = nn.Embedding(vocab_size, d_model)
        self.pos_embedding = nn.Embedding(max_seq_len, d_model)

        # Transformer å±‚
        self.layers = nn.ModuleList([
            TransformerEncoderBlock(d_model, num_heads, d_ff, dropout)
            for _ in range(num_layers)
        ])

        self.norm = nn.LayerNorm(d_model)
        self.dropout = nn.Dropout(dropout)

        # åˆ†ç±»å¤´
        self.classifier = nn.Linear(d_model, num_classes)

        # åˆå§‹åŒ–
        self._init_weights()

    def _init_weights(self):
        for p in self.parameters():
            if p.dim() > 1:
                nn.init.xavier_uniform_(p)

    def forward(self, x, mask=None):
        batch_size, seq_len = x.shape

        # ä½ç½®ç´¢å¼•
        positions = torch.arange(seq_len, device=x.device).unsqueeze(0).expand(batch_size, -1)

        # è¯åµŒå…¥ + ä½ç½®åµŒå…¥
        x = self.embedding(x) + self.pos_embedding(positions)
        x = self.dropout(x)

        # Transformer å±‚
        for layer in self.layers:
            x, _ = layer(x, mask)

        x = self.norm(x)

        # å– [CLS] ä½ç½®ï¼ˆå‡è®¾æ˜¯ç¬¬ä¸€ä¸ª tokenï¼‰åšåˆ†ç±»
        cls_output = x[:, 0]
        logits = self.classifier(cls_output)

        return logits

# æµ‹è¯•
model = Transformer(
    vocab_size=10000,
    d_model=256,
    num_heads=4,
    num_layers=4,
    d_ff=1024,
    num_classes=2
)

x = torch.randint(0, 10000, (2, 128))  # [batch, seq_len]
output = model(x)
print(f"è¾“å…¥: {x.shape}")
print(f"è¾“å‡º: {output.shape}")

# å‚æ•°ç»Ÿè®¡
total_params = sum(p.numel() for p in model.parameters())
print(f"å‚æ•°é‡: {total_params:,}")
```

---

## 8. ç»ƒä¹ é¢˜

### åŸºç¡€ç»ƒä¹ 

1. æ‰‹åŠ¨è®¡ç®—ä¸€ä¸ª 4x4 çš„æ³¨æ„åŠ›çŸ©é˜µï¼ˆç»™å®š Qã€Kã€Vï¼‰
2. å®ç°å¸¦ causal mask çš„æ³¨æ„åŠ›ï¼ˆåªèƒ½çœ‹å‰é¢çš„ tokenï¼‰
3. ä¿®æ”¹ FFN ä¸º SwiGLU

### å‚è€ƒç­”æ¡ˆ

<details>
<summary>ç‚¹å‡»æŸ¥çœ‹ç­”æ¡ˆ</summary>

```python
# 1. æ‰‹åŠ¨è®¡ç®—æ³¨æ„åŠ›
Q = torch.tensor([[1., 0.], [0., 1.], [1., 1.], [0., 0.]])
K = torch.tensor([[1., 0.], [0., 1.], [0., 0.], [1., 1.]])
V = torch.tensor([[1., 2.], [3., 4.], [5., 6.], [7., 8.]])

d_k = Q.size(-1)
scores = torch.matmul(Q, K.T) / math.sqrt(d_k)
print(f"æ³¨æ„åŠ›åˆ†æ•°:\n{scores}")

weights = F.softmax(scores, dim=-1)
print(f"æ³¨æ„åŠ›æƒé‡:\n{weights}")

output = torch.matmul(weights, V)
print(f"è¾“å‡º:\n{output}")


# 2. Causal Mask
def create_causal_mask(seq_len):
    """åˆ›å»ºå› æœæ©ç ï¼ˆä¸‹ä¸‰è§’çŸ©é˜µï¼‰"""
    mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1)
    return mask == 0  # True è¡¨ç¤ºå¯ä»¥çœ‹ï¼ŒFalse è¡¨ç¤ºä¸èƒ½çœ‹

mask = create_causal_mask(4)
print(f"å› æœæ©ç :\n{mask}")

# ä½¿ç”¨
scores = torch.randn(4, 4)
scores = scores.masked_fill(~mask, float('-inf'))
weights = F.softmax(scores, dim=-1)
print(f"å¸¦å› æœæ©ç çš„æƒé‡:\n{weights}")
# æ¯è¡Œåªæœ‰å¯¹è§’çº¿åŠå·¦è¾¹æœ‰å€¼ï¼Œå…¶ä»–éƒ½æ˜¯ 0


# 3. SwiGLU FFN
class SwiGLU(nn.Module):
    def __init__(self, d_model, d_ff):
        super().__init__()
        self.w1 = nn.Linear(d_model, d_ff)
        self.w2 = nn.Linear(d_ff, d_model)
        self.w3 = nn.Linear(d_model, d_ff)

    def forward(self, x):
        return self.w2(F.silu(self.w1(x)) * self.w3(x))

# æµ‹è¯•
swiglu = SwiGLU(512, 2048)
x = torch.randn(2, 10, 512)
y = swiglu(x)
print(f"SwiGLU: {x.shape} -> {y.shape}")
```

</details>

---

## ğŸ“– å…³é”®æ€»ç»“

```
Transformer =
  è¯åµŒå…¥ + ä½ç½®åµŒå…¥
  + N Ã— (Multi-Head Attention + FFN + æ®‹å·® + LayerNorm)

Self-Attention:
  1. Qã€Kã€V åˆ†åˆ«æŠ•å½±
  2. QK^T / âˆšd_k è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°
  3. Softmax å½’ä¸€åŒ–
  4. åŠ æƒæ±‚å’Œ V

Multi-Head:
  - å¹¶è¡Œå¤šä¸ªæ³¨æ„åŠ›å¤´
  - æ¯ä¸ªå¤´å­¦ä¹ ä¸åŒæ¨¡å¼
  - æ‹¼æ¥åæŠ•å½±
```

---

## â¡ï¸ ä¸‹ä¸€æ­¥

å­¦å®Œæœ¬èŠ‚åï¼Œç»§ç»­å­¦ä¹  [02-æ³¨æ„åŠ›æœºåˆ¶è¯¦è§£.md](./02-æ³¨æ„åŠ›æœºåˆ¶è¯¦è§£.md)

