# ğŸ‘ï¸ 02 - æ³¨æ„åŠ›æœºåˆ¶è¯¦è§£

> æ·±å…¥ç†è§£æ³¨æ„åŠ›è®¡ç®—ã€å„ç§æ©ç ã€æ³¨æ„åŠ›å¯è§†åŒ–

---

## ç›®å½•

1. [æ³¨æ„åŠ›è®¡ç®—è¯¦è§£](#1-æ³¨æ„åŠ›è®¡ç®—è¯¦è§£)
2. [æ³¨æ„åŠ›æ©ç ](#2-æ³¨æ„åŠ›æ©ç )
3. [Cross-Attention](#3-cross-attention)
4. [æ³¨æ„åŠ›å¯è§†åŒ–](#4-æ³¨æ„åŠ›å¯è§†åŒ–)
5. [æ³¨æ„åŠ›çš„å˜ä½“](#5-æ³¨æ„åŠ›çš„å˜ä½“)
6. [ç»ƒä¹ é¢˜](#6-ç»ƒä¹ é¢˜)

---

## 1. æ³¨æ„åŠ›è®¡ç®—è¯¦è§£

### 1.1 é€æ­¥è®¡ç®—ç¤ºä¾‹

```python
import torch
import torch.nn.functional as F
import math

# ç®€å•ç¤ºä¾‹ï¼š3 ä¸ª tokenï¼Œç»´åº¦ 4
# å¥å­ï¼š"I love AI"
seq_len = 3
d_model = 4

# å‡è®¾å·²ç»å¾—åˆ°çš„åµŒå…¥
X = torch.tensor([
    [1.0, 0.0, 1.0, 0.0],  # "I"
    [0.0, 2.0, 0.0, 2.0],  # "love"
    [1.0, 1.0, 1.0, 1.0],  # "AI"
])

# æƒé‡çŸ©é˜µï¼ˆå®é™…æ˜¯å­¦ä¹ å¾—åˆ°çš„ï¼‰
W_q = torch.randn(d_model, d_model)
W_k = torch.randn(d_model, d_model)
W_v = torch.randn(d_model, d_model)

# Step 1: çº¿æ€§æŠ•å½±å¾—åˆ° Q, K, V
Q = X @ W_q
K = X @ W_k
V = X @ W_v
print(f"Q:\n{Q}")
print(f"K:\n{K}")
print(f"V:\n{V}")

# Step 2: è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°
# scores[i][j] = Q[i] å’Œ K[j] çš„ç›¸ä¼¼åº¦
scores = Q @ K.T
print(f"\nåŸå§‹åˆ†æ•° (QK^T):\n{scores}")

# Step 3: ç¼©æ”¾
d_k = d_model
scores_scaled = scores / math.sqrt(d_k)
print(f"\nç¼©æ”¾ååˆ†æ•°:\n{scores_scaled}")

# Step 4: Softmax å½’ä¸€åŒ–
attention_weights = F.softmax(scores_scaled, dim=-1)
print(f"\næ³¨æ„åŠ›æƒé‡:\n{attention_weights}")
print(f"æ¯è¡Œå’Œ: {attention_weights.sum(dim=-1)}")  # åº”è¯¥éƒ½æ˜¯ 1

# Step 5: åŠ æƒæ±‚å’Œ
output = attention_weights @ V
print(f"\nè¾“å‡º:\n{output}")

# ç†è§£ï¼šoutput[i] æ˜¯ V çš„åŠ æƒå’Œï¼Œæƒé‡ç”± Q[i] å’Œ K å†³å®š
```

### 1.2 çŸ©é˜µç»´åº¦è¿½è¸ª

```
è¾“å…¥ X: [batch, seq_len, d_model]
         [2,     10,      512]

æŠ•å½±åï¼š
  Q: [batch, seq_len, d_model] â†’ [2, 10, 512]
  K: [batch, seq_len, d_model] â†’ [2, 10, 512]
  V: [batch, seq_len, d_model] â†’ [2, 10, 512]

æ³¨æ„åŠ›åˆ†æ•°ï¼š
  QK^T: [batch, seq_len, d_model] @ [batch, d_model, seq_len]
        [2, 10, 512] @ [2, 512, 10]
      = [2, 10, 10]

æ³¨æ„åŠ›æƒé‡ï¼ˆsoftmax åï¼‰ï¼š
  [batch, seq_len, seq_len] = [2, 10, 10]
  æ¯ä¸ªä½ç½® i å¯¹æ‰€æœ‰ä½ç½® j çš„æ³¨æ„åŠ›æƒé‡

è¾“å‡ºï¼š
  weights @ V: [batch, seq_len, seq_len] @ [batch, seq_len, d_model]
               [2, 10, 10] @ [2, 10, 512]
             = [2, 10, 512]
```

### 1.3 ä¸ºä»€ä¹ˆè¦ç¼©æ”¾ï¼Ÿ

```python
# ä¸ç¼©æ”¾çš„é—®é¢˜ï¼šå½“ d_k å¾ˆå¤§æ—¶ï¼Œç‚¹ç§¯ç»“æœä¹Ÿå¾ˆå¤§
d_k = 512
q = torch.randn(d_k)
k = torch.randn(d_k)

dot = (q * k).sum()
print(f"d_k={d_k} æ—¶çš„ç‚¹ç§¯: {dot.item():.2f}")
print(f"ç‚¹ç§¯çš„æœŸæœ›æ–¹å·®: {d_k}")  # æ–¹å·® â‰ˆ d_k

# å¤§æ•°å€¼ç»è¿‡ softmax åä¼šé¥±å’Œ
large_scores = torch.tensor([1.0, 2.0, 100.0])
print(f"å¤§åˆ†æ•°çš„ softmax: {F.softmax(large_scores, dim=0)}")
# â†’ å‡ ä¹æ˜¯ [0, 0, 1]ï¼Œæ¢¯åº¦è¶‹è¿‘äº 0

# ç¼©æ”¾å
scaled_scores = large_scores / math.sqrt(512)
print(f"ç¼©æ”¾åçš„ softmax: {F.softmax(scaled_scores, dim=0)}")
# â†’ æ›´å‡åŒ€ï¼Œæ¢¯åº¦æ›´å¥½
```

---

## 2. æ³¨æ„åŠ›æ©ç 

### 2.1 Padding Mask

```python
def create_padding_mask(seq, pad_idx=0):
    """
    åˆ›å»º padding mask
    pad ä½ç½®ä¸º Falseï¼Œé pad ä½ç½®ä¸º True
    """
    # seq: [batch, seq_len]
    mask = (seq != pad_idx)  # [batch, seq_len]
    return mask.unsqueeze(1)  # [batch, 1, seq_len] ç”¨äºå¹¿æ’­

# ç¤ºä¾‹
seq = torch.tensor([
    [1, 2, 3, 0, 0],  # æœ‰ 2 ä¸ª padding
    [4, 5, 6, 7, 0],  # æœ‰ 1 ä¸ª padding
])

mask = create_padding_mask(seq)
print(f"Padding mask:\n{mask}")

# åœ¨æ³¨æ„åŠ›ä¸­ä½¿ç”¨
scores = torch.randn(2, 5, 5)  # [batch, seq_len, seq_len]
scores = scores.masked_fill(~mask.unsqueeze(2), float('-inf'))
weights = F.softmax(scores, dim=-1)
print(f"ç¬¬ä¸€ä¸ªæ ·æœ¬çš„æƒé‡ï¼ˆæœ€åä¸¤åˆ—åº”ä¸º 0ï¼‰:\n{weights[0]}")
```

### 2.2 Causal Maskï¼ˆå› æœæ©ç ï¼‰

```python
def create_causal_mask(seq_len):
    """
    åˆ›å»ºå› æœæ©ç ï¼ˆç”¨äº Decoderï¼‰
    ä½ç½® i åªèƒ½çœ‹åˆ°ä½ç½® 0 åˆ° i
    """
    # ä¸‹ä¸‰è§’çŸ©é˜µï¼šmask[i][j] = True if j <= i
    mask = torch.tril(torch.ones(seq_len, seq_len)).bool()
    return mask

# ç¤ºä¾‹
mask = create_causal_mask(5)
print(f"å› æœæ©ç :\n{mask.int()}")

# å¯è§†åŒ–
"""
ä½ç½®:  0  1  2  3  4
    0  1  0  0  0  0   â† ä½ç½® 0 åªèƒ½çœ‹è‡ªå·±
    1  1  1  0  0  0   â† ä½ç½® 1 å¯ä»¥çœ‹ 0, 1
    2  1  1  1  0  0   â† ä½ç½® 2 å¯ä»¥çœ‹ 0, 1, 2
    3  1  1  1  1  0
    4  1  1  1  1  1
"""

# åœ¨æ³¨æ„åŠ›ä¸­ä½¿ç”¨
scores = torch.randn(5, 5)
scores = scores.masked_fill(~mask, float('-inf'))
weights = F.softmax(scores, dim=-1)
print(f"å› æœæ³¨æ„åŠ›æƒé‡:\n{weights}")
# ä¸Šä¸‰è§’éƒ¨åˆ†éƒ½æ˜¯ 0
```

### 2.3 ç»„åˆæ©ç 

```python
def create_combined_mask(seq, pad_idx=0):
    """
    ç»„åˆ padding mask å’Œ causal mask
    ç”¨äº Decoder çš„è‡ªæ³¨æ„åŠ›
    """
    batch_size, seq_len = seq.shape

    # Padding mask: [batch, 1, seq_len]
    padding_mask = (seq != pad_idx).unsqueeze(1)

    # Causal mask: [seq_len, seq_len]
    causal_mask = torch.tril(torch.ones(seq_len, seq_len)).bool()

    # ç»„åˆ: [batch, seq_len, seq_len]
    combined = padding_mask & causal_mask.unsqueeze(0)

    return combined

# ç¤ºä¾‹
seq = torch.tensor([
    [1, 2, 3, 0, 0],
    [4, 5, 6, 7, 8],
])

mask = create_combined_mask(seq)
print(f"ç»„åˆæ©ç ï¼ˆç¬¬ä¸€ä¸ªæ ·æœ¬ï¼‰:\n{mask[0].int()}")
# ä¸‹ä¸‰è§’ AND é padding
```

### 2.4 Sliding Window Mask

```python
def create_sliding_window_mask(seq_len, window_size):
    """
    æ»‘åŠ¨çª—å£æ³¨æ„åŠ›æ©ç ï¼ˆç”¨äºé•¿åºåˆ—ï¼‰
    æ¯ä¸ªä½ç½®åªèƒ½çœ‹å‰å window_size ä¸ªä½ç½®
    """
    mask = torch.zeros(seq_len, seq_len).bool()

    for i in range(seq_len):
        start = max(0, i - window_size)
        end = min(seq_len, i + window_size + 1)
        mask[i, start:end] = True

    return mask

# ç¤ºä¾‹
mask = create_sliding_window_mask(8, window_size=2)
print(f"æ»‘åŠ¨çª—å£æ©ç :\n{mask.int()}")
# æ¯è¡Œåªæœ‰ä¸­é—´ä¸€æ®µæ˜¯ 1ï¼Œå¯¹åº”çª—å£èŒƒå›´
```

---

## 3. Cross-Attention

### 3.1 Cross-Attention vs Self-Attention

```
Self-Attention:
  Q, K, V éƒ½æ¥è‡ªåŒä¸€ä¸ªåºåˆ—
  ç”¨é€”ï¼šè®©åºåˆ—å†…éƒ¨äº¤äº’

Cross-Attention:
  Q æ¥è‡ªä¸€ä¸ªåºåˆ—ï¼ˆDecoderï¼‰
  K, V æ¥è‡ªå¦ä¸€ä¸ªåºåˆ—ï¼ˆEncoderï¼‰
  ç”¨é€”ï¼šè®© Decoder "çœ‹åˆ°" Encoder çš„ä¿¡æ¯

åº”ç”¨åœºæ™¯ï¼š
  - æœºå™¨ç¿»è¯‘ï¼šDecoder æŸ¥è¯¢ Encoder çš„æºè¯­è¨€ä¿¡æ¯
  - å›¾åƒæè¿°ï¼šæ–‡æœ¬ Decoder æŸ¥è¯¢å›¾åƒ Encoder
  - é—®ç­”ç³»ç»Ÿï¼šç­”æ¡ˆç”Ÿæˆå™¨æŸ¥è¯¢é—®é¢˜ç¼–ç 
```

### 3.2 ä»£ç å®ç°

```python
class CrossAttention(nn.Module):
    """Cross-Attention: Query æ¥è‡ª Decoderï¼ŒKey/Value æ¥è‡ª Encoder"""
    def __init__(self, d_model, num_heads, dropout=0.1):
        super().__init__()
        self.d_model = d_model
        self.num_heads = num_heads
        self.d_k = d_model // num_heads

        self.W_q = nn.Linear(d_model, d_model)
        self.W_k = nn.Linear(d_model, d_model)
        self.W_v = nn.Linear(d_model, d_model)
        self.W_o = nn.Linear(d_model, d_model)

        self.dropout = nn.Dropout(dropout)

    def forward(self, decoder_hidden, encoder_output, mask=None):
        """
        decoder_hidden: [batch, tgt_len, d_model] - Decoder çš„éšè—çŠ¶æ€
        encoder_output: [batch, src_len, d_model] - Encoder çš„è¾“å‡º
        mask: [batch, 1, src_len] - Encoder çš„ padding mask
        """
        batch_size = decoder_hidden.size(0)

        # Q æ¥è‡ª Decoderï¼ŒK/V æ¥è‡ª Encoder
        Q = self.W_q(decoder_hidden)  # [batch, tgt_len, d_model]
        K = self.W_k(encoder_output)  # [batch, src_len, d_model]
        V = self.W_v(encoder_output)  # [batch, src_len, d_model]

        # æ‹†åˆ†å¤šå¤´
        Q = Q.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)
        K = K.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)
        V = V.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)

        # æ³¨æ„åŠ›
        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)
        # scores: [batch, num_heads, tgt_len, src_len]

        if mask is not None:
            scores = scores.masked_fill(mask.unsqueeze(1) == 0, float('-inf'))

        weights = F.softmax(scores, dim=-1)
        weights = self.dropout(weights)

        context = torch.matmul(weights, V)
        context = context.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)

        return self.W_o(context), weights

# æµ‹è¯•
cross_attn = CrossAttention(d_model=256, num_heads=4)

encoder_output = torch.randn(2, 20, 256)  # Encoder è¾“å‡º 20 ä¸ª token
decoder_hidden = torch.randn(2, 10, 256)  # Decoder å½“å‰ 10 ä¸ª token

output, weights = cross_attn(decoder_hidden, encoder_output)
print(f"Cross-Attention è¾“å‡º: {output.shape}")  # [2, 10, 256]
print(f"æ³¨æ„åŠ›æƒé‡: {weights.shape}")  # [2, 4, 10, 20]
# æ¯ä¸ª Decoder ä½ç½®å¯¹ 20 ä¸ª Encoder ä½ç½®çš„æ³¨æ„åŠ›
```

---

## 4. æ³¨æ„åŠ›å¯è§†åŒ–

### 4.1 ä½¿ç”¨ BertViz

```python
# pip install bertviz
from bertviz import head_view
from transformers import BertTokenizer, BertModel

tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
model = BertModel.from_pretrained('bert-base-uncased', output_attentions=True)

text = "The cat sat on the mat."
inputs = tokenizer(text, return_tensors='pt')
outputs = model(**inputs)

# outputs.attentions: tuple of (batch, num_heads, seq_len, seq_len)
attention = outputs.attentions
tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])

# å¯è§†åŒ–ï¼ˆåœ¨ Jupyter ä¸­ï¼‰
# head_view(attention, tokens)
```

### 4.2 æ‰‹åŠ¨å¯è§†åŒ–

```python
import matplotlib.pyplot as plt
import seaborn as sns

def visualize_attention_heads(attention_weights, tokens, layer=0):
    """
    å¯è§†åŒ–å¤šå¤´æ³¨æ„åŠ›
    attention_weights: [num_heads, seq_len, seq_len]
    """
    num_heads = attention_weights.shape[0]
    fig, axes = plt.subplots(2, 4, figsize=(16, 8))

    for i, ax in enumerate(axes.flat):
        if i < num_heads:
            sns.heatmap(
                attention_weights[i].detach().numpy(),
                ax=ax,
                cmap='Blues',
                xticklabels=tokens,
                yticklabels=tokens,
                vmin=0, vmax=1
            )
            ax.set_title(f'Head {i+1}')
            ax.tick_params(axis='x', rotation=45)

    plt.suptitle(f'Layer {layer} Attention Heads')
    plt.tight_layout()
    plt.show()

# ç¤ºä¾‹
tokens = ["[CLS]", "The", "cat", "sat", "[SEP]"]
attn = torch.softmax(torch.randn(8, 5, 5), dim=-1)
visualize_attention_heads(attn, tokens)
```

### 4.3 æ³¨æ„åŠ›åˆ†æ

```python
def analyze_attention(attention_weights, tokens):
    """åˆ†ææ³¨æ„åŠ›æ¨¡å¼"""
    # attention_weights: [num_heads, seq_len, seq_len]

    # 1. æ¯ä¸ªå¤´çš„æ³¨æ„åŠ›ç†µï¼ˆè¶Šä½è¶Šé›†ä¸­ï¼‰
    entropy = -(attention_weights * torch.log(attention_weights + 1e-9)).sum(dim=-1)
    avg_entropy = entropy.mean(dim=-1)
    print(f"æ¯ä¸ªå¤´çš„å¹³å‡æ³¨æ„åŠ›ç†µ: {avg_entropy}")

    # 2. å“ªäº› token è¢«å…³æ³¨æœ€å¤š
    total_attention = attention_weights.mean(dim=0).sum(dim=0)  # [seq_len]
    print(f"\nè¢«å…³æ³¨åº¦ï¼ˆæ¯ä¸ª token æ”¶åˆ°çš„æ€»æ³¨æ„åŠ›ï¼‰:")
    for i, token in enumerate(tokens):
        print(f"  {token}: {total_attention[i].item():.4f}")

    # 3. å“ªäº› token æœ€"å¤–å‘"ï¼ˆåˆ†æ•£æ³¨æ„åŠ›ç»™å…¶ä»– tokenï¼‰
    outgoing = attention_weights.mean(dim=0).sum(dim=1)
    print(f"\nå¤–å‘åº¦ï¼ˆæ¯è¡Œå’Œï¼Œåº”è¯¥éƒ½æ¥è¿‘ 1ï¼‰: {outgoing}")

    # 4. è‡ªæ³¨æ„åŠ›ç¨‹åº¦ï¼ˆå¯¹è§’çº¿ï¼‰
    self_attention = attention_weights.mean(dim=0).diagonal()
    print(f"\nè‡ªæ³¨æ„åŠ›ç¨‹åº¦: {self_attention}")

# ç¤ºä¾‹
tokens = ["I", "love", "AI"]
attn = torch.softmax(torch.randn(4, 3, 3), dim=-1)
analyze_attention(attn, tokens)
```

---

## 5. æ³¨æ„åŠ›çš„å˜ä½“

### 5.1 ç›¸å¯¹ä½ç½®æ³¨æ„åŠ›

```python
class RelativePositionAttention(nn.Module):
    """ç›¸å¯¹ä½ç½®æ³¨æ„åŠ›ï¼ˆT5 ä½¿ç”¨ï¼‰"""
    def __init__(self, d_model, num_heads, max_rel_pos=32):
        super().__init__()
        self.num_heads = num_heads
        self.d_k = d_model // num_heads
        self.max_rel_pos = max_rel_pos

        # ç›¸å¯¹ä½ç½®åç½®
        self.rel_pos_bias = nn.Embedding(2 * max_rel_pos + 1, num_heads)

    def get_relative_positions(self, seq_len):
        """è®¡ç®—ç›¸å¯¹ä½ç½®çŸ©é˜µ"""
        positions = torch.arange(seq_len)
        rel_pos = positions.unsqueeze(0) - positions.unsqueeze(1)
        rel_pos = rel_pos.clamp(-self.max_rel_pos, self.max_rel_pos)
        rel_pos = rel_pos + self.max_rel_pos  # åç§»åˆ°éè´Ÿ
        return rel_pos

    def forward(self, Q, K, V, mask=None):
        batch_size, seq_len, _ = Q.shape

        # è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°
        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)

        # æ·»åŠ ç›¸å¯¹ä½ç½®åç½®
        rel_pos = self.get_relative_positions(seq_len).to(Q.device)
        rel_bias = self.rel_pos_bias(rel_pos)  # [seq_len, seq_len, num_heads]
        rel_bias = rel_bias.permute(2, 0, 1).unsqueeze(0)  # [1, num_heads, seq_len, seq_len]
        scores = scores + rel_bias

        if mask is not None:
            scores = scores.masked_fill(~mask, float('-inf'))

        weights = F.softmax(scores, dim=-1)
        return torch.matmul(weights, V), weights
```

### 5.2 ç¨€ç–æ³¨æ„åŠ›

```python
def sparse_attention_mask(seq_len, stride=4):
    """
    ç¨€ç–æ³¨æ„åŠ›ï¼šåªå…³æ³¨å±€éƒ¨ + å…¨å±€
    å‡å°‘è®¡ç®—å¤æ‚åº¦ O(nÂ²) â†’ O(n)
    """
    mask = torch.zeros(seq_len, seq_len).bool()

    # å±€éƒ¨ï¼šæ¯ä¸ªä½ç½®å…³æ³¨å‰åå‡ ä¸ªä½ç½®
    for i in range(seq_len):
        start = max(0, i - stride)
        end = min(seq_len, i + stride + 1)
        mask[i, start:end] = True

    # å…¨å±€ï¼šæ¯ä¸ªä½ç½®å…³æ³¨å›ºå®šçš„å…¨å±€ä½ç½®ï¼ˆå¦‚æ¯ stride ä¸ªï¼‰
    global_positions = list(range(0, seq_len, stride))
    for i in range(seq_len):
        for g in global_positions:
            mask[i, g] = True
            mask[g, i] = True

    return mask

# å¯è§†åŒ–
mask = sparse_attention_mask(16, stride=4)
plt.figure(figsize=(8, 8))
plt.imshow(mask.int(), cmap='Blues')
plt.title('Sparse Attention Pattern')
plt.colorbar()
plt.show()
```

### 5.3 çº¿æ€§æ³¨æ„åŠ›

```python
def linear_attention(Q, K, V, feature_map=None):
    """
    çº¿æ€§æ³¨æ„åŠ›ï¼šO(nÂ²) â†’ O(n)
    æ ¸å¿ƒï¼šæ”¹å˜è®¡ç®—é¡ºåºï¼Œå…ˆç®— KV å†ä¹˜ Q

    æ ‡å‡†ï¼šsoftmax(QK^T) V
    çº¿æ€§ï¼šÏ†(Q) (Ï†(K)^T V)  -- ç»“åˆå¾‹
    """
    if feature_map is None:
        # ç®€å•çš„ç‰¹å¾æ˜ å°„ï¼šELU + 1
        feature_map = lambda x: F.elu(x) + 1

    Q = feature_map(Q)
    K = feature_map(K)

    # å…ˆè®¡ç®— K^T V: [d_k, d_v]
    KV = torch.matmul(K.transpose(-2, -1), V)

    # å†è®¡ç®— Q (KV): [seq_len, d_v]
    output = torch.matmul(Q, KV)

    # å½’ä¸€åŒ–
    K_sum = K.sum(dim=-2, keepdim=True)  # [1, d_k]
    normalizer = torch.matmul(Q, K_sum.transpose(-2, -1))  # [seq_len, 1]
    output = output / (normalizer + 1e-6)

    return output
```

---

## 6. ç»ƒä¹ é¢˜

### åŸºç¡€ç»ƒä¹ 

1. å®ç°å¸¦ padding mask å’Œ causal mask çš„æ³¨æ„åŠ›
2. å¯è§†åŒ– BERT åœ¨ä¸€ä¸ªå¥å­ä¸Šçš„æ³¨æ„åŠ›æ¨¡å¼
3. æ¯”è¾ƒæ ‡å‡†æ³¨æ„åŠ›å’Œçº¿æ€§æ³¨æ„åŠ›çš„è®¡ç®—æ—¶é—´

### å‚è€ƒç­”æ¡ˆ

<details>
<summary>ç‚¹å‡»æŸ¥çœ‹ç­”æ¡ˆ</summary>

```python
# 1. å®Œæ•´çš„æ©ç æ³¨æ„åŠ›
def masked_attention(Q, K, V, padding_mask=None, causal=False):
    batch_size, seq_len, d_k = Q.shape

    scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(d_k)

    # å› æœæ©ç 
    if causal:
        causal_mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1).bool()
        scores = scores.masked_fill(causal_mask.to(Q.device), float('-inf'))

    # Padding æ©ç 
    if padding_mask is not None:
        # padding_mask: [batch, seq_len], True è¡¨ç¤ºæœ‰æ•ˆ
        mask = padding_mask.unsqueeze(1)  # [batch, 1, seq_len]
        scores = scores.masked_fill(~mask, float('-inf'))

    weights = F.softmax(scores, dim=-1)
    output = torch.matmul(weights, V)

    return output, weights

# æµ‹è¯•
Q = K = V = torch.randn(2, 5, 8)
padding_mask = torch.tensor([[1, 1, 1, 0, 0], [1, 1, 1, 1, 0]]).bool()

output, weights = masked_attention(Q, K, V, padding_mask, causal=True)
print(f"è¾“å‡º: {output.shape}")
print(f"æƒé‡ï¼ˆåº”è¯¥æ˜¯ä¸‹ä¸‰è§’ä¸” padding ä½ç½®ä¸º 0ï¼‰:\n{weights[0]}")


# 2. BERT æ³¨æ„åŠ›å¯è§†åŒ–
from transformers import BertTokenizer, BertModel

tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
model = BertModel.from_pretrained('bert-base-uncased', output_attentions=True)

text = "The quick brown fox jumps over the lazy dog."
inputs = tokenizer(text, return_tensors='pt')
outputs = model(**inputs)

# ç¬¬ 6 å±‚çš„æ³¨æ„åŠ›
layer_6_attn = outputs.attentions[5][0]  # [num_heads, seq_len, seq_len]
tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])

# å¯è§†åŒ–ç¬¬ä¸€ä¸ªå¤´
plt.figure(figsize=(10, 8))
sns.heatmap(layer_6_attn[0].detach().numpy(),
            xticklabels=tokens, yticklabels=tokens, cmap='Blues')
plt.title('BERT Layer 6 Head 1')
plt.show()


# 3. è®¡ç®—æ—¶é—´å¯¹æ¯”
import time

seq_len = 1024
d_k = 64
batch_size = 8

Q = K = V = torch.randn(batch_size, seq_len, d_k)

# æ ‡å‡†æ³¨æ„åŠ›
start = time.time()
for _ in range(100):
    scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(d_k)
    weights = F.softmax(scores, dim=-1)
    output = torch.matmul(weights, V)
standard_time = time.time() - start
print(f"æ ‡å‡†æ³¨æ„åŠ›æ—¶é—´: {standard_time:.4f}s")

# çº¿æ€§æ³¨æ„åŠ›
start = time.time()
for _ in range(100):
    output = linear_attention(Q, K, V)
linear_time = time.time() - start
print(f"çº¿æ€§æ³¨æ„åŠ›æ—¶é—´: {linear_time:.4f}s")
print(f"åŠ é€Ÿæ¯”: {standard_time/linear_time:.2f}x")
```

</details>

---

## â¡ï¸ ä¸‹ä¸€æ­¥

å­¦å®Œæœ¬èŠ‚åï¼Œç»§ç»­å­¦ä¹  [03-ä½ç½®ç¼–ç .md](./03-ä½ç½®ç¼–ç .md)

