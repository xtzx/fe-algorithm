# ğŸ” 08 - Embedding ä¸å‘é‡æ£€ç´¢

> Embedding å°†æ–‡æœ¬è½¬ä¸ºå‘é‡ï¼Œå‘é‡æ£€ç´¢æ˜¯ RAG çš„æ ¸å¿ƒæŠ€æœ¯

---

## ç›®å½•

1. [æ–‡æœ¬ Embedding](#1-æ–‡æœ¬-embedding)
2. [ç›¸ä¼¼åº¦è®¡ç®—](#2-ç›¸ä¼¼åº¦è®¡ç®—)
3. [å‘é‡æ•°æ®åº“](#3-å‘é‡æ•°æ®åº“)
4. [å®æˆ˜ï¼šè¯­ä¹‰æœç´¢](#4-å®æˆ˜è¯­ä¹‰æœç´¢)
5. [ç»ƒä¹ é¢˜](#5-ç»ƒä¹ é¢˜)

---

## 1. æ–‡æœ¬ Embedding

### 1.1 ä»€ä¹ˆæ˜¯ Embedding

```
Embeddingï¼šå°†ç¦»æ•£å¯¹è±¡ï¼ˆæ–‡æœ¬ã€å›¾åƒï¼‰æ˜ å°„åˆ°è¿ç»­å‘é‡ç©ºé—´

æ–‡æœ¬ â†’ Embedding æ¨¡å‹ â†’ å‘é‡ï¼ˆå¦‚ 768 ç»´ï¼‰

ç‰¹ç‚¹ï¼š
- è¯­ä¹‰ç›¸ä¼¼çš„æ–‡æœ¬ï¼Œå‘é‡ä¹Ÿç›¸è¿‘
- å¯ä»¥ç”¨å‘é‡è·ç¦»è¡¡é‡è¯­ä¹‰ç›¸ä¼¼åº¦
- æ˜¯ RAGã€è¯­ä¹‰æœç´¢çš„åŸºç¡€
```

### 1.2 è·å– Embedding

```python
from transformers import AutoTokenizer, AutoModel
import torch
import torch.nn.functional as F

# æ–¹æ³• 1ï¼šä½¿ç”¨ BERT è·å– Embedding
def get_bert_embedding(text, model_name="bert-base-uncased"):
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModel.from_pretrained(model_name)

    inputs = tokenizer(text, return_tensors="pt", padding=True, truncation=True)

    with torch.no_grad():
        outputs = model(**inputs)

    # æ–¹æ³• 1: [CLS] token
    cls_embedding = outputs.last_hidden_state[:, 0, :]

    # æ–¹æ³• 2: å¹³å‡æ± åŒ–ï¼ˆé€šå¸¸æ•ˆæœæ›´å¥½ï¼‰
    attention_mask = inputs["attention_mask"]
    token_embeddings = outputs.last_hidden_state
    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()
    mean_embedding = torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)

    return mean_embedding

# æµ‹è¯•
text = "Hello, how are you?"
embedding = get_bert_embedding(text)
print(f"Embedding shape: {embedding.shape}")  # [1, 768]
```

### 1.3 ä¸“ç”¨ Embedding æ¨¡å‹

```python
from sentence_transformers import SentenceTransformer

# æ–¹æ³• 2ï¼šä½¿ç”¨ Sentence Transformersï¼ˆæ¨èï¼‰
model = SentenceTransformer('all-MiniLM-L6-v2')

sentences = [
    "This is a sentence.",
    "This is another sentence.",
    "Completely different topic."
]

# è·å– embedding
embeddings = model.encode(sentences)
print(f"Embeddings shape: {embeddings.shape}")  # [3, 384]

# å¸¸ç”¨çš„ Embedding æ¨¡å‹
"""
é€šç”¨ï¼š
- sentence-transformers/all-MiniLM-L6-v2 (384ç»´ï¼Œå¿«)
- sentence-transformers/all-mpnet-base-v2 (768ç»´ï¼Œæ•ˆæœå¥½)

ä¸­æ–‡ï¼š
- shibing624/text2vec-base-chinese
- BAAI/bge-base-zh-v1.5
- moka-ai/m3e-base

å¤šè¯­è¨€ï¼š
- BAAI/bge-m3
- intfloat/multilingual-e5-large
"""

# ä½¿ç”¨ BGE æ¨¡å‹
from transformers import AutoTokenizer, AutoModel

def get_bge_embedding(texts, model_name="BAAI/bge-base-en-v1.5"):
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModel.from_pretrained(model_name)

    # BGE æ¨èåŠ å‰ç¼€
    texts = ["passage: " + t for t in texts]

    inputs = tokenizer(texts, padding=True, truncation=True, return_tensors="pt", max_length=512)

    with torch.no_grad():
        outputs = model(**inputs)
        embeddings = outputs.last_hidden_state[:, 0, :]
        embeddings = F.normalize(embeddings, p=2, dim=1)

    return embeddings

# æµ‹è¯•
embeddings = get_bge_embedding(["Hello world", "ä½ å¥½ä¸–ç•Œ"])
print(f"BGE embeddings: {embeddings.shape}")
```

---

## 2. ç›¸ä¼¼åº¦è®¡ç®—

### 2.1 ä½™å¼¦ç›¸ä¼¼åº¦

```python
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity

def cosine_sim(a, b):
    """ä½™å¼¦ç›¸ä¼¼åº¦ï¼š[-1, 1]ï¼Œè¶Šå¤§è¶Šç›¸ä¼¼"""
    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))

# ä½¿ç”¨ sklearn
embeddings = model.encode([
    "I love machine learning",
    "I enjoy deep learning",
    "The weather is nice today"
])

similarity_matrix = cosine_similarity(embeddings)
print("ç›¸ä¼¼åº¦çŸ©é˜µ:")
print(similarity_matrix)

# ç»“æœï¼šå‰ä¸¤å¥ç›¸ä¼¼åº¦é«˜ï¼Œç¬¬ä¸‰å¥ä¸å‰ä¸¤å¥ç›¸ä¼¼åº¦ä½
```

### 2.2 å…¶ä»–è·ç¦»åº¦é‡

```python
from scipy.spatial.distance import euclidean, cityblock

# æ¬§æ°è·ç¦»ï¼ˆL2ï¼‰
def euclidean_distance(a, b):
    return np.sqrt(np.sum((a - b) ** 2))

# æ›¼å“ˆé¡¿è·ç¦»ï¼ˆL1ï¼‰
def manhattan_distance(a, b):
    return np.sum(np.abs(a - b))

# ç‚¹ç§¯ï¼ˆå¦‚æœå‘é‡å·²å½’ä¸€åŒ–ï¼Œç­‰ä»·äºä½™å¼¦ç›¸ä¼¼åº¦ï¼‰
def dot_product(a, b):
    return np.dot(a, b)

# æ¯”è¾ƒ
a = embeddings[0]
b = embeddings[1]
c = embeddings[2]

print(f"a-b ä½™å¼¦ç›¸ä¼¼åº¦: {cosine_sim(a, b):.4f}")
print(f"a-c ä½™å¼¦ç›¸ä¼¼åº¦: {cosine_sim(a, c):.4f}")
print(f"a-b æ¬§æ°è·ç¦»: {euclidean_distance(a, b):.4f}")
print(f"a-c æ¬§æ°è·ç¦»: {euclidean_distance(a, c):.4f}")
```

---

## 3. å‘é‡æ•°æ®åº“

### 3.1 Faiss

```python
import faiss
import numpy as np

# å‡†å¤‡æ•°æ®
d = 384  # å‘é‡ç»´åº¦
nb = 10000  # æ•°æ®åº“å¤§å°
nq = 5  # æŸ¥è¯¢æ•°é‡

np.random.seed(42)
xb = np.random.random((nb, d)).astype('float32')  # æ•°æ®åº“å‘é‡
xq = np.random.random((nq, d)).astype('float32')  # æŸ¥è¯¢å‘é‡

# åˆ›å»ºç´¢å¼•
# 1. ç²¾ç¡®æœç´¢ï¼ˆé€‚åˆå°æ•°æ®é›†ï¼‰
index_flat = faiss.IndexFlatL2(d)

# 2. IVFï¼ˆé€‚åˆå¤§æ•°æ®é›†ï¼‰
nlist = 100  # èšç±»æ•°
quantizer = faiss.IndexFlatL2(d)
index_ivf = faiss.IndexIVFFlat(quantizer, d, nlist)
index_ivf.train(xb)  # éœ€è¦è®­ç»ƒ

# 3. HNSWï¼ˆé«˜ç²¾åº¦ï¼Œå†…å­˜æ¢é€Ÿåº¦ï¼‰
index_hnsw = faiss.IndexHNSWFlat(d, 32)  # 32 æ˜¯ M å‚æ•°

# æ·»åŠ å‘é‡
index_flat.add(xb)
print(f"ç´¢å¼•å¤§å°: {index_flat.ntotal}")

# æœç´¢
k = 4  # è¿”å›æœ€è¿‘çš„ k ä¸ª
D, I = index_flat.search(xq, k)
print(f"è·ç¦»:\n{D}")
print(f"ç´¢å¼•:\n{I}")

# å¸¦ ID çš„ç´¢å¼•
index_with_ids = faiss.IndexIDMap(faiss.IndexFlatL2(d))
ids = np.arange(nb).astype('int64')
index_with_ids.add_with_ids(xb, ids)
```

### 3.2 ChromaDB

```python
import chromadb
from chromadb.config import Settings

# åˆ›å»ºå®¢æˆ·ç«¯
client = chromadb.Client()
# æˆ–æŒä¹…åŒ–å­˜å‚¨
# client = chromadb.PersistentClient(path="./chroma_db")

# åˆ›å»ºé›†åˆ
collection = client.create_collection(
    name="my_collection",
    metadata={"hnsw:space": "cosine"}  # ä½¿ç”¨ä½™å¼¦ç›¸ä¼¼åº¦
)

# æ·»åŠ æ–‡æ¡£
collection.add(
    documents=[
        "This is a document about machine learning",
        "Deep learning is a subset of machine learning",
        "The weather is sunny today"
    ],
    metadatas=[
        {"source": "doc1"},
        {"source": "doc2"},
        {"source": "doc3"}
    ],
    ids=["id1", "id2", "id3"]
)

# æŸ¥è¯¢
results = collection.query(
    query_texts=["What is deep learning?"],
    n_results=2
)

print("æŸ¥è¯¢ç»“æœ:")
print(f"Documents: {results['documents']}")
print(f"Distances: {results['distances']}")
print(f"IDs: {results['ids']}")

# ä½¿ç”¨è‡ªå®šä¹‰ embedding
from sentence_transformers import SentenceTransformer

model = SentenceTransformer('all-MiniLM-L6-v2')

class CustomEmbedding:
    def __call__(self, texts):
        return model.encode(texts).tolist()

# collection = client.create_collection(
#     name="custom_embedding_collection",
#     embedding_function=CustomEmbedding()
# )
```

### 3.3 å‘é‡æ•°æ®åº“å¯¹æ¯”

```
| æ•°æ®åº“ | ç‰¹ç‚¹ | é€‚ç”¨åœºæ™¯ |
|--------|------|---------|
| Faiss | é«˜æ€§èƒ½ï¼ŒMeta å¼€å‘ | å¤§è§„æ¨¡ç›¸ä¼¼åº¦æœç´¢ |
| ChromaDB | ç®€å•æ˜“ç”¨ï¼Œå†…ç½® embedding | åŸå‹å¼€å‘ï¼Œå°è§„æ¨¡ |
| Pinecone | äº‘æœåŠ¡ï¼Œæ‰˜ç®¡ | ç”Ÿäº§ç¯å¢ƒï¼Œä¸æƒ³è¿ç»´ |
| Milvus | åˆ†å¸ƒå¼ï¼Œé«˜å¯ç”¨ | å¤§è§„æ¨¡ç”Ÿäº§ç¯å¢ƒ |
| Weaviate | æ”¯æŒæ··åˆæœç´¢ | éœ€è¦å…³é”®è¯+å‘é‡æœç´¢ |
| Qdrant | Rust å®ç°ï¼Œé«˜æ€§èƒ½ | é«˜æ€§èƒ½éœ€æ±‚ |
```

---

## 4. å®æˆ˜ï¼šè¯­ä¹‰æœç´¢

### 4.1 å®Œæ•´è¯­ä¹‰æœç´¢å¼•æ“

```python
from sentence_transformers import SentenceTransformer
import faiss
import numpy as np

class SemanticSearchEngine:
    def __init__(self, model_name='all-MiniLM-L6-v2'):
        self.model = SentenceTransformer(model_name)
        self.index = None
        self.documents = []

    def build_index(self, documents):
        """æ„å»ºç´¢å¼•"""
        self.documents = documents

        # è·å– embeddings
        embeddings = self.model.encode(documents, show_progress_bar=True)
        embeddings = embeddings.astype('float32')

        # å½’ä¸€åŒ–ï¼ˆç”¨äºä½™å¼¦ç›¸ä¼¼åº¦ï¼‰
        faiss.normalize_L2(embeddings)

        # åˆ›å»ºç´¢å¼•
        d = embeddings.shape[1]
        self.index = faiss.IndexFlatIP(d)  # IP = Inner Productï¼ˆå½’ä¸€åŒ–åç­‰äºä½™å¼¦ï¼‰
        self.index.add(embeddings)

        print(f"ç´¢å¼•æ„å»ºå®Œæˆï¼Œå…± {self.index.ntotal} ä¸ªæ–‡æ¡£")

    def search(self, query, k=5):
        """æœç´¢"""
        # æŸ¥è¯¢ embedding
        query_embedding = self.model.encode([query]).astype('float32')
        faiss.normalize_L2(query_embedding)

        # æœç´¢
        scores, indices = self.index.search(query_embedding, k)

        results = []
        for i, (score, idx) in enumerate(zip(scores[0], indices[0])):
            if idx < len(self.documents):
                results.append({
                    'rank': i + 1,
                    'score': float(score),
                    'document': self.documents[idx]
                })

        return results

    def save(self, path):
        """ä¿å­˜ç´¢å¼•"""
        faiss.write_index(self.index, f"{path}/index.faiss")
        np.save(f"{path}/documents.npy", self.documents)

    def load(self, path):
        """åŠ è½½ç´¢å¼•"""
        self.index = faiss.read_index(f"{path}/index.faiss")
        self.documents = np.load(f"{path}/documents.npy", allow_pickle=True).tolist()

# ä½¿ç”¨ç¤ºä¾‹
documents = [
    "Python is a popular programming language for machine learning.",
    "TensorFlow is an open-source machine learning framework by Google.",
    "PyTorch is developed by Facebook and is widely used in research.",
    "Natural language processing deals with text and speech data.",
    "Computer vision is about teaching computers to understand images.",
    "Reinforcement learning is about learning through trial and error.",
    "Deep learning uses neural networks with many layers.",
    "Transfer learning allows reusing pre-trained models.",
]

# åˆ›å»ºæœç´¢å¼•æ“
engine = SemanticSearchEngine()
engine.build_index(documents)

# æœç´¢
query = "What framework should I use for neural networks?"
results = engine.search(query, k=3)

print(f"\næŸ¥è¯¢: {query}")
print("-" * 50)
for r in results:
    print(f"[{r['rank']}] (score: {r['score']:.4f})")
    print(f"    {r['document']}")
```

### 4.2 æ··åˆæœç´¢

```python
from rank_bm25 import BM25Okapi
import numpy as np

class HybridSearchEngine:
    """æ··åˆæœç´¢ï¼šBM25 å…³é”®è¯åŒ¹é… + è¯­ä¹‰å‘é‡æœç´¢"""

    def __init__(self, model_name='all-MiniLM-L6-v2'):
        self.model = SentenceTransformer(model_name)
        self.documents = []
        self.bm25 = None
        self.embeddings = None

    def build_index(self, documents):
        self.documents = documents

        # BM25 ç´¢å¼•
        tokenized = [doc.lower().split() for doc in documents]
        self.bm25 = BM25Okapi(tokenized)

        # å‘é‡ç´¢å¼•
        self.embeddings = self.model.encode(documents)
        self.embeddings = self.embeddings / np.linalg.norm(self.embeddings, axis=1, keepdims=True)

    def search(self, query, k=5, alpha=0.5):
        """
        æ··åˆæœç´¢
        alpha: è¯­ä¹‰æœç´¢æƒé‡ï¼ˆ0-1ï¼‰
        """
        # BM25 åˆ†æ•°
        bm25_scores = self.bm25.get_scores(query.lower().split())
        bm25_scores = (bm25_scores - bm25_scores.min()) / (bm25_scores.max() - bm25_scores.min() + 1e-6)

        # è¯­ä¹‰åˆ†æ•°
        query_emb = self.model.encode([query])
        query_emb = query_emb / np.linalg.norm(query_emb)
        semantic_scores = np.dot(self.embeddings, query_emb.T).flatten()
        semantic_scores = (semantic_scores + 1) / 2  # å½’ä¸€åŒ–åˆ° [0, 1]

        # æ··åˆåˆ†æ•°
        hybrid_scores = alpha * semantic_scores + (1 - alpha) * bm25_scores

        # æ’åº
        top_indices = np.argsort(hybrid_scores)[::-1][:k]

        results = []
        for idx in top_indices:
            results.append({
                'document': self.documents[idx],
                'score': float(hybrid_scores[idx]),
                'bm25_score': float(bm25_scores[idx]),
                'semantic_score': float(semantic_scores[idx])
            })

        return results

# æµ‹è¯•
hybrid_engine = HybridSearchEngine()
hybrid_engine.build_index(documents)

results = hybrid_engine.search("Python deep learning", k=3, alpha=0.7)
print("\næ··åˆæœç´¢ç»“æœ:")
for r in results:
    print(f"Score: {r['score']:.4f} (BM25: {r['bm25_score']:.4f}, Semantic: {r['semantic_score']:.4f})")
    print(f"  {r['document']}")
```

---

## 5. ç»ƒä¹ é¢˜

### åŸºç¡€ç»ƒä¹ 

1. æ¯”è¾ƒä¸åŒ Embedding æ¨¡å‹åœ¨è¯­ä¹‰ç›¸ä¼¼åº¦ä»»åŠ¡ä¸Šçš„æ•ˆæœ
2. ç”¨ ChromaDB æ„å»ºä¸€ä¸ªç®€å•çš„æ–‡æ¡£æ£€ç´¢ç³»ç»Ÿ
3. å®ç°ä¸€ä¸ªæ”¯æŒå¢é‡æ›´æ–°çš„å‘é‡ç´¢å¼•

### å‚è€ƒç­”æ¡ˆ

<details>
<summary>ç‚¹å‡»æŸ¥çœ‹ç­”æ¡ˆ</summary>

```python
# 1. Embedding æ¨¡å‹å¯¹æ¯”
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np

def compare_models(sentences, similar_pairs, dissimilar_pairs):
    models = [
        'all-MiniLM-L6-v2',
        'all-mpnet-base-v2',
    ]

    for model_name in models:
        print(f"\n{model_name}:")
        model = SentenceTransformer(model_name)
        embeddings = model.encode(sentences)

        # ç›¸ä¼¼å¯¹çš„å¹³å‡ç›¸ä¼¼åº¦
        similar_scores = []
        for i, j in similar_pairs:
            sim = cosine_similarity([embeddings[i]], [embeddings[j]])[0][0]
            similar_scores.append(sim)

        # ä¸ç›¸ä¼¼å¯¹çš„å¹³å‡ç›¸ä¼¼åº¦
        dissimilar_scores = []
        for i, j in dissimilar_pairs:
            sim = cosine_similarity([embeddings[i]], [embeddings[j]])[0][0]
            dissimilar_scores.append(sim)

        print(f"  ç›¸ä¼¼å¯¹å¹³å‡åˆ†: {np.mean(similar_scores):.4f}")
        print(f"  ä¸ç›¸ä¼¼å¯¹å¹³å‡åˆ†: {np.mean(dissimilar_scores):.4f}")
        print(f"  åŒºåˆ†åº¦: {np.mean(similar_scores) - np.mean(dissimilar_scores):.4f}")

sentences = [
    "I love machine learning",      # 0
    "Machine learning is great",    # 1
    "The weather is nice today",    # 2
    "It's sunny outside",           # 3
]

similar_pairs = [(0, 1), (2, 3)]
dissimilar_pairs = [(0, 2), (0, 3), (1, 2), (1, 3)]

compare_models(sentences, similar_pairs, dissimilar_pairs)


# 2. ChromaDB æ–‡æ¡£æ£€ç´¢
import chromadb

client = chromadb.Client()
collection = client.create_collection("docs")

documents = [
    "Python tutorial for beginners",
    "Advanced machine learning techniques",
    "Introduction to deep learning",
    "Web development with Django",
]

collection.add(
    documents=documents,
    ids=[f"doc{i}" for i in range(len(documents))]
)

# æ£€ç´¢
results = collection.query(
    query_texts=["How to learn Python?"],
    n_results=2
)
print("æ£€ç´¢ç»“æœ:", results['documents'])


# 3. å¢é‡æ›´æ–°ç´¢å¼•
class IncrementalIndex:
    def __init__(self, model_name='all-MiniLM-L6-v2'):
        self.model = SentenceTransformer(model_name)
        self.documents = []
        self.embeddings = None

    def add(self, documents):
        """å¢é‡æ·»åŠ æ–‡æ¡£"""
        new_embeddings = self.model.encode(documents)

        if self.embeddings is None:
            self.embeddings = new_embeddings
        else:
            self.embeddings = np.vstack([self.embeddings, new_embeddings])

        self.documents.extend(documents)
        print(f"æ·»åŠ  {len(documents)} ä¸ªæ–‡æ¡£ï¼Œæ€»è®¡ {len(self.documents)} ä¸ª")

    def search(self, query, k=3):
        query_emb = self.model.encode([query])
        scores = cosine_similarity(query_emb, self.embeddings)[0]
        top_k = np.argsort(scores)[::-1][:k]
        return [(self.documents[i], scores[i]) for i in top_k]

# æµ‹è¯•
idx = IncrementalIndex()
idx.add(["Document 1", "Document 2"])
idx.add(["Document 3"])
print(idx.search("doc"))
```

</details>

---

## â¡ï¸ ä¸‹ä¸€æ­¥

å­¦å®Œæœ¬èŠ‚åï¼Œç»§ç»­å­¦ä¹  [09-å¤šæ¨¡æ€åŸºç¡€.md](./09-å¤šæ¨¡æ€åŸºç¡€.md)

