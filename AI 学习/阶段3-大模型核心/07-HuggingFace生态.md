# ğŸ¤— 07 - Hugging Face ç”Ÿæ€

> Hugging Face æ˜¯ NLP/LLM çš„æ ¸å¿ƒå·¥å…·åº“ï¼ŒæŒæ¡å®ƒèƒ½å¤§å¹…æå‡å¼€å‘æ•ˆç‡

---

## ç›®å½•

1. [Hugging Face æ¦‚è§ˆ](#1-hugging-face-æ¦‚è§ˆ)
2. [Transformers åº“](#2-transformers-åº“)
3. [Datasets åº“](#3-datasets-åº“)
4. [Model Hub](#4-model-hub)
5. [Pipeline å¿«é€Ÿæ¨ç†](#5-pipeline-å¿«é€Ÿæ¨ç†)
6. [æ¨¡å‹å¾®è°ƒ](#6-æ¨¡å‹å¾®è°ƒ)
7. [ç»ƒä¹ é¢˜](#7-ç»ƒä¹ é¢˜)

---

## 1. Hugging Face æ¦‚è§ˆ

### 1.1 æ ¸å¿ƒç»„ä»¶

```
Hugging Face ç”Ÿæ€ï¼š

1. transformers - é¢„è®­ç»ƒæ¨¡å‹åº“
   - åŠ è½½å„ç§ Transformer æ¨¡å‹
   - BERT, GPT, LLaMA, T5, Whisper...

2. datasets - æ•°æ®é›†åº“
   - åŠ è½½å…¬å¼€æ•°æ®é›†
   - é«˜æ•ˆæ•°æ®å¤„ç†

3. Model Hub - æ¨¡å‹ä»“åº“
   - å­˜å‚¨/åˆ†äº«æ¨¡å‹
   - è¶…è¿‡ 50 ä¸‡ä¸ªæ¨¡å‹

4. tokenizers - é«˜æ€§èƒ½åˆ†è¯å™¨
   - Rust å®ç°ï¼Œé€Ÿåº¦å¿«

5. accelerate - åˆ†å¸ƒå¼è®­ç»ƒ
   - ç®€åŒ–å¤š GPU è®­ç»ƒ

6. PEFT - é«˜æ•ˆå¾®è°ƒ
   - LoRA, Prefix Tuning
```

### 1.2 å®‰è£…

```bash
# æ ¸å¿ƒåº“
pip install transformers datasets tokenizers

# å¯é€‰
pip install accelerate  # åŠ é€Ÿ/åˆ†å¸ƒå¼
pip install peft       # é«˜æ•ˆå¾®è°ƒ
pip install evaluate   # è¯„ä¼°æŒ‡æ ‡
pip install sentencepiece  # æŸäº›æ¨¡å‹éœ€è¦
```

---

## 2. Transformers åº“

### 2.1 Auto ç±»ï¼ˆæ¨èæ–¹å¼ï¼‰

```python
from transformers import (
    AutoTokenizer,
    AutoModel,
    AutoModelForSequenceClassification,
    AutoModelForCausalLM,
    AutoModelForMaskedLM,
    AutoConfig
)

# è‡ªåŠ¨æ ¹æ®æ¨¡å‹ååŠ è½½æ­£ç¡®çš„ç±»
model_name = "bert-base-uncased"

# åŠ è½½ Tokenizer
tokenizer = AutoTokenizer.from_pretrained(model_name)

# åŠ è½½æ¨¡å‹ï¼ˆåŸºç¡€ç‰ˆï¼Œä¸å¸¦ä»»åŠ¡å¤´ï¼‰
model = AutoModel.from_pretrained(model_name)

# åŠ è½½åˆ†ç±»æ¨¡å‹
classifier = AutoModelForSequenceClassification.from_pretrained(
    model_name,
    num_labels=2
)

# åŠ è½½ç”Ÿæˆæ¨¡å‹
generator = AutoModelForCausalLM.from_pretrained("gpt2")

# åŠ è½½ MLM æ¨¡å‹
mlm_model = AutoModelForMaskedLM.from_pretrained(model_name)
```

### 2.2 æ¨¡å‹æ¨ç†

```python
from transformers import AutoTokenizer, AutoModel
import torch

# åŠ è½½
tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")
model = AutoModel.from_pretrained("bert-base-uncased")

# å‡†å¤‡è¾“å…¥
text = "Hello, how are you?"
inputs = tokenizer(text, return_tensors="pt")
print(f"Inputs: {inputs}")

# æ¨ç†
model.eval()
with torch.no_grad():
    outputs = model(**inputs)

# è¾“å‡º
print(f"Last hidden state: {outputs.last_hidden_state.shape}")
# [1, seq_len, 768]

print(f"Pooler output: {outputs.pooler_output.shape}")
# [1, 768] - [CLS] token çš„è¾“å‡º

# è·å–è¯å‘é‡
word_embeddings = outputs.last_hidden_state
print(f"ç¬¬ä¸€ä¸ª token çš„å‘é‡: {word_embeddings[0, 0, :5]}")
```

### 2.3 ç”Ÿæˆæ–‡æœ¬

```python
from transformers import AutoTokenizer, AutoModelForCausalLM

# åŠ è½½ GPT-2
tokenizer = AutoTokenizer.from_pretrained("gpt2")
model = AutoModelForCausalLM.from_pretrained("gpt2")

# å‡†å¤‡è¾“å…¥
prompt = "Once upon a time"
inputs = tokenizer(prompt, return_tensors="pt")

# ç”Ÿæˆ
outputs = model.generate(
    **inputs,
    max_new_tokens=50,
    num_return_sequences=1,
    temperature=0.8,
    top_p=0.9,
    do_sample=True,
    pad_token_id=tokenizer.eos_token_id
)

# è§£ç 
generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
print(generated_text)
```

### 2.4 ç”Ÿæˆå‚æ•°è¯¦è§£

```python
# ç”Ÿæˆç­–ç•¥å‚æ•°
generation_config = {
    # åŸºç¡€å‚æ•°
    "max_new_tokens": 100,       # æœ€å¤§ç”Ÿæˆ token æ•°
    "min_new_tokens": 10,        # æœ€å°ç”Ÿæˆ token æ•°

    # é‡‡æ ·å‚æ•°
    "do_sample": True,           # æ˜¯å¦é‡‡æ ·ï¼ˆFalse=è´ªå©ªï¼‰
    "temperature": 0.7,          # æ¸©åº¦ï¼ˆè¶Šé«˜è¶Šéšæœºï¼‰
    "top_k": 50,                 # Top-K é‡‡æ ·
    "top_p": 0.9,                # Top-P (Nucleus) é‡‡æ ·

    # é‡å¤æ§åˆ¶
    "repetition_penalty": 1.1,   # é‡å¤æƒ©ç½š
    "no_repeat_ngram_size": 3,   # ç¦æ­¢é‡å¤çš„ n-gram å¤§å°

    # æŸæœç´¢
    "num_beams": 5,              # æŸå®½åº¦
    "num_return_sequences": 3,   # è¿”å›åºåˆ—æ•°

    # åœæ­¢æ¡ä»¶
    "eos_token_id": tokenizer.eos_token_id,
    "pad_token_id": tokenizer.pad_token_id,
}

outputs = model.generate(**inputs, **generation_config)
```

---

## 3. Datasets åº“

### 3.1 åŠ è½½æ•°æ®é›†

```python
from datasets import load_dataset

# åŠ è½½å†…ç½®æ•°æ®é›†
dataset = load_dataset("imdb")
print(dataset)
# DatasetDict({
#     train: Dataset({features: ['text', 'label'], num_rows: 25000})
#     test: Dataset({features: ['text', 'label'], num_rows: 25000})
# })

# æŸ¥çœ‹æ ·æœ¬
print(dataset['train'][0])

# åŠ è½½æ•°æ®é›†çš„å­é›†
dataset = load_dataset("imdb", split="train[:1000]")

# ä» CSV åŠ è½½
# dataset = load_dataset("csv", data_files="data.csv")

# ä» JSON åŠ è½½
# dataset = load_dataset("json", data_files="data.json")

# åŠ è½½ä¸­æ–‡æ•°æ®é›†
# dataset = load_dataset("nlpcc/c3")
```

### 3.2 æ•°æ®å¤„ç†

```python
from datasets import load_dataset
from transformers import AutoTokenizer

# åŠ è½½æ•°æ®
dataset = load_dataset("imdb", split="train[:1000]")
tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")

# å®šä¹‰å¤„ç†å‡½æ•°
def tokenize_function(examples):
    return tokenizer(
        examples["text"],
        padding="max_length",
        truncation=True,
        max_length=256
    )

# åº”ç”¨å¤„ç†ï¼ˆæ‰¹é‡å¤„ç†ï¼Œé€Ÿåº¦å¿«ï¼‰
tokenized_dataset = dataset.map(
    tokenize_function,
    batched=True,
    remove_columns=["text"]  # åˆ é™¤åŸå§‹æ–‡æœ¬åˆ—
)

print(tokenized_dataset[0].keys())
# dict_keys(['label', 'input_ids', 'token_type_ids', 'attention_mask'])

# è®¾ç½®æ ¼å¼ä¸º PyTorch
tokenized_dataset.set_format("torch")
```

### 3.3 æ•°æ®é›†æ“ä½œ

```python
# è¿‡æ»¤
positive_reviews = dataset.filter(lambda x: x["label"] == 1)

# æ’åº
sorted_dataset = dataset.sort("label")

# æ‰“ä¹±
shuffled = dataset.shuffle(seed=42)

# é€‰æ‹©åˆ—
text_only = dataset.select_columns(["text"])

# åˆ‡åˆ†
train_test = dataset.train_test_split(test_size=0.2)
print(train_test)
# DatasetDict({
#     train: Dataset({...})
#     test: Dataset({...})
# })

# åˆå¹¶
from datasets import concatenate_datasets
combined = concatenate_datasets([dataset1, dataset2])
```

---

## 4. Model Hub

### 4.1 æµè§ˆå’Œæœç´¢

```python
from huggingface_hub import HfApi, list_models

# æœç´¢æ¨¡å‹
api = HfApi()
models = api.list_models(
    filter="text-classification",
    sort="downloads",
    direction=-1,
    limit=5
)

for model in models:
    print(f"{model.id}: {model.downloads} downloads")

# æœç´¢ç‰¹å®šæ¨¡å‹
# models = api.list_models(search="bert-base")
```

### 4.2 ä¸‹è½½å’Œä½¿ç”¨

```python
from transformers import AutoModel, AutoTokenizer

# ä» Hub åŠ è½½ï¼ˆè‡ªåŠ¨ä¸‹è½½å¹¶ç¼“å­˜ï¼‰
model_name = "sentence-transformers/all-MiniLM-L6-v2"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModel.from_pretrained(model_name)

# æŒ‡å®šç¼“å­˜ç›®å½•
model = AutoModel.from_pretrained(
    model_name,
    cache_dir="./model_cache"
)

# åŠ è½½ç‰¹å®šç‰ˆæœ¬
model = AutoModel.from_pretrained(
    model_name,
    revision="v1.0"  # æˆ– commit hash
)

# ä¿¡ä»»è¿œç¨‹ä»£ç ï¼ˆæŸäº›æ¨¡å‹éœ€è¦ï¼‰
model = AutoModel.from_pretrained(
    "some-model",
    trust_remote_code=True
)
```

### 4.3 ä¸Šä¼ æ¨¡å‹

```python
from huggingface_hub import HfApi, login

# ç™»å½•
login()  # æˆ–è®¾ç½® HF_TOKEN ç¯å¢ƒå˜é‡

# ä¸Šä¼ æ¨¡å‹
api = HfApi()

# æ–¹æ³• 1ï¼šä½¿ç”¨ push_to_hub
model.push_to_hub("my-username/my-model-name")
tokenizer.push_to_hub("my-username/my-model-name")

# æ–¹æ³• 2ï¼šä½¿ç”¨ API
api.upload_folder(
    folder_path="./my-model",
    repo_id="my-username/my-model-name",
    repo_type="model"
)
```

---

## 5. Pipeline å¿«é€Ÿæ¨ç†

### 5.1 å„ç§ Pipeline

```python
from transformers import pipeline

# æ–‡æœ¬åˆ†ç±»
classifier = pipeline("text-classification")
result = classifier("I love this movie!")
print(result)  # [{'label': 'POSITIVE', 'score': 0.9998}]

# æƒ…æ„Ÿåˆ†æï¼ˆæŒ‡å®šæ¨¡å‹ï¼‰
sentiment = pipeline(
    "sentiment-analysis",
    model="nlptown/bert-base-multilingual-uncased-sentiment"
)
print(sentiment("This product is amazing!"))

# å‘½åå®ä½“è¯†åˆ«
ner = pipeline("ner", grouped_entities=True)
print(ner("My name is John and I work at Google in New York."))

# é—®ç­”
qa = pipeline("question-answering")
result = qa(
    question="What is the capital of France?",
    context="France is a country in Europe. Paris is the capital of France."
)
print(result)

# æ–‡æœ¬ç”Ÿæˆ
generator = pipeline("text-generation", model="gpt2")
print(generator("Once upon a time", max_length=50))

# å¡«ç©º
fill_mask = pipeline("fill-mask", model="bert-base-uncased")
print(fill_mask("The capital of France is [MASK]."))

# æ‘˜è¦
summarizer = pipeline("summarization", model="facebook/bart-large-cnn")
text = "..." # é•¿æ–‡æœ¬
print(summarizer(text, max_length=130, min_length=30))

# ç¿»è¯‘
translator = pipeline("translation_en_to_fr", model="Helsinki-NLP/opus-mt-en-fr")
print(translator("Hello, how are you?"))

# é›¶æ ·æœ¬åˆ†ç±»
classifier = pipeline("zero-shot-classification")
result = classifier(
    "This is a tutorial about NLP",
    candidate_labels=["technology", "sports", "politics"]
)
print(result)
```

### 5.2 æ‰¹é‡å¤„ç†

```python
# Pipeline æ”¯æŒæ‰¹é‡å¤„ç†
classifier = pipeline("text-classification", device=0)  # GPU

texts = [
    "I love this product!",
    "This is terrible.",
    "It's okay, nothing special."
]

# æ‰¹é‡æ¨ç†
results = classifier(texts)
for text, result in zip(texts, results):
    print(f"{text}: {result}")

# ä½¿ç”¨ Dataset
from datasets import load_dataset
dataset = load_dataset("imdb", split="test[:100]")
results = classifier(dataset["text"])
```

---

## 6. æ¨¡å‹å¾®è°ƒ

### 6.1 ä½¿ç”¨ Trainer

```python
from transformers import (
    AutoTokenizer,
    AutoModelForSequenceClassification,
    Trainer,
    TrainingArguments,
    DataCollatorWithPadding
)
from datasets import load_dataset
import numpy as np
from sklearn.metrics import accuracy_score

# åŠ è½½æ•°æ®å’Œæ¨¡å‹
dataset = load_dataset("imdb")
tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")
model = AutoModelForSequenceClassification.from_pretrained(
    "bert-base-uncased",
    num_labels=2
)

# é¢„å¤„ç†
def tokenize_function(examples):
    return tokenizer(
        examples["text"],
        truncation=True,
        max_length=256
    )

tokenized_datasets = dataset.map(tokenize_function, batched=True)
data_collator = DataCollatorWithPadding(tokenizer=tokenizer)

# è¯„ä¼°æŒ‡æ ‡
def compute_metrics(eval_pred):
    logits, labels = eval_pred
    predictions = np.argmax(logits, axis=-1)
    return {"accuracy": accuracy_score(labels, predictions)}

# è®­ç»ƒå‚æ•°
training_args = TrainingArguments(
    output_dir="./results",
    num_train_epochs=3,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=64,
    warmup_steps=500,
    weight_decay=0.01,
    logging_dir="./logs",
    logging_steps=100,
    evaluation_strategy="epoch",
    save_strategy="epoch",
    load_best_model_at_end=True,
)

# åˆ›å»º Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_datasets["train"].select(range(1000)),  # æ¼”ç¤ºç”¨å°æ•°æ®
    eval_dataset=tokenized_datasets["test"].select(range(500)),
    tokenizer=tokenizer,
    data_collator=data_collator,
    compute_metrics=compute_metrics,
)

# è®­ç»ƒ
trainer.train()

# è¯„ä¼°
results = trainer.evaluate()
print(results)

# ä¿å­˜
trainer.save_model("./my_fine_tuned_model")
```

### 6.2 æ‰‹åŠ¨è®­ç»ƒå¾ªç¯

```python
import torch
from torch.utils.data import DataLoader
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from transformers import AdamW, get_scheduler
from tqdm import tqdm

# å‡†å¤‡æ•°æ®
tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")
model = AutoModelForSequenceClassification.from_pretrained(
    "bert-base-uncased", num_labels=2
)

# åˆ›å»º DataLoader
train_dataloader = DataLoader(
    tokenized_datasets["train"].select(range(1000)),
    shuffle=True,
    batch_size=16,
    collate_fn=data_collator
)

# ä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨
optimizer = AdamW(model.parameters(), lr=5e-5)
num_training_steps = len(train_dataloader) * 3
lr_scheduler = get_scheduler(
    "linear",
    optimizer=optimizer,
    num_warmup_steps=0,
    num_training_steps=num_training_steps
)

# è®­ç»ƒ
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

model.train()
for epoch in range(3):
    total_loss = 0
    progress_bar = tqdm(train_dataloader, desc=f"Epoch {epoch+1}")

    for batch in progress_bar:
        batch = {k: v.to(device) for k, v in batch.items()}

        outputs = model(**batch)
        loss = outputs.loss

        loss.backward()
        optimizer.step()
        lr_scheduler.step()
        optimizer.zero_grad()

        total_loss += loss.item()
        progress_bar.set_postfix({"loss": loss.item()})

    print(f"Epoch {epoch+1} Average Loss: {total_loss/len(train_dataloader):.4f}")
```

---

## 7. ç»ƒä¹ é¢˜

### åŸºç¡€ç»ƒä¹ 

1. ä½¿ç”¨ pipeline å®ç°ä¸€ä¸ªæƒ…æ„Ÿåˆ†ææœåŠ¡
2. åŠ è½½ IMDB æ•°æ®é›†å¹¶ç”¨ BERT å¾®è°ƒ
3. ç”¨ GPT-2 ç”Ÿæˆä¸€æ®µæ•…äº‹

### å‚è€ƒç­”æ¡ˆ

<details>
<summary>ç‚¹å‡»æŸ¥çœ‹ç­”æ¡ˆ</summary>

```python
# 1. æƒ…æ„Ÿåˆ†ææœåŠ¡
from transformers import pipeline

def sentiment_service():
    classifier = pipeline(
        "sentiment-analysis",
        model="distilbert-base-uncased-finetuned-sst-2-english"
    )

    while True:
        text = input("è¾“å…¥æ–‡æœ¬ï¼ˆq é€€å‡ºï¼‰: ")
        if text.lower() == 'q':
            break
        result = classifier(text)[0]
        print(f"æƒ…æ„Ÿ: {result['label']}, ç½®ä¿¡åº¦: {result['score']:.4f}")

# sentiment_service()


# 2. BERT å¾®è°ƒï¼ˆç®€åŒ–ç‰ˆï¼‰
from transformers import (
    AutoTokenizer,
    AutoModelForSequenceClassification,
    Trainer,
    TrainingArguments
)
from datasets import load_dataset

# åŠ è½½
dataset = load_dataset("imdb", split="train[:500]")
tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")
model = AutoModelForSequenceClassification.from_pretrained(
    "bert-base-uncased", num_labels=2
)

# å¤„ç†
def tokenize(examples):
    return tokenizer(examples["text"], truncation=True, max_length=128)

dataset = dataset.map(tokenize, batched=True)
dataset = dataset.train_test_split(test_size=0.1)

# è®­ç»ƒ
args = TrainingArguments(
    output_dir="./bert-imdb",
    num_train_epochs=1,
    per_device_train_batch_size=8,
    logging_steps=50,
)

trainer = Trainer(
    model=model,
    args=args,
    train_dataset=dataset["train"],
    eval_dataset=dataset["test"],
)

# trainer.train()


# 3. GPT-2 æ•…äº‹ç”Ÿæˆ
from transformers import pipeline

def generate_story(prompt, max_length=200):
    generator = pipeline("text-generation", model="gpt2")

    result = generator(
        prompt,
        max_length=max_length,
        num_return_sequences=1,
        temperature=0.8,
        top_p=0.9,
        do_sample=True
    )

    return result[0]['generated_text']

story = generate_story("In a magical forest, there lived a")
print(story)
```

</details>

---

## â¡ï¸ ä¸‹ä¸€æ­¥

å­¦å®Œæœ¬èŠ‚åï¼Œç»§ç»­å­¦ä¹  [08-Embeddingä¸å‘é‡æ£€ç´¢.md](./08-Embeddingä¸å‘é‡æ£€ç´¢.md)

