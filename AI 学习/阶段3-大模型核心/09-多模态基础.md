# ğŸ¨ 09 - å¤šæ¨¡æ€åŸºç¡€

> äº†è§£ Diffusion æ¨¡å‹ã€CLIPã€å¤šæ¨¡æ€ LLM çš„åŸºæœ¬åŸç†

---

## ç›®å½•

1. [å¤šæ¨¡æ€ AI æ¦‚è§ˆ](#1-å¤šæ¨¡æ€-ai-æ¦‚è§ˆ)
2. [CLIP å›¾æ–‡å¯¹é½](#2-clip-å›¾æ–‡å¯¹é½)
3. [Diffusion æ‰©æ•£æ¨¡å‹](#3-diffusion-æ‰©æ•£æ¨¡å‹)
4. [å¤šæ¨¡æ€ LLM](#4-å¤šæ¨¡æ€-llm)
5. [ç»ƒä¹ é¢˜](#5-ç»ƒä¹ é¢˜)

---

## 1. å¤šæ¨¡æ€ AI æ¦‚è§ˆ

### 1.1 ä»€ä¹ˆæ˜¯å¤šæ¨¡æ€

```
å•æ¨¡æ€ï¼šåªå¤„ç†ä¸€ç§æ•°æ®ç±»å‹
  - æ–‡æœ¬æ¨¡å‹ï¼šGPTã€BERT
  - å›¾åƒæ¨¡å‹ï¼šResNetã€ViT
  - éŸ³é¢‘æ¨¡å‹ï¼šWhisper

å¤šæ¨¡æ€ï¼šå¤„ç†å¤šç§æ•°æ®ç±»å‹
  - å›¾æ–‡ï¼šCLIPã€DALL-Eã€Stable Diffusion
  - æ–‡æœ¬+å›¾åƒç†è§£ï¼šGPT-4Vã€LLaVAã€Qwen-VL
  - æ–‡æœ¬+éŸ³é¢‘ï¼šGPT-4o
  - å…¨æ¨¡æ€ï¼šGemini
```

### 1.2 å¤šæ¨¡æ€ä»»åŠ¡

```
1. å›¾æ–‡åŒ¹é…ï¼šç»™å®šå›¾åƒå’Œæ–‡æœ¬ï¼Œåˆ¤æ–­æ˜¯å¦åŒ¹é…
2. å›¾åƒç”Ÿæˆï¼šæ ¹æ®æ–‡æœ¬ç”Ÿæˆå›¾åƒï¼ˆText-to-Imageï¼‰
3. å›¾åƒç†è§£ï¼šæè¿°å›¾åƒå†…å®¹ï¼ˆImage Captioningï¼‰
4. è§†è§‰é—®ç­”ï¼šæ ¹æ®å›¾åƒå›ç­”é—®é¢˜ï¼ˆVQAï¼‰
5. å›¾åƒç¼–è¾‘ï¼šæ ¹æ®æŒ‡ä»¤ç¼–è¾‘å›¾åƒ
```

---

## 2. CLIP å›¾æ–‡å¯¹é½

### 2.1 CLIP æ ¸å¿ƒæ€æƒ³

```
CLIP (Contrastive Language-Image Pre-training)

æ ¸å¿ƒæ€æƒ³ï¼šå­¦ä¹ å›¾åƒå’Œæ–‡æœ¬çš„å…±åŒè¡¨ç¤ºç©ºé—´

è®­ç»ƒï¼š
  - è¾“å…¥ï¼šå¤§é‡ (å›¾åƒ, æ–‡æœ¬) å¯¹
  - å›¾åƒç¼–ç å™¨ï¼šå›¾åƒ â†’ å‘é‡
  - æ–‡æœ¬ç¼–ç å™¨ï¼šæ–‡æœ¬ â†’ å‘é‡
  - ç›®æ ‡ï¼šåŒ¹é…çš„å›¾æ–‡å¯¹å‘é‡æ¥è¿‘ï¼Œä¸åŒ¹é…çš„è¿œç¦»

å¯¹æ¯”å­¦ä¹ ï¼š
  [å›¾åƒ1] â”€â”€â”€â”€â”€â”
  [å›¾åƒ2] â”€â”€â”  â”‚
  [å›¾åƒ3] â”€â”€â”¼â”€â”€â”¼â”€â”€â†’ ç›¸ä¼¼åº¦çŸ©é˜µ
  [æ–‡æœ¬1] â”€â”€â”¼â”€â”€â”˜
  [æ–‡æœ¬2] â”€â”€â”˜
  [æ–‡æœ¬3] â”€â”€â”€â”€â”€â”€â†’

å¯¹è§’çº¿åº”è¯¥æ˜¯é«˜ç›¸ä¼¼åº¦ï¼ˆåŒ¹é…å¯¹ï¼‰
```

### 2.2 ä½¿ç”¨ CLIP

```python
from transformers import CLIPProcessor, CLIPModel
from PIL import Image
import torch
import requests

# åŠ è½½æ¨¡å‹
model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32")
processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")

# å‡†å¤‡å›¾åƒ
url = "https://upload.wikimedia.org/wikipedia/commons/thumb/3/3a/Cat03.jpg/1200px-Cat03.jpg"
image = Image.open(requests.get(url, stream=True).raw)

# å‡†å¤‡æ–‡æœ¬é€‰é¡¹
texts = ["a photo of a cat", "a photo of a dog", "a photo of a bird"]

# å¤„ç†è¾“å…¥
inputs = processor(text=texts, images=image, return_tensors="pt", padding=True)

# å‰å‘ä¼ æ’­
outputs = model(**inputs)

# è·å–ç›¸ä¼¼åº¦
logits_per_image = outputs.logits_per_image  # [1, 3]
probs = logits_per_image.softmax(dim=1)

print("å›¾åƒ-æ–‡æœ¬ç›¸ä¼¼åº¦:")
for text, prob in zip(texts, probs[0]):
    print(f"  {text}: {prob:.4f}")
```

### 2.3 CLIP çš„åº”ç”¨

```python
# 1. é›¶æ ·æœ¬å›¾åƒåˆ†ç±»
def zero_shot_classify(image, class_names, model, processor):
    texts = [f"a photo of a {name}" for name in class_names]
    inputs = processor(text=texts, images=image, return_tensors="pt", padding=True)
    outputs = model(**inputs)
    probs = outputs.logits_per_image.softmax(dim=1)
    return {name: prob.item() for name, prob in zip(class_names, probs[0])}

# 2. å›¾åƒæœç´¢
def search_images(query, images, model, processor):
    """æ ¹æ®æ–‡æœ¬æœç´¢æœ€ç›¸å…³çš„å›¾åƒ"""
    # ç¼–ç æ–‡æœ¬
    text_inputs = processor(text=[query], return_tensors="pt", padding=True)
    text_features = model.get_text_features(**text_inputs)

    # ç¼–ç å›¾åƒ
    image_inputs = processor(images=images, return_tensors="pt")
    image_features = model.get_image_features(**image_inputs)

    # è®¡ç®—ç›¸ä¼¼åº¦
    text_features = text_features / text_features.norm(dim=-1, keepdim=True)
    image_features = image_features / image_features.norm(dim=-1, keepdim=True)
    similarities = (text_features @ image_features.T).squeeze(0)

    # è¿”å›æœ€ç›¸å…³çš„å›¾åƒç´¢å¼•
    return similarities.argsort(descending=True)

# 3. å›¾åƒ-å›¾åƒæœç´¢
def image_to_image_search(query_image, database_images, model, processor):
    """æ ¹æ®å›¾åƒæœç´¢ç›¸ä¼¼å›¾åƒ"""
    all_images = [query_image] + database_images
    inputs = processor(images=all_images, return_tensors="pt")
    features = model.get_image_features(**inputs)
    features = features / features.norm(dim=-1, keepdim=True)

    query_feature = features[0:1]
    db_features = features[1:]

    similarities = (query_feature @ db_features.T).squeeze(0)
    return similarities.argsort(descending=True)
```

---

## 3. Diffusion æ‰©æ•£æ¨¡å‹

### 3.1 æ ¸å¿ƒæ€æƒ³

```
æ‰©æ•£æ¨¡å‹ï¼šå­¦ä¹ ä»å™ªå£°ç”Ÿæˆå›¾åƒ

å‰å‘è¿‡ç¨‹ï¼ˆåŠ å™ªï¼‰ï¼š
  åŸå›¾ â†’ åŠ ä¸€ç‚¹å™ªå£° â†’ åŠ æ›´å¤šå™ªå£° â†’ ... â†’ çº¯å™ªå£°
  xâ‚€   â†’    xâ‚      â†’    xâ‚‚      â†’ ... â†’   xâ‚œ

é€†å‘è¿‡ç¨‹ï¼ˆå»å™ªï¼‰ï¼š
  çº¯å™ªå£° â†’ å»ä¸€ç‚¹å™ªå£° â†’ å»æ›´å¤šå™ªå£° â†’ ... â†’ æ¸…æ™°å›¾åƒ
   xâ‚œ    â†’   xâ‚œâ‚‹â‚    â†’   xâ‚œâ‚‹â‚‚    â†’ ... â†’   xâ‚€

è®­ç»ƒï¼šå­¦ä¹ é¢„æµ‹æ¯ä¸€æ­¥çš„å™ªå£°
ç”Ÿæˆï¼šä»çº¯å™ªå£°å¼€å§‹ï¼Œé€æ­¥å»å™ª
```

### 3.2 æ•°å­¦ç›´è§‰

```python
import torch
import numpy as np
import matplotlib.pyplot as plt

def forward_diffusion(x0, t, beta_schedule):
    """å‰å‘æ‰©æ•£ï¼šç»™å›¾åƒåŠ å™ªå£°"""
    # ç´¯ç§¯å™ªå£°ç³»æ•°
    alpha = 1 - beta_schedule
    alpha_cumprod = torch.cumprod(alpha, dim=0)

    sqrt_alpha_cumprod = torch.sqrt(alpha_cumprod[t])
    sqrt_one_minus_alpha_cumprod = torch.sqrt(1 - alpha_cumprod[t])

    # é‡‡æ ·å™ªå£°
    noise = torch.randn_like(x0)

    # åŠ å™ªåçš„å›¾åƒ
    xt = sqrt_alpha_cumprod * x0 + sqrt_one_minus_alpha_cumprod * noise

    return xt, noise

# å¯è§†åŒ–åŠ å™ªè¿‡ç¨‹
def visualize_diffusion():
    # åˆ›å»ºä¸€ä¸ªç®€å•çš„å›¾åƒ
    x0 = torch.zeros(1, 1, 32, 32)
    x0[:, :, 8:24, 8:24] = 1.0  # ç™½è‰²æ–¹å—

    # å®šä¹‰å™ªå£°è®¡åˆ’
    T = 1000
    beta = torch.linspace(0.0001, 0.02, T)

    # ä¸åŒæ—¶é—´æ­¥çš„å›¾åƒ
    fig, axes = plt.subplots(1, 6, figsize=(15, 3))
    timesteps = [0, 100, 250, 500, 750, 999]

    for ax, t in zip(axes, timesteps):
        xt, _ = forward_diffusion(x0, t, beta)
        ax.imshow(xt[0, 0].numpy(), cmap='gray')
        ax.set_title(f't={t}')
        ax.axis('off')

    plt.suptitle('Diffusion Forward Process')
    plt.show()

# visualize_diffusion()
```

### 3.3 ä½¿ç”¨ Stable Diffusion

```python
from diffusers import StableDiffusionPipeline
import torch

# åŠ è½½æ¨¡å‹
pipe = StableDiffusionPipeline.from_pretrained(
    "runwayml/stable-diffusion-v1-5",
    torch_dtype=torch.float16
)
pipe = pipe.to("cuda")

# ç”Ÿæˆå›¾åƒ
prompt = "a beautiful sunset over mountains, digital art, highly detailed"
image = pipe(prompt, num_inference_steps=50).images[0]
image.save("sunset.png")

# æ›´å¤šå‚æ•°
image = pipe(
    prompt=prompt,
    negative_prompt="blurry, low quality",  # è´Ÿé¢æç¤º
    num_inference_steps=50,  # å»å™ªæ­¥æ•°
    guidance_scale=7.5,      # æç¤ºå¼ºåº¦
    height=512,
    width=512,
).images[0]

# å›¾åƒåˆ°å›¾åƒ
from diffusers import StableDiffusionImg2ImgPipeline

img2img_pipe = StableDiffusionImg2ImgPipeline.from_pretrained(
    "runwayml/stable-diffusion-v1-5",
    torch_dtype=torch.float16
)
img2img_pipe = img2img_pipe.to("cuda")

# åŸºäºå·²æœ‰å›¾åƒç”Ÿæˆ
from PIL import Image
init_image = Image.open("input.png").resize((512, 512))
result = img2img_pipe(
    prompt="make it look like a painting",
    image=init_image,
    strength=0.75,  # å˜åŒ–ç¨‹åº¦
).images[0]
```

---

## 4. å¤šæ¨¡æ€ LLM

### 4.1 æ¶æ„æ¦‚è¿°

```
å¤šæ¨¡æ€ LLM çš„å…¸å‹æ¶æ„ï¼š

          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   å›¾åƒ â†’ â”‚  è§†è§‰ç¼–ç å™¨   â”‚ â†’ å›¾åƒç‰¹å¾
          â”‚  (CLIP ViT)   â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚
                â†“ æŠ•å½±å±‚
                â”‚
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   æ–‡æœ¬ â†’ â”‚     LLM       â”‚ â†’ æ–‡æœ¬è¾“å‡º
          â”‚ (LLaMA/Qwen)  â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ä»£è¡¨æ¨¡å‹ï¼š
- LLaVAï¼šCLIP + LLaMA
- Qwen-VLï¼šCLIP + Qwen
- GPT-4Vï¼šæœªå…¬å¼€æ¶æ„
```

### 4.2 ä½¿ç”¨å¤šæ¨¡æ€ LLM

```python
# ä½¿ç”¨ LLaVA
from transformers import LlavaProcessor, LlavaForConditionalGeneration
from PIL import Image
import requests

# åŠ è½½æ¨¡å‹
model = LlavaForConditionalGeneration.from_pretrained(
    "llava-hf/llava-1.5-7b-hf",
    torch_dtype=torch.float16,
    device_map="auto"
)
processor = LlavaProcessor.from_pretrained("llava-hf/llava-1.5-7b-hf")

# å‡†å¤‡è¾“å…¥
url = "https://example.com/image.jpg"
image = Image.open(requests.get(url, stream=True).raw)

prompt = "USER: <image>\nDescribe this image in detail.\nASSISTANT:"

# å¤„ç†
inputs = processor(text=prompt, images=image, return_tensors="pt").to("cuda")

# ç”Ÿæˆ
outputs = model.generate(**inputs, max_new_tokens=200)
response = processor.decode(outputs[0], skip_special_tokens=True)
print(response)
```

### 4.3 ä½¿ç”¨ Qwen-VL

```python
from transformers import AutoModelForCausalLM, AutoTokenizer

# åŠ è½½ Qwen-VL
tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen-VL-Chat", trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen-VL-Chat",
    device_map="cuda",
    trust_remote_code=True
).eval()

# å‡†å¤‡å¯¹è¯
query = tokenizer.from_list_format([
    {'image': 'path/to/image.jpg'},
    {'text': 'æè¿°è¿™å¼ å›¾ç‰‡'},
])

# ç”Ÿæˆ
response, history = model.chat(tokenizer, query=query, history=None)
print(response)

# å¤šè½®å¯¹è¯
response, history = model.chat(tokenizer, query='å›¾ç‰‡ä¸­æœ‰ä»€ä¹ˆåŠ¨ç‰©ï¼Ÿ', history=history)
print(response)
```

### 4.4 ç®€åŒ–çš„å¤šæ¨¡æ€æ¶æ„

```python
import torch
import torch.nn as nn
from transformers import CLIPModel, AutoModelForCausalLM, AutoTokenizer

class SimpleVLM(nn.Module):
    """ç®€åŒ–ç‰ˆå¤šæ¨¡æ€ LLM"""
    def __init__(self, clip_model_name, llm_model_name):
        super().__init__()

        # è§†è§‰ç¼–ç å™¨
        self.clip = CLIPModel.from_pretrained(clip_model_name)
        self.vision_encoder = self.clip.vision_model

        # å†»ç»“è§†è§‰ç¼–ç å™¨
        for param in self.vision_encoder.parameters():
            param.requires_grad = False

        # LLM
        self.llm = AutoModelForCausalLM.from_pretrained(llm_model_name)
        self.tokenizer = AutoTokenizer.from_pretrained(llm_model_name)

        # æŠ•å½±å±‚ï¼šå°†è§†è§‰ç‰¹å¾æ˜ å°„åˆ° LLM çš„ç»´åº¦
        vision_dim = self.clip.config.vision_config.hidden_size
        llm_dim = self.llm.config.hidden_size
        self.vision_projector = nn.Linear(vision_dim, llm_dim)

    def encode_image(self, pixel_values):
        """ç¼–ç å›¾åƒ"""
        vision_outputs = self.vision_encoder(pixel_values)
        image_features = vision_outputs.last_hidden_state  # [batch, num_patches, vision_dim]
        projected = self.vision_projector(image_features)  # [batch, num_patches, llm_dim]
        return projected

    def forward(self, pixel_values, input_ids, attention_mask):
        """å‰å‘ä¼ æ’­"""
        # ç¼–ç å›¾åƒ
        image_embeds = self.encode_image(pixel_values)

        # è·å–æ–‡æœ¬åµŒå…¥
        text_embeds = self.llm.get_input_embeddings()(input_ids)

        # æ‹¼æ¥å›¾åƒå’Œæ–‡æœ¬åµŒå…¥
        # [image_tokens, text_tokens]
        inputs_embeds = torch.cat([image_embeds, text_embeds], dim=1)

        # æ›´æ–° attention mask
        image_attention = torch.ones(
            image_embeds.shape[:2],
            dtype=attention_mask.dtype,
            device=attention_mask.device
        )
        full_attention_mask = torch.cat([image_attention, attention_mask], dim=1)

        # LLM å‰å‘ä¼ æ’­
        outputs = self.llm(
            inputs_embeds=inputs_embeds,
            attention_mask=full_attention_mask
        )

        return outputs

# æ³¨æ„ï¼šè¿™åªæ˜¯æ¦‚å¿µæ¼”ç¤ºï¼Œå®é™…å®ç°æ›´å¤æ‚
```

---

## 5. ç»ƒä¹ é¢˜

### åŸºç¡€ç»ƒä¹ 

1. ç”¨ CLIP å®ç°é›¶æ ·æœ¬å›¾åƒåˆ†ç±»
2. ç†è§£ Diffusion çš„å‰å‘å’Œé€†å‘è¿‡ç¨‹
3. å°è¯•ä½¿ç”¨å¤šæ¨¡æ€ LLM æè¿°å›¾åƒ

### å‚è€ƒç­”æ¡ˆ

<details>
<summary>ç‚¹å‡»æŸ¥çœ‹ç­”æ¡ˆ</summary>

```python
# 1. CLIP é›¶æ ·æœ¬åˆ†ç±»
from transformers import CLIPProcessor, CLIPModel
from PIL import Image
import torch

model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32")
processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")

def zero_shot_classify(image_path, classes):
    image = Image.open(image_path)
    texts = [f"a photo of a {c}" for c in classes]

    inputs = processor(text=texts, images=image, return_tensors="pt", padding=True)
    outputs = model(**inputs)
    probs = outputs.logits_per_image.softmax(dim=1)[0]

    results = {c: p.item() for c, p in zip(classes, probs)}
    return sorted(results.items(), key=lambda x: -x[1])

# æµ‹è¯•
classes = ["cat", "dog", "bird", "car", "airplane"]
# results = zero_shot_classify("test.jpg", classes)
# print(results)


# 2. Diffusion å‰å‘è¿‡ç¨‹å¯è§†åŒ–
import torch
import matplotlib.pyplot as plt

def simple_diffusion_demo():
    # ç®€å•çš„ 1D æ•°æ®
    x0 = torch.tensor([1.0])  # åŸå§‹æ•°æ®ç‚¹

    T = 100  # æ—¶é—´æ­¥
    betas = torch.linspace(0.0001, 0.02, T)
    alphas = 1 - betas
    alpha_cumprod = torch.cumprod(alphas, dim=0)

    # å‰å‘è¿‡ç¨‹
    trajectory = [x0.item()]
    for t in range(T):
        noise = torch.randn_like(x0)
        x_t = torch.sqrt(alpha_cumprod[t]) * x0 + torch.sqrt(1 - alpha_cumprod[t]) * noise
        trajectory.append(x_t.item())

    plt.figure(figsize=(10, 4))
    plt.plot(trajectory, label='Forward diffusion')
    plt.xlabel('Time step')
    plt.ylabel('Value')
    plt.title('Diffusion Forward Process')
    plt.axhline(y=0, color='r', linestyle='--', label='Mean (pure noise)')
    plt.legend()
    plt.show()

# simple_diffusion_demo()


# 3. å¤šæ¨¡æ€ LLMï¼ˆæ¦‚å¿µä»£ç ï¼‰
"""
# ä½¿ç”¨ Hugging Face Transformers
from transformers import LlavaProcessor, LlavaForConditionalGeneration

model = LlavaForConditionalGeneration.from_pretrained("llava-hf/llava-1.5-7b-hf")
processor = LlavaProcessor.from_pretrained("llava-hf/llava-1.5-7b-hf")

image = Image.open("image.jpg")
prompt = "Describe this image:"

inputs = processor(text=prompt, images=image, return_tensors="pt")
outputs = model.generate(**inputs, max_new_tokens=100)
print(processor.decode(outputs[0]))
"""
print("å¤šæ¨¡æ€ LLM éœ€è¦è¾ƒå¤§çš„æ˜¾å­˜ï¼Œå»ºè®®åœ¨ GPU ä¸Šè¿è¡Œ")
```

</details>

---

## â¡ï¸ ä¸‹ä¸€æ­¥

å­¦å®Œæœ¬èŠ‚åï¼Œç»§ç»­å­¦ä¹  [10-è§£ç ç­–ç•¥ä¸é‡‡æ ·.md](./10-è§£ç ç­–ç•¥ä¸é‡‡æ ·.md)

