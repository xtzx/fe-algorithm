# ğŸ­ 12 - é¡¹ç›®ï¼šTransformer æƒ…æ„Ÿåˆ†ç±»

> å…¥é—¨çº§é¡¹ç›®ï¼šä»é›¶å®ç°ä¸€ä¸ª Transformer åˆ†ç±»å™¨ï¼Œç†è§£æ ¸å¿ƒç»„ä»¶

---

## ç›®å½•

1. [é¡¹ç›®æ¦‚è¿°](#1-é¡¹ç›®æ¦‚è¿°)
2. [æ•°æ®å‡†å¤‡](#2-æ•°æ®å‡†å¤‡)
3. [ä»é›¶å®ç° Transformer](#3-ä»é›¶å®ç°-transformer)
4. [è®­ç»ƒä¸è¯„ä¼°](#4-è®­ç»ƒä¸è¯„ä¼°)
5. [ç»“æœåˆ†æ](#5-ç»“æœåˆ†æ)
6. [æ‰©å±•ä»»åŠ¡](#6-æ‰©å±•ä»»åŠ¡)

---

## 1. é¡¹ç›®æ¦‚è¿°

### 1.1 ä»»åŠ¡è¯´æ˜

```
ä»»åŠ¡ï¼šæ–‡æœ¬æƒ…æ„Ÿåˆ†ç±»ï¼ˆæ­£é¢/è´Ÿé¢ï¼‰
æ•°æ®é›†ï¼šIMDB ç”µå½±è¯„è®º
éš¾åº¦ï¼šâ­â­ï¼ˆå…¥é—¨çº§ï¼‰

ç›®æ ‡ï¼š
1. ä»é›¶å®ç° Transformer Encoder
2. ç†è§£ Self-Attention çš„å®é™…åº”ç”¨
3. å®Œæˆå®Œæ•´çš„è®­ç»ƒå’Œè¯„ä¼°æµç¨‹

è¿™æ˜¯è¿›å…¥ BERT/GPT å‰çš„å¿…å¤‡ç»ƒä¹ ï¼
```

### 1.2 é¡¹ç›®ç»“æ„

```
transformer_sentiment/
â”œâ”€â”€ model.py          # Transformer å®ç°
â”œâ”€â”€ dataset.py        # æ•°æ®å¤„ç†
â”œâ”€â”€ train.py          # è®­ç»ƒè„šæœ¬
â”œâ”€â”€ evaluate.py       # è¯„ä¼°è„šæœ¬
â””â”€â”€ checkpoints/      # æ¨¡å‹ä¿å­˜
```

---

## 2. æ•°æ®å‡†å¤‡

```python
"""
Transformer æƒ…æ„Ÿåˆ†ç±»é¡¹ç›®
ä»é›¶å®ç° Transformer Encoderï¼Œå®Œæˆæ–‡æœ¬åˆ†ç±»
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
import numpy as np
import matplotlib.pyplot as plt
from collections import Counter
import re

# è®¾ç½®éšæœºç§å­
torch.manual_seed(42)
np.random.seed(42)

# è®¾å¤‡
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"ä½¿ç”¨è®¾å¤‡: {device}")

# ============================================================
# 2. æ•°æ®å‡†å¤‡
# ============================================================
print("\n" + "=" * 60)
print("1. æ•°æ®å‡†å¤‡")
print("=" * 60)

# ä½¿ç”¨ç®€å•çš„ç¤ºä¾‹æ•°æ®ï¼ˆå®é™…é¡¹ç›®å¯ä½¿ç”¨ IMDBï¼‰
positive_samples = [
    "I love this movie it is great",
    "This film is wonderful and amazing",
    "Excellent performance by the actors",
    "A masterpiece of cinema",
    "Highly recommended great story",
    "The best movie I have ever seen",
    "Absolutely brilliant and moving",
    "Perfect in every way loved it",
    "Outstanding film with great acting",
    "A beautiful and touching story",
] * 100  # æ‰©å±•æ•°æ®

negative_samples = [
    "This movie is terrible and boring",
    "Worst film I have ever watched",
    "Complete waste of time and money",
    "Awful acting and bad script",
    "I hated every minute of it",
    "Disappointing and poorly made",
    "Do not watch this garbage",
    "Terrible plot and bad acting",
    "A total disaster of a movie",
    "Boring and uninteresting story",
] * 100

# åˆå¹¶æ•°æ®
texts = positive_samples + negative_samples
labels = [1] * len(positive_samples) + [0] * len(negative_samples)

# æ‰“ä¹±æ•°æ®
indices = np.random.permutation(len(texts))
texts = [texts[i] for i in indices]
labels = [labels[i] for i in indices]

print(f"æ€»æ ·æœ¬æ•°: {len(texts)}")
print(f"æ­£é¢æ ·æœ¬: {sum(labels)}")
print(f"è´Ÿé¢æ ·æœ¬: {len(labels) - sum(labels)}")

# ============================================================
# æ„å»ºè¯è¡¨
# ============================================================
def tokenize(text):
    """ç®€å•åˆ†è¯"""
    text = text.lower()
    text = re.sub(r'[^a-z\s]', '', text)
    return text.split()

# æ„å»ºè¯è¡¨
word_freq = Counter()
for text in texts:
    word_freq.update(tokenize(text))

# ç‰¹æ®Š token
PAD_TOKEN = "<PAD>"
UNK_TOKEN = "<UNK>"
CLS_TOKEN = "<CLS>"

# åˆ›å»ºè¯è¡¨
vocab = {PAD_TOKEN: 0, UNK_TOKEN: 1, CLS_TOKEN: 2}
for word, _ in word_freq.most_common(10000):
    if word not in vocab:
        vocab[word] = len(vocab)

print(f"è¯è¡¨å¤§å°: {len(vocab)}")

# ============================================================
# æ•°æ®é›†ç±»
# ============================================================
class SentimentDataset(Dataset):
    def __init__(self, texts, labels, vocab, max_len=32):
        self.texts = texts
        self.labels = labels
        self.vocab = vocab
        self.max_len = max_len

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        text = self.texts[idx]
        label = self.labels[idx]

        # åˆ†è¯
        tokens = [CLS_TOKEN] + tokenize(text)[:self.max_len - 1]

        # è½¬æ¢ä¸º ID
        token_ids = [self.vocab.get(t, self.vocab[UNK_TOKEN]) for t in tokens]

        # å¡«å……
        padding_len = self.max_len - len(token_ids)
        token_ids = token_ids + [self.vocab[PAD_TOKEN]] * padding_len

        # æ³¨æ„åŠ›æ©ç 
        attention_mask = [1] * (self.max_len - padding_len) + [0] * padding_len

        return {
            'input_ids': torch.tensor(token_ids, dtype=torch.long),
            'attention_mask': torch.tensor(attention_mask, dtype=torch.long),
            'label': torch.tensor(label, dtype=torch.long)
        }

# åˆ’åˆ†æ•°æ®é›†
train_size = int(0.8 * len(texts))
train_texts, val_texts = texts[:train_size], texts[train_size:]
train_labels, val_labels = labels[:train_size], labels[train_size:]

train_dataset = SentimentDataset(train_texts, train_labels, vocab)
val_dataset = SentimentDataset(val_texts, val_labels, vocab)

train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=64)

print(f"è®­ç»ƒé›†: {len(train_dataset)}")
print(f"éªŒè¯é›†: {len(val_dataset)}")

# æŸ¥çœ‹ä¸€ä¸ªæ ·æœ¬
sample = train_dataset[0]
print(f"\næ ·æœ¬ç¤ºä¾‹:")
print(f"  input_ids: {sample['input_ids'][:10]}...")
print(f"  attention_mask: {sample['attention_mask'][:10]}...")
print(f"  label: {sample['label']}")
```

---

## 3. ä»é›¶å®ç° Transformer

### 3.1 ä½ç½®ç¼–ç 

```python
# ============================================================
# 3. ä»é›¶å®ç° Transformer
# ============================================================
print("\n" + "=" * 60)
print("2. ä»é›¶å®ç° Transformer")
print("=" * 60)

class PositionalEncoding(nn.Module):
    """æ­£å¼¦ä½ç½®ç¼–ç """
    def __init__(self, d_model, max_len=512):
        super().__init__()

        # åˆ›å»ºä½ç½®ç¼–ç çŸ©é˜µ
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))

        pe[:, 0::2] = torch.sin(position * div_term)  # å¶æ•°ä½ç½®
        pe[:, 1::2] = torch.cos(position * div_term)  # å¥‡æ•°ä½ç½®

        pe = pe.unsqueeze(0)  # [1, max_len, d_model]
        self.register_buffer('pe', pe)

    def forward(self, x):
        # x: [batch, seq_len, d_model]
        return x + self.pe[:, :x.size(1), :]
```

### 3.2 Multi-Head Attention

```python
class MultiHeadAttention(nn.Module):
    """å¤šå¤´è‡ªæ³¨æ„åŠ›"""
    def __init__(self, d_model, n_heads, dropout=0.1):
        super().__init__()
        assert d_model % n_heads == 0

        self.d_model = d_model
        self.n_heads = n_heads
        self.head_dim = d_model // n_heads

        self.q_proj = nn.Linear(d_model, d_model)
        self.k_proj = nn.Linear(d_model, d_model)
        self.v_proj = nn.Linear(d_model, d_model)
        self.out_proj = nn.Linear(d_model, d_model)

        self.dropout = nn.Dropout(dropout)
        self.scale = self.head_dim ** -0.5

    def forward(self, x, attention_mask=None):
        B, T, D = x.shape

        # è®¡ç®— Q, K, V
        Q = self.q_proj(x).view(B, T, self.n_heads, self.head_dim).transpose(1, 2)
        K = self.k_proj(x).view(B, T, self.n_heads, self.head_dim).transpose(1, 2)
        V = self.v_proj(x).view(B, T, self.n_heads, self.head_dim).transpose(1, 2)

        # è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°
        scores = torch.matmul(Q, K.transpose(-2, -1)) * self.scale

        # åº”ç”¨æ©ç 
        if attention_mask is not None:
            # attention_mask: [B, T] -> [B, 1, 1, T]
            mask = attention_mask.unsqueeze(1).unsqueeze(2)
            scores = scores.masked_fill(mask == 0, float('-inf'))

        # Softmax + Dropout
        attn_weights = F.softmax(scores, dim=-1)
        attn_weights = self.dropout(attn_weights)

        # åŠ æƒæ±‚å’Œ
        out = torch.matmul(attn_weights, V)

        # åˆå¹¶å¤šå¤´
        out = out.transpose(1, 2).contiguous().view(B, T, D)
        out = self.out_proj(out)

        return out, attn_weights
```

### 3.3 Feed Forward Network

```python
class FeedForward(nn.Module):
    """å‰é¦ˆç½‘ç»œ"""
    def __init__(self, d_model, d_ff, dropout=0.1):
        super().__init__()
        self.fc1 = nn.Linear(d_model, d_ff)
        self.fc2 = nn.Linear(d_ff, d_model)
        self.dropout = nn.Dropout(dropout)

    def forward(self, x):
        x = self.fc1(x)
        x = F.gelu(x)  # ä½¿ç”¨ GELU æ¿€æ´»
        x = self.dropout(x)
        x = self.fc2(x)
        return x
```

### 3.4 Transformer Encoder Layer

```python
class TransformerEncoderLayer(nn.Module):
    """Transformer Encoder å±‚"""
    def __init__(self, d_model, n_heads, d_ff, dropout=0.1):
        super().__init__()
        self.attn = MultiHeadAttention(d_model, n_heads, dropout)
        self.ff = FeedForward(d_model, d_ff, dropout)
        self.norm1 = nn.LayerNorm(d_model)
        self.norm2 = nn.LayerNorm(d_model)
        self.dropout1 = nn.Dropout(dropout)
        self.dropout2 = nn.Dropout(dropout)

    def forward(self, x, attention_mask=None):
        # Self-Attention + æ®‹å·®è¿æ¥
        attn_out, attn_weights = self.attn(self.norm1(x), attention_mask)
        x = x + self.dropout1(attn_out)

        # FFN + æ®‹å·®è¿æ¥
        ff_out = self.ff(self.norm2(x))
        x = x + self.dropout2(ff_out)

        return x, attn_weights
```

### 3.5 å®Œæ•´çš„ Transformer åˆ†ç±»å™¨

```python
class TransformerClassifier(nn.Module):
    """Transformer æ–‡æœ¬åˆ†ç±»å™¨"""
    def __init__(self, vocab_size, d_model=128, n_heads=4, n_layers=2,
                 d_ff=256, num_classes=2, max_len=128, dropout=0.1):
        super().__init__()

        self.d_model = d_model

        # è¯åµŒå…¥
        self.embedding = nn.Embedding(vocab_size, d_model, padding_idx=0)

        # ä½ç½®ç¼–ç 
        self.pos_encoding = PositionalEncoding(d_model, max_len)

        # Transformer å±‚
        self.layers = nn.ModuleList([
            TransformerEncoderLayer(d_model, n_heads, d_ff, dropout)
            for _ in range(n_layers)
        ])

        # åˆ†ç±»å¤´
        self.norm = nn.LayerNorm(d_model)
        self.classifier = nn.Linear(d_model, num_classes)

        self.dropout = nn.Dropout(dropout)

    def forward(self, input_ids, attention_mask=None):
        # è¯åµŒå…¥
        x = self.embedding(input_ids)  # [B, T, D]
        x = x * (self.d_model ** 0.5)  # ç¼©æ”¾

        # ä½ç½®ç¼–ç 
        x = self.pos_encoding(x)
        x = self.dropout(x)

        # Transformer å±‚
        all_attn_weights = []
        for layer in self.layers:
            x, attn_weights = layer(x, attention_mask)
            all_attn_weights.append(attn_weights)

        x = self.norm(x)

        # å– [CLS] token çš„è¡¨ç¤º
        cls_output = x[:, 0, :]  # [B, D]

        # åˆ†ç±»
        logits = self.classifier(cls_output)  # [B, num_classes]

        return logits, all_attn_weights

# åˆ›å»ºæ¨¡å‹
model = TransformerClassifier(
    vocab_size=len(vocab),
    d_model=128,
    n_heads=4,
    n_layers=2,
    d_ff=256,
    num_classes=2,
    max_len=32,
    dropout=0.1
).to(device)

# å‚æ•°ç»Ÿè®¡
total_params = sum(p.numel() for p in model.parameters())
print(f"æ¨¡å‹å‚æ•°é‡: {total_params:,}")
print(model)

# æµ‹è¯•å‰å‘ä¼ æ’­
sample_batch = next(iter(train_loader))
sample_input = sample_batch['input_ids'].to(device)
sample_mask = sample_batch['attention_mask'].to(device)
sample_output, _ = model(sample_input, sample_mask)
print(f"\nè¾“å…¥å½¢çŠ¶: {sample_input.shape}")
print(f"è¾“å‡ºå½¢çŠ¶: {sample_output.shape}")
```

---

## 4. è®­ç»ƒä¸è¯„ä¼°

```python
# ============================================================
# 4. è®­ç»ƒä¸è¯„ä¼°
# ============================================================
print("\n" + "=" * 60)
print("3. è®­ç»ƒä¸è¯„ä¼°")
print("=" * 60)

def train_one_epoch(model, train_loader, criterion, optimizer, device):
    model.train()
    total_loss = 0
    correct = 0
    total = 0

    for batch in train_loader:
        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        labels = batch['label'].to(device)

        optimizer.zero_grad()

        logits, _ = model(input_ids, attention_mask)
        loss = criterion(logits, labels)

        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)  # æ¢¯åº¦è£å‰ª
        optimizer.step()

        total_loss += loss.item()
        preds = logits.argmax(dim=1)
        correct += (preds == labels).sum().item()
        total += labels.size(0)

    return total_loss / len(train_loader), correct / total


def evaluate(model, val_loader, criterion, device):
    model.eval()
    total_loss = 0
    correct = 0
    total = 0

    with torch.no_grad():
        for batch in val_loader:
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            labels = batch['label'].to(device)

            logits, _ = model(input_ids, attention_mask)
            loss = criterion(logits, labels)

            total_loss += loss.item()
            preds = logits.argmax(dim=1)
            correct += (preds == labels).sum().item()
            total += labels.size(0)

    return total_loss / len(val_loader), correct / total


# è®­ç»ƒè®¾ç½®
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=0.01)
scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=20)

# è®­ç»ƒå¾ªç¯
num_epochs = 20
train_losses, val_losses = [], []
train_accs, val_accs = [], []

print("\nå¼€å§‹è®­ç»ƒ...")
print("-" * 50)

for epoch in range(num_epochs):
    train_loss, train_acc = train_one_epoch(model, train_loader, criterion, optimizer, device)
    val_loss, val_acc = evaluate(model, val_loader, criterion, device)
    scheduler.step()

    train_losses.append(train_loss)
    val_losses.append(val_loss)
    train_accs.append(train_acc)
    val_accs.append(val_acc)

    print(f"Epoch {epoch+1:2d}/{num_epochs}: "
          f"Train Loss={train_loss:.4f}, Train Acc={train_acc:.4f}, "
          f"Val Loss={val_loss:.4f}, Val Acc={val_acc:.4f}")

print(f"\næœ€ç»ˆéªŒè¯å‡†ç¡®ç‡: {val_accs[-1]:.4f}")
```

---

## 5. ç»“æœåˆ†æ

### 5.1 è®­ç»ƒæ›²çº¿

```python
# ============================================================
# 5. ç»“æœåˆ†æ
# ============================================================
print("\n" + "=" * 60)
print("4. ç»“æœåˆ†æ")
print("=" * 60)

# ç»˜åˆ¶è®­ç»ƒæ›²çº¿
fig, axes = plt.subplots(1, 2, figsize=(14, 5))

# Loss æ›²çº¿
axes[0].plot(train_losses, label='Train', linewidth=2)
axes[0].plot(val_losses, label='Validation', linewidth=2)
axes[0].set_xlabel('Epoch')
axes[0].set_ylabel('Loss')
axes[0].set_title('Loss Curves')
axes[0].legend()
axes[0].grid(True)

# Accuracy æ›²çº¿
axes[1].plot(train_accs, label='Train', linewidth=2)
axes[1].plot(val_accs, label='Validation', linewidth=2)
axes[1].set_xlabel('Epoch')
axes[1].set_ylabel('Accuracy')
axes[1].set_title('Accuracy Curves')
axes[1].legend()
axes[1].grid(True)

plt.tight_layout()
plt.savefig('transformer_training_curves.png', dpi=150)
plt.show()
```

### 5.2 æ³¨æ„åŠ›å¯è§†åŒ–

```python
def visualize_attention(model, text, vocab, device):
    """å¯è§†åŒ–æ³¨æ„åŠ›æƒé‡"""
    model.eval()

    # åˆ†è¯å’Œç¼–ç 
    tokens = [CLS_TOKEN] + tokenize(text)[:31]
    token_ids = [vocab.get(t, vocab[UNK_TOKEN]) for t in tokens]
    padding_len = 32 - len(token_ids)
    token_ids = token_ids + [vocab[PAD_TOKEN]] * padding_len
    attention_mask = [1] * (32 - padding_len) + [0] * padding_len

    input_ids = torch.tensor([token_ids], dtype=torch.long).to(device)
    mask = torch.tensor([attention_mask], dtype=torch.long).to(device)

    # å‰å‘ä¼ æ’­
    with torch.no_grad():
        logits, attn_weights = model(input_ids, mask)

    # é¢„æµ‹ç»“æœ
    pred = logits.argmax(dim=1).item()
    prob = F.softmax(logits, dim=1)[0, pred].item()
    sentiment = "æ­£é¢" if pred == 1 else "è´Ÿé¢"

    print(f"æ–‡æœ¬: {text}")
    print(f"é¢„æµ‹: {sentiment} (ç½®ä¿¡åº¦: {prob:.2%})")

    # å¯è§†åŒ–æœ€åä¸€å±‚ç¬¬ä¸€ä¸ªå¤´çš„æ³¨æ„åŠ›
    attn = attn_weights[-1][0, 0].cpu().numpy()  # [T, T]

    # åªæ˜¾ç¤ºæœ‰æ•ˆ token
    valid_len = len(tokens)
    attn = attn[:valid_len, :valid_len]

    plt.figure(figsize=(10, 8))
    plt.imshow(attn, cmap='Blues')
    plt.colorbar()
    plt.xticks(range(valid_len), tokens, rotation=45, ha='right')
    plt.yticks(range(valid_len), tokens)
    plt.xlabel('Key')
    plt.ylabel('Query')
    plt.title(f'Attention Weights (Pred: {sentiment})')
    plt.tight_layout()
    plt.savefig('attention_visualization.png', dpi=150)
    plt.show()

# å¯è§†åŒ–ç¤ºä¾‹
visualize_attention(model, "I love this amazing movie", vocab, device)
visualize_attention(model, "This movie is terrible", vocab, device)
```

### 5.3 é¢„æµ‹æµ‹è¯•

```python
def predict(model, text, vocab, device):
    """é¢„æµ‹å•æ¡æ–‡æœ¬"""
    model.eval()

    tokens = [CLS_TOKEN] + tokenize(text)[:31]
    token_ids = [vocab.get(t, vocab[UNK_TOKEN]) for t in tokens]
    padding_len = 32 - len(token_ids)
    token_ids = token_ids + [vocab[PAD_TOKEN]] * padding_len
    attention_mask = [1] * (32 - padding_len) + [0] * padding_len

    input_ids = torch.tensor([token_ids], dtype=torch.long).to(device)
    mask = torch.tensor([attention_mask], dtype=torch.long).to(device)

    with torch.no_grad():
        logits, _ = model(input_ids, mask)
        probs = F.softmax(logits, dim=1)

    pred = logits.argmax(dim=1).item()
    confidence = probs[0, pred].item()

    return "æ­£é¢" if pred == 1 else "è´Ÿé¢", confidence

# æµ‹è¯•
test_texts = [
    "This is the best movie ever!",
    "Absolutely terrible, waste of time",
    "Pretty good, I enjoyed it",
    "Not great but not bad either",
    "Boring and disappointing film",
]

print("\né¢„æµ‹æµ‹è¯•:")
print("-" * 50)
for text in test_texts:
    sentiment, conf = predict(model, text, vocab, device)
    print(f"[{sentiment}] ({conf:.2%}) {text}")
```

---

## 6. æ‰©å±•ä»»åŠ¡

### 6.1 ä¿å­˜å’ŒåŠ è½½æ¨¡å‹

```python
# ä¿å­˜æ¨¡å‹
torch.save({
    'model_state_dict': model.state_dict(),
    'vocab': vocab,
    'config': {
        'vocab_size': len(vocab),
        'd_model': 128,
        'n_heads': 4,
        'n_layers': 2,
        'd_ff': 256,
        'num_classes': 2,
        'max_len': 32,
    }
}, 'transformer_sentiment.pth')

print("æ¨¡å‹å·²ä¿å­˜ï¼")

# åŠ è½½æ¨¡å‹
checkpoint = torch.load('transformer_sentiment.pth')
loaded_model = TransformerClassifier(**checkpoint['config']).to(device)
loaded_model.load_state_dict(checkpoint['model_state_dict'])
loaded_vocab = checkpoint['vocab']

print("æ¨¡å‹å·²åŠ è½½ï¼")
```

### 6.2 è¿›é˜¶æŒ‘æˆ˜

```python
"""
è¿›é˜¶ä»»åŠ¡æ¸…å•ï¼š

1. ä½¿ç”¨çœŸå® IMDB æ•°æ®é›†
   from datasets import load_dataset
   dataset = load_dataset("imdb")

2. å¢åŠ æ¨¡å‹å®¹é‡
   - æ›´å¤šå±‚ï¼ˆn_layers=4ï¼‰
   - æ›´å¤§ç»´åº¦ï¼ˆd_model=256ï¼‰
   - æ›´å¤šæ³¨æ„åŠ›å¤´ï¼ˆn_heads=8ï¼‰

3. æ·»åŠ ä½ç½®åµŒå…¥ï¼ˆå¯å­¦ä¹ ï¼‰
   self.pos_embedding = nn.Embedding(max_len, d_model)

4. å®ç°æ›´å¤šæ± åŒ–ç­–ç•¥
   - Mean Pooling
   - Max Pooling
   - Attention Pooling

5. æ·»åŠ é¢„è®­ç»ƒè¯åµŒå…¥
   - ä½¿ç”¨ GloVe æˆ– FastText
"""
```

---

## é¡¹ç›®æ€»ç»“

```
ğŸ¯ æœ¬é¡¹ç›®å®Œæˆçš„ä»»åŠ¡ï¼š

1. âœ… ä»é›¶å®ç° Multi-Head Attention
2. âœ… ä»é›¶å®ç° Transformer Encoder
3. âœ… å®Œæˆæ–‡æœ¬æƒ…æ„Ÿåˆ†ç±»ä»»åŠ¡
4. âœ… å¯è§†åŒ–æ³¨æ„åŠ›æƒé‡
5. âœ… æ¨¡å‹ä¿å­˜å’ŒåŠ è½½

ğŸ“Š å…¸å‹ç»“æœï¼š
- éªŒè¯å‡†ç¡®ç‡ï¼š~90-95%
- è®­ç»ƒæ—¶é—´ï¼šå‡ åˆ†é’Ÿï¼ˆCPUï¼‰

ğŸ“ å­¦åˆ°çš„çŸ¥è¯†ç‚¹ï¼š
- Self-Attention çš„è®¡ç®—è¿‡ç¨‹
- Multi-Head Attention çš„å®ç°
- Position Encoding çš„ä½œç”¨
- LayerNorm + æ®‹å·®è¿æ¥
- [CLS] token ç”¨äºåˆ†ç±»

ğŸ”— ä¸ BERT çš„è”ç³»ï¼š
BERT = é¢„è®­ç»ƒçš„ Transformer Encoder
æœ¬é¡¹ç›® = ä»é›¶è®­ç»ƒçš„ç®€åŒ–ç‰ˆ Transformer Encoder
```

---

## â¡ï¸ ä¸‹ä¸€æ­¥

å®Œæˆæœ¬å…¥é—¨é¡¹ç›®åï¼Œç»§ç»­æŒ‘æˆ˜ [13-é¡¹ç›®-BERTæ–‡æœ¬åˆ†ç±».md](./13-é¡¹ç›®-BERTæ–‡æœ¬åˆ†ç±».md)

