# ğŸ“ 03 - ä½ç½®ç¼–ç 

> Transformer æ²¡æœ‰å†…åœ¨çš„ä½ç½®æ„ŸçŸ¥ï¼Œä½ç½®ç¼–ç è®©æ¨¡å‹çŸ¥é“ token çš„é¡ºåº

---

## ç›®å½•

1. [ä¸ºä»€ä¹ˆéœ€è¦ä½ç½®ç¼–ç ](#1-ä¸ºä»€ä¹ˆéœ€è¦ä½ç½®ç¼–ç )
2. [ç»å¯¹ä½ç½®ç¼–ç ](#2-ç»å¯¹ä½ç½®ç¼–ç )
3. [RoPE æ—‹è½¬ä½ç½®ç¼–ç ](#3-rope-æ—‹è½¬ä½ç½®ç¼–ç )
4. [ALiBi](#4-alibi)
5. [ä½ç½®ç¼–ç å¯¹æ¯”](#5-ä½ç½®ç¼–ç å¯¹æ¯”)
6. [ç»ƒä¹ é¢˜](#6-ç»ƒä¹ é¢˜)

---

## 1. ä¸ºä»€ä¹ˆéœ€è¦ä½ç½®ç¼–ç 

### 1.1 é—®é¢˜

```
Self-Attention æ˜¯ä½ç½®æ— å…³çš„ï¼š

è¾“å…¥ ["I", "love", "AI"] â†’ æ³¨æ„åŠ› â†’ è¾“å‡º
è¾“å…¥ ["AI", "love", "I"] â†’ æ³¨æ„åŠ› â†’ ç›¸åŒè¾“å‡ºï¼ˆé¡ºåºå˜äº†ä½†ç»“æœä¸€æ ·ï¼ï¼‰

å› ä¸º Attention(Q, K, V) = softmax(QK^T/âˆšd) V
åªå…³å¿ƒ token ä¹‹é—´çš„ç›¸ä¼¼åº¦ï¼Œä¸å…³å¿ƒé¡ºåº

è€Œè¯­è¨€æ˜¯æœ‰é¡ºåºçš„ï¼š
"ç‹—å’¬äºº" â‰  "äººå’¬ç‹—"
```

### 1.2 è§£å†³æ–¹æ¡ˆ

```
ç»™æ¯ä¸ªä½ç½®ä¸€ä¸ªç‹¬ç‰¹çš„"èº«ä»½ä¿¡æ¯"

token embedding + position embedding â†’ è¾“å…¥
[è¯­ä¹‰ä¿¡æ¯]      + [ä½ç½®ä¿¡æ¯]       â†’ [å¸¦ä½ç½®çš„è¯­ä¹‰]

æ–¹æ³•ï¼š
1. å¯å­¦ä¹ çš„ä½ç½®åµŒå…¥ï¼ˆLearnedï¼‰
2. æ­£å¼¦ä½ç½®ç¼–ç ï¼ˆSinusoidalï¼‰
3. æ—‹è½¬ä½ç½®ç¼–ç ï¼ˆRoPEï¼‰- ç°ä»£ LLM ä¸»æµ
4. ALiBi - æ— éœ€è®­ç»ƒçš„ä½ç½®åç½®
```

---

## 2. ç»å¯¹ä½ç½®ç¼–ç 

### 2.1 å¯å­¦ä¹ ä½ç½®åµŒå…¥

```python
import torch
import torch.nn as nn
import math

class LearnedPositionalEncoding(nn.Module):
    """å¯å­¦ä¹ çš„ä½ç½®åµŒå…¥ï¼ˆBERTã€GPT-2 ä½¿ç”¨ï¼‰"""
    def __init__(self, max_len, d_model):
        super().__init__()
        self.pos_embedding = nn.Embedding(max_len, d_model)

    def forward(self, x):
        # x: [batch, seq_len, d_model]
        seq_len = x.size(1)
        positions = torch.arange(seq_len, device=x.device)
        pos_emb = self.pos_embedding(positions)  # [seq_len, d_model]
        return x + pos_emb

# ä½¿ç”¨
pos_enc = LearnedPositionalEncoding(max_len=512, d_model=256)
x = torch.randn(2, 100, 256)
y = pos_enc(x)
print(f"è¾“å‡º: {y.shape}")
```

**ç‰¹ç‚¹**ï¼š
- ç®€å•ç›´æ¥
- ä½ç½®ä¿¡æ¯å®Œå…¨ä»æ•°æ®å­¦ä¹ 
- æ— æ³•æ³›åŒ–åˆ°è®­ç»ƒæ—¶æœªè§è¿‡çš„é•¿åº¦

### 2.2 æ­£å¼¦ä½ç½®ç¼–ç 

```python
class SinusoidalPositionalEncoding(nn.Module):
    """
    æ­£å¼¦ä½ç½®ç¼–ç ï¼ˆåŸå§‹ Transformerï¼‰

    PE(pos, 2i)   = sin(pos / 10000^(2i/d_model))
    PE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))
    """
    def __init__(self, d_model, max_len=5000):
        super().__init__()

        # åˆ›å»ºä½ç½®ç¼–ç çŸ©é˜µ
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len).unsqueeze(1).float()

        # è®¡ç®—é™¤æ•°
        div_term = torch.exp(
            torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)
        )

        # äº¤æ›¿ä½¿ç”¨ sin å’Œ cos
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)

        # æ³¨å†Œä¸º bufferï¼ˆä¸æ˜¯å‚æ•°ï¼Œä½†ä¼šä¿å­˜åˆ°æ¨¡å‹ä¸­ï¼‰
        self.register_buffer('pe', pe.unsqueeze(0))  # [1, max_len, d_model]

    def forward(self, x):
        # x: [batch, seq_len, d_model]
        seq_len = x.size(1)
        return x + self.pe[:, :seq_len]

# å¯è§†åŒ–
import matplotlib.pyplot as plt

pe = SinusoidalPositionalEncoding(d_model=64, max_len=100)
pos_encoding = pe.pe[0].numpy()

plt.figure(figsize=(12, 6))
plt.imshow(pos_encoding.T, cmap='RdBu', aspect='auto')
plt.colorbar()
plt.xlabel('Position')
plt.ylabel('Dimension')
plt.title('Sinusoidal Positional Encoding')
plt.show()

# è§‚å¯Ÿï¼šä½ç»´å˜åŒ–å¿«ï¼ˆé«˜é¢‘ï¼‰ï¼Œé«˜ç»´å˜åŒ–æ…¢ï¼ˆä½é¢‘ï¼‰
```

**æ­£å¼¦ç¼–ç çš„ä¼˜ç‚¹**ï¼š
1. å¯ä»¥æ³›åŒ–åˆ°æ›´é•¿åºåˆ—
2. ç›¸å¯¹ä½ç½®å¯ä»¥é€šè¿‡çº¿æ€§å˜æ¢è¡¨ç¤ºï¼šPE(pos+k) å¯ä»¥ä» PE(pos) çº¿æ€§å˜æ¢å¾—åˆ°

---

## 3. RoPE æ—‹è½¬ä½ç½®ç¼–ç 

### 3.1 æ ¸å¿ƒæ€æƒ³

```
RoPEï¼ˆRotary Position Embeddingï¼‰æ˜¯ç°ä»£ LLM çš„æ ‡å‡†é€‰æ‹©
- LLaMAã€Qwenã€Mistral ç­‰éƒ½ä½¿ç”¨ RoPE

æ ¸å¿ƒæ€æƒ³ï¼š
- ä¸æ˜¯æŠŠä½ç½®ä¿¡æ¯"åŠ "åˆ°åµŒå…¥ä¸Š
- è€Œæ˜¯æŠŠä½ç½®ä¿¡æ¯"æ—‹è½¬"åˆ°åµŒå…¥ä¸Š
- æ—‹è½¬çš„è§’åº¦ç”±ä½ç½®å†³å®š

ä¼˜åŠ¿ï¼š
1. ç›¸å¯¹ä½ç½®ä¿¡æ¯ç›´æ¥ç¼–ç åˆ°æ³¨æ„åŠ›åˆ†æ•°ä¸­
2. å¯ä»¥å¾ˆå¥½åœ°æ³›åŒ–åˆ°æ›´é•¿åºåˆ—
3. è®¡ç®—é«˜æ•ˆ
```

### 3.2 ç›´è§‚ç†è§£

```
æŠŠå‘é‡æƒ³è±¡æˆ 2D å¹³é¢ä¸Šçš„ç‚¹ï¼š

ä½ç½® 0: å‘é‡ [x, y]
ä½ç½® 1: æ—‹è½¬ Î¸ è§’åº¦ â†’ [x', y']
ä½ç½® 2: æ—‹è½¬ 2Î¸ è§’åº¦ â†’ [x'', y'']

ä¸¤ä¸ªä½ç½®çš„æ³¨æ„åŠ›åˆ†æ•°ï¼ˆç‚¹ç§¯ï¼‰åªå–å†³äºå®ƒä»¬çš„ç›¸å¯¹ä½ç½®ï¼š
q_m Â· k_n = f(m - n)  // åªå’Œç›¸å¯¹è·ç¦»æœ‰å…³

è¿™æ­£æ˜¯æˆ‘ä»¬æƒ³è¦çš„ç›¸å¯¹ä½ç½®ä¿¡æ¯ï¼
```

### 3.3 ä»£ç å®ç°

```python
class RotaryPositionalEncoding(nn.Module):
    """RoPE æ—‹è½¬ä½ç½®ç¼–ç """
    def __init__(self, d_model, max_len=4096, base=10000):
        super().__init__()
        self.d_model = d_model
        self.max_len = max_len
        self.base = base

        # è®¡ç®—é¢‘ç‡
        inv_freq = 1.0 / (base ** (torch.arange(0, d_model, 2).float() / d_model))
        self.register_buffer('inv_freq', inv_freq)

        # é¢„è®¡ç®—
        self._set_cos_sin_cache(max_len)

    def _set_cos_sin_cache(self, seq_len):
        t = torch.arange(seq_len, dtype=self.inv_freq.dtype)
        freqs = torch.einsum('i,j->ij', t, self.inv_freq)  # [seq_len, d_model/2]

        # æ‰©å±•åˆ°å®Œæ•´ç»´åº¦
        emb = torch.cat([freqs, freqs], dim=-1)  # [seq_len, d_model]

        self.register_buffer('cos_cached', emb.cos())
        self.register_buffer('sin_cached', emb.sin())

    def forward(self, q, k, position_ids=None):
        """
        åº”ç”¨ RoPE åˆ° Q å’Œ K
        q, k: [batch, num_heads, seq_len, d_k]
        """
        seq_len = q.size(2)

        cos = self.cos_cached[:seq_len].unsqueeze(0).unsqueeze(0)  # [1, 1, seq_len, d_k]
        sin = self.sin_cached[:seq_len].unsqueeze(0).unsqueeze(0)

        # åº”ç”¨æ—‹è½¬
        q_embed = self._apply_rotary_emb(q, cos, sin)
        k_embed = self._apply_rotary_emb(k, cos, sin)

        return q_embed, k_embed

    def _apply_rotary_emb(self, x, cos, sin):
        """åº”ç”¨æ—‹è½¬"""
        # æŠŠå‘é‡åˆ†æˆä¸¤åŠï¼Œäº¤å‰æ—‹è½¬
        x1, x2 = x[..., :x.size(-1)//2], x[..., x.size(-1)//2:]

        # æ—‹è½¬ï¼š[x1, x2] â†’ [x1*cos - x2*sin, x1*sin + x2*cos]
        rotated = torch.cat([
            x1 * cos[..., :x1.size(-1)] - x2 * sin[..., :x1.size(-1)],
            x1 * sin[..., :x1.size(-1)] + x2 * cos[..., :x1.size(-1)]
        ], dim=-1)

        return rotated

# ä½¿ç”¨ç¤ºä¾‹
d_model = 64
num_heads = 4
d_k = d_model // num_heads

rope = RotaryPositionalEncoding(d_k)

q = torch.randn(2, num_heads, 10, d_k)
k = torch.randn(2, num_heads, 10, d_k)

q_rope, k_rope = rope(q, k)
print(f"Q (with RoPE): {q_rope.shape}")
print(f"K (with RoPE): {k_rope.shape}")

# éªŒè¯ï¼šç›¸å¯¹ä½ç½®æ€§è´¨
# ä½ç½® 3 å’Œ 5 çš„æ³¨æ„åŠ›åˆ†æ•°åº”è¯¥å’Œä½ç½® 0 å’Œ 2 ç›¸åŒï¼ˆç›¸å¯¹è·ç¦»éƒ½æ˜¯ 2ï¼‰
```

### 3.4 LLaMA é£æ ¼çš„ RoPE

```python
def precompute_freqs_cis(dim: int, end: int, theta: float = 10000.0):
    """é¢„è®¡ç®— RoPE çš„å¤æ•°å½¢å¼ï¼ˆLLaMA é£æ ¼ï¼‰"""
    freqs = 1.0 / (theta ** (torch.arange(0, dim, 2).float() / dim))
    t = torch.arange(end)
    freqs = torch.outer(t, freqs)  # [seq_len, dim/2]
    # å¤æ•°å½¢å¼ï¼še^(iÎ¸) = cos(Î¸) + i*sin(Î¸)
    freqs_cis = torch.polar(torch.ones_like(freqs), freqs)  # [seq_len, dim/2]
    return freqs_cis

def apply_rotary_emb(xq, xk, freqs_cis):
    """åº”ç”¨ RoPEï¼ˆå¤æ•°ä¹˜æ³•å®ç°ï¼‰"""
    # æŠŠå®æ•°å‘é‡è½¬ä¸ºå¤æ•°
    xq_ = torch.view_as_complex(xq.float().reshape(*xq.shape[:-1], -1, 2))
    xk_ = torch.view_as_complex(xk.float().reshape(*xk.shape[:-1], -1, 2))

    # å¤æ•°ä¹˜æ³• = æ—‹è½¬
    freqs_cis = freqs_cis.unsqueeze(0).unsqueeze(0)  # [1, 1, seq_len, dim/2]
    xq_out = torch.view_as_real(xq_ * freqs_cis).flatten(3)
    xk_out = torch.view_as_real(xk_ * freqs_cis).flatten(3)

    return xq_out.type_as(xq), xk_out.type_as(xk)

# æµ‹è¯•
freqs_cis = precompute_freqs_cis(64, 100)
xq = torch.randn(2, 4, 100, 64)  # [batch, heads, seq_len, dim]
xk = torch.randn(2, 4, 100, 64)

xq_rope, xk_rope = apply_rotary_emb(xq, xk, freqs_cis)
print(f"RoPE åçš„ Q: {xq_rope.shape}")
```

---

## 4. ALiBi

### 4.1 æ ¸å¿ƒæ€æƒ³

```
ALiBiï¼ˆAttention with Linear Biasesï¼‰
- ä¸ä¿®æ”¹åµŒå…¥ï¼Œç›´æ¥åœ¨æ³¨æ„åŠ›åˆ†æ•°ä¸ŠåŠ åç½®
- åç½®ä¸ç›¸å¯¹è·ç¦»æˆçº¿æ€§å…³ç³»
- è·ç¦»è¶Šè¿œï¼Œæƒ©ç½šè¶Šå¤§

scores_new = scores - m * |i - j|

m æ˜¯æ¯ä¸ªå¤´çš„æ–œç‡ï¼ˆé¢„è®¾çš„è¶…å‚æ•°ï¼‰
```

### 4.2 ä»£ç å®ç°

```python
class ALiBi(nn.Module):
    """ALiBi ä½ç½®ç¼–ç """
    def __init__(self, num_heads, max_len=4096):
        super().__init__()

        # è®¡ç®—æ¯ä¸ªå¤´çš„æ–œç‡
        # m_i = 2^(-8i/n) å…¶ä¸­ n æ˜¯å¤´æ•°
        slopes = self._get_slopes(num_heads)
        self.register_buffer('slopes', slopes)

        # é¢„è®¡ç®—åç½®çŸ©é˜µ
        bias = self._build_alibi_bias(max_len)
        self.register_buffer('bias', bias)

    def _get_slopes(self, n):
        """è®¡ç®—æ¯ä¸ªå¤´çš„æ–œç‡"""
        def get_slopes_power_of_2(n):
            start = 2 ** (-(2 ** -(math.log2(n) - 3)))
            ratio = start
            return [start * (ratio ** i) for i in range(n)]

        if math.log2(n).is_integer():
            return torch.tensor(get_slopes_power_of_2(n))
        else:
            # é 2 çš„å¹‚æ¬¡ï¼Œéœ€è¦æ’å€¼
            closest_power_of_2 = 2 ** math.floor(math.log2(n))
            return torch.tensor(
                get_slopes_power_of_2(closest_power_of_2) +
                get_slopes_power_of_2(2 * closest_power_of_2)[0::2][:n - closest_power_of_2]
            )

    def _build_alibi_bias(self, seq_len):
        """æ„å»º ALiBi åç½®çŸ©é˜µ"""
        # ç›¸å¯¹è·ç¦»çŸ©é˜µ
        positions = torch.arange(seq_len)
        relative_positions = positions.unsqueeze(0) - positions.unsqueeze(1)
        # å–ç»å¯¹å€¼ï¼ˆæˆ–åªå–è´Ÿæ–¹å‘ï¼Œç”¨äºå› æœæ©ç ï¼‰
        relative_positions = -torch.abs(relative_positions)
        return relative_positions.float()

    def forward(self, attention_scores, seq_len):
        """
        æ·»åŠ  ALiBi åç½®åˆ°æ³¨æ„åŠ›åˆ†æ•°
        attention_scores: [batch, num_heads, seq_len, seq_len]
        """
        bias = self.bias[:seq_len, :seq_len]  # [seq_len, seq_len]

        # æ¯ä¸ªå¤´ä¹˜ä»¥ä¸åŒçš„æ–œç‡
        # slopes: [num_heads], bias: [seq_len, seq_len]
        alibi = self.slopes.view(1, -1, 1, 1) * bias.unsqueeze(0).unsqueeze(0)
        # alibi: [1, num_heads, seq_len, seq_len]

        return attention_scores + alibi

# æµ‹è¯•
alibi = ALiBi(num_heads=8)
scores = torch.randn(2, 8, 64, 64)
scores_with_alibi = alibi(scores, seq_len=64)

print(f"ALiBi åç½®ç¤ºä¾‹ï¼ˆæ–œç‡ï¼‰: {alibi.slopes}")
print(f"è·ç¦»çŸ©é˜µç¤ºä¾‹:\n{alibi.bias[:5, :5]}")
```

### 4.3 ALiBi çš„ä¼˜åŠ¿

```
1. é›¶è®­ç»ƒæˆæœ¬ï¼šä¸éœ€è¦é¢å¤–çš„å‚æ•°
2. å¤–æ¨èƒ½åŠ›å¼ºï¼šåœ¨çŸ­åºåˆ—ä¸Šè®­ç»ƒï¼Œå¯ä»¥æ¨ç†æ›´é•¿åºåˆ—
3. ç®€å•é«˜æ•ˆï¼šåªæ˜¯åŠ ä¸€ä¸ªé¢„è®¡ç®—çš„åç½®

åœ¨ MPTã€BLOOM ç­‰æ¨¡å‹ä¸­ä½¿ç”¨
```

---

## 5. ä½ç½®ç¼–ç å¯¹æ¯”

### 5.1 å¯¹æ¯”è¡¨

| æ–¹æ³• | ç‰¹ç‚¹ | å¤–æ¨èƒ½åŠ› | ä½¿ç”¨æ¨¡å‹ |
|------|------|---------|---------|
| Learned | ç®€å•ï¼Œå®Œå…¨æ•°æ®é©±åŠ¨ | å·® | BERT, GPT-2 |
| Sinusoidal | æ— éœ€å­¦ä¹ ï¼Œæ•°å­¦ä¼˜é›… | ä¸€èˆ¬ | åŸå§‹ Transformer |
| RoPE | ç›¸å¯¹ä½ç½®ï¼Œé«˜æ•ˆ | å¥½ | LLaMA, Qwen, Mistral |
| ALiBi | æ— éœ€è®­ç»ƒï¼Œçº¿æ€§åç½® | å¾ˆå¥½ | MPT, BLOOM |

### 5.2 å¦‚ä½•é€‰æ‹©

```
1. çŸ­åºåˆ—ï¼ˆ<512ï¼‰ï¼šLearned æˆ– Sinusoidal éƒ½å¯ä»¥
2. é•¿åºåˆ—ï¼ˆ>2048ï¼‰ï¼šRoPE æˆ– ALiBi
3. éœ€è¦å¤–æ¨ï¼šALiBi æœ€å¥½
4. ç°ä»£ LLMï¼šRoPE æ˜¯ä¸»æµé€‰æ‹©

å®è·µå»ºè®®ï¼š
- å¦‚æœä»é›¶å¼€å§‹è®­ç»ƒï¼šç”¨ RoPE
- å¦‚æœéœ€è¦è¶…é•¿ä¸Šä¸‹æ–‡ï¼šè€ƒè™‘ ALiBi æˆ– RoPE + NTK ç¼©æ”¾
```

### 5.3 é•¿ä¸Šä¸‹æ–‡æ‰©å±•

```python
# RoPE å¤–æ¨æ–¹æ³•ï¼šNTK-Aware Scaling
def ntk_aware_rope(dim, max_len, base=10000, scaling_factor=2.0):
    """NTK-Aware RoPEï¼šé€šè¿‡è°ƒæ•´ base å®ç°é•¿åº¦å¤–æ¨"""
    # æ‰©å±• base
    base = base * (scaling_factor ** (dim / (dim - 2)))
    inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))
    return inv_freq

# ä½¿ç”¨æ›´å¤§çš„ base å¯ä»¥è®©æ¨¡å‹æ³›åŒ–åˆ°æ›´é•¿åºåˆ—
# LLaMA 2 Long ç­‰æ¨¡å‹ä½¿ç”¨ç±»ä¼¼æŠ€æœ¯
```

---

## 6. ç»ƒä¹ é¢˜

### åŸºç¡€ç»ƒä¹ 

1. å®ç°æ­£å¼¦ä½ç½®ç¼–ç å¹¶å¯è§†åŒ–
2. éªŒè¯ RoPE çš„ç›¸å¯¹ä½ç½®æ€§è´¨
3. æ¯”è¾ƒä¸åŒä½ç½®ç¼–ç å¯¹æ¨¡å‹æ•ˆæœçš„å½±å“

### å‚è€ƒç­”æ¡ˆ

<details>
<summary>ç‚¹å‡»æŸ¥çœ‹ç­”æ¡ˆ</summary>

```python
import torch
import torch.nn.functional as F
import matplotlib.pyplot as plt
import math

# 1. æ­£å¼¦ä½ç½®ç¼–ç å¯è§†åŒ–
def sinusoidal_position_encoding(max_len, d_model):
    pe = torch.zeros(max_len, d_model)
    position = torch.arange(0, max_len).unsqueeze(1).float()
    div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))
    pe[:, 0::2] = torch.sin(position * div_term)
    pe[:, 1::2] = torch.cos(position * div_term)
    return pe

pe = sinusoidal_position_encoding(100, 64)

fig, axes = plt.subplots(1, 2, figsize=(14, 5))

# çƒ­åŠ›å›¾
axes[0].imshow(pe.T, cmap='RdBu', aspect='auto')
axes[0].set_xlabel('Position')
axes[0].set_ylabel('Dimension')
axes[0].set_title('Positional Encoding Heatmap')

# å‡ ä¸ªç»´åº¦çš„æ›²çº¿
for dim in [0, 1, 10, 11, 30, 31]:
    axes[1].plot(pe[:, dim], label=f'dim {dim}')
axes[1].set_xlabel('Position')
axes[1].set_ylabel('Value')
axes[1].set_title('Positional Encoding Curves')
axes[1].legend()

plt.tight_layout()
plt.show()


# 2. RoPE ç›¸å¯¹ä½ç½®éªŒè¯
def simple_rope(x, position):
    """ç®€åŒ–çš„ RoPEï¼ˆ2Dï¼‰"""
    theta = 0.1 * position
    cos_theta = math.cos(theta)
    sin_theta = math.sin(theta)
    x_rot = torch.tensor([
        x[0] * cos_theta - x[1] * sin_theta,
        x[0] * sin_theta + x[1] * cos_theta
    ])
    return x_rot

# éªŒè¯ç›¸å¯¹ä½ç½®æ€§è´¨
q = torch.tensor([1.0, 0.5])
k = torch.tensor([0.8, 0.3])

# ä½ç½® 0 å’Œ 2
q_0 = simple_rope(q, 0)
k_2 = simple_rope(k, 2)
score_02 = torch.dot(q_0, k_2)

# ä½ç½® 3 å’Œ 5ï¼ˆç›¸å¯¹è·ç¦»ä¹Ÿæ˜¯ 2ï¼‰
q_3 = simple_rope(q, 3)
k_5 = simple_rope(k, 5)
score_35 = torch.dot(q_3, k_5)

print(f"ä½ç½® 0-2 çš„æ³¨æ„åŠ›åˆ†æ•°: {score_02:.4f}")
print(f"ä½ç½® 3-5 çš„æ³¨æ„åŠ›åˆ†æ•°: {score_35:.4f}")
print(f"å·®å¼‚: {abs(score_02 - score_35):.6f}")  # åº”è¯¥éå¸¸å°


# 3. ä¸åŒä½ç½®ç¼–ç å¯¹æ¯”ï¼ˆç®€åŒ–æµ‹è¯•ï¼‰
# å®é™…åº”è¯¥åœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸Šæµ‹è¯•
print("\nä½ç½®ç¼–ç å¯¹æ¯”ï¼ˆç®€åŒ–ï¼‰:")
print("- Learned: å‚æ•°é‡ = max_len * d_model")
print("- Sinusoidal: å‚æ•°é‡ = 0")
print("- RoPE: å‚æ•°é‡ = 0ï¼ˆä½†éœ€è¦é¢„è®¡ç®—ï¼‰")
print("- ALiBi: å‚æ•°é‡ = 0")
```

</details>

---

## â¡ï¸ ä¸‹ä¸€æ­¥

å­¦å®Œæœ¬èŠ‚åï¼Œç»§ç»­å­¦ä¹  [04-Encoderä¸Decoder.md](./04-Encoderä¸Decoder.md)

