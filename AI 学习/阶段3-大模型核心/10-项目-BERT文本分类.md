# ğŸ“ é¡¹ç›®ï¼šBERT æ–‡æœ¬åˆ†ç±»

> ä½¿ç”¨ BERT å¾®è°ƒå®Œæˆæ–‡æœ¬åˆ†ç±»ä»»åŠ¡

---

## é¡¹ç›®æ¦‚è¿°

### ä»»åŠ¡è¯´æ˜

```
æ•°æ®é›†ï¼šIMDB ç”µå½±è¯„è®ºæƒ…æ„Ÿåˆ†ç±»
- 25,000 æ¡è®­ç»ƒæ ·æœ¬
- 25,000 æ¡æµ‹è¯•æ ·æœ¬
- äºŒåˆ†ç±»ï¼šæ­£é¢/è´Ÿé¢

ç›®æ ‡ï¼š
- ä½¿ç”¨é¢„è®­ç»ƒ BERT è¿›è¡Œå¾®è°ƒ
- è¾¾åˆ° 90%+ çš„å‡†ç¡®ç‡
```

---

## å®Œæ•´ä»£ç 

```python
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
from transformers import (
    BertTokenizer,
    BertForSequenceClassification,
    AdamW,
    get_linear_schedule_with_warmup
)
from datasets import load_dataset
from sklearn.metrics import accuracy_score, classification_report
from tqdm import tqdm
import numpy as np

# ========== é…ç½® ==========
class Config:
    model_name = "bert-base-uncased"
    max_length = 256
    batch_size = 16
    learning_rate = 2e-5
    num_epochs = 3
    warmup_ratio = 0.1
    weight_decay = 0.01
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

config = Config()
print(f"ä½¿ç”¨è®¾å¤‡: {config.device}")

# ========== æ•°æ®å‡†å¤‡ ==========
print("åŠ è½½æ•°æ®...")
dataset = load_dataset("imdb")

# ä½¿ç”¨éƒ¨åˆ†æ•°æ®è¿›è¡Œæ¼”ç¤º
train_data = dataset["train"].select(range(5000))
test_data = dataset["test"].select(range(1000))

print(f"è®­ç»ƒé›†: {len(train_data)}")
print(f"æµ‹è¯•é›†: {len(test_data)}")
print(f"æ ·æœ¬: {train_data[0]}")

# ========== Tokenizer ==========
tokenizer = BertTokenizer.from_pretrained(config.model_name)

def tokenize_function(examples):
    return tokenizer(
        examples["text"],
        padding="max_length",
        truncation=True,
        max_length=config.max_length,
        return_tensors="pt"
    )

# å¤„ç†æ•°æ®
print("Tokenizing...")
train_encodings = tokenizer(
    train_data["text"],
    padding="max_length",
    truncation=True,
    max_length=config.max_length,
    return_tensors="pt"
)

test_encodings = tokenizer(
    test_data["text"],
    padding="max_length",
    truncation=True,
    max_length=config.max_length,
    return_tensors="pt"
)

# ========== Dataset ç±» ==========
class IMDBDataset(torch.utils.data.Dataset):
    def __init__(self, encodings, labels):
        self.encodings = encodings
        self.labels = labels

    def __len__(self):
        return len(self.labels)

    def __getitem__(self, idx):
        return {
            "input_ids": self.encodings["input_ids"][idx],
            "attention_mask": self.encodings["attention_mask"][idx],
            "labels": torch.tensor(self.labels[idx])
        }

train_dataset = IMDBDataset(train_encodings, train_data["label"])
test_dataset = IMDBDataset(test_encodings, test_data["label"])

train_loader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=config.batch_size * 2)

# ========== æ¨¡å‹ ==========
print("åŠ è½½æ¨¡å‹...")
model = BertForSequenceClassification.from_pretrained(
    config.model_name,
    num_labels=2
)
model = model.to(config.device)

# ========== ä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨ ==========
optimizer = AdamW(
    model.parameters(),
    lr=config.learning_rate,
    weight_decay=config.weight_decay
)

total_steps = len(train_loader) * config.num_epochs
warmup_steps = int(total_steps * config.warmup_ratio)

scheduler = get_linear_schedule_with_warmup(
    optimizer,
    num_warmup_steps=warmup_steps,
    num_training_steps=total_steps
)

# ========== è®­ç»ƒå‡½æ•° ==========
def train_epoch(model, loader, optimizer, scheduler, device):
    model.train()
    total_loss = 0
    predictions = []
    true_labels = []

    progress_bar = tqdm(loader, desc="Training")
    for batch in progress_bar:
        input_ids = batch["input_ids"].to(device)
        attention_mask = batch["attention_mask"].to(device)
        labels = batch["labels"].to(device)

        optimizer.zero_grad()

        outputs = model(
            input_ids=input_ids,
            attention_mask=attention_mask,
            labels=labels
        )

        loss = outputs.loss
        logits = outputs.logits

        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
        optimizer.step()
        scheduler.step()

        total_loss += loss.item()
        preds = torch.argmax(logits, dim=1).cpu().numpy()
        predictions.extend(preds)
        true_labels.extend(labels.cpu().numpy())

        progress_bar.set_postfix({"loss": loss.item()})

    avg_loss = total_loss / len(loader)
    accuracy = accuracy_score(true_labels, predictions)

    return avg_loss, accuracy

# ========== è¯„ä¼°å‡½æ•° ==========
def evaluate(model, loader, device):
    model.eval()
    predictions = []
    true_labels = []

    with torch.no_grad():
        for batch in tqdm(loader, desc="Evaluating"):
            input_ids = batch["input_ids"].to(device)
            attention_mask = batch["attention_mask"].to(device)
            labels = batch["labels"]

            outputs = model(
                input_ids=input_ids,
                attention_mask=attention_mask
            )

            preds = torch.argmax(outputs.logits, dim=1).cpu().numpy()
            predictions.extend(preds)
            true_labels.extend(labels.numpy())

    accuracy = accuracy_score(true_labels, predictions)
    return accuracy, predictions, true_labels

# ========== è®­ç»ƒå¾ªç¯ ==========
print("\nå¼€å§‹è®­ç»ƒ...")
best_accuracy = 0

for epoch in range(config.num_epochs):
    print(f"\n{'='*50}")
    print(f"Epoch {epoch + 1}/{config.num_epochs}")
    print('='*50)

    train_loss, train_acc = train_epoch(
        model, train_loader, optimizer, scheduler, config.device
    )
    print(f"è®­ç»ƒ - Loss: {train_loss:.4f}, Accuracy: {train_acc:.4f}")

    test_acc, _, _ = evaluate(model, test_loader, config.device)
    print(f"æµ‹è¯• - Accuracy: {test_acc:.4f}")

    if test_acc > best_accuracy:
        best_accuracy = test_acc
        torch.save(model.state_dict(), "best_bert_model.pth")
        print("âœ“ ä¿å­˜æœ€ä½³æ¨¡å‹")

print(f"\nè®­ç»ƒå®Œæˆï¼æœ€ä½³æµ‹è¯•å‡†ç¡®ç‡: {best_accuracy:.4f}")

# ========== è¯¦ç»†è¯„ä¼° ==========
print("\nåŠ è½½æœ€ä½³æ¨¡å‹è¿›è¡Œè¯„ä¼°...")
model.load_state_dict(torch.load("best_bert_model.pth"))
accuracy, predictions, true_labels = evaluate(model, test_loader, config.device)

print("\nåˆ†ç±»æŠ¥å‘Š:")
print(classification_report(
    true_labels, predictions,
    target_names=["Negative", "Positive"]
))

# ========== æ¨ç†ç¤ºä¾‹ ==========
def predict(text, model, tokenizer, device):
    model.eval()

    encoding = tokenizer(
        text,
        padding="max_length",
        truncation=True,
        max_length=config.max_length,
        return_tensors="pt"
    )

    input_ids = encoding["input_ids"].to(device)
    attention_mask = encoding["attention_mask"].to(device)

    with torch.no_grad():
        outputs = model(input_ids=input_ids, attention_mask=attention_mask)
        probs = torch.softmax(outputs.logits, dim=1)
        pred = torch.argmax(probs, dim=1).item()

    labels = ["Negative", "Positive"]
    return {
        "prediction": labels[pred],
        "confidence": probs[0][pred].item(),
        "probabilities": {
            "Negative": probs[0][0].item(),
            "Positive": probs[0][1].item()
        }
    }

# æµ‹è¯•æ¨ç†
test_texts = [
    "This movie is absolutely fantastic! I loved every minute of it.",
    "Terrible film. Complete waste of time and money.",
    "It was okay, nothing special but not bad either."
]

print("\næ¨ç†æµ‹è¯•:")
print("-" * 50)
for text in test_texts:
    result = predict(text, model, tokenizer, config.device)
    print(f"æ–‡æœ¬: {text[:50]}...")
    print(f"é¢„æµ‹: {result['prediction']} (ç½®ä¿¡åº¦: {result['confidence']:.4f})")
    print("-" * 50)
```

---

## ä½¿ç”¨ Trainer API

```python
from transformers import Trainer, TrainingArguments, DataCollatorWithPadding

# æ•°æ®å¤„ç†
def preprocess_function(examples):
    return tokenizer(
        examples["text"],
        truncation=True,
        max_length=256
    )

tokenized_train = train_data.map(preprocess_function, batched=True)
tokenized_test = test_data.map(preprocess_function, batched=True)

# æ•°æ®æ•´ç†å™¨
data_collator = DataCollatorWithPadding(tokenizer=tokenizer)

# è¯„ä¼°æŒ‡æ ‡
def compute_metrics(eval_pred):
    predictions, labels = eval_pred
    predictions = np.argmax(predictions, axis=1)
    return {"accuracy": accuracy_score(labels, predictions)}

# è®­ç»ƒå‚æ•°
training_args = TrainingArguments(
    output_dir="./results",
    num_train_epochs=3,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=32,
    warmup_ratio=0.1,
    weight_decay=0.01,
    logging_dir="./logs",
    logging_steps=100,
    evaluation_strategy="epoch",
    save_strategy="epoch",
    load_best_model_at_end=True,
    metric_for_best_model="accuracy",
)

# åˆ›å»º Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_train,
    eval_dataset=tokenized_test,
    tokenizer=tokenizer,
    data_collator=data_collator,
    compute_metrics=compute_metrics,
)

# è®­ç»ƒ
trainer.train()

# è¯„ä¼°
results = trainer.evaluate()
print(f"æœ€ç»ˆç»“æœ: {results}")
```

---

## ä¼˜åŒ–æ–¹å‘

```python
# 1. ä½¿ç”¨æ›´å¥½çš„é¢„è®­ç»ƒæ¨¡å‹
model = BertForSequenceClassification.from_pretrained("roberta-base", num_labels=2)

# 2. å†»ç»“éƒ¨åˆ†å±‚
for param in model.bert.embeddings.parameters():
    param.requires_grad = False
for layer in model.bert.encoder.layer[:6]:
    for param in layer.parameters():
        param.requires_grad = False

# 3. å­¦ä¹ ç‡åˆ†å±‚
optimizer_grouped_parameters = [
    {"params": model.bert.encoder.layer[-4:].parameters(), "lr": 2e-5},
    {"params": model.classifier.parameters(), "lr": 1e-4},
]
optimizer = AdamW(optimizer_grouped_parameters)

# 4. æ•°æ®å¢å¼º
# - å›è¯‘
# - åŒä¹‰è¯æ›¿æ¢
# - éšæœºåˆ é™¤

# 5. é›†æˆå­¦ä¹ 
# è®­ç»ƒå¤šä¸ªæ¨¡å‹ï¼ŒæŠ•ç¥¨æˆ–å¹³å‡æ¦‚ç‡
```

---

## é¢„æœŸæ•ˆæœ

```
è®­ç»ƒé…ç½®: BERT-base, 3 epochs, lr=2e-5
è®­ç»ƒé›†: 25,000 (å®Œæ•´)
æµ‹è¯•é›†: 25,000 (å®Œæ•´)

é¢„æœŸå‡†ç¡®ç‡: 92-94%
```

---

## â¡ï¸ ä¸‹ä¸€æ­¥

ç»§ç»­ [11-é¡¹ç›®-è¯­ä¹‰æœç´¢å¼•æ“.md](./11-é¡¹ç›®-è¯­ä¹‰æœç´¢å¼•æ“.md)

