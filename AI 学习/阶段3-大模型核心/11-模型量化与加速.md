# ⚡ 11 - 模型量化与加速

> 让大模型跑得更快、用更少显存

---

## 目录

1. [为什么需要量化](#1-为什么需要量化)
2. [量化基础知识](#2-量化基础知识)
3. [PyTorch 量化实践](#3-pytorch-量化实践)
4. [LLM 专用量化](#4-llm-专用量化)
5. [推理加速技术](#5-推理加速技术)
6. [实战：量化并部署模型](#6-实战量化并部署模型)
7. [练习题](#7-练习题)

---

## 1. 为什么需要量化

### 1.1 大模型的困境

```
LLaMA-7B 模型：
- 参数量：7B = 70 亿
- FP32：7B × 4 bytes = 28 GB
- FP16：7B × 2 bytes = 14 GB
- INT8：7B × 1 byte = 7 GB
- INT4：7B × 0.5 bytes = 3.5 GB

量化的目标：
├── 减少显存占用（更小的模型）
├── 加速推理（更快的计算）
└── 降低部署成本（消费级 GPU 也能跑）
```

### 1.2 量化 vs 精度权衡

```
精度损失可以接受吗？

实验结果（LLaMA-7B）：
| 量化方法 | 显存 | MMLU准确率 | 损失 |
|---------|------|-----------|------|
| FP16    | 14GB | 35.1%     | 基准 |
| INT8    | 7GB  | 34.8%     | -0.3% |
| INT4    | 3.5GB| 33.9%     | -1.2% |

结论：量化损失很小，但收益巨大
```

---

## 2. 量化基础知识

### 2.1 数据类型回顾

```python
import torch
import numpy as np

# 常见数据类型
print("=" * 50)
print(f"{'Type':<15} {'Bytes':<8} {'Range':<30}")
print("=" * 50)

types = [
    ('FP32', 4, '±3.4e38'),
    ('FP16', 2, '±65504'),
    ('BF16', 2, '±3.4e38 (低精度)'),
    ('INT8', 1, '-128 ~ 127'),
    ('INT4', 0.5, '-8 ~ 7'),
]

for name, bytes, range_ in types:
    print(f"{name:<15} {bytes:<8} {range_:<30}")
```

### 2.2 量化原理

```python
import torch

def symmetric_quantize(x, num_bits=8):
    """对称量化：将浮点数映射到整数

    x_int = round(x / scale)
    x_dequant = x_int * scale
    """
    # 计算缩放因子
    qmax = 2 ** (num_bits - 1) - 1  # 127 for INT8
    scale = x.abs().max() / qmax

    # 量化
    x_int = torch.round(x / scale).clamp(-qmax, qmax).to(torch.int8)

    # 反量化
    x_dequant = x_int.float() * scale

    return x_int, scale, x_dequant

# 示例
x = torch.randn(4, 4)
x_int, scale, x_dequant = symmetric_quantize(x)

print(f"原始张量:\n{x}")
print(f"\n量化后 (INT8):\n{x_int}")
print(f"\n反量化:\n{x_dequant}")
print(f"\n量化误差: {(x - x_dequant).abs().mean():.6f}")
```

### 2.3 非对称量化

```python
def asymmetric_quantize(x, num_bits=8):
    """非对称量化：适用于非对称分布（如 ReLU 后的激活值）

    x_int = round((x - zero_point) / scale)
    x_dequant = x_int * scale + zero_point
    """
    qmin = 0
    qmax = 2 ** num_bits - 1  # 255 for UINT8

    x_min, x_max = x.min(), x.max()
    scale = (x_max - x_min) / (qmax - qmin)
    zero_point = qmin - x_min / scale

    # 量化
    x_int = torch.round(x / scale + zero_point).clamp(qmin, qmax).to(torch.uint8)

    # 反量化
    x_dequant = (x_int.float() - zero_point) * scale

    return x_int, scale, zero_point, x_dequant

# 示例：ReLU 后的激活值（全为正）
x = torch.relu(torch.randn(4, 4))
x_int, scale, zp, x_dequant = asymmetric_quantize(x)

print(f"原始张量:\n{x}")
print(f"\n量化后 (UINT8):\n{x_int}")
print(f"\n量化误差: {(x - x_dequant).abs().mean():.6f}")
```

### 2.4 量化粒度

```
量化粒度（Granularity）：
├── 逐张量（Per-Tensor）：整个张量一个 scale
├── 逐通道（Per-Channel）：每个输出通道一个 scale
├── 逐组（Per-Group）：每 N 个元素一个 scale
└── 逐行（Per-Row）：矩阵每行一个 scale

粒度越细，精度损失越小，但开销越大
```

```python
def per_channel_quantize(weight, num_bits=8):
    """逐通道量化（推荐用于权重）"""
    out_channels = weight.shape[0]
    qmax = 2 ** (num_bits - 1) - 1

    # 每个输出通道计算 scale
    scales = weight.view(out_channels, -1).abs().max(dim=1).values / qmax
    scales = scales.view(-1, *([1] * (weight.dim() - 1)))

    # 量化
    w_int = torch.round(weight / scales).clamp(-qmax, qmax).to(torch.int8)

    return w_int, scales

# 示例
weight = torch.randn(64, 32)  # [out_channels, in_channels]
w_int, scales = per_channel_quantize(weight)
print(f"权重形状: {weight.shape}")
print(f"量化后形状: {w_int.shape}")
print(f"Scales 形状: {scales.shape}")
```

---

## 3. PyTorch 量化实践

### 3.1 动态量化

```python
import torch
import torch.nn as nn

# 定义一个简单模型
class SimpleModel(nn.Module):
    def __init__(self):
        super().__init__()
        self.fc1 = nn.Linear(784, 256)
        self.fc2 = nn.Linear(256, 10)

    def forward(self, x):
        x = torch.relu(self.fc1(x))
        return self.fc2(x)

# 创建模型
model = SimpleModel()
model.eval()

# 动态量化：推理时动态量化激活值
quantized_model = torch.quantization.quantize_dynamic(
    model,
    {nn.Linear},  # 只量化 Linear 层
    dtype=torch.qint8
)

# 对比大小
import os

torch.save(model.state_dict(), 'model_fp32.pt')
torch.save(quantized_model.state_dict(), 'model_int8.pt')

size_fp32 = os.path.getsize('model_fp32.pt')
size_int8 = os.path.getsize('model_int8.pt')

print(f"FP32 模型大小: {size_fp32 / 1024:.2f} KB")
print(f"INT8 模型大小: {size_int8 / 1024:.2f} KB")
print(f"压缩比: {size_fp32 / size_int8:.2f}x")

# 测试推理速度
import time

x = torch.randn(100, 784)

# FP32
start = time.time()
for _ in range(1000):
    _ = model(x)
print(f"FP32 推理时间: {time.time() - start:.3f}s")

# INT8
start = time.time()
for _ in range(1000):
    _ = quantized_model(x)
print(f"INT8 推理时间: {time.time() - start:.3f}s")

# 清理
os.remove('model_fp32.pt')
os.remove('model_int8.pt')
```

### 3.2 静态量化

```python
import torch
import torch.nn as nn
from torch.quantization import QuantStub, DeQuantStub

class QuantizableModel(nn.Module):
    def __init__(self):
        super().__init__()
        self.quant = QuantStub()
        self.fc1 = nn.Linear(784, 256)
        self.fc2 = nn.Linear(256, 10)
        self.dequant = DeQuantStub()

    def forward(self, x):
        x = self.quant(x)  # 输入量化
        x = torch.relu(self.fc1(x))
        x = self.fc2(x)
        x = self.dequant(x)  # 输出反量化
        return x

# 创建模型
model = QuantizableModel()
model.eval()

# 配置量化
model.qconfig = torch.quantization.get_default_qconfig('fbgemm')

# 准备量化
model_prepared = torch.quantization.prepare(model)

# 校准（用代表性数据跑几个 batch）
calibration_data = torch.randn(100, 784)
with torch.no_grad():
    model_prepared(calibration_data)

# 转换为量化模型
model_quantized = torch.quantization.convert(model_prepared)

print("静态量化完成！")
print(model_quantized)
```

### 3.3 量化感知训练 (QAT)

```python
import torch
import torch.nn as nn

class QATModel(nn.Module):
    def __init__(self):
        super().__init__()
        self.quant = torch.quantization.QuantStub()
        self.fc1 = nn.Linear(784, 256)
        self.bn1 = nn.BatchNorm1d(256)
        self.fc2 = nn.Linear(256, 10)
        self.dequant = torch.quantization.DeQuantStub()

    def forward(self, x):
        x = self.quant(x)
        x = torch.relu(self.bn1(self.fc1(x)))
        x = self.fc2(x)
        x = self.dequant(x)
        return x

# 创建模型
model = QATModel()
model.train()

# 配置 QAT
model.qconfig = torch.quantization.get_default_qat_qconfig('fbgemm')

# 准备 QAT（插入 fake quantize）
model_qat = torch.quantization.prepare_qat(model)

# 训练（正常训练流程）
optimizer = torch.optim.Adam(model_qat.parameters(), lr=0.001)
criterion = nn.CrossEntropyLoss()

# 模拟训练
for epoch in range(3):
    x = torch.randn(32, 784)
    y = torch.randint(0, 10, (32,))

    optimizer.zero_grad()
    output = model_qat(x)
    loss = criterion(output, y)
    loss.backward()
    optimizer.step()

    print(f"Epoch {epoch+1}, Loss: {loss.item():.4f}")

# 转换为量化模型
model_qat.eval()
model_quantized = torch.quantization.convert(model_qat)
print("\nQAT 训练完成！")
```

---

## 4. LLM 专用量化

### 4.1 使用 bitsandbytes

```python
# pip install bitsandbytes

from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
import torch

# INT8 量化配置
bnb_config_8bit = BitsAndBytesConfig(
    load_in_8bit=True,
)

# INT4 量化配置
bnb_config_4bit = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",  # NormalFloat4
    bnb_4bit_compute_dtype=torch.bfloat16,
    bnb_4bit_use_double_quant=True,  # 嵌套量化
)

# 加载 8-bit 模型
model_8bit = AutoModelForCausalLM.from_pretrained(
    "meta-llama/Llama-2-7b-hf",
    quantization_config=bnb_config_8bit,
    device_map="auto"
)

# 加载 4-bit 模型
model_4bit = AutoModelForCausalLM.from_pretrained(
    "meta-llama/Llama-2-7b-hf",
    quantization_config=bnb_config_4bit,
    device_map="auto"
)

# 查看显存占用
print(f"8-bit 显存: {torch.cuda.memory_allocated() / 1024**3:.2f} GB")
```

### 4.2 GPTQ 量化

```python
# pip install auto-gptq

from transformers import AutoTokenizer
from auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig

# 配置
quantize_config = BaseQuantizeConfig(
    bits=4,
    group_size=128,
    desc_act=False,
)

# 加载模型
model_name = "meta-llama/Llama-2-7b-hf"
tokenizer = AutoTokenizer.from_pretrained(model_name)

# 量化（需要校准数据）
model = AutoGPTQForCausalLM.from_pretrained(
    model_name,
    quantize_config
)

# 准备校准数据
calibration_texts = [
    "The meaning of life is",
    "Artificial intelligence can",
    "The future of technology",
]
calibration_data = [tokenizer(t, return_tensors="pt") for t in calibration_texts]

# 执行量化
model.quantize(calibration_data)

# 保存量化模型
model.save_quantized("llama2-7b-gptq")

# 加载量化模型
model_quantized = AutoGPTQForCausalLM.from_quantized(
    "llama2-7b-gptq",
    device="cuda:0"
)
```

### 4.3 AWQ 量化

```python
# pip install autoawq

from awq import AutoAWQForCausalLM
from transformers import AutoTokenizer

model_name = "meta-llama/Llama-2-7b-hf"

# 加载模型
model = AutoAWQForCausalLM.from_pretrained(model_name)
tokenizer = AutoTokenizer.from_pretrained(model_name)

# 量化配置
quant_config = {
    "zero_point": True,
    "q_group_size": 128,
    "w_bit": 4,
}

# 执行量化
model.quantize(tokenizer, quant_config=quant_config)

# 保存
model.save_quantized("llama2-7b-awq")
```

### 4.4 量化方法对比

| 方法 | 精度损失 | 速度 | 显存节省 | 适用场景 |
|------|---------|------|---------|---------|
| **动态量化** | 较小 | 中等 | 较少 | 小模型 |
| **bitsandbytes** | 小 | 快 | 大 | 推理部署 |
| **GPTQ** | 很小 | 很快 | 大 | 生产部署 |
| **AWQ** | 很小 | 最快 | 大 | 高性能推理 |
| **QAT** | 最小 | 最快 | 大 | 对精度敏感 |

---

## 5. 推理加速技术

### 5.1 Flash Attention

```python
# PyTorch 2.0+ 原生支持
import torch
import torch.nn.functional as F

def flash_attention(Q, K, V, is_causal=False):
    """使用 PyTorch 的 scaled_dot_product_attention
    底层自动使用 Flash Attention
    """
    # PyTorch 2.0+ 自动选择最优实现
    # 包括 Flash Attention、Memory Efficient Attention 等
    output = F.scaled_dot_product_attention(
        Q, K, V,
        attn_mask=None,
        dropout_p=0.0,
        is_causal=is_causal
    )
    return output

# 示例
B, H, T, D = 2, 8, 1024, 64
Q = torch.randn(B, H, T, D, device='cuda')
K = torch.randn(B, H, T, D, device='cuda')
V = torch.randn(B, H, T, D, device='cuda')

output = flash_attention(Q, K, V, is_causal=True)
print(f"输出形状: {output.shape}")
```

### 5.2 KV Cache 实现

```python
import torch
import torch.nn as nn

class CachedAttention(nn.Module):
    def __init__(self, d_model, n_heads):
        super().__init__()
        self.n_heads = n_heads
        self.head_dim = d_model // n_heads

        self.q_proj = nn.Linear(d_model, d_model)
        self.k_proj = nn.Linear(d_model, d_model)
        self.v_proj = nn.Linear(d_model, d_model)
        self.out_proj = nn.Linear(d_model, d_model)

    def forward(self, x, kv_cache=None, use_cache=True):
        B, T, D = x.shape

        Q = self.q_proj(x).view(B, T, self.n_heads, self.head_dim).transpose(1, 2)
        K = self.k_proj(x).view(B, T, self.n_heads, self.head_dim).transpose(1, 2)
        V = self.v_proj(x).view(B, T, self.n_heads, self.head_dim).transpose(1, 2)

        # 使用 KV Cache
        if kv_cache is not None:
            past_K, past_V = kv_cache
            K = torch.cat([past_K, K], dim=2)
            V = torch.cat([past_V, V], dim=2)

        # 计算注意力
        scores = torch.matmul(Q, K.transpose(-2, -1)) / (self.head_dim ** 0.5)
        attn = torch.softmax(scores, dim=-1)
        out = torch.matmul(attn, V)

        out = out.transpose(1, 2).contiguous().view(B, T, D)
        out = self.out_proj(out)

        new_cache = (K, V) if use_cache else None
        return out, new_cache

# 使用示例
attn = CachedAttention(512, 8)

# 首次推理
x1 = torch.randn(1, 10, 512)
out1, cache = attn(x1, kv_cache=None)

# 增量推理（只计算新 token）
x2 = torch.randn(1, 1, 512)  # 只有 1 个新 token
out2, cache = attn(x2, kv_cache=cache)

print(f"首次推理输出: {out1.shape}")
print(f"增量推理输出: {out2.shape}")
print(f"缓存 K 形状: {cache[0].shape}")  # 累积了 11 个 token
```

### 5.3 Continuous Batching

```python
class ContinuousBatcher:
    """连续批处理：动态管理请求，最大化 GPU 利用率"""

    def __init__(self, model, tokenizer, max_batch_size=8):
        self.model = model
        self.tokenizer = tokenizer
        self.max_batch_size = max_batch_size
        self.pending_requests = []
        self.active_requests = []

    def add_request(self, prompt, max_tokens):
        """添加新请求"""
        self.pending_requests.append({
            'prompt': prompt,
            'max_tokens': max_tokens,
            'generated': [],
            'input_ids': self.tokenizer.encode(prompt),
        })

    def step(self):
        """执行一步推理"""
        # 填充活跃请求
        while len(self.active_requests) < self.max_batch_size and self.pending_requests:
            self.active_requests.append(self.pending_requests.pop(0))

        if not self.active_requests:
            return []

        # 批量推理
        # ... (实际实现需要处理不同长度的序列)

        # 返回完成的请求
        completed = []
        self.active_requests = [r for r in self.active_requests
                                if len(r['generated']) < r['max_tokens']]

        return completed
```

### 5.4 Speculative Decoding

```python
def speculative_decode(draft_model, target_model, tokenizer, prompt,
                       gamma=4, max_length=50):
    """投机解码：用小模型加速大模型推理

    1. 小模型（draft）快速生成 gamma 个 token
    2. 大模型（target）一次验证这些 token
    3. 接受正确的，拒绝错误的
    """
    input_ids = tokenizer.encode(prompt, return_tensors='pt')
    generated = input_ids

    while generated.shape[1] < max_length:
        # 1. Draft: 小模型生成 gamma 个候选
        draft_tokens = []
        draft_input = generated

        for _ in range(gamma):
            with torch.no_grad():
                draft_output = draft_model(draft_input)
                draft_logits = draft_output.logits[:, -1, :]
                draft_token = torch.argmax(draft_logits, dim=-1, keepdim=True)
                draft_tokens.append(draft_token)
                draft_input = torch.cat([draft_input, draft_token], dim=-1)

        # 2. Target: 大模型一次性验证
        candidate = torch.cat([generated] + draft_tokens, dim=-1)

        with torch.no_grad():
            target_output = target_model(candidate)
            target_logits = target_output.logits

        # 3. 验证每个 token
        accepted = 0
        for i in range(gamma):
            pos = generated.shape[1] + i
            target_token = torch.argmax(target_logits[:, pos-1, :], dim=-1)

            if target_token == draft_tokens[i]:
                accepted += 1
            else:
                # 使用 target 的 token
                generated = torch.cat([generated, target_token.unsqueeze(-1)], dim=-1)
                break
        else:
            # 全部接受，再从 target 采样一个
            next_token = torch.argmax(target_logits[:, -1, :], dim=-1, keepdim=True)
            generated = torch.cat([candidate, next_token], dim=-1)

        if accepted == 0:
            continue

        generated = candidate[:, :generated.shape[1] + accepted]

    return tokenizer.decode(generated[0])
```

---

## 6. 实战：量化并部署模型

### 6.1 完整流程

```python
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
import torch
import time

# 1. 配置量化
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.float16,
    bnb_4bit_use_double_quant=True,
)

# 2. 加载模型
model_name = "TinyLlama/TinyLlama-1.1B-Chat-v1.0"  # 用小模型演示
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    quantization_config=bnb_config,
    device_map="auto"
)

print(f"模型已加载，显存占用: {torch.cuda.memory_allocated() / 1024**3:.2f} GB")

# 3. 测试推理
def generate(prompt, max_length=100):
    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)

    start = time.time()
    outputs = model.generate(
        **inputs,
        max_length=max_length,
        do_sample=True,
        temperature=0.7,
        top_p=0.9,
    )
    elapsed = time.time() - start

    text = tokenizer.decode(outputs[0], skip_special_tokens=True)
    tokens_generated = outputs.shape[1] - inputs['input_ids'].shape[1]

    print(f"生成 {tokens_generated} tokens，耗时 {elapsed:.2f}s")
    print(f"速度: {tokens_generated / elapsed:.1f} tokens/s")

    return text

# 测试
prompt = "Explain quantum computing in simple terms:"
result = generate(prompt)
print(f"\n{result}")
```

### 6.2 性能对比脚本

```python
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
import time
import gc

def benchmark_model(model_name, quantization=None, num_runs=5):
    """基准测试"""
    # 清理显存
    gc.collect()
    torch.cuda.empty_cache()

    # 加载配置
    load_kwargs = {"device_map": "auto"}

    if quantization == "8bit":
        load_kwargs["quantization_config"] = BitsAndBytesConfig(load_in_8bit=True)
    elif quantization == "4bit":
        load_kwargs["quantization_config"] = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_quant_type="nf4",
            bnb_4bit_compute_dtype=torch.float16,
        )
    else:
        load_kwargs["torch_dtype"] = torch.float16

    # 加载
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModelForCausalLM.from_pretrained(model_name, **load_kwargs)

    memory = torch.cuda.memory_allocated() / 1024**3

    # 预热
    prompt = "Hello, how are you?"
    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
    _ = model.generate(**inputs, max_length=20)

    # 测试
    times = []
    for _ in range(num_runs):
        torch.cuda.synchronize()
        start = time.time()
        _ = model.generate(**inputs, max_length=50, do_sample=False)
        torch.cuda.synchronize()
        times.append(time.time() - start)

    avg_time = sum(times) / len(times)

    del model
    gc.collect()
    torch.cuda.empty_cache()

    return {
        "quantization": quantization or "fp16",
        "memory_gb": memory,
        "avg_time_s": avg_time,
    }

# 运行对比
model_name = "TinyLlama/TinyLlama-1.1B-Chat-v1.0"
results = []

for quant in [None, "8bit", "4bit"]:
    print(f"Testing {quant or 'fp16'}...")
    result = benchmark_model(model_name, quant)
    results.append(result)
    print(result)

# 打印对比表
print("\n" + "=" * 50)
print(f"{'Quantization':<15} {'Memory (GB)':<15} {'Time (s)':<15}")
print("=" * 50)
for r in results:
    print(f"{r['quantization']:<15} {r['memory_gb']:<15.2f} {r['avg_time_s']:<15.3f}")
```

---

## 7. 练习题

### 基础练习

1. 实现对称量化和反量化函数
2. 使用 PyTorch 动态量化一个简单模型
3. 使用 bitsandbytes 加载一个 4-bit 模型

### 进阶练习

4. 实现 KV Cache，对比有无缓存的推理速度
5. 使用 GPTQ 量化一个模型，对比精度损失

### 参考答案

<details>
<summary>练习 1 参考答案</summary>

```python
import torch

def symmetric_quantize(x, num_bits=8):
    qmax = 2 ** (num_bits - 1) - 1
    scale = x.abs().max() / qmax
    x_int = torch.round(x / scale).clamp(-qmax, qmax).to(torch.int8)
    return x_int, scale

def dequantize(x_int, scale):
    return x_int.float() * scale

# 测试
x = torch.randn(4, 4)
x_int, scale = symmetric_quantize(x)
x_recovered = dequantize(x_int, scale)

print(f"原始: {x}")
print(f"恢复: {x_recovered}")
print(f"误差: {(x - x_recovered).abs().mean():.6f}")
```

</details>

---

## ➡️ 下一步

学完本节后，继续学习 [12-项目-Transformer情感分类.md](./12-项目-Transformer情感分类.md)

