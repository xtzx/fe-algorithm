# ğŸ² 10 - è§£ç ç­–ç•¥ä¸é‡‡æ ·

> ç†è§£ LLM å¦‚ä½•ç”Ÿæˆæ–‡æœ¬ï¼šè´ªå©ªè§£ç ã€æŸæœç´¢ã€é‡‡æ ·ç­–ç•¥

---

## ç›®å½•

1. [è§£ç é—®é¢˜æ¦‚è¿°](#1-è§£ç é—®é¢˜æ¦‚è¿°)
2. [è´ªå©ªè§£ç ](#2-è´ªå©ªè§£ç )
3. [æŸæœç´¢](#3-æŸæœç´¢)
4. [é‡‡æ ·ç­–ç•¥](#4-é‡‡æ ·ç­–ç•¥)
5. [é«˜çº§é‡‡æ ·æŠ€æœ¯](#5-é«˜çº§é‡‡æ ·æŠ€æœ¯)
6. [å®è·µæŒ‡å—](#6-å®è·µæŒ‡å—)
7. [ç»ƒä¹ é¢˜](#7-ç»ƒä¹ é¢˜)

---

## 1. è§£ç é—®é¢˜æ¦‚è¿°

### 1.1 ä»€ä¹ˆæ˜¯è§£ç 

```
è¯­è¨€æ¨¡å‹è¾“å‡ºçš„æ˜¯æ¦‚ç‡åˆ†å¸ƒï¼Œä¸æ˜¯æ–‡æœ¬

è¾“å…¥: "The cat sat on the"
æ¨¡å‹è¾“å‡º: P(next_token | context)
    "mat"   : 0.15
    "floor" : 0.12
    "bed"   : 0.10
    "chair" : 0.08
    ...

è§£ç ç­–ç•¥ï¼šå¦‚ä½•ä»æ¦‚ç‡åˆ†å¸ƒä¸­é€‰æ‹©ä¸‹ä¸€ä¸ª tokenï¼Ÿ
```

### 1.2 è§£ç ç­–ç•¥åˆ†ç±»

```
è§£ç ç­–ç•¥
â”œâ”€â”€ ç¡®å®šæ€§è§£ç 
â”‚   â”œâ”€â”€ è´ªå©ªè§£ç ï¼ˆGreedyï¼‰
â”‚   â””â”€â”€ æŸæœç´¢ï¼ˆBeam Searchï¼‰
â”‚
â””â”€â”€ éšæœºé‡‡æ ·
    â”œâ”€â”€ çº¯éšæœºé‡‡æ ·
    â”œâ”€â”€ Temperature é‡‡æ ·
    â”œâ”€â”€ Top-K é‡‡æ ·
    â”œâ”€â”€ Top-P (Nucleus) é‡‡æ ·
    â””â”€â”€ ç»„åˆç­–ç•¥
```

---

## 2. è´ªå©ªè§£ç 

### 2.1 åŸç†

```
æ¯ä¸€æ­¥éƒ½é€‰æ‹©æ¦‚ç‡æœ€é«˜çš„ token

Step 1: argmax P(w1 | context) â†’ "mat"
Step 2: argmax P(w2 | context, "mat") â†’ "was"
Step 3: argmax P(w3 | context, "mat", "was") â†’ "soft"
...
```

### 2.2 å®ç°

```python
import torch
import torch.nn.functional as F
from transformers import GPT2LMHeadModel, GPT2Tokenizer

def greedy_decode(model, tokenizer, prompt, max_length=50):
    """è´ªå©ªè§£ç """
    model.eval()
    input_ids = tokenizer.encode(prompt, return_tensors='pt')

    generated = input_ids

    for _ in range(max_length):
        with torch.no_grad():
            outputs = model(generated)
            logits = outputs.logits[:, -1, :]  # å–æœ€åä¸€ä¸ªä½ç½®çš„ logits

        # è´ªå©ªï¼šé€‰æ‹©æ¦‚ç‡æœ€å¤§çš„ token
        next_token = torch.argmax(logits, dim=-1, keepdim=True)

        # æ‹¼æ¥
        generated = torch.cat([generated, next_token], dim=-1)

        # é‡åˆ° EOS åœæ­¢
        if next_token.item() == tokenizer.eos_token_id:
            break

    return tokenizer.decode(generated[0], skip_special_tokens=True)

# æµ‹è¯•
model = GPT2LMHeadModel.from_pretrained('gpt2')
tokenizer = GPT2Tokenizer.from_pretrained('gpt2')

prompt = "The future of AI is"
result = greedy_decode(model, tokenizer, prompt)
print(f"Greedy: {result}")
```

### 2.3 ä¼˜ç¼ºç‚¹

| ä¼˜ç‚¹ | ç¼ºç‚¹ |
|------|------|
| ç®€å•å¿«é€Ÿ | å®¹æ˜“é‡å¤ |
| ç¡®å®šæ€§è¾“å‡º | ç¼ºä¹å¤šæ ·æ€§ |
| é€‚åˆçŸ­æ–‡æœ¬ | å¯èƒ½é™·å…¥å±€éƒ¨æœ€ä¼˜ |

---

## 3. æŸæœç´¢

### 3.1 åŸç†

```
åŒæ—¶ç»´æŠ¤ K ä¸ªæœ€ä½³å€™é€‰åºåˆ—ï¼ˆbeamï¼‰

beam_size = 3

Step 1:
  Beam 1: "The cat sat on the mat" (score: -2.3)
  Beam 2: "The cat sat on the floor" (score: -2.5)
  Beam 3: "The cat sat on the bed" (score: -2.8)

Step 2: æ¯ä¸ª beam æ‰©å±•æ‰€æœ‰å¯èƒ½ï¼Œä¿ç•™æ€»åˆ†æœ€é«˜çš„ K ä¸ª
  ...

æœ€ç»ˆé€‰æ‹©æ€»åˆ†æœ€é«˜çš„åºåˆ—
```

### 3.2 å®ç°

```python
import torch
import torch.nn.functional as F
from dataclasses import dataclass
from typing import List
import heapq

@dataclass
class BeamHypothesis:
    tokens: List[int]
    score: float

    def __lt__(self, other):
        return self.score > other.score  # åˆ†æ•°è¶Šé«˜è¶Šå¥½

def beam_search(model, tokenizer, prompt, beam_size=5, max_length=50):
    """æŸæœç´¢è§£ç """
    model.eval()
    input_ids = tokenizer.encode(prompt, return_tensors='pt')

    # åˆå§‹åŒ– beam
    beams = [BeamHypothesis(tokens=input_ids[0].tolist(), score=0.0)]
    completed = []

    for step in range(max_length):
        all_candidates = []

        for beam in beams:
            if beam.tokens[-1] == tokenizer.eos_token_id:
                completed.append(beam)
                continue

            # è·å–ä¸‹ä¸€ä¸ª token çš„æ¦‚ç‡
            input_tensor = torch.tensor([beam.tokens])
            with torch.no_grad():
                outputs = model(input_tensor)
                logits = outputs.logits[:, -1, :]
                log_probs = F.log_softmax(logits, dim=-1)

            # è·å– top-k å€™é€‰
            top_log_probs, top_indices = torch.topk(log_probs[0], beam_size * 2)

            for log_prob, token_id in zip(top_log_probs, top_indices):
                new_tokens = beam.tokens + [token_id.item()]
                new_score = beam.score + log_prob.item()
                all_candidates.append(BeamHypothesis(tokens=new_tokens, score=new_score))

        # é€‰æ‹© top-k beams
        all_candidates.sort()
        beams = all_candidates[:beam_size]

        if len(beams) == 0:
            break

    # åˆå¹¶å®Œæˆçš„å’Œæœªå®Œæˆçš„
    all_hypotheses = completed + beams
    all_hypotheses.sort()

    # è¿”å›æœ€ä½³åºåˆ—
    best = all_hypotheses[0]
    return tokenizer.decode(best.tokens, skip_special_tokens=True)

# æµ‹è¯•
result = beam_search(model, tokenizer, "The future of AI is", beam_size=5)
print(f"Beam Search: {result}")
```

### 3.3 é•¿åº¦å½’ä¸€åŒ–

```python
def length_normalized_score(score, length, alpha=0.6):
    """é•¿åº¦æƒ©ç½šï¼Œé¿å…åå¥½çŸ­åºåˆ—"""
    # Google NMT è®ºæ–‡çš„å…¬å¼
    lp = ((5 + length) / 6) ** alpha
    return score / lp
```

### 3.4 ä¼˜ç¼ºç‚¹

| ä¼˜ç‚¹ | ç¼ºç‚¹ |
|------|------|
| æ¯”è´ªå©ªæ›´ä¼˜ | è®¡ç®—é‡å¤§ |
| é€‚åˆç¿»è¯‘ç­‰ä»»åŠ¡ | ä»ç„¶ç¼ºä¹å¤šæ ·æ€§ |
| è€ƒè™‘å¤šä¸ªå€™é€‰ | å®¹æ˜“ç”Ÿæˆé€šç”¨/æ— èŠæ–‡æœ¬ |

---

## 4. é‡‡æ ·ç­–ç•¥

### 4.1 çº¯éšæœºé‡‡æ ·

```python
def random_sample(model, tokenizer, prompt, max_length=50):
    """çº¯éšæœºé‡‡æ ·"""
    model.eval()
    input_ids = tokenizer.encode(prompt, return_tensors='pt')
    generated = input_ids

    for _ in range(max_length):
        with torch.no_grad():
            outputs = model(generated)
            logits = outputs.logits[:, -1, :]
            probs = F.softmax(logits, dim=-1)

        # æŒ‰æ¦‚ç‡é‡‡æ ·
        next_token = torch.multinomial(probs, num_samples=1)
        generated = torch.cat([generated, next_token], dim=-1)

        if next_token.item() == tokenizer.eos_token_id:
            break

    return tokenizer.decode(generated[0], skip_special_tokens=True)
```

### 4.2 Temperature é‡‡æ ·

```python
def temperature_sample(model, tokenizer, prompt, temperature=1.0, max_length=50):
    """Temperature é‡‡æ ·

    temperature < 1: æ›´ç¡®å®šï¼ˆåˆ†å¸ƒæ›´å°–é”ï¼‰
    temperature = 1: åŸå§‹åˆ†å¸ƒ
    temperature > 1: æ›´éšæœºï¼ˆåˆ†å¸ƒæ›´å¹³å¦ï¼‰
    """
    model.eval()
    input_ids = tokenizer.encode(prompt, return_tensors='pt')
    generated = input_ids

    for _ in range(max_length):
        with torch.no_grad():
            outputs = model(generated)
            logits = outputs.logits[:, -1, :] / temperature  # ç¼©æ”¾ logits
            probs = F.softmax(logits, dim=-1)

        next_token = torch.multinomial(probs, num_samples=1)
        generated = torch.cat([generated, next_token], dim=-1)

        if next_token.item() == tokenizer.eos_token_id:
            break

    return tokenizer.decode(generated[0], skip_special_tokens=True)

# å¯¹æ¯”ä¸åŒ temperature
prompt = "Once upon a time"
for temp in [0.5, 1.0, 1.5]:
    result = temperature_sample(model, tokenizer, prompt, temperature=temp)
    print(f"Temp={temp}: {result[:100]}...")
```

**Temperature å¯è§†åŒ–**ï¼š

```python
import matplotlib.pyplot as plt
import numpy as np

def visualize_temperature(logits, temperatures=[0.5, 1.0, 2.0]):
    """å¯è§†åŒ– temperature å¯¹åˆ†å¸ƒçš„å½±å“"""
    fig, axes = plt.subplots(1, len(temperatures), figsize=(15, 4))

    for ax, temp in zip(axes, temperatures):
        scaled_logits = logits / temp
        probs = F.softmax(torch.tensor(scaled_logits), dim=-1).numpy()

        ax.bar(range(len(probs)), probs)
        ax.set_title(f'Temperature = {temp}')
        ax.set_xlabel('Token ID')
        ax.set_ylabel('Probability')

    plt.tight_layout()
    plt.show()

# ç¤ºä¾‹ logits
logits = np.array([2.0, 1.5, 1.0, 0.5, 0.0, -0.5, -1.0])
visualize_temperature(logits)
```

### 4.3 Top-K é‡‡æ ·

```python
def top_k_sample(model, tokenizer, prompt, k=50, temperature=1.0, max_length=50):
    """Top-K é‡‡æ ·ï¼šåªä»æ¦‚ç‡æœ€é«˜çš„ K ä¸ª token ä¸­é‡‡æ ·"""
    model.eval()
    input_ids = tokenizer.encode(prompt, return_tensors='pt')
    generated = input_ids

    for _ in range(max_length):
        with torch.no_grad():
            outputs = model(generated)
            logits = outputs.logits[:, -1, :] / temperature

        # åªä¿ç•™ top-k
        top_k_logits, top_k_indices = torch.topk(logits, k, dim=-1)

        # åœ¨ top-k ä¸­é‡‡æ ·
        probs = F.softmax(top_k_logits, dim=-1)
        sampled_index = torch.multinomial(probs, num_samples=1)
        next_token = top_k_indices.gather(-1, sampled_index)

        generated = torch.cat([generated, next_token], dim=-1)

        if next_token.item() == tokenizer.eos_token_id:
            break

    return tokenizer.decode(generated[0], skip_special_tokens=True)
```

### 4.4 Top-P (Nucleus) é‡‡æ ·

```python
def top_p_sample(model, tokenizer, prompt, p=0.9, temperature=1.0, max_length=50):
    """Top-P (Nucleus) é‡‡æ ·ï¼šä»ç´¯ç§¯æ¦‚ç‡è¾¾åˆ° P çš„æœ€å°é›†åˆä¸­é‡‡æ ·"""
    model.eval()
    input_ids = tokenizer.encode(prompt, return_tensors='pt')
    generated = input_ids

    for _ in range(max_length):
        with torch.no_grad():
            outputs = model(generated)
            logits = outputs.logits[:, -1, :] / temperature

        # æ’åº
        sorted_logits, sorted_indices = torch.sort(logits, descending=True, dim=-1)
        sorted_probs = F.softmax(sorted_logits, dim=-1)
        cumulative_probs = torch.cumsum(sorted_probs, dim=-1)

        # æ‰¾åˆ°ç´¯ç§¯æ¦‚ç‡è¶…è¿‡ p çš„ä½ç½®
        sorted_indices_to_remove = cumulative_probs > p
        # ä¿ç•™ç¬¬ä¸€ä¸ªè¶…è¿‡ p çš„ token
        sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()
        sorted_indices_to_remove[..., 0] = False

        # å°†è¦ç§»é™¤çš„ token çš„ logits è®¾ä¸º -inf
        indices_to_remove = sorted_indices_to_remove.scatter(
            dim=-1, index=sorted_indices, src=sorted_indices_to_remove
        )
        logits[indices_to_remove] = float('-inf')

        # é‡‡æ ·
        probs = F.softmax(logits, dim=-1)
        next_token = torch.multinomial(probs, num_samples=1)

        generated = torch.cat([generated, next_token], dim=-1)

        if next_token.item() == tokenizer.eos_token_id:
            break

    return tokenizer.decode(generated[0], skip_special_tokens=True)
```

### 4.5 Top-K vs Top-P å¯¹æ¯”

```
Top-K çš„é—®é¢˜ï¼š
- å›ºå®š K å€¼ä¸å¤Ÿçµæ´»
- å½“åˆ†å¸ƒå¾ˆå°–é”æ—¶ï¼ŒK=50 å¯èƒ½åŒ…å«å¤ªå¤šä½æ¦‚ç‡ token
- å½“åˆ†å¸ƒå¾ˆå¹³å¦æ—¶ï¼ŒK=50 å¯èƒ½ä¸å¤Ÿ

Top-P çš„ä¼˜åŠ¿ï¼š
- è‡ªé€‚åº”ï¼šæ ¹æ®åˆ†å¸ƒè‡ªåŠ¨è°ƒæ•´å€™é€‰æ•°é‡
- åˆ†å¸ƒå°–é”æ—¶ï¼Œåªé€‰å°‘æ•° token
- åˆ†å¸ƒå¹³å¦æ—¶ï¼Œé€‰æ‹©æ›´å¤š token

æ¨èï¼šä½¿ç”¨ Top-Pï¼ˆnucleus samplingï¼‰
```

---

## 5. é«˜çº§é‡‡æ ·æŠ€æœ¯

### 5.1 ç»„åˆç­–ç•¥

```python
def advanced_sample(model, tokenizer, prompt,
                    temperature=0.8, top_k=50, top_p=0.95,
                    repetition_penalty=1.2, max_length=100):
    """ç»„åˆå¤šç§é‡‡æ ·ç­–ç•¥"""
    model.eval()
    input_ids = tokenizer.encode(prompt, return_tensors='pt')
    generated = input_ids

    for _ in range(max_length):
        with torch.no_grad():
            outputs = model(generated)
            logits = outputs.logits[:, -1, :]

        # 1. é‡å¤æƒ©ç½š
        for token_id in set(generated[0].tolist()):
            logits[0, token_id] /= repetition_penalty

        # 2. Temperature
        logits = logits / temperature

        # 3. Top-K è¿‡æ»¤
        if top_k > 0:
            top_k_values, _ = torch.topk(logits, top_k)
            min_top_k = top_k_values[:, -1].unsqueeze(-1)
            logits = torch.where(logits < min_top_k,
                                torch.full_like(logits, float('-inf')),
                                logits)

        # 4. Top-P è¿‡æ»¤
        if top_p < 1.0:
            sorted_logits, sorted_indices = torch.sort(logits, descending=True)
            sorted_probs = F.softmax(sorted_logits, dim=-1)
            cumulative_probs = torch.cumsum(sorted_probs, dim=-1)

            sorted_indices_to_remove = cumulative_probs > top_p
            sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()
            sorted_indices_to_remove[..., 0] = False

            indices_to_remove = sorted_indices_to_remove.scatter(
                dim=-1, index=sorted_indices, src=sorted_indices_to_remove
            )
            logits[indices_to_remove] = float('-inf')

        # é‡‡æ ·
        probs = F.softmax(logits, dim=-1)
        next_token = torch.multinomial(probs, num_samples=1)

        generated = torch.cat([generated, next_token], dim=-1)

        if next_token.item() == tokenizer.eos_token_id:
            break

    return tokenizer.decode(generated[0], skip_special_tokens=True)
```

### 5.2 é‡å¤æƒ©ç½š

```python
def apply_repetition_penalty(logits, generated_tokens, penalty=1.2):
    """æƒ©ç½šå·²ç”Ÿæˆçš„ tokenï¼Œé¿å…é‡å¤"""
    for token_id in set(generated_tokens):
        if logits[token_id] > 0:
            logits[token_id] /= penalty
        else:
            logits[token_id] *= penalty
    return logits
```

### 5.3 Contrastive Search

```python
def contrastive_search(model, tokenizer, prompt, k=5, alpha=0.6, max_length=50):
    """å¯¹æ¯”æœç´¢ï¼šå¹³è¡¡æ¦‚ç‡å’Œä¸ä¸Šæ–‡çš„å·®å¼‚æ€§

    score = (1 - alpha) * log_prob + alpha * max_similarity
    é€‰æ‹© score æœ€é«˜çš„ token
    """
    model.eval()
    input_ids = tokenizer.encode(prompt, return_tensors='pt')
    generated = input_ids
    past_hidden_states = []

    for _ in range(max_length):
        with torch.no_grad():
            outputs = model(generated, output_hidden_states=True)
            logits = outputs.logits[:, -1, :]
            hidden = outputs.hidden_states[-1][:, -1, :]  # æœ€åä¸€å±‚çš„æœ€åä¸€ä¸ªä½ç½®

        # è·å– top-k å€™é€‰
        top_logits, top_indices = torch.topk(logits, k, dim=-1)
        top_probs = F.softmax(top_logits, dim=-1)

        if len(past_hidden_states) == 0:
            # ç¬¬ä¸€æ­¥ç›´æ¥é€‰æ¦‚ç‡æœ€é«˜çš„
            next_token = top_indices[:, 0:1]
        else:
            # è®¡ç®—æ¯ä¸ªå€™é€‰ä¸å†å²éšè—çŠ¶æ€çš„ç›¸ä¼¼åº¦
            scores = []
            past_hidden = torch.stack(past_hidden_states, dim=1)  # [1, seq_len, hidden]

            for i in range(k):
                # æ¨¡æ‹Ÿé€‰æ‹©è¿™ä¸ª token åçš„éšè—çŠ¶æ€
                candidate_hidden = hidden  # ç®€åŒ–ï¼šç”¨å½“å‰éšè—çŠ¶æ€è¿‘ä¼¼

                # è®¡ç®—ä¸å†å²çš„æœ€å¤§ç›¸ä¼¼åº¦
                similarity = F.cosine_similarity(
                    candidate_hidden.unsqueeze(1),
                    past_hidden,
                    dim=-1
                ).max()

                # è®¡ç®—å¾—åˆ†
                score = (1 - alpha) * torch.log(top_probs[0, i]) - alpha * similarity
                scores.append(score)

            best_idx = torch.argmax(torch.tensor(scores))
            next_token = top_indices[:, best_idx:best_idx+1]

        past_hidden_states.append(hidden.squeeze(0))
        generated = torch.cat([generated, next_token], dim=-1)

        if next_token.item() == tokenizer.eos_token_id:
            break

    return tokenizer.decode(generated[0], skip_special_tokens=True)
```

---

## 6. å®è·µæŒ‡å—

### 6.1 Hugging Face ä¸­ä½¿ç”¨

```python
from transformers import AutoModelForCausalLM, AutoTokenizer

model_name = "gpt2"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name)

prompt = "The meaning of life is"
inputs = tokenizer(prompt, return_tensors="pt")

# è´ªå©ªè§£ç 
greedy_output = model.generate(
    **inputs,
    max_length=50,
    do_sample=False  # è´ªå©ª
)

# æŸæœç´¢
beam_output = model.generate(
    **inputs,
    max_length=50,
    num_beams=5,
    early_stopping=True
)

# Top-K + Top-P é‡‡æ ·
sample_output = model.generate(
    **inputs,
    max_length=50,
    do_sample=True,
    temperature=0.8,
    top_k=50,
    top_p=0.95,
    repetition_penalty=1.2
)

# å¯¹æ¯”æœç´¢
contrastive_output = model.generate(
    **inputs,
    max_length=50,
    penalty_alpha=0.6,
    top_k=5
)

print("Greedy:", tokenizer.decode(greedy_output[0]))
print("Beam:", tokenizer.decode(beam_output[0]))
print("Sample:", tokenizer.decode(sample_output[0]))
print("Contrastive:", tokenizer.decode(contrastive_output[0]))
```

### 6.2 å‚æ•°é€‰æ‹©æŒ‡å—

| ä»»åŠ¡ç±»å‹ | æ¨èç­–ç•¥ | å‚æ•° |
|---------|---------|------|
| æœºå™¨ç¿»è¯‘ | Beam Search | beam_size=4-6 |
| åˆ›æ„å†™ä½œ | Top-P é‡‡æ · | temp=0.9, top_p=0.95 |
| ä»£ç ç”Ÿæˆ | ä½æ¸©é‡‡æ · | temp=0.2-0.5, top_p=0.9 |
| å¯¹è¯ | ç»„åˆç­–ç•¥ | temp=0.7, top_k=50, top_p=0.9 |
| æ‘˜è¦ | Beam + é‡‡æ · | diverse_beam=True |

### 6.3 å¸¸è§é—®é¢˜

```
1. ç”Ÿæˆé‡å¤ï¼Ÿ
   â†’ å¢åŠ  repetition_penalty (1.1-1.5)
   â†’ ä½¿ç”¨ no_repeat_ngram_size=3

2. ç”Ÿæˆå¤ªéšæœºï¼Ÿ
   â†’ é™ä½ temperature
   â†’ é™ä½ top_p æˆ– top_k

3. ç”Ÿæˆå¤ªæ— èŠï¼Ÿ
   â†’ æé«˜ temperature
   â†’ ä½¿ç”¨å¯¹æ¯”æœç´¢

4. é•¿åº¦ä¸å¤Ÿï¼Ÿ
   â†’ è°ƒæ•´ max_length
   â†’ ä½¿ç”¨ min_length
```

---

## 7. ç»ƒä¹ é¢˜

### åŸºç¡€ç»ƒä¹ 

1. å®ç° Temperature é‡‡æ ·ï¼Œå¯¹æ¯” 0.5, 1.0, 1.5 çš„æ•ˆæœ
2. å®ç° Top-K é‡‡æ ·ï¼Œè§‚å¯Ÿä¸åŒ K å€¼çš„å½±å“
3. ä½¿ç”¨ Hugging Face çš„ generate æ–¹æ³•ï¼Œå°è¯•ä¸åŒå‚æ•°

### è¿›é˜¶ç»ƒä¹ 

4. å®ç°å¸¦é‡å¤æƒ©ç½šçš„é‡‡æ ·
5. å¯¹æ¯” Greedyã€Beam Searchã€Top-P åœ¨ç¿»è¯‘ä»»åŠ¡ä¸Šçš„è¡¨ç°

### å‚è€ƒç­”æ¡ˆ

<details>
<summary>ç»ƒä¹  1 å‚è€ƒç­”æ¡ˆ</summary>

```python
from transformers import GPT2LMHeadModel, GPT2Tokenizer

model = GPT2LMHeadModel.from_pretrained('gpt2')
tokenizer = GPT2Tokenizer.from_pretrained('gpt2')

prompt = "In the beginning"
inputs = tokenizer(prompt, return_tensors="pt")

for temp in [0.5, 1.0, 1.5]:
    output = model.generate(
        **inputs,
        max_length=50,
        do_sample=True,
        temperature=temp,
        top_k=0,  # ç¦ç”¨ top-k
        top_p=1.0  # ç¦ç”¨ top-p
    )
    print(f"Temp={temp}:")
    print(tokenizer.decode(output[0]))
    print("-" * 50)
```

</details>

---

## â¡ï¸ ä¸‹ä¸€æ­¥

å­¦å®Œæœ¬èŠ‚åï¼Œç»§ç»­å­¦ä¹  [11-æ¨¡å‹é‡åŒ–ä¸åŠ é€Ÿ.md](./11-æ¨¡å‹é‡åŒ–ä¸åŠ é€Ÿ.md)

