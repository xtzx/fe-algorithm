# ğŸ”¤ 06 - Tokenization

> Tokenization æ˜¯ LLM å¤„ç†æ–‡æœ¬çš„ç¬¬ä¸€æ­¥ï¼Œç†è§£å®ƒå¯¹äºä½¿ç”¨å’Œä¼˜åŒ–æ¨¡å‹è‡³å…³é‡è¦

---

## ç›®å½•

1. [ä»€ä¹ˆæ˜¯ Tokenization](#1-ä»€ä¹ˆæ˜¯-tokenization)
2. [BPE ç®—æ³•](#2-bpe-ç®—æ³•)
3. [å…¶ä»–åˆ†è¯ç®—æ³•](#3-å…¶ä»–åˆ†è¯ç®—æ³•)
4. [Hugging Face Tokenizers](#4-hugging-face-tokenizers)
5. [ç‰¹æ®Š Token](#5-ç‰¹æ®Š-token)
6. [å®è·µæ³¨æ„äº‹é¡¹](#6-å®è·µæ³¨æ„äº‹é¡¹)
7. [ç»ƒä¹ é¢˜](#7-ç»ƒä¹ é¢˜)

---

## 1. ä»€ä¹ˆæ˜¯ Tokenization

### 1.1 ä¸ºä»€ä¹ˆéœ€è¦ Tokenization

```
è®¡ç®—æœºä¸èƒ½ç›´æ¥ç†è§£æ–‡å­—ï¼Œéœ€è¦è½¬æ¢ä¸ºæ•°å­—

ç®€å•æ–¹æ³•ï¼š
1. å­—ç¬¦çº§ï¼š'H','e','l','l','o' â†’ [1,2,3,3,4]
   - è¯è¡¨å°ï¼Œä½†åºåˆ—é•¿

2. è¯çº§ï¼š'Hello', 'World' â†’ [1234, 5678]
   - åºåˆ—çŸ­ï¼Œä½†è¯è¡¨å¤§
   - æœªè§è¯ï¼ˆOOVï¼‰é—®é¢˜

å­è¯ Tokenizationï¼ˆç°ä»£æ–¹æ³•ï¼‰ï¼š
- å¸¸è§è¯ä¿æŒå®Œæ•´
- ç½•è§è¯æ‹†æˆå­è¯
- å¹³è¡¡è¯è¡¨å¤§å°å’Œåºåˆ—é•¿åº¦
```

### 1.2 Token vs å­—ç¬¦ vs è¯

```python
text = "Hello, ChatGPT! ä½ å¥½ï¼Œä¸–ç•Œï¼"

# å­—ç¬¦çº§
chars = list(text)
print(f"å­—ç¬¦æ•°: {len(chars)}")  # 20

# è¯çº§ï¼ˆç®€å•ç©ºæ ¼åˆ†å‰²ï¼‰
words = text.split()
print(f"è¯æ•°: {len(words)}")  # 3

# å­è¯çº§ï¼ˆGPT Tokenizerï¼‰
from transformers import GPT2Tokenizer
tokenizer = GPT2Tokenizer.from_pretrained('gpt2')
tokens = tokenizer.tokenize(text)
print(f"Token æ•°: {len(tokens)}")  # çº¦ 10-15
print(f"Tokens: {tokens}")

# ä¸­æ–‡ï¼šæ¯ä¸ªå­—é€šå¸¸æ˜¯ 1-2 ä¸ª token
# è‹±æ–‡ï¼šå¸¸è§è¯ 1 ä¸ª tokenï¼Œç½•è§è¯å¤šä¸ª token
```

### 1.3 Token ä¸æˆæœ¬

```
API å®šä»·é€šå¸¸æŒ‰ token è®¡è´¹ï¼š
- GPT-4: ~$0.03 / 1K input tokens
- Claude: ~$0.003 / 1K input tokens

ç»éªŒæ³•åˆ™ï¼š
- è‹±æ–‡ï¼š1 token â‰ˆ 4 å­—ç¬¦ â‰ˆ 0.75 è¯
- ä¸­æ–‡ï¼š1 token â‰ˆ 1-2 æ±‰å­—

"The quick brown fox" = 4 tokens
"æ•æ·çš„æ£•è‰²ç‹ç‹¸è·³è¿‡" â‰ˆ 8-10 tokens
```

---

## 2. BPE ç®—æ³•

### 2.1 BPE åŸç†

```
BPEï¼ˆByte Pair Encodingï¼‰ï¼šè¿­ä»£åˆå¹¶æœ€å¸¸è§çš„å­—ç¬¦å¯¹

åˆå§‹ï¼šè¯è¡¨æ˜¯æ‰€æœ‰å•å­—ç¬¦
è¿­ä»£ï¼š
  1. ç»Ÿè®¡æ‰€æœ‰ç›¸é‚»å­—ç¬¦å¯¹çš„é¢‘ç‡
  2. åˆå¹¶é¢‘ç‡æœ€é«˜çš„å­—ç¬¦å¯¹ä¸ºæ–° token
  3. æ›´æ–°è¯è¡¨
  4. é‡å¤ç›´åˆ°è¾¾åˆ°ç›®æ ‡è¯è¡¨å¤§å°

ç¤ºä¾‹ï¼š
åŸå§‹æ–‡æœ¬ï¼š"aaabdaaabac"

Step 1: æœ€å¸¸è§å¯¹ "aa" â†’ åˆå¹¶ä¸º "Z"
  â†’ "ZabdZabac" (Z=aa)

Step 2: æœ€å¸¸è§å¯¹ "Za" â†’ åˆå¹¶ä¸º "Y"
  â†’ "YbdYbac" (Y=Za=aaa)

Step 3: ...ç»§ç»­
```

### 2.2 BPE å®ç°

```python
from collections import Counter, defaultdict

def get_pair_stats(vocab):
    """ç»Ÿè®¡ç›¸é‚»å­—ç¬¦å¯¹çš„é¢‘ç‡"""
    pairs = defaultdict(int)
    for word, freq in vocab.items():
        symbols = word.split()
        for i in range(len(symbols) - 1):
            pairs[(symbols[i], symbols[i+1])] += freq
    return pairs

def merge_vocab(pair, vocab):
    """åˆå¹¶æœ€å¸¸è§çš„å­—ç¬¦å¯¹"""
    new_vocab = {}
    bigram = ' '.join(pair)
    replacement = ''.join(pair)

    for word, freq in vocab.items():
        new_word = word.replace(bigram, replacement)
        new_vocab[new_word] = freq

    return new_vocab

def train_bpe(text, num_merges):
    """è®­ç»ƒ BPE"""
    # åˆå§‹åŒ–ï¼šæ¯ä¸ªå­—ç¬¦åŠ ç©ºæ ¼ï¼Œè¯å°¾åŠ  </w>
    words = text.split()
    vocab = Counter()
    for word in words:
        word_with_space = ' '.join(list(word)) + ' </w>'
        vocab[word_with_space] += 1

    merges = []

    for i in range(num_merges):
        pairs = get_pair_stats(vocab)
        if not pairs:
            break

        best_pair = max(pairs, key=pairs.get)
        merges.append(best_pair)
        vocab = merge_vocab(best_pair, vocab)

        print(f"Merge {i+1}: {best_pair} â†’ {''.join(best_pair)}")

    return vocab, merges

# ç¤ºä¾‹
text = "low lower lowest low lower new newer newest"
vocab, merges = train_bpe(text, num_merges=10)

print("\næœ€ç»ˆè¯è¡¨:")
for word, freq in sorted(vocab.items(), key=lambda x: -x[1])[:10]:
    print(f"  {word}: {freq}")
```

### 2.3 BPE åˆ†è¯

```python
def bpe_tokenize(text, merges):
    """ä½¿ç”¨å­¦åˆ°çš„ BPE è§„åˆ™åˆ†è¯"""
    words = text.split()
    tokens = []

    for word in words:
        # åˆå§‹åŒ–ä¸ºå­—ç¬¦
        word_tokens = list(word) + ['</w>']

        # åº”ç”¨åˆå¹¶è§„åˆ™
        while True:
            # æ‰¾åˆ°å¯ä»¥åˆå¹¶çš„å¯¹
            pairs = [(word_tokens[i], word_tokens[i+1])
                     for i in range(len(word_tokens) - 1)]

            # æ‰¾åˆ°åœ¨ merges ä¸­æ’åæœ€é«˜çš„å¯¹
            best_pair = None
            best_idx = float('inf')
            for pair in pairs:
                if pair in merges:
                    idx = merges.index(pair)
                    if idx < best_idx:
                        best_idx = idx
                        best_pair = pair

            if best_pair is None:
                break

            # åˆå¹¶
            new_tokens = []
            i = 0
            while i < len(word_tokens):
                if i < len(word_tokens) - 1 and \
                   (word_tokens[i], word_tokens[i+1]) == best_pair:
                    new_tokens.append(''.join(best_pair))
                    i += 2
                else:
                    new_tokens.append(word_tokens[i])
                    i += 1
            word_tokens = new_tokens

        tokens.extend(word_tokens)

    return tokens

# æµ‹è¯•
tokens = bpe_tokenize("lowest", merges)
print(f"'lowest' â†’ {tokens}")
```

---

## 3. å…¶ä»–åˆ†è¯ç®—æ³•

### 3.1 WordPieceï¼ˆBERTï¼‰

```
WordPieceï¼šç±»ä¼¼ BPEï¼Œä½†ä½¿ç”¨ä¸åŒçš„åˆå¹¶ç­–ç•¥

BPEï¼šåˆå¹¶é¢‘ç‡æœ€é«˜çš„å¯¹
WordPieceï¼šåˆå¹¶èƒ½æœ€å¤§åŒ–è¯­è¨€æ¨¡å‹ä¼¼ç„¶çš„å¯¹

ç‰¹ç‚¹ï¼š
- å­è¯ç”¨ ## å‰ç¼€è¡¨ç¤ºï¼ˆé™¤äº†è¯é¦–ï¼‰
- "unbelievable" â†’ ["un", "##believ", "##able"]
```

### 3.2 Unigramï¼ˆT5ã€ALBERTï¼‰

```
Unigramï¼šä»å¤§è¯è¡¨å¼€å§‹ï¼Œé€æ­¥åˆ å‡

1. åˆå§‹åŒ–ï¼šåŒ…å«æ‰€æœ‰å¯èƒ½å­ä¸²çš„å¤§è¯è¡¨
2. è®¡ç®—æ¯ä¸ªå­è¯çš„é‡è¦æ€§ï¼ˆåŸºäºè¯­è¨€æ¨¡å‹ï¼‰
3. åˆ é™¤ä¸é‡è¦çš„å­è¯
4. é‡å¤ç›´åˆ°è¾¾åˆ°ç›®æ ‡å¤§å°

ä¼˜ç‚¹ï¼šå¯ä»¥è¾“å‡ºå¤šç§åˆ†è¯ç»“æœçš„æ¦‚ç‡
```

### 3.3 SentencePiece

```python
# SentencePieceï¼šè¯­è¨€æ— å…³çš„åˆ†è¯å™¨
# ä¸ä¾èµ–é¢„åˆ†è¯ï¼ˆç©ºæ ¼åˆ†å‰²ï¼‰ï¼Œç›´æ¥åœ¨åŸå§‹æ–‡æœ¬ä¸Šè®­ç»ƒ

# å®‰è£…
# pip install sentencepiece

import sentencepiece as spm

# è®­ç»ƒ
spm.SentencePieceTrainer.train(
    input='data.txt',
    model_prefix='mymodel',
    vocab_size=8000,
    model_type='bpe'  # æˆ– 'unigram'
)

# åŠ è½½
sp = spm.SentencePieceProcessor()
sp.load('mymodel.model')

# åˆ†è¯
text = "Hello, world! ä½ å¥½ä¸–ç•Œ"
tokens = sp.encode_as_pieces(text)
print(f"Tokens: {tokens}")

ids = sp.encode_as_ids(text)
print(f"IDs: {ids}")

# è§£ç 
decoded = sp.decode_pieces(tokens)
print(f"Decoded: {decoded}")
```

### 3.4 ç®—æ³•å¯¹æ¯”

| ç®—æ³• | åˆå¹¶ç­–ç•¥ | ä½¿ç”¨æ¨¡å‹ |
|------|---------|---------|
| BPE | é¢‘ç‡æœ€é«˜ | GPT, LLaMA |
| WordPiece | ä¼¼ç„¶æœ€å¤§ | BERT |
| Unigram | è¯­è¨€æ¨¡å‹ | T5, ALBERT |
| SentencePiece | BPE/Unigram | å¤šè¯­è¨€æ¨¡å‹ |

---

## 4. Hugging Face Tokenizers

### 4.1 åŸºæœ¬ä½¿ç”¨

```python
from transformers import AutoTokenizer

# åŠ è½½é¢„è®­ç»ƒ Tokenizer
tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")

# åˆ†è¯
text = "Hello, how are you doing today?"

# æ–¹æ³• 1ï¼šåŸºç¡€åˆ†è¯
tokens = tokenizer.tokenize(text)
print(f"Tokens: {tokens}")

# æ–¹æ³• 2ï¼šå®Œæ•´ç¼–ç ï¼ˆæ¨èï¼‰
encoding = tokenizer(text)
print(f"Input IDs: {encoding['input_ids']}")
print(f"Attention Mask: {encoding['attention_mask']}")

# æ–¹æ³• 3ï¼šç¼–ç å¹¶å¡«å……åˆ°å›ºå®šé•¿åº¦
encoding = tokenizer(
    text,
    padding='max_length',
    max_length=20,
    truncation=True,
    return_tensors='pt'
)
print(f"Padded IDs: {encoding['input_ids']}")

# è§£ç 
decoded = tokenizer.decode(encoding['input_ids'][0])
print(f"Decoded: {decoded}")
```

### 4.2 æ‰¹é‡å¤„ç†

```python
texts = [
    "Hello, world!",
    "This is a longer sentence that will need different handling.",
    "Short."
]

# æ‰¹é‡ç¼–ç ï¼ˆè‡ªåŠ¨å¡«å……åˆ°æœ€é•¿åºåˆ—ï¼‰
batch_encoding = tokenizer(
    texts,
    padding=True,
    truncation=True,
    max_length=32,
    return_tensors='pt'
)

print(f"Batch shape: {batch_encoding['input_ids'].shape}")
print(f"Attention masks:\n{batch_encoding['attention_mask']}")
```

### 4.3 ä¸åŒæ¨¡å‹çš„ Tokenizer

```python
# GPT-2 Tokenizerï¼ˆBPEï¼‰
gpt2_tokenizer = AutoTokenizer.from_pretrained("gpt2")
print(gpt2_tokenizer.tokenize("Hello, world!"))
# ['Hello', ',', 'Ä world', '!']  # Ä  è¡¨ç¤ºå‰é¢æœ‰ç©ºæ ¼

# BERT Tokenizerï¼ˆWordPieceï¼‰
bert_tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")
print(bert_tokenizer.tokenize("unbelievable"))
# ['un', '##believable'] æˆ– ['un', '##believ', '##able']

# LLaMA Tokenizerï¼ˆSentencePieceï¼‰
# llama_tokenizer = AutoTokenizer.from_pretrained("meta-llama/Llama-2-7b-hf")

# ä¸­æ–‡ Tokenizer
chinese_tokenizer = AutoTokenizer.from_pretrained("bert-base-chinese")
print(chinese_tokenizer.tokenize("ä½ å¥½ä¸–ç•Œ"))
# ['ä½ ', 'å¥½', 'ä¸–', 'ç•Œ']
```

### 4.4 Tokenizer çš„è¯è¡¨

```python
tokenizer = AutoTokenizer.from_pretrained("gpt2")

# è¯è¡¨å¤§å°
print(f"è¯è¡¨å¤§å°: {len(tokenizer)}")  # 50257

# æŸ¥çœ‹è¯è¡¨
vocab = tokenizer.get_vocab()
print(f"å‰ 10 ä¸ª token: {list(vocab.items())[:10]}")

# Token ID è½¬æ¢
token = "hello"
token_id = tokenizer.convert_tokens_to_ids(token)
print(f"'{token}' â†’ ID: {token_id}")

# ID è½¬ Token
token_back = tokenizer.convert_ids_to_tokens(token_id)
print(f"ID {token_id} â†’ '{token_back}'")

# æ·»åŠ æ–° token
num_added = tokenizer.add_tokens(['[CUSTOM]', '[SPECIAL]'])
print(f"æ·»åŠ äº† {num_added} ä¸ªæ–° token")
print(f"æ–°è¯è¡¨å¤§å°: {len(tokenizer)}")
```

---

## 5. ç‰¹æ®Š Token

### 5.1 å¸¸è§ç‰¹æ®Š Token

```python
from transformers import AutoTokenizer

# BERT
bert_tok = AutoTokenizer.from_pretrained("bert-base-uncased")
print(f"BERT ç‰¹æ®Š token:")
print(f"  [PAD]: {bert_tok.pad_token} (ID: {bert_tok.pad_token_id})")
print(f"  [UNK]: {bert_tok.unk_token} (ID: {bert_tok.unk_token_id})")
print(f"  [CLS]: {bert_tok.cls_token} (ID: {bert_tok.cls_token_id})")
print(f"  [SEP]: {bert_tok.sep_token} (ID: {bert_tok.sep_token_id})")
print(f"  [MASK]: {bert_tok.mask_token} (ID: {bert_tok.mask_token_id})")

# GPT-2
gpt2_tok = AutoTokenizer.from_pretrained("gpt2")
print(f"\nGPT-2 ç‰¹æ®Š token:")
print(f"  BOS: {gpt2_tok.bos_token}")  # å¯èƒ½ä¸º None
print(f"  EOS: {gpt2_tok.eos_token} (ID: {gpt2_tok.eos_token_id})")
```

### 5.2 ç‰¹æ®Š Token çš„ä½œç”¨

```
[PAD] - å¡«å…… tokenï¼Œç”¨äºå¯¹é½ä¸åŒé•¿åº¦çš„åºåˆ—
[UNK] - æœªçŸ¥ tokenï¼Œç”¨äºè¯è¡¨å¤–çš„è¯
[CLS] - å¥å­å¼€å§‹ tokenï¼Œå…¶è¾“å‡ºç”¨äºå¥å­çº§ä»»åŠ¡ï¼ˆBERTï¼‰
[SEP] - åˆ†éš” tokenï¼Œç”¨äºåˆ†éš”å¥å­å¯¹
[MASK] - æ©ç  tokenï¼Œç”¨äº MLM è®­ç»ƒ
[BOS] - åºåˆ—å¼€å§‹ tokenï¼ˆBegin of Sequenceï¼‰
[EOS] - åºåˆ—ç»“æŸ tokenï¼ˆEnd of Sequenceï¼‰

LLM ä¸­çš„ç‰¹æ®Š tokenï¼š
<|im_start|> - æ¶ˆæ¯å¼€å§‹
<|im_end|> - æ¶ˆæ¯ç»“æŸ
<|system|>, <|user|>, <|assistant|> - è§’è‰²æ ‡è®°
```

### 5.3 Chat Template

```python
from transformers import AutoTokenizer

# ä½¿ç”¨ chat template
tokenizer = AutoTokenizer.from_pretrained("microsoft/DialoGPT-medium")

# æ„å»ºå¯¹è¯
messages = [
    {"role": "user", "content": "Hello!"},
    {"role": "assistant", "content": "Hi there! How can I help you?"},
    {"role": "user", "content": "What's the weather like?"}
]

# åº”ç”¨ chat templateï¼ˆå¦‚æœæ”¯æŒï¼‰
try:
    formatted = tokenizer.apply_chat_template(messages, tokenize=False)
    print(f"Formatted:\n{formatted}")
except:
    print("This tokenizer doesn't have a chat template")

# æ‰‹åŠ¨æ„å»ºï¼ˆé€šç”¨æ–¹æ³•ï¼‰
def format_chat(messages, system_prompt="You are a helpful assistant."):
    formatted = f"<|system|>\n{system_prompt}<|end|>\n"
    for msg in messages:
        role = msg['role']
        content = msg['content']
        formatted += f"<|{role}|>\n{content}<|end|>\n"
    formatted += "<|assistant|>\n"
    return formatted
```

---

## 6. å®è·µæ³¨æ„äº‹é¡¹

### 6.1 ä¸Šä¸‹æ–‡çª—å£

```python
# æ£€æŸ¥ä¸Šä¸‹æ–‡çª—å£é™åˆ¶
tokenizer = AutoTokenizer.from_pretrained("gpt2")
print(f"GPT-2 æœ€å¤§é•¿åº¦: {tokenizer.model_max_length}")  # 1024

# å¤„ç†è¶…é•¿æ–‡æœ¬
long_text = "..." * 2000  # å¾ˆé•¿çš„æ–‡æœ¬

encoding = tokenizer(
    long_text,
    truncation=True,
    max_length=512,
    return_overflowing_tokens=True,  # è¿”å›æº¢å‡ºçš„ token
    stride=50  # æ»‘åŠ¨çª—å£é‡å 
)

print(f"åˆ†å—æ•°: {len(encoding['input_ids'])}")
```

### 6.2 Tokenization å½±å“

```python
# Token æ•°é‡å½±å“ç”Ÿæˆè´¨é‡å’Œæˆæœ¬

def analyze_tokenization(text, model_name):
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    tokens = tokenizer.tokenize(text)

    print(f"\n{model_name}:")
    print(f"  æ–‡æœ¬é•¿åº¦: {len(text)} å­—ç¬¦")
    print(f"  Token æ•°: {len(tokens)}")
    print(f"  å‹ç¼©ç‡: {len(text) / len(tokens):.2f} å­—ç¬¦/token")
    print(f"  Tokens: {tokens[:10]}...")

text_en = "The quick brown fox jumps over the lazy dog."
text_zh = "æ•æ·çš„æ£•è‰²ç‹ç‹¸è·³è¿‡äº†æ‡’æƒ°çš„ç‹—ã€‚"

for text in [text_en, text_zh]:
    print(f"\n{'='*50}")
    print(f"Text: {text}")
    analyze_tokenization(text, "gpt2")
    # analyze_tokenization(text, "bert-base-chinese")
```

### 6.3 å¤„ç†ç‰¹æ®Šæƒ…å†µ

```python
# å¤„ç†ä»£ç 
code = """
def hello():
    print("Hello, World!")
"""

tokenizer = AutoTokenizer.from_pretrained("gpt2")
tokens = tokenizer.tokenize(code)
print(f"ä»£ç  tokens: {tokens}")
# æ³¨æ„ï¼šç¼©è¿›å’Œæ¢è¡Œä¹Ÿä¼šè¢« tokenize

# å¤„ç† URL
url = "https://www.example.com/path?param=value"
tokens = tokenizer.tokenize(url)
print(f"URL tokens: {tokens}")
# URL é€šå¸¸ä¼šè¢«æ‹†æˆå¾ˆå¤š token

# å¤„ç†æ•°å­—
numbers = "The year is 2024 and pi is 3.14159"
tokens = tokenizer.tokenize(numbers)
print(f"æ•°å­— tokens: {tokens}")
# æ•°å­—å¯èƒ½æ¯ä½ä¸€ä¸ª token
```

---

## 7. ç»ƒä¹ é¢˜

### åŸºç¡€ç»ƒä¹ 

1. æ‰‹åŠ¨å®ç° BPE è®­ç»ƒå’Œåˆ†è¯
2. æ¯”è¾ƒä¸åŒæ¨¡å‹çš„ Tokenizer åœ¨ä¸­è‹±æ–‡ä¸Šçš„è¡¨ç°
3. ä¼°ç®—ä¸€æ®µæ–‡æœ¬çš„ API æˆæœ¬

### å‚è€ƒç­”æ¡ˆ

<details>
<summary>ç‚¹å‡»æŸ¥çœ‹ç­”æ¡ˆ</summary>

```python
# 1. BPE å®ç°è§ä¸Šæ–‡

# 2. Tokenizer å¯¹æ¯”
from transformers import AutoTokenizer

def compare_tokenizers(text):
    models = [
        ("GPT-2", "gpt2"),
        ("BERT", "bert-base-uncased"),
        ("BERT-Chinese", "bert-base-chinese"),
    ]

    print(f"Text: {text}")
    print("-" * 50)

    for name, model_name in models:
        try:
            tok = AutoTokenizer.from_pretrained(model_name)
            tokens = tok.tokenize(text)
            ids = tok.encode(text)
            print(f"{name}:")
            print(f"  Tokens: {len(tokens)}")
            print(f"  {tokens[:10]}...")
        except:
            print(f"{name}: åŠ è½½å¤±è´¥")
        print()

# è‹±æ–‡æµ‹è¯•
compare_tokenizers("Hello, how are you doing today?")

# ä¸­æ–‡æµ‹è¯•
compare_tokenizers("ä½ å¥½ï¼Œä»Šå¤©å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿ")


# 3. API æˆæœ¬ä¼°ç®—
def estimate_cost(text, model="gpt-4"):
    """ä¼°ç®— API è°ƒç”¨æˆæœ¬"""
    tokenizer = AutoTokenizer.from_pretrained("gpt2")  # è¿‘ä¼¼
    num_tokens = len(tokenizer.encode(text))

    # ä»·æ ¼ï¼ˆç¾å…ƒ/1K tokensï¼‰
    prices = {
        "gpt-4": {"input": 0.03, "output": 0.06},
        "gpt-3.5-turbo": {"input": 0.0015, "output": 0.002},
        "claude-3-opus": {"input": 0.015, "output": 0.075},
    }

    price = prices.get(model, {"input": 0.01, "output": 0.01})

    # å‡è®¾è¾“å‡ºå’Œè¾“å…¥ä¸€æ ·é•¿
    input_cost = num_tokens / 1000 * price["input"]
    output_cost = num_tokens / 1000 * price["output"]

    print(f"æ–‡æœ¬: {text[:50]}...")
    print(f"Token æ•°: {num_tokens}")
    print(f"æ¨¡å‹: {model}")
    print(f"  è¾“å…¥æˆæœ¬: ${input_cost:.4f}")
    print(f"  è¾“å‡ºæˆæœ¬: ${output_cost:.4f}")
    print(f"  æ€»æˆæœ¬: ${input_cost + output_cost:.4f}")

# æµ‹è¯•
long_text = "è¿™æ˜¯ä¸€æ®µå¾ˆé•¿çš„æ–‡æœ¬..." * 100
estimate_cost(long_text, "gpt-4")
```

</details>

---

## â¡ï¸ ä¸‹ä¸€æ­¥

å­¦å®Œæœ¬èŠ‚åï¼Œç»§ç»­å­¦ä¹  [07-HuggingFaceç”Ÿæ€.md](./07-HuggingFaceç”Ÿæ€.md)

