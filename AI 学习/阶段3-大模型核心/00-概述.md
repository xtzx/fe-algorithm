# ğŸ› é˜¶æ®µ 3ï¼šå¤§æ¨¡å‹æ ¸å¿ƒæŠ€æœ¯ï¼ˆLLM Coreï¼‰

> **ç›®æ ‡**ï¼šåƒé€ Transformerï¼Œç†è§£ç°ä»£ LLM çš„å…³é”®æŠ€æœ¯ï¼ŒæŒæ¡ Hugging Face ç”Ÿæ€
> **å‰ç½®è¦æ±‚**ï¼šPyTorch åŸºç¡€ï¼ˆé˜¶æ®µ 2ï¼‰

---

## ğŸ¯ æœ¬é˜¶æ®µç›®æ ‡

å®Œæˆæœ¬é˜¶æ®µåï¼Œä½ å°†èƒ½å¤Ÿï¼š

1. **åƒé€ Transformer**ï¼šç†è§£ Self-Attentionã€Multi-Head Attentionã€ä½ç½®ç¼–ç 
2. **ç†è§£ç°ä»£ LLM æŠ€æœ¯**ï¼šKV Cacheã€Flash Attentionã€GQAã€MoE
3. **æŒæ¡è§£ç ä¸é‡åŒ–**ï¼šé‡‡æ ·ç­–ç•¥ã€æ¨¡å‹é‡åŒ–ã€æ¨ç†åŠ é€Ÿ
4. **æŒæ¡ Hugging Face ç”Ÿæ€**ï¼štransformersã€datasetsã€Model Hub
5. **å®Œæˆå®æˆ˜é¡¹ç›®**ï¼šTransformer æƒ…æ„Ÿåˆ†ç±» + BERT æ–‡æœ¬åˆ†ç±» + è¯­ä¹‰æœç´¢ + æ‰‹å†™æç®€ GPT

---

## ğŸ“ æœ¬é˜¶æ®µæ–‡ä»¶ç»“æ„

```
é˜¶æ®µ3-å¤§æ¨¡å‹æ ¸å¿ƒ/
â”œâ”€â”€ 00-æ¦‚è¿°.md                      # æœ¬æ–‡ä»¶
â”œâ”€â”€ 01-Transformeræ¶æ„.md           # Self-Attentionã€Multi-Headã€FFN
â”œâ”€â”€ 02-æ³¨æ„åŠ›æœºåˆ¶è¯¦è§£.md             # æ³¨æ„åŠ›è®¡ç®—ã€æ©ç ã€å¯è§†åŒ–
â”œâ”€â”€ 03-ä½ç½®ç¼–ç .md                  # ç»å¯¹ä½ç½®ã€RoPEã€ALiBi
â”œâ”€â”€ 04-Encoderä¸Decoder.md         # BERT vs GPT vs T5
â”œâ”€â”€ 05-LLMä¼˜åŒ–æŠ€æœ¯.md               # KV Cacheã€Flash Attentionã€GQAã€MoE
â”œâ”€â”€ 06-Tokenization.md             # BPEã€WordPieceã€SentencePiece
â”œâ”€â”€ 07-HuggingFaceç”Ÿæ€.md          # transformersã€datasetsã€pipeline
â”œâ”€â”€ 08-Embeddingä¸å‘é‡æ£€ç´¢.md       # è¯­ä¹‰å‘é‡ã€Faissã€ChromaDB
â”œâ”€â”€ 09-å¤šæ¨¡æ€åŸºç¡€.md                # Diffusionã€CLIPã€å¤šæ¨¡æ€ LLM
â”œâ”€â”€ 10-è§£ç ç­–ç•¥ä¸é‡‡æ ·.md            # Greedyã€Beamã€Top-K/P âœ¨ æ–°å¢
â”œâ”€â”€ 11-æ¨¡å‹é‡åŒ–ä¸åŠ é€Ÿ.md            # INT8/INT4ã€æ¨ç†ä¼˜åŒ– âœ¨ æ–°å¢
â”œâ”€â”€ 12-é¡¹ç›®-Transformeræƒ…æ„Ÿåˆ†ç±».md  # ä»é›¶å®ç° Transformer âœ¨ æ–°å¢ï¼ˆå…¥é—¨ï¼‰
â”œâ”€â”€ 13-é¡¹ç›®-BERTæ–‡æœ¬åˆ†ç±».md         # å®Œæ•´ BERT å¾®è°ƒé¡¹ç›®
â”œâ”€â”€ 14-é¡¹ç›®-è¯­ä¹‰æœç´¢å¼•æ“.md         # Embedding + å‘é‡æ£€ç´¢
â”œâ”€â”€ 15-é¡¹ç›®-æ‰‹å†™æç®€GPT.md          # nanoGPT é£æ ¼çš„å®ç°
â””â”€â”€ 16-è‡ªæµ‹æ¸…å•.md                  # æ ¸å¿ƒé—®é¢˜è‡ªæµ‹
```

---

## ğŸ—ºï¸ LLM æŠ€æœ¯å…¨æ™¯å›¾

```
å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰
â”‚
â”œâ”€â”€ æ ¸å¿ƒæ¶æ„ï¼šTransformer
â”‚   â”œâ”€â”€ Self-Attentionï¼ˆæ ¸å¿ƒï¼ï¼‰
â”‚   â”œâ”€â”€ Multi-Head Attention
â”‚   â”œâ”€â”€ Feed Forward Network
â”‚   â”œâ”€â”€ æ®‹å·®è¿æ¥ + LayerNorm
â”‚   â””â”€â”€ ä½ç½®ç¼–ç ï¼ˆRoPEã€ALiBiï¼‰
â”‚
â”œâ”€â”€ æ¨¡å‹ç±»å‹
â”‚   â”œâ”€â”€ Encoder-only: BERTï¼ˆç†è§£ï¼‰
â”‚   â”œâ”€â”€ Decoder-only: GPT/LLaMAï¼ˆç”Ÿæˆï¼‰
â”‚   â””â”€â”€ Encoder-Decoder: T5ï¼ˆç¿»è¯‘ã€æ‘˜è¦ï¼‰
â”‚
â”œâ”€â”€ ä¼˜åŒ–æŠ€æœ¯
â”‚   â”œâ”€â”€ KV Cacheï¼ˆæ¨ç†åŠ é€Ÿï¼‰
â”‚   â”œâ”€â”€ Flash Attentionï¼ˆæ˜¾å­˜ä¼˜åŒ–ï¼‰
â”‚   â”œâ”€â”€ GQA/MQAï¼ˆå‡å°‘ KV å¤´æ•°ï¼‰
â”‚   â””â”€â”€ MoEï¼ˆç¨€ç–æ¿€æ´»ï¼‰
â”‚
â”œâ”€â”€ è§£ç ä¸é‡‡æ ·
â”‚   â”œâ”€â”€ Greedy Decoding
â”‚   â”œâ”€â”€ Beam Search
â”‚   â”œâ”€â”€ Top-K / Top-P Sampling
â”‚   â””â”€â”€ Contrastive Search
â”‚
â”œâ”€â”€ æ¨¡å‹é‡åŒ–
â”‚   â”œâ”€â”€ INT8 / INT4 é‡åŒ–
â”‚   â”œâ”€â”€ GPTQ / AWQ
â”‚   â””â”€â”€ æ¨ç†åŠ é€ŸæŠ€æœ¯
â”‚
â”œâ”€â”€ è¾“å…¥å¤„ç†
â”‚   â”œâ”€â”€ Tokenizationï¼ˆBPEã€SentencePieceï¼‰
â”‚   â””â”€â”€ Embeddingï¼ˆè¯åµŒå…¥ + ä½ç½®åµŒå…¥ï¼‰
â”‚
â”œâ”€â”€ ç”Ÿæ€å·¥å…·
â”‚   â”œâ”€â”€ Hugging Face transformers
â”‚   â”œâ”€â”€ Hugging Face datasets
â”‚   â””â”€â”€ Model Hub
â”‚
â””â”€â”€ åº”ç”¨æ–¹å‘
    â”œâ”€â”€ æ–‡æœ¬åˆ†ç±»/NERï¼ˆBERTï¼‰
    â”œâ”€â”€ æ–‡æœ¬ç”Ÿæˆï¼ˆGPTï¼‰
    â”œâ”€â”€ è¯­ä¹‰æœç´¢ï¼ˆEmbeddingï¼‰
    â””â”€â”€ å¤šæ¨¡æ€ï¼ˆCLIPã€LLaVAï¼‰
```

---

## ğŸ› ï¸ ç¯å¢ƒå‡†å¤‡

### å®‰è£…ä¾èµ–

```bash
# åˆ›å»ºç¯å¢ƒ
conda create -n llm_env python=3.11
conda activate llm_env

# æ ¸å¿ƒåº“
pip install torch torchvision torchaudio
pip install transformers datasets tokenizers
pip install accelerate  # å¤§æ¨¡å‹åŠ è½½
pip install sentencepiece  # Tokenization

# é‡åŒ–ç›¸å…³
pip install bitsandbytes  # INT8/INT4 é‡åŒ–
pip install auto-gptq     # GPTQ é‡åŒ–

# å‘é‡æ£€ç´¢
pip install faiss-cpu  # æˆ– faiss-gpu
pip install chromadb

# å¯è§†åŒ–
pip install matplotlib seaborn
pip install bertviz  # æ³¨æ„åŠ›å¯è§†åŒ–

# å¯é€‰ï¼šJupyter
pip install jupyter notebook
```

### éªŒè¯å®‰è£…

```python
import torch
from transformers import AutoTokenizer, AutoModel

print(f"PyTorch: {torch.__version__}")
print(f"CUDA: {torch.cuda.is_available()}")

# æµ‹è¯• Hugging Face
tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")
tokens = tokenizer("Hello, world!")
print(f"Tokens: {tokens}")
```

---

## ğŸ“– æ ¸å¿ƒæ¦‚å¿µé¢„è§ˆ

### Transformer æ˜¯ä»€ä¹ˆï¼Ÿ

```
ä¼ ç»Ÿåºåˆ—æ¨¡å‹ï¼ˆRNNï¼‰ï¼šé¡ºåºå¤„ç†ï¼Œæ— æ³•å¹¶è¡Œ
      â†“
Transformerï¼ˆ2017ï¼‰ï¼šå¹¶è¡Œå¤„ç†ï¼Œæ³¨æ„åŠ›æœºåˆ¶
      â†“
ç°ä»£ LLMï¼ˆGPTã€LLaMAï¼‰ï¼šDecoder-only Transformer + å¤§è§„æ¨¡è®­ç»ƒ
```

### Self-Attention ç›´è§‰

```
è¾“å…¥ï¼š["I", "love", "AI"]

Self-Attention åšçš„äº‹ï¼š
è®©æ¯ä¸ªè¯"çœ‹"åˆ°å…¶ä»–æ‰€æœ‰è¯ï¼Œå­¦ä¹ è¯ä¹‹é—´çš„å…³ç³»

"love" å…³æ³¨ï¼š
- "I" (è°çˆ±ï¼Ÿ) â†’ é«˜æƒé‡
- "AI" (çˆ±ä»€ä¹ˆï¼Ÿ) â†’ é«˜æƒé‡

è¾“å‡ºï¼šæ¯ä¸ªè¯çš„è¡¨ç¤ºèåˆäº†ä¸Šä¸‹æ–‡ä¿¡æ¯
```

### KV Cache ç›´è§‰

```
ç”Ÿæˆæ—¶ï¼š
Step 1: "The"     â†’ è®¡ç®— K,V
Step 2: "The cat" â†’ é‡æ–°è®¡ç®—æ‰€æœ‰ K,Vï¼Ÿå¤ªæµªè´¹ï¼

KV Cacheï¼š
- ç¼“å­˜å·²è®¡ç®—çš„ K,V
- æ–° token åªè®¡ç®—æ–°çš„ K,V
- å¤§å¹…åŠ é€Ÿæ¨ç†
```

### æ¨¡å‹é‡åŒ–ç›´è§‰

```
LLaMA-7B æ˜¾å­˜éœ€æ±‚ï¼š
- FP32: 28 GB
- FP16: 14 GB
- INT8: 7 GB
- INT4: 3.5 GB

é‡åŒ– = ç”¨æ›´å°‘çš„æ¯”ç‰¹è¡¨ç¤ºæƒé‡
ç²¾åº¦æŸå¤±å¾ˆå°ï¼Œä½†æ˜¾å­˜èŠ‚çœå·¨å¤§
```

---

## ğŸ“š æ¨èèµ„æº

### å¿…è¯»
- [Attention Is All You Need](https://arxiv.org/abs/1706.03762) - Transformer åŸè®ºæ–‡
- [The Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/) - å¯è§†åŒ–è®²è§£
- [nanoGPT](https://github.com/karpathy/nanoGPT) - Karpathy çš„æç®€ GPT

### æ¨èè¯¾ç¨‹
- [Andrej Karpathy: Let's build GPT](https://www.youtube.com/watch?v=kCc8FmEb1nY)
- [Hugging Face NLP Course](https://huggingface.co/learn/nlp-course)
- [æå®æ¯… Transformer è®²è§£](https://www.bilibili.com/video/BV1J94y1f7u5)

### å®ç”¨å·¥å…·
- [Hugging Face Model Hub](https://huggingface.co/models)
- [Tokenizer Playground](https://platform.openai.com/tokenizer)

---

## â¡ï¸ ä¸‹ä¸€æ­¥

å‡†å¤‡å¥½äº†å—ï¼Ÿå¼€å§‹å­¦ä¹  [01-Transformeræ¶æ„.md](./01-Transformeræ¶æ„.md)ï¼
