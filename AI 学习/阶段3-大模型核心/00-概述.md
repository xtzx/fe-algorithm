# ğŸ› é˜¶æ®µ 3ï¼šå¤§æ¨¡å‹æ ¸å¿ƒæŠ€æœ¯ï¼ˆLLM Coreï¼‰

> **ç›®æ ‡**ï¼šåƒé€ Transformerï¼Œç†è§£ç°ä»£ LLM çš„å…³é”®æŠ€æœ¯ï¼ŒæŒæ¡ Hugging Face ç”Ÿæ€
> **å»ºè®®æ—¶é•¿**ï¼š4ï½6 å‘¨ï¼ˆæ¯å¤© 2ï½3 å°æ—¶ï¼‰
> **å‰ç½®è¦æ±‚**ï¼šPyTorch åŸºç¡€ï¼ˆé˜¶æ®µ 2ï¼‰

---

## ğŸ¯ æœ¬é˜¶æ®µç›®æ ‡

å®Œæˆæœ¬é˜¶æ®µåï¼Œä½ å°†èƒ½å¤Ÿï¼š

1. **åƒé€ Transformer**ï¼šç†è§£ Self-Attentionã€Multi-Head Attentionã€ä½ç½®ç¼–ç 
2. **ç†è§£ç°ä»£ LLM æŠ€æœ¯**ï¼šKV Cacheã€Flash Attentionã€GQAã€MoE
3. **æŒæ¡ Hugging Face ç”Ÿæ€**ï¼štransformersã€datasetsã€Model Hub
4. **å®Œæˆå®æˆ˜é¡¹ç›®**ï¼šBERT æ–‡æœ¬åˆ†ç±» + è¯­ä¹‰æœç´¢ + æ‰‹å†™æç®€ GPT

---

## ğŸ“ æœ¬é˜¶æ®µæ–‡ä»¶ç»“æ„

```
é˜¶æ®µ3-å¤§æ¨¡å‹æ ¸å¿ƒ/
â”œâ”€â”€ 00-æ¦‚è¿°.md                      # æœ¬æ–‡ä»¶
â”œâ”€â”€ 01-Transformeræ¶æ„.md           # Self-Attentionã€Multi-Headã€FFN
â”œâ”€â”€ 02-æ³¨æ„åŠ›æœºåˆ¶è¯¦è§£.md             # æ³¨æ„åŠ›è®¡ç®—ã€æ©ç ã€å¯è§†åŒ–
â”œâ”€â”€ 03-ä½ç½®ç¼–ç .md                  # ç»å¯¹ä½ç½®ã€RoPEã€ALiBi
â”œâ”€â”€ 04-Encoderä¸Decoder.md         # BERT vs GPT vs T5
â”œâ”€â”€ 05-LLMä¼˜åŒ–æŠ€æœ¯.md               # KV Cacheã€Flash Attentionã€GQAã€MoE
â”œâ”€â”€ 06-Tokenization.md             # BPEã€WordPieceã€SentencePiece
â”œâ”€â”€ 07-HuggingFaceç”Ÿæ€.md          # transformersã€datasetsã€pipeline
â”œâ”€â”€ 08-Embeddingä¸å‘é‡æ£€ç´¢.md       # è¯­ä¹‰å‘é‡ã€Faissã€ChromaDB
â”œâ”€â”€ 09-å¤šæ¨¡æ€åŸºç¡€.md                # Diffusionã€CLIPã€å¤šæ¨¡æ€ LLM
â”œâ”€â”€ 10-é¡¹ç›®-BERTæ–‡æœ¬åˆ†ç±».md         # å®Œæ•´ BERT å¾®è°ƒé¡¹ç›®
â”œâ”€â”€ 11-é¡¹ç›®-è¯­ä¹‰æœç´¢å¼•æ“.md         # Embedding + å‘é‡æ£€ç´¢
â”œâ”€â”€ 12-é¡¹ç›®-æ‰‹å†™æç®€GPT.md          # nanoGPT é£æ ¼çš„å®ç°
â””â”€â”€ 13-è‡ªæµ‹æ¸…å•.md                  # 15 ä¸ªæ ¸å¿ƒé—®é¢˜
```

---

## ğŸ“… å»ºè®®å­¦ä¹ è®¡åˆ’

### ç¬¬ 1-2 å‘¨ï¼šTransformer æ ¸å¿ƒ

| å¤©æ•° | å†…å®¹ | æ–‡ä»¶ |
|------|------|------|
| Day 1-2 | Transformer æ•´ä½“æ¶æ„ | `01-Transformeræ¶æ„.md` |
| Day 3-4 | æ³¨æ„åŠ›æœºåˆ¶æ·±å…¥ | `02-æ³¨æ„åŠ›æœºåˆ¶è¯¦è§£.md` |
| Day 5-6 | ä½ç½®ç¼–ç  | `03-ä½ç½®ç¼–ç .md` |
| Day 7 | Encoder vs Decoder | `04-Encoderä¸Decoder.md` |

### ç¬¬ 3 å‘¨ï¼šç°ä»£ LLM æŠ€æœ¯

| å¤©æ•° | å†…å®¹ | æ–‡ä»¶ |
|------|------|------|
| Day 8-10 | LLM ä¼˜åŒ–æŠ€æœ¯ | `05-LLMä¼˜åŒ–æŠ€æœ¯.md` |
| Day 11-12 | Tokenization | `06-Tokenization.md` |
| Day 13-14 | Hugging Face ç”Ÿæ€ | `07-HuggingFaceç”Ÿæ€.md` |

### ç¬¬ 4 å‘¨ï¼šEmbedding ä¸å¤šæ¨¡æ€

| å¤©æ•° | å†…å®¹ | æ–‡ä»¶ |
|------|------|------|
| Day 15-17 | Embedding ä¸å‘é‡æ£€ç´¢ | `08-Embeddingä¸å‘é‡æ£€ç´¢.md` |
| Day 18-19 | å¤šæ¨¡æ€åŸºç¡€ | `09-å¤šæ¨¡æ€åŸºç¡€.md` |
| Day 20-21 | BERT æ–‡æœ¬åˆ†ç±»é¡¹ç›® | `10-é¡¹ç›®-BERTæ–‡æœ¬åˆ†ç±».md` |

### ç¬¬ 5-6 å‘¨ï¼šé¡¹ç›®å®æˆ˜

| å¤©æ•° | å†…å®¹ | æ–‡ä»¶ |
|------|------|------|
| Day 22-25 | è¯­ä¹‰æœç´¢å¼•æ“ | `11-é¡¹ç›®-è¯­ä¹‰æœç´¢å¼•æ“.md` |
| Day 26-35 | æ‰‹å†™æç®€ GPT | `12-é¡¹ç›®-æ‰‹å†™æç®€GPT.md` |
| Day 36+ | è‡ªæµ‹ä¸æ€»ç»“ | `13-è‡ªæµ‹æ¸…å•.md` |

---

## ğŸ—ºï¸ LLM æŠ€æœ¯å…¨æ™¯å›¾

```
å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰
â”‚
â”œâ”€â”€ æ ¸å¿ƒæ¶æ„ï¼šTransformer
â”‚   â”œâ”€â”€ Self-Attentionï¼ˆæ ¸å¿ƒï¼ï¼‰
â”‚   â”œâ”€â”€ Multi-Head Attention
â”‚   â”œâ”€â”€ Feed Forward Network
â”‚   â”œâ”€â”€ æ®‹å·®è¿æ¥ + LayerNorm
â”‚   â””â”€â”€ ä½ç½®ç¼–ç ï¼ˆRoPEã€ALiBiï¼‰
â”‚
â”œâ”€â”€ æ¨¡å‹ç±»å‹
â”‚   â”œâ”€â”€ Encoder-only: BERTï¼ˆç†è§£ï¼‰
â”‚   â”œâ”€â”€ Decoder-only: GPT/LLaMAï¼ˆç”Ÿæˆï¼‰
â”‚   â””â”€â”€ Encoder-Decoder: T5ï¼ˆç¿»è¯‘ã€æ‘˜è¦ï¼‰
â”‚
â”œâ”€â”€ ä¼˜åŒ–æŠ€æœ¯
â”‚   â”œâ”€â”€ KV Cacheï¼ˆæ¨ç†åŠ é€Ÿï¼‰
â”‚   â”œâ”€â”€ Flash Attentionï¼ˆæ˜¾å­˜ä¼˜åŒ–ï¼‰
â”‚   â”œâ”€â”€ GQA/MQAï¼ˆå‡å°‘ KV å¤´æ•°ï¼‰
â”‚   â””â”€â”€ MoEï¼ˆç¨€ç–æ¿€æ´»ï¼‰
â”‚
â”œâ”€â”€ è¾“å…¥å¤„ç†
â”‚   â”œâ”€â”€ Tokenizationï¼ˆBPEã€SentencePieceï¼‰
â”‚   â””â”€â”€ Embeddingï¼ˆè¯åµŒå…¥ + ä½ç½®åµŒå…¥ï¼‰
â”‚
â”œâ”€â”€ ç”Ÿæ€å·¥å…·
â”‚   â”œâ”€â”€ Hugging Face transformers
â”‚   â”œâ”€â”€ Hugging Face datasets
â”‚   â””â”€â”€ Model Hub
â”‚
â””â”€â”€ åº”ç”¨æ–¹å‘
    â”œâ”€â”€ æ–‡æœ¬åˆ†ç±»/NERï¼ˆBERTï¼‰
    â”œâ”€â”€ æ–‡æœ¬ç”Ÿæˆï¼ˆGPTï¼‰
    â”œâ”€â”€ è¯­ä¹‰æœç´¢ï¼ˆEmbeddingï¼‰
    â””â”€â”€ å¤šæ¨¡æ€ï¼ˆCLIPã€LLaVAï¼‰
```

---

## ğŸ› ï¸ ç¯å¢ƒå‡†å¤‡

### å®‰è£…ä¾èµ–

```bash
# åˆ›å»ºç¯å¢ƒ
conda create -n llm_env python=3.11
conda activate llm_env

# æ ¸å¿ƒåº“
pip install torch torchvision torchaudio
pip install transformers datasets tokenizers
pip install accelerate  # å¤§æ¨¡å‹åŠ è½½
pip install sentencepiece  # Tokenization

# å‘é‡æ£€ç´¢
pip install faiss-cpu  # æˆ– faiss-gpu
pip install chromadb

# å¯è§†åŒ–
pip install matplotlib seaborn
pip install bertviz  # æ³¨æ„åŠ›å¯è§†åŒ–

# å¯é€‰ï¼šJupyter
pip install jupyter notebook
```

### éªŒè¯å®‰è£…

```python
import torch
from transformers import AutoTokenizer, AutoModel

print(f"PyTorch: {torch.__version__}")
print(f"CUDA: {torch.cuda.is_available()}")

# æµ‹è¯• Hugging Face
tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")
tokens = tokenizer("Hello, world!")
print(f"Tokens: {tokens}")
```

---

## ğŸ“– æ ¸å¿ƒæ¦‚å¿µé¢„è§ˆ

### Transformer æ˜¯ä»€ä¹ˆï¼Ÿ

```
ä¼ ç»Ÿåºåˆ—æ¨¡å‹ï¼ˆRNNï¼‰ï¼šé¡ºåºå¤„ç†ï¼Œæ— æ³•å¹¶è¡Œ
      â†“
Transformerï¼ˆ2017ï¼‰ï¼šå¹¶è¡Œå¤„ç†ï¼Œæ³¨æ„åŠ›æœºåˆ¶
      â†“
ç°ä»£ LLMï¼ˆGPTã€LLaMAï¼‰ï¼šDecoder-only Transformer + å¤§è§„æ¨¡è®­ç»ƒ
```

### Self-Attention ç›´è§‰

```
è¾“å…¥ï¼š["I", "love", "AI"]

Self-Attention åšçš„äº‹ï¼š
è®©æ¯ä¸ªè¯"çœ‹"åˆ°å…¶ä»–æ‰€æœ‰è¯ï¼Œå­¦ä¹ è¯ä¹‹é—´çš„å…³ç³»

"love" å…³æ³¨ï¼š
- "I" (è°çˆ±ï¼Ÿ) â†’ é«˜æƒé‡
- "AI" (çˆ±ä»€ä¹ˆï¼Ÿ) â†’ é«˜æƒé‡

è¾“å‡ºï¼šæ¯ä¸ªè¯çš„è¡¨ç¤ºèåˆäº†ä¸Šä¸‹æ–‡ä¿¡æ¯
```

### KV Cache ç›´è§‰

```
ç”Ÿæˆæ—¶ï¼š
Step 1: "The"     â†’ è®¡ç®— K,V
Step 2: "The cat" â†’ é‡æ–°è®¡ç®—æ‰€æœ‰ K,Vï¼Ÿå¤ªæµªè´¹ï¼

KV Cacheï¼š
- ç¼“å­˜å·²è®¡ç®—çš„ K,V
- æ–° token åªè®¡ç®—æ–°çš„ K,V
- å¤§å¹…åŠ é€Ÿæ¨ç†
```

---

## ğŸ“š æ¨èèµ„æº

### å¿…è¯»
- [Attention Is All You Need](https://arxiv.org/abs/1706.03762) - Transformer åŸè®ºæ–‡
- [The Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/) - å¯è§†åŒ–è®²è§£
- [nanoGPT](https://github.com/karpathy/nanoGPT) - Karpathy çš„æç®€ GPT

### æ¨èè¯¾ç¨‹
- [Andrej Karpathy: Let's build GPT](https://www.youtube.com/watch?v=kCc8FmEb1nY)
- [Hugging Face NLP Course](https://huggingface.co/learn/nlp-course)
- [æå®æ¯… Transformer è®²è§£](https://www.bilibili.com/video/BV1J94y1f7u5)

### å®ç”¨å·¥å…·
- [Hugging Face Model Hub](https://huggingface.co/models)
- [Tokenizer Playground](https://platform.openai.com/tokenizer)

---

## â¡ï¸ ä¸‹ä¸€æ­¥

å‡†å¤‡å¥½äº†å—ï¼Ÿå¼€å§‹å­¦ä¹  [01-Transformeræ¶æ„.md](./01-Transformeræ¶æ„.md)ï¼

