# ğŸ“Š 11 - LLM åº”ç”¨è¯„ä¼°

> å¦‚ä½•è¯„ä¼°å’Œç›‘æ§ LLM åº”ç”¨çš„è´¨é‡ã€æ€§èƒ½å’Œæˆæœ¬

---

## ç›®å½•

1. [ä¸ºä»€ä¹ˆéœ€è¦è¯„ä¼°](#1-ä¸ºä»€ä¹ˆéœ€è¦è¯„ä¼°)
2. [è¯„ä¼°æŒ‡æ ‡ä½“ç³»](#2-è¯„ä¼°æŒ‡æ ‡ä½“ç³»)
3. [è‡ªåŠ¨åŒ–è¯„ä¼°æ–¹æ³•](#3-è‡ªåŠ¨åŒ–è¯„ä¼°æ–¹æ³•)
4. [äººå·¥è¯„ä¼°](#4-äººå·¥è¯„ä¼°)
5. [RAG ä¸“é¡¹è¯„ä¼°](#5-rag-ä¸“é¡¹è¯„ä¼°)
6. [ç”Ÿäº§ç›‘æ§](#6-ç”Ÿäº§ç›‘æ§)
7. [A/B æµ‹è¯•](#7-ab-æµ‹è¯•)
8. [ç»ƒä¹ é¢˜](#8-ç»ƒä¹ é¢˜)

---

## 1. ä¸ºä»€ä¹ˆéœ€è¦è¯„ä¼°

### 1.1 è¯„ä¼°çš„ä»·å€¼

```
ä¸ºä»€ä¹ˆè¦è¯„ä¼° LLM åº”ç”¨ï¼Ÿ

1. è´¨é‡ä¿éšœ
   â””â”€â”€ ç¡®ä¿è¾“å‡ºç¬¦åˆé¢„æœŸ

2. è¿­ä»£æ”¹è¿›
   â””â”€â”€ é‡åŒ–æ”¹è¿›æ•ˆæœ

3. é—®é¢˜è¯Šæ–­
   â””â”€â”€ å‘ç°è–„å¼±ç¯èŠ‚

4. æˆæœ¬ä¼˜åŒ–
   â””â”€â”€ å¹³è¡¡è´¨é‡ä¸æˆæœ¬

5. åˆè§„è¦æ±‚
   â””â”€â”€ æ»¡è¶³ä¸šåŠ¡/æ³•è§„æ ‡å‡†
```

### 1.2 è¯„ä¼°çš„æŒ‘æˆ˜

```
LLM è¯„ä¼°ä¸ºä»€ä¹ˆéš¾ï¼Ÿ

1. å¼€æ”¾æ€§è¾“å‡º
   â””â”€â”€ åŒä¸€é—®é¢˜æœ‰å¤šä¸ªæ­£ç¡®ç­”æ¡ˆ

2. ä¸»è§‚æ€§å¼º
   â””â”€â”€ è´¨é‡åˆ¤æ–­å› äººè€Œå¼‚

3. é•¿å°¾é—®é¢˜
   â””â”€â”€ è¾¹ç¼˜åœºæ™¯éš¾ä»¥è¦†ç›–

4. åŠ¨æ€æ€§
   â””â”€â”€ æ¨¡å‹å‡çº§å¯èƒ½å¯¼è‡´è¡Œä¸ºå˜åŒ–
```

---

## 2. è¯„ä¼°æŒ‡æ ‡ä½“ç³»

### 2.1 é€šç”¨æŒ‡æ ‡

```python
from dataclasses import dataclass
from typing import Optional
from enum import Enum

class QualityDimension(Enum):
    RELEVANCE = "ç›¸å…³æ€§"      # å›ç­”æ˜¯å¦åˆ‡é¢˜
    ACCURACY = "å‡†ç¡®æ€§"       # äº‹å®æ˜¯å¦æ­£ç¡®
    COHERENCE = "è¿è´¯æ€§"      # é€»è¾‘æ˜¯å¦é€šé¡º
    FLUENCY = "æµç•…æ€§"        # è¯­è¨€æ˜¯å¦è‡ªç„¶
    HELPFULNESS = "æœ‰ç”¨æ€§"    # æ˜¯å¦è§£å†³ç”¨æˆ·é—®é¢˜
    SAFETY = "å®‰å…¨æ€§"         # æ˜¯å¦åŒ…å«æœ‰å®³å†…å®¹

@dataclass
class EvaluationResult:
    """è¯„ä¼°ç»“æœ"""
    question: str
    answer: str
    reference: Optional[str] = None

    # å„ç»´åº¦å¾—åˆ† (0-1)
    relevance: float = 0.0
    accuracy: float = 0.0
    coherence: float = 0.0
    fluency: float = 0.0
    helpfulness: float = 0.0
    safety: float = 0.0

    # ç»¼åˆå¾—åˆ†
    overall: float = 0.0

    # å…ƒä¿¡æ¯
    latency_ms: float = 0.0
    token_count: int = 0
    cost_usd: float = 0.0
```

### 2.2 ä»»åŠ¡ç‰¹å®šæŒ‡æ ‡

```python
# 1. åˆ†ç±»ä»»åŠ¡
from sklearn.metrics import precision_score, recall_score, f1_score

def evaluate_classification(predictions, labels):
    return {
        "precision": precision_score(labels, predictions, average='weighted'),
        "recall": recall_score(labels, predictions, average='weighted'),
        "f1": f1_score(labels, predictions, average='weighted'),
    }

# 2. ç”Ÿæˆä»»åŠ¡
from rouge_score import rouge_scorer

def evaluate_generation(generated, reference):
    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)
    scores = scorer.score(reference, generated)
    return {
        "rouge1": scores['rouge1'].fmeasure,
        "rouge2": scores['rouge2'].fmeasure,
        "rougeL": scores['rougeL'].fmeasure,
    }

# 3. QA ä»»åŠ¡
def evaluate_qa(predicted, expected):
    """ç²¾ç¡®åŒ¹é…å’Œ F1"""
    # ç²¾ç¡®åŒ¹é…
    exact_match = predicted.strip().lower() == expected.strip().lower()

    # Token-level F1
    pred_tokens = set(predicted.lower().split())
    exp_tokens = set(expected.lower().split())

    if len(pred_tokens) == 0 or len(exp_tokens) == 0:
        f1 = 0.0
    else:
        precision = len(pred_tokens & exp_tokens) / len(pred_tokens)
        recall = len(pred_tokens & exp_tokens) / len(exp_tokens)
        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0

    return {"exact_match": exact_match, "f1": f1}
```

---

## 3. è‡ªåŠ¨åŒ–è¯„ä¼°æ–¹æ³•

### 3.1 LLM-as-Judge

```python
from openai import OpenAI
import json

client = OpenAI()

def llm_judge(question: str, answer: str, criteria: str = None) -> dict:
    """ä½¿ç”¨ LLM ä½œä¸ºè¯„åˆ¤è€…"""

    if criteria is None:
        criteria = """
1. ç›¸å…³æ€§ (relevance): å›ç­”æ˜¯å¦åˆ‡é¢˜
2. å‡†ç¡®æ€§ (accuracy): ä¿¡æ¯æ˜¯å¦æ­£ç¡®
3. å®Œæ•´æ€§ (completeness): æ˜¯å¦å……åˆ†å›ç­”é—®é¢˜
4. æ¸…æ™°åº¦ (clarity): è¡¨è¾¾æ˜¯å¦æ¸…æ¥š
"""

    prompt = f"""
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å›ç­”è´¨é‡è¯„ä¼°å‘˜ã€‚è¯·è¯„ä¼°ä»¥ä¸‹é—®ç­”çš„è´¨é‡ã€‚

é—®é¢˜ï¼š{question}

å›ç­”ï¼š{answer}

è¯„ä¼°ç»´åº¦ï¼š
{criteria}

è¯·ä¸ºæ¯ä¸ªç»´åº¦æ‰“åˆ†ï¼ˆ1-5åˆ†ï¼‰ï¼Œå¹¶ç»™å‡ºç®€çŸ­ç†ç”±ã€‚
è¿”å› JSON æ ¼å¼ï¼š
{{
    "relevance": {{"score": 1-5, "reason": "..."}},
    "accuracy": {{"score": 1-5, "reason": "..."}},
    "completeness": {{"score": 1-5, "reason": "..."}},
    "clarity": {{"score": 1-5, "reason": "..."}},
    "overall": 1-5,
    "feedback": "æ€»ä½“è¯„ä»·..."
}}
"""

    response = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[{"role": "user", "content": prompt}],
        response_format={"type": "json_object"}
    )

    return json.loads(response.choices[0].message.content)

# ä½¿ç”¨
result = llm_judge(
    question="ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ",
    answer="æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œè®©è®¡ç®—æœºèƒ½ä»æ•°æ®ä¸­å­¦ä¹ è§„å¾‹ï¼Œè€Œæ— éœ€æ˜¾å¼ç¼–ç¨‹ã€‚"
)
print(json.dumps(result, ensure_ascii=False, indent=2))
```

### 3.2 æˆå¯¹æ¯”è¾ƒ (Pairwise Comparison)

```python
def pairwise_compare(question: str, answer_a: str, answer_b: str) -> dict:
    """æˆå¯¹æ¯”è¾ƒä¸¤ä¸ªå›ç­”"""

    prompt = f"""
è¯·æ¯”è¾ƒä»¥ä¸‹ä¸¤ä¸ªå›ç­”çš„è´¨é‡ã€‚

é—®é¢˜ï¼š{question}

å›ç­” Aï¼š
{answer_a}

å›ç­” Bï¼š
{answer_b}

è¯·åˆ¤æ–­å“ªä¸ªå›ç­”æ›´å¥½ï¼Œè¿”å› JSONï¼š
{{
    "winner": "A" æˆ– "B" æˆ– "tie",
    "reason": "é€‰æ‹©ç†ç”±...",
    "a_strengths": ["A çš„ä¼˜ç‚¹..."],
    "a_weaknesses": ["A çš„ç¼ºç‚¹..."],
    "b_strengths": ["B çš„ä¼˜ç‚¹..."],
    "b_weaknesses": ["B çš„ç¼ºç‚¹..."]
}}
"""

    response = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[{"role": "user", "content": prompt}],
        response_format={"type": "json_object"}
    )

    return json.loads(response.choices[0].message.content)

# ä½¿ç”¨ï¼šæ¯”è¾ƒä¸åŒ prompt ç­–ç•¥çš„æ•ˆæœ
result = pairwise_compare(
    question="å¦‚ä½•å­¦ä¹  Pythonï¼Ÿ",
    answer_a="é¦–å…ˆå®‰è£… Pythonï¼Œç„¶åå­¦ä¹ åŸºç¡€è¯­æ³•ã€‚",
    answer_b="å­¦ä¹  Python å»ºè®®åˆ†å‡ æ­¥ï¼š1) å®‰è£…ç¯å¢ƒ 2) å­¦ä¹ åŸºç¡€è¯­æ³• 3) åšå°é¡¹ç›® 4) æ·±å…¥æ¡†æ¶ã€‚æ¨èä»å®˜æ–¹æ•™ç¨‹å¼€å§‹ã€‚"
)
print(f"èƒœè€…: {result['winner']}, åŸå› : {result['reason']}")
```

### 3.3 åŸºäºå‚è€ƒç­”æ¡ˆçš„è¯„ä¼°

```python
def evaluate_with_reference(question: str, answer: str, reference: str) -> dict:
    """ä¸å‚è€ƒç­”æ¡ˆå¯¹æ¯”è¯„ä¼°"""

    prompt = f"""
è¯·è¯„ä¼°å›ç­”ä¸å‚è€ƒç­”æ¡ˆçš„ä¸€è‡´æ€§å’Œè´¨é‡ã€‚

é—®é¢˜ï¼š{question}

å‚è€ƒç­”æ¡ˆï¼š{reference}

å¾…è¯„ä¼°å›ç­”ï¼š{answer}

è¯„ä¼°è¦ç‚¹ï¼š
1. äº‹å®ä¸€è‡´æ€§ï¼šå›ç­”ä¸­çš„äº‹å®æ˜¯å¦ä¸å‚è€ƒç­”æ¡ˆä¸€è‡´
2. ä¿¡æ¯è¦†ç›–åº¦ï¼šå‚è€ƒç­”æ¡ˆä¸­çš„è¦ç‚¹è¦†ç›–äº†å¤šå°‘
3. é¢å¤–ä¿¡æ¯ï¼šæ˜¯å¦æœ‰è¶…å‡ºå‚è€ƒç­”æ¡ˆçš„æ­£ç¡®/é”™è¯¯ä¿¡æ¯

è¿”å› JSONï¼š
{{
    "factual_consistency": 0-1,
    "coverage": 0-1,
    "extra_correct": ["æ­£ç¡®çš„é¢å¤–ä¿¡æ¯..."],
    "extra_incorrect": ["é”™è¯¯çš„é¢å¤–ä¿¡æ¯..."],
    "missing": ["ç¼ºå¤±çš„è¦ç‚¹..."],
    "overall_score": 0-1
}}
"""

    response = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[{"role": "user", "content": prompt}],
        response_format={"type": "json_object"}
    )

    return json.loads(response.choices[0].message.content)
```

---

## 4. äººå·¥è¯„ä¼°

### 4.1 è¯„ä¼°ç•Œé¢è®¾è®¡

```python
import streamlit as st
from datetime import datetime
import json

def create_evaluation_ui():
    """åˆ›å»ºäººå·¥è¯„ä¼°ç•Œé¢"""

    st.title("LLM å›ç­”è´¨é‡è¯„ä¼°")

    # åŠ è½½å¾…è¯„ä¼°æ•°æ®
    if 'eval_data' not in st.session_state:
        st.session_state.eval_data = load_eval_samples()
        st.session_state.current_idx = 0
        st.session_state.results = []

    data = st.session_state.eval_data
    idx = st.session_state.current_idx

    if idx >= len(data):
        st.success("è¯„ä¼°å®Œæˆï¼")
        st.json(st.session_state.results)
        return

    sample = data[idx]

    # æ˜¾ç¤ºé—®ç­”
    st.subheader(f"é—®é¢˜ {idx + 1}/{len(data)}")
    st.write(f"**é—®é¢˜ï¼š** {sample['question']}")
    st.write(f"**å›ç­”ï¼š** {sample['answer']}")

    # è¯„åˆ†
    st.subheader("è´¨é‡è¯„åˆ†")

    relevance = st.slider("ç›¸å…³æ€§", 1, 5, 3, help="å›ç­”æ˜¯å¦åˆ‡é¢˜")
    accuracy = st.slider("å‡†ç¡®æ€§", 1, 5, 3, help="ä¿¡æ¯æ˜¯å¦æ­£ç¡®")
    helpfulness = st.slider("æœ‰ç”¨æ€§", 1, 5, 3, help="æ˜¯å¦è§£å†³ç”¨æˆ·é—®é¢˜")

    # æ–‡å­—åé¦ˆ
    feedback = st.text_area("è¡¥å……åé¦ˆï¼ˆå¯é€‰ï¼‰")

    # æäº¤
    if st.button("æäº¤å¹¶ä¸‹ä¸€ä¸ª"):
        result = {
            "sample_id": sample.get("id", idx),
            "question": sample["question"],
            "answer": sample["answer"],
            "relevance": relevance,
            "accuracy": accuracy,
            "helpfulness": helpfulness,
            "feedback": feedback,
            "timestamp": datetime.now().isoformat(),
            "evaluator": "human"
        }

        st.session_state.results.append(result)
        st.session_state.current_idx += 1
        st.rerun()

def load_eval_samples():
    """åŠ è½½è¯„ä¼°æ ·æœ¬"""
    # å®é™…åº”ç”¨ä¸­ä»æ•°æ®åº“æˆ–æ–‡ä»¶åŠ è½½
    return [
        {"id": 1, "question": "ä»€ä¹ˆæ˜¯ Pythonï¼Ÿ", "answer": "Python æ˜¯ä¸€ç§ç¼–ç¨‹è¯­è¨€..."},
        {"id": 2, "question": "å¦‚ä½•å­¦ä¹ æœºå™¨å­¦ä¹ ï¼Ÿ", "answer": "é¦–å…ˆå­¦ä¹ æ•°å­¦åŸºç¡€..."},
    ]
```

### 4.2 è¯„ä¼°è€…ä¸€è‡´æ€§

```python
from scipy.stats import pearsonr, spearmanr
import numpy as np

def calculate_inter_rater_agreement(ratings_a: list, ratings_b: list) -> dict:
    """è®¡ç®—è¯„ä¼°è€…é—´ä¸€è‡´æ€§"""

    # Pearson ç›¸å…³ç³»æ•°
    pearson_r, pearson_p = pearsonr(ratings_a, ratings_b)

    # Spearman ç§©ç›¸å…³
    spearman_r, spearman_p = spearmanr(ratings_a, ratings_b)

    # ç²¾ç¡®ä¸€è‡´ç‡
    exact_agreement = sum(a == b for a, b in zip(ratings_a, ratings_b)) / len(ratings_a)

    # å…è®¸1åˆ†å·®å¼‚çš„ä¸€è‡´ç‡
    near_agreement = sum(abs(a - b) <= 1 for a, b in zip(ratings_a, ratings_b)) / len(ratings_a)

    return {
        "pearson_r": pearson_r,
        "spearman_r": spearman_r,
        "exact_agreement": exact_agreement,
        "near_agreement": near_agreement,
    }

# ä½¿ç”¨
evaluator_1 = [4, 3, 5, 2, 4, 3, 5, 4]
evaluator_2 = [5, 3, 4, 2, 4, 4, 5, 3]

agreement = calculate_inter_rater_agreement(evaluator_1, evaluator_2)
print(f"Pearson ç›¸å…³: {agreement['pearson_r']:.3f}")
print(f"ç²¾ç¡®ä¸€è‡´ç‡: {agreement['exact_agreement']:.2%}")
```

---

## 5. RAG ä¸“é¡¹è¯„ä¼°

### 5.1 RAG è¯„ä¼°ç»´åº¦

```python
@dataclass
class RAGEvaluationResult:
    """RAG ç³»ç»Ÿè¯„ä¼°ç»“æœ"""

    # æ£€ç´¢è´¨é‡
    retrieval_precision: float    # æ£€ç´¢ç»“æœçš„ç›¸å…³æ€§
    retrieval_recall: float       # æ˜¯å¦å¬å›æ‰€æœ‰ç›¸å…³æ–‡æ¡£
    retrieval_mrr: float          # å¹³å‡å€’æ•°æ’å

    # ç”Ÿæˆè´¨é‡
    answer_relevance: float       # å›ç­”ç›¸å…³æ€§
    answer_faithfulness: float    # å›ç­”å¿ å®åº¦ï¼ˆæ˜¯å¦åŸºäºæ£€ç´¢å†…å®¹ï¼‰
    answer_correctness: float     # å›ç­”æ­£ç¡®æ€§

    # æ•´ä½“è´¨é‡
    context_utilization: float    # ä¸Šä¸‹æ–‡åˆ©ç”¨ç‡
    hallucination_rate: float     # å¹»è§‰ç‡
```

### 5.2 å¿ å®åº¦è¯„ä¼°

```python
def evaluate_faithfulness(answer: str, contexts: list[str]) -> dict:
    """è¯„ä¼°å›ç­”å¯¹æ£€ç´¢å†…å®¹çš„å¿ å®åº¦"""

    prompt = f"""
è¯·è¯„ä¼°å›ç­”æ˜¯å¦å¿ å®äºç»™å®šçš„ä¸Šä¸‹æ–‡ã€‚

ä¸Šä¸‹æ–‡ï¼š
{chr(10).join([f"[{i+1}] {c}" for i, c in enumerate(contexts)])}

å›ç­”ï¼š
{answer}

åˆ†æï¼š
1. å›ç­”ä¸­çš„æ¯ä¸ªé™ˆè¿°æ˜¯å¦éƒ½èƒ½åœ¨ä¸Šä¸‹æ–‡ä¸­æ‰¾åˆ°ä¾æ®ï¼Ÿ
2. æ˜¯å¦æœ‰å‡­ç©ºç¼–é€ çš„å†…å®¹ï¼ˆå¹»è§‰ï¼‰ï¼Ÿ

è¿”å› JSONï¼š
{{
    "faithfulness_score": 0-1,
    "supported_claims": ["æœ‰ä¾æ®çš„é™ˆè¿°..."],
    "unsupported_claims": ["æ— ä¾æ®çš„é™ˆè¿°..."],
    "hallucinations": ["å¹»è§‰å†…å®¹..."],
    "analysis": "è¯¦ç»†åˆ†æ..."
}}
"""

    response = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[{"role": "user", "content": prompt}],
        response_format={"type": "json_object"}
    )

    return json.loads(response.choices[0].message.content)
```

### 5.3 æ£€ç´¢è¯„ä¼°

```python
from typing import List

def evaluate_retrieval(
    query: str,
    retrieved_docs: List[str],
    relevant_docs: List[str]  # çœŸå®ç›¸å…³æ–‡æ¡£
) -> dict:
    """è¯„ä¼°æ£€ç´¢è´¨é‡"""

    # è®¡ç®— Precision@K
    k = len(retrieved_docs)
    relevant_retrieved = set(retrieved_docs) & set(relevant_docs)
    precision_at_k = len(relevant_retrieved) / k if k > 0 else 0

    # è®¡ç®— Recall
    recall = len(relevant_retrieved) / len(relevant_docs) if relevant_docs else 0

    # è®¡ç®— MRR (Mean Reciprocal Rank)
    mrr = 0.0
    for i, doc in enumerate(retrieved_docs):
        if doc in relevant_docs:
            mrr = 1.0 / (i + 1)
            break

    # è®¡ç®— NDCG
    def dcg(relevances, k):
        return sum((2**rel - 1) / np.log2(i + 2) for i, rel in enumerate(relevances[:k]))

    relevances = [1 if doc in relevant_docs else 0 for doc in retrieved_docs]
    ideal_relevances = sorted(relevances, reverse=True)

    dcg_score = dcg(relevances, k)
    idcg_score = dcg(ideal_relevances, k)
    ndcg = dcg_score / idcg_score if idcg_score > 0 else 0

    return {
        "precision_at_k": precision_at_k,
        "recall": recall,
        "mrr": mrr,
        "ndcg": ndcg,
        "relevant_count": len(relevant_retrieved),
        "total_retrieved": k,
        "total_relevant": len(relevant_docs)
    }
```

---

## 6. ç”Ÿäº§ç›‘æ§

### 6.1 æ ¸å¿ƒç›‘æ§æŒ‡æ ‡

```python
from dataclasses import dataclass, field
from datetime import datetime
from typing import Dict, List
import time

@dataclass
class RequestMetrics:
    """å•æ¬¡è¯·æ±‚æŒ‡æ ‡"""
    request_id: str
    timestamp: datetime

    # æ€§èƒ½
    latency_ms: float
    time_to_first_token_ms: float = 0.0

    # æˆæœ¬
    input_tokens: int = 0
    output_tokens: int = 0
    cost_usd: float = 0.0

    # è´¨é‡
    success: bool = True
    error_type: str = None

    # ä¸šåŠ¡
    user_id: str = None
    session_id: str = None
    feedback: int = None  # 1=positive, -1=negative, 0=neutral

class MetricsCollector:
    """æŒ‡æ ‡æ”¶é›†å™¨"""

    def __init__(self):
        self.metrics: List[RequestMetrics] = []

    def record(self, metrics: RequestMetrics):
        self.metrics.append(metrics)

    def get_summary(self, window_minutes: int = 60) -> dict:
        """è·å–æŒ‡æ ‡æ‘˜è¦"""
        cutoff = datetime.now().timestamp() - window_minutes * 60
        recent = [m for m in self.metrics if m.timestamp.timestamp() > cutoff]

        if not recent:
            return {"message": "No data in window"}

        latencies = [m.latency_ms for m in recent]
        costs = [m.cost_usd for m in recent]
        success_rate = sum(1 for m in recent if m.success) / len(recent)

        return {
            "request_count": len(recent),
            "success_rate": success_rate,
            "latency_p50": np.percentile(latencies, 50),
            "latency_p95": np.percentile(latencies, 95),
            "latency_p99": np.percentile(latencies, 99),
            "total_cost_usd": sum(costs),
            "avg_cost_per_request": np.mean(costs),
        }

# ä½¿ç”¨è£…é¥°å™¨è‡ªåŠ¨æ”¶é›†æŒ‡æ ‡
collector = MetricsCollector()

def track_request(func):
    def wrapper(*args, **kwargs):
        request_id = str(time.time())
        start = time.time()

        try:
            result = func(*args, **kwargs)
            success = True
            error_type = None
        except Exception as e:
            success = False
            error_type = type(e).__name__
            raise
        finally:
            latency = (time.time() - start) * 1000

            metrics = RequestMetrics(
                request_id=request_id,
                timestamp=datetime.now(),
                latency_ms=latency,
                success=success,
                error_type=error_type
            )
            collector.record(metrics)

        return result
    return wrapper
```

### 6.2 å‘Šè­¦è§„åˆ™

```python
from typing import Callable
from dataclasses import dataclass

@dataclass
class AlertRule:
    name: str
    condition: Callable[[dict], bool]
    severity: str  # "critical", "warning", "info"
    message_template: str

class AlertManager:
    def __init__(self):
        self.rules: List[AlertRule] = []
        self.triggered_alerts: List[dict] = []

    def add_rule(self, rule: AlertRule):
        self.rules.append(rule)

    def check(self, metrics_summary: dict):
        for rule in self.rules:
            if rule.condition(metrics_summary):
                alert = {
                    "rule": rule.name,
                    "severity": rule.severity,
                    "message": rule.message_template.format(**metrics_summary),
                    "timestamp": datetime.now().isoformat()
                }
                self.triggered_alerts.append(alert)
                self._notify(alert)

    def _notify(self, alert: dict):
        # å‘é€é€šçŸ¥ï¼ˆé‚®ä»¶ã€Slackã€PagerDuty ç­‰ï¼‰
        print(f"[{alert['severity'].upper()}] {alert['message']}")

# é…ç½®å‘Šè­¦è§„åˆ™
alert_manager = AlertManager()

alert_manager.add_rule(AlertRule(
    name="high_error_rate",
    condition=lambda m: m.get("success_rate", 1) < 0.95,
    severity="critical",
    message_template="é”™è¯¯ç‡è¿‡é«˜ï¼š{success_rate:.1%}"
))

alert_manager.add_rule(AlertRule(
    name="high_latency",
    condition=lambda m: m.get("latency_p95", 0) > 5000,
    severity="warning",
    message_template="P95 å»¶è¿Ÿè¿‡é«˜ï¼š{latency_p95:.0f}ms"
))

alert_manager.add_rule(AlertRule(
    name="cost_spike",
    condition=lambda m: m.get("avg_cost_per_request", 0) > 0.1,
    severity="warning",
    message_template="å•æ¬¡è¯·æ±‚æˆæœ¬è¿‡é«˜ï¼š${avg_cost_per_request:.3f}"
))
```

### 6.3 ç”¨æˆ·åé¦ˆæ”¶é›†

```python
from enum import IntEnum

class FeedbackType(IntEnum):
    THUMBS_UP = 1
    THUMBS_DOWN = -1
    NEUTRAL = 0

class FeedbackCollector:
    def __init__(self):
        self.feedbacks = []

    def record(self, request_id: str, feedback: FeedbackType,
               comment: str = None, categories: List[str] = None):
        """è®°å½•ç”¨æˆ·åé¦ˆ"""
        self.feedbacks.append({
            "request_id": request_id,
            "feedback": feedback.value,
            "comment": comment,
            "categories": categories or [],
            "timestamp": datetime.now().isoformat()
        })

    def get_satisfaction_rate(self, window_hours: int = 24) -> dict:
        """è®¡ç®—æ»¡æ„åº¦"""
        cutoff = datetime.now().timestamp() - window_hours * 3600
        recent = [f for f in self.feedbacks
                  if datetime.fromisoformat(f["timestamp"]).timestamp() > cutoff]

        if not recent:
            return {"message": "No feedback in window"}

        positive = sum(1 for f in recent if f["feedback"] == 1)
        negative = sum(1 for f in recent if f["feedback"] == -1)

        return {
            "total_feedback": len(recent),
            "positive": positive,
            "negative": negative,
            "satisfaction_rate": positive / len(recent),
            "nps": (positive - negative) / len(recent) * 100  # Net Promoter Score
        }
```

---

## 7. A/B æµ‹è¯•

### 7.1 å®éªŒæ¡†æ¶

```python
import random
from typing import Dict, Any
from dataclasses import dataclass

@dataclass
class Experiment:
    name: str
    variants: Dict[str, Any]  # variant_name -> config
    traffic_split: Dict[str, float]  # variant_name -> æµé‡æ¯”ä¾‹

class ABTestManager:
    def __init__(self):
        self.experiments: Dict[str, Experiment] = {}
        self.assignments: Dict[str, str] = {}  # user_id -> variant

    def create_experiment(self, experiment: Experiment):
        # éªŒè¯æµé‡åˆ†é…æ€»å’Œä¸º 1
        assert abs(sum(experiment.traffic_split.values()) - 1.0) < 0.001
        self.experiments[experiment.name] = experiment

    def get_variant(self, experiment_name: str, user_id: str) -> tuple[str, Any]:
        """è·å–ç”¨æˆ·åˆ†é…çš„å˜ä½“"""
        exp = self.experiments[experiment_name]

        # æ£€æŸ¥æ˜¯å¦å·²åˆ†é…
        key = f"{experiment_name}:{user_id}"
        if key in self.assignments:
            variant_name = self.assignments[key]
            return variant_name, exp.variants[variant_name]

        # åŸºäºç”¨æˆ· ID çš„ç¡®å®šæ€§åˆ†é…ï¼ˆä¿è¯åŒä¸€ç”¨æˆ·å§‹ç»ˆçœ‹åˆ°åŒä¸€å˜ä½“ï¼‰
        hash_value = hash(key) % 10000 / 10000

        cumulative = 0.0
        for variant_name, proportion in exp.traffic_split.items():
            cumulative += proportion
            if hash_value < cumulative:
                self.assignments[key] = variant_name
                return variant_name, exp.variants[variant_name]

        # é»˜è®¤è¿”å›ç¬¬ä¸€ä¸ª
        first_variant = list(exp.variants.keys())[0]
        self.assignments[key] = first_variant
        return first_variant, exp.variants[first_variant]

# ä½¿ç”¨
ab_manager = ABTestManager()

# åˆ›å»ºå®éªŒï¼šæµ‹è¯•ä¸åŒçš„ prompt
ab_manager.create_experiment(Experiment(
    name="prompt_optimization",
    variants={
        "control": {"prompt": "è¯·å›ç­”ä»¥ä¸‹é—®é¢˜ï¼š"},
        "treatment_a": {"prompt": "ä½ æ˜¯ä¸“å®¶ï¼Œè¯·è¯¦ç»†å›ç­”ï¼š"},
        "treatment_b": {"prompt": "è¯·ç”¨ç®€æ´çš„è¯­è¨€å›ç­”ï¼š"}
    },
    traffic_split={
        "control": 0.34,
        "treatment_a": 0.33,
        "treatment_b": 0.33
    }
))

# è·å–ç”¨æˆ·åˆ†é…
user_id = "user_123"
variant_name, config = ab_manager.get_variant("prompt_optimization", user_id)
print(f"ç”¨æˆ· {user_id} åˆ†é…åˆ°ï¼š{variant_name}")
print(f"ä½¿ç”¨ promptï¼š{config['prompt']}")
```

### 7.2 ç»Ÿè®¡æ˜¾è‘—æ€§æ£€éªŒ

```python
from scipy import stats
import numpy as np

def calculate_significance(
    control_conversions: int,
    control_total: int,
    treatment_conversions: int,
    treatment_total: int
) -> dict:
    """è®¡ç®— A/B æµ‹è¯•çš„ç»Ÿè®¡æ˜¾è‘—æ€§"""

    # è½¬åŒ–ç‡
    control_rate = control_conversions / control_total
    treatment_rate = treatment_conversions / treatment_total

    # ç›¸å¯¹æå‡
    relative_lift = (treatment_rate - control_rate) / control_rate if control_rate > 0 else 0

    # Z æ£€éªŒ
    pooled_rate = (control_conversions + treatment_conversions) / (control_total + treatment_total)
    se = np.sqrt(pooled_rate * (1 - pooled_rate) * (1/control_total + 1/treatment_total))
    z_score = (treatment_rate - control_rate) / se if se > 0 else 0
    p_value = 2 * (1 - stats.norm.cdf(abs(z_score)))  # åŒå°¾æ£€éªŒ

    # ç½®ä¿¡åŒºé—´
    ci_95 = 1.96 * se

    return {
        "control_rate": control_rate,
        "treatment_rate": treatment_rate,
        "relative_lift": relative_lift,
        "z_score": z_score,
        "p_value": p_value,
        "is_significant": p_value < 0.05,
        "confidence_interval": (treatment_rate - control_rate - ci_95,
                                 treatment_rate - control_rate + ci_95)
    }

# ä½¿ç”¨
result = calculate_significance(
    control_conversions=120,
    control_total=1000,
    treatment_conversions=150,
    treatment_total=1000
)

print(f"Control è½¬åŒ–ç‡: {result['control_rate']:.2%}")
print(f"Treatment è½¬åŒ–ç‡: {result['treatment_rate']:.2%}")
print(f"ç›¸å¯¹æå‡: {result['relative_lift']:.2%}")
print(f"P-value: {result['p_value']:.4f}")
print(f"ç»Ÿè®¡æ˜¾è‘—: {result['is_significant']}")
```

---

## 8. ç»ƒä¹ é¢˜

### åŸºç¡€ç»ƒä¹ 

1. å®ç°ä¸€ä¸ª LLM-as-Judge è¯„ä¼°å‡½æ•°
2. è®¡ç®— ROUGE åˆ†æ•°è¯„ä¼°ç”Ÿæˆè´¨é‡
3. å®ç°ç”¨æˆ·åé¦ˆæ”¶é›†å’Œæ»¡æ„åº¦è®¡ç®—

### è¿›é˜¶ç»ƒä¹ 

4. è®¾è®¡ä¸€ä¸ªå®Œæ•´çš„ RAG è¯„ä¼° Pipeline
5. å®ç° A/B æµ‹è¯•æ¡†æ¶å’Œç»Ÿè®¡æ˜¾è‘—æ€§æ£€éªŒ

### å‚è€ƒç­”æ¡ˆ

<details>
<summary>ç»ƒä¹  1 å‚è€ƒç­”æ¡ˆ</summary>

```python
from openai import OpenAI
import json

client = OpenAI()

def llm_judge_simple(question: str, answer: str) -> dict:
    """ç®€å•çš„ LLM è¯„ä¼°"""

    prompt = f"""
è¯„ä¼°ä»¥ä¸‹é—®ç­”çš„è´¨é‡ï¼Œè¿”å› JSON æ ¼å¼è¯„åˆ†ã€‚

é—®é¢˜ï¼š{question}
å›ç­”ï¼š{answer}

è¿”å›æ ¼å¼ï¼š
{{"score": 1-5, "reason": "è¯„åˆ†ç†ç”±"}}
"""

    response = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[{"role": "user", "content": prompt}],
        response_format={"type": "json_object"}
    )

    return json.loads(response.choices[0].message.content)

# æµ‹è¯•
result = llm_judge_simple(
    question="Python å’Œ Java çš„åŒºåˆ«æ˜¯ä»€ä¹ˆï¼Ÿ",
    answer="Python æ˜¯åŠ¨æ€ç±»å‹è¯­è¨€ï¼Œè¯­æ³•ç®€æ´ï¼›Java æ˜¯é™æ€ç±»å‹è¯­è¨€ï¼Œéœ€è¦ç¼–è¯‘ã€‚"
)
print(f"è¯„åˆ†: {result['score']}, ç†ç”±: {result['reason']}")
```

</details>

---

## â¡ï¸ ä¸‹ä¸€æ­¥

å­¦å®Œæœ¬èŠ‚åï¼Œç»§ç»­å®æˆ˜é¡¹ç›® [12-é¡¹ç›®-RAGçŸ¥è¯†åº“.md](./12-é¡¹ç›®-RAGçŸ¥è¯†åº“.md)

