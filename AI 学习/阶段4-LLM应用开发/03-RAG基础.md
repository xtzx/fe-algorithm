# ğŸ“š RAG åŸºç¡€

> æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRetrieval-Augmented Generationï¼‰

---

## ä»€ä¹ˆæ˜¯ RAG

```
RAG é€šè¿‡æ£€ç´¢å¤–éƒ¨çŸ¥è¯†æ¥å¢å¼ºå¤§æ¨¡å‹çš„ç”Ÿæˆèƒ½åŠ›ï¼š

ç”¨æˆ·é—®é¢˜ â†’ æ£€ç´¢ç›¸å…³æ–‡æ¡£ â†’ æ„é€ ä¸Šä¸‹æ–‡ â†’ LLM ç”Ÿæˆç­”æ¡ˆ

ä¼˜åŠ¿ï¼š
1. çŸ¥è¯†å¯æ›´æ–°ï¼šæ— éœ€é‡æ–°è®­ç»ƒæ¨¡å‹
2. å‡å°‘å¹»è§‰ï¼šåŸºäºçœŸå®æ–‡æ¡£å›ç­”
3. å¯è¿½æº¯ï¼šç­”æ¡ˆæœ‰æ¥æºå‡ºå¤„
4. æˆæœ¬ä½ï¼šæ¯”å¾®è°ƒæ›´ç»æµ
```

---

## RAG æ ¸å¿ƒæµç¨‹

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        ç¦»çº¿ç´¢å¼•é˜¶æ®µ                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚  æ–‡æ¡£ â†’ åˆ‡åˆ†(Chunking) â†’ Embedding â†’ å­˜å…¥å‘é‡æ•°æ®åº“            â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        åœ¨çº¿æŸ¥è¯¢é˜¶æ®µ                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚  é—®é¢˜ â†’ Embedding â†’ å‘é‡æ£€ç´¢ â†’ Top-K æ–‡æ¡£ â†’ æ„é€  Prompt â†’ LLM â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ä»é›¶å®ç° RAG

### å®Œæ•´ä»£ç 

```python
import os
from typing import List, Dict
import numpy as np

# ========== 1. æ–‡æ¡£åˆ‡åˆ† ==========
def simple_chunk(text: str, chunk_size: int = 500, overlap: int = 50) -> List[str]:
    """ç®€å•çš„å›ºå®šé•¿åº¦åˆ‡åˆ†"""
    chunks = []
    start = 0
    while start < len(text):
        end = start + chunk_size
        chunk = text[start:end]
        chunks.append(chunk)
        start = end - overlap
    return chunks


def recursive_chunk(text: str, chunk_size: int = 500, separators: List[str] = None) -> List[str]:
    """é€’å½’åˆ‡åˆ†ï¼šä¼˜å…ˆæŒ‰è¯­ä¹‰è¾¹ç•Œåˆ‡åˆ†"""
    if separators is None:
        separators = ["\n\n", "\n", "ã€‚", ".", " ", ""]

    chunks = []

    def split_text(text: str, sep_index: int = 0) -> List[str]:
        if len(text) <= chunk_size:
            return [text] if text.strip() else []

        if sep_index >= len(separators):
            # æ— æ³•å†åˆ†ï¼Œå¼ºåˆ¶åˆ‡åˆ†
            return [text[i:i+chunk_size] for i in range(0, len(text), chunk_size)]

        sep = separators[sep_index]
        if sep:
            parts = text.split(sep)
        else:
            parts = list(text)

        result = []
        current = ""

        for part in parts:
            test = current + sep + part if current else part
            if len(test) <= chunk_size:
                current = test
            else:
                if current:
                    result.append(current)
                # é€’å½’å¤„ç†è¿‡é•¿çš„éƒ¨åˆ†
                if len(part) > chunk_size:
                    result.extend(split_text(part, sep_index + 1))
                else:
                    current = part

        if current:
            result.append(current)

        return result

    return split_text(text)


# ========== 2. Embedding ==========
from sentence_transformers import SentenceTransformer

class EmbeddingModel:
    def __init__(self, model_name: str = "all-MiniLM-L6-v2"):
        self.model = SentenceTransformer(model_name)

    def embed(self, texts: List[str]) -> np.ndarray:
        """æ‰¹é‡ç”Ÿæˆ embedding"""
        return self.model.encode(texts, show_progress_bar=True)

    def embed_query(self, query: str) -> np.ndarray:
        """å•æ¡æŸ¥è¯¢çš„ embedding"""
        return self.model.encode([query])[0]


# ========== 3. å‘é‡å­˜å‚¨ ==========
import faiss

class VectorStore:
    def __init__(self, dimension: int = 384):
        self.dimension = dimension
        self.index = faiss.IndexFlatIP(dimension)  # å†…ç§¯ï¼ˆä½™å¼¦ç›¸ä¼¼åº¦ï¼‰
        self.documents: List[Dict] = []

    def add(self, embeddings: np.ndarray, documents: List[Dict]):
        """æ·»åŠ æ–‡æ¡£"""
        # å½’ä¸€åŒ–ä»¥ä½¿ç”¨å†…ç§¯è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
        faiss.normalize_L2(embeddings)
        self.index.add(embeddings.astype('float32'))
        self.documents.extend(documents)

    def search(self, query_embedding: np.ndarray, top_k: int = 5) -> List[Dict]:
        """æ£€ç´¢"""
        query_embedding = query_embedding.reshape(1, -1).astype('float32')
        faiss.normalize_L2(query_embedding)

        scores, indices = self.index.search(query_embedding, top_k)

        results = []
        for score, idx in zip(scores[0], indices[0]):
            if idx < len(self.documents):
                doc = self.documents[idx].copy()
                doc['score'] = float(score)
                results.append(doc)

        return results


# ========== 4. ç”Ÿæˆ ==========
from openai import OpenAI

client = OpenAI()

def generate_answer(query: str, contexts: List[str], system_prompt: str = None) -> str:
    """åŸºäºæ£€ç´¢ç»“æœç”Ÿæˆç­”æ¡ˆ"""

    if system_prompt is None:
        system_prompt = """ä½ æ˜¯ä¸€ä¸ªçŸ¥è¯†é—®ç­”åŠ©æ‰‹ã€‚è¯·åŸºäºæä¾›çš„å‚è€ƒèµ„æ–™å›ç­”ç”¨æˆ·é—®é¢˜ã€‚
å¦‚æœå‚è€ƒèµ„æ–™ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·æ˜ç¡®è¯´æ˜ã€‚
å›ç­”è¦å‡†ç¡®ã€ç®€æ´ï¼Œå¹¶åœ¨é€‚å½“æ—¶å€™å¼•ç”¨æ¥æºã€‚"""

    context_text = "\n\n".join([f"[æ–‡æ¡£{i+1}]\n{ctx}" for i, ctx in enumerate(contexts)])

    user_prompt = f"""å‚è€ƒèµ„æ–™ï¼š
{context_text}

ç”¨æˆ·é—®é¢˜ï¼š{query}

è¯·åŸºäºå‚è€ƒèµ„æ–™å›ç­”é—®é¢˜ï¼š"""

    response = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt}
        ],
        temperature=0.3
    )

    return response.choices[0].message.content


# ========== 5. RAG Pipeline ==========
class SimpleRAG:
    def __init__(self, embedding_model: str = "all-MiniLM-L6-v2"):
        self.embedder = EmbeddingModel(embedding_model)
        self.vector_store = VectorStore(dimension=384)

    def add_documents(self, documents: List[str], metadatas: List[Dict] = None):
        """æ·»åŠ æ–‡æ¡£åˆ°çŸ¥è¯†åº“"""
        if metadatas is None:
            metadatas = [{}] * len(documents)

        # åˆ‡åˆ†
        all_chunks = []
        all_metadata = []

        for doc, meta in zip(documents, metadatas):
            chunks = recursive_chunk(doc, chunk_size=500)
            for i, chunk in enumerate(chunks):
                all_chunks.append(chunk)
                all_metadata.append({
                    **meta,
                    "chunk_index": i,
                    "content": chunk
                })

        print(f"åˆ‡åˆ†ä¸º {len(all_chunks)} ä¸ªå—")

        # Embedding
        embeddings = self.embedder.embed(all_chunks)

        # å­˜å‚¨
        self.vector_store.add(embeddings, all_metadata)

        print(f"å·²æ·»åŠ  {len(all_chunks)} ä¸ªæ–‡æ¡£å—")

    def query(self, question: str, top_k: int = 5) -> Dict:
        """æŸ¥è¯¢"""
        # æ£€ç´¢
        query_embedding = self.embedder.embed_query(question)
        results = self.vector_store.search(query_embedding, top_k)

        # æ„é€ ä¸Šä¸‹æ–‡
        contexts = [r['content'] for r in results]

        # ç”Ÿæˆç­”æ¡ˆ
        answer = generate_answer(question, contexts)

        return {
            "question": question,
            "answer": answer,
            "sources": results
        }


# ========== ä½¿ç”¨ç¤ºä¾‹ ==========
if __name__ == "__main__":
    # åˆ›å»º RAG å®ä¾‹
    rag = SimpleRAG()

    # æ·»åŠ æ–‡æ¡£
    documents = [
        """
        Python æ˜¯ä¸€ç§é«˜çº§ç¼–ç¨‹è¯­è¨€ï¼Œç”± Guido van Rossum äº 1991 å¹´å‘å¸ƒã€‚
        Python ä»¥å…¶ç®€æ´çš„è¯­æ³•å’Œå¼ºå¤§çš„æ ‡å‡†åº“è€Œé—»åï¼Œå¹¿æ³›åº”ç”¨äº Web å¼€å‘ã€
        æ•°æ®ç§‘å­¦ã€äººå·¥æ™ºèƒ½ç­‰é¢†åŸŸã€‚Python æ”¯æŒå¤šç§ç¼–ç¨‹èŒƒå¼ï¼ŒåŒ…æ‹¬é¢å‘å¯¹è±¡ã€
        å‡½æ•°å¼å’Œè¿‡ç¨‹å¼ç¼–ç¨‹ã€‚
        """,
        """
        æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œä¸“æ³¨äºè®©è®¡ç®—æœºä»æ•°æ®ä¸­å­¦ä¹ ã€‚
        å¸¸è§çš„æœºå™¨å­¦ä¹ ç®—æ³•åŒ…æ‹¬çº¿æ€§å›å½’ã€å†³ç­–æ ‘ã€éšæœºæ£®æ—ã€æ”¯æŒå‘é‡æœºç­‰ã€‚
        æ·±åº¦å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„å­é¢†åŸŸï¼Œä½¿ç”¨å¤šå±‚ç¥ç»ç½‘ç»œå¤„ç†å¤æ‚ä»»åŠ¡ã€‚
        """,
        """
        å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ˜¯åŸºäº Transformer æ¶æ„çš„é¢„è®­ç»ƒæ¨¡å‹ã€‚
        ä»£è¡¨æ€§çš„ LLM åŒ…æ‹¬ GPTã€BERTã€LLaMA ç­‰ã€‚è¿™äº›æ¨¡å‹é€šè¿‡åœ¨å¤§è§„æ¨¡
        æ–‡æœ¬æ•°æ®ä¸Šè®­ç»ƒï¼Œå­¦ä¹ è¯­è¨€çš„ç»Ÿè®¡è§„å¾‹ï¼Œèƒ½å¤Ÿå®Œæˆæ–‡æœ¬ç”Ÿæˆã€é—®ç­”ã€
        æ‘˜è¦ç­‰å¤šç§ä»»åŠ¡ã€‚
        """
    ]

    metadatas = [
        {"source": "python_intro.txt"},
        {"source": "ml_basics.txt"},
        {"source": "llm_overview.txt"}
    ]

    rag.add_documents(documents, metadatas)

    # æŸ¥è¯¢
    result = rag.query("ä»€ä¹ˆæ˜¯å¤§è¯­è¨€æ¨¡å‹ï¼Ÿå®ƒå’Œæœºå™¨å­¦ä¹ æœ‰ä»€ä¹ˆå…³ç³»ï¼Ÿ")

    print("é—®é¢˜:", result["question"])
    print("\nç­”æ¡ˆ:", result["answer"])
    print("\næ¥æº:")
    for source in result["sources"][:3]:
        print(f"  - {source.get('source', 'unknown')} (score: {source['score']:.4f})")
```

---

## Chunking ç­–ç•¥è¯¦è§£

### 1. å›ºå®šé•¿åº¦åˆ‡åˆ†

```python
def fixed_size_chunk(text: str, chunk_size: int = 500, overlap: int = 50):
    """
    ä¼˜ç‚¹ï¼šç®€å•ã€å¯é¢„æµ‹
    ç¼ºç‚¹ï¼šå¯èƒ½åœ¨å¥å­ä¸­é—´æˆªæ–­
    """
    chunks = []
    for i in range(0, len(text), chunk_size - overlap):
        chunks.append(text[i:i + chunk_size])
    return chunks
```

### 2. å¥å­çº§åˆ‡åˆ†

```python
import re

def sentence_chunk(text: str, max_chunk_size: int = 500):
    """æŒ‰å¥å­åˆ‡åˆ†ï¼Œå°½é‡ä¸è¶…è¿‡æœ€å¤§é•¿åº¦"""
    # åˆ†å¥
    sentences = re.split(r'(?<=[ã€‚ï¼ï¼Ÿ.!?])\s*', text)

    chunks = []
    current_chunk = ""

    for sentence in sentences:
        if len(current_chunk) + len(sentence) <= max_chunk_size:
            current_chunk += sentence
        else:
            if current_chunk:
                chunks.append(current_chunk)
            current_chunk = sentence

    if current_chunk:
        chunks.append(current_chunk)

    return chunks
```

### 3. è¯­ä¹‰åˆ‡åˆ†

```python
from sentence_transformers import SentenceTransformer
import numpy as np

def semantic_chunk(text: str, model: SentenceTransformer,
                   threshold: float = 0.7, max_chunk_size: int = 1000):
    """åŸºäºè¯­ä¹‰ç›¸ä¼¼åº¦çš„åˆ‡åˆ†"""
    # åˆ†å¥
    sentences = re.split(r'(?<=[ã€‚ï¼ï¼Ÿ.!?])\s*', text)
    sentences = [s for s in sentences if s.strip()]

    if len(sentences) <= 1:
        return sentences

    # è®¡ç®—å¥å­ embedding
    embeddings = model.encode(sentences)

    # è®¡ç®—ç›¸é‚»å¥å­çš„ç›¸ä¼¼åº¦
    chunks = []
    current_chunk = [sentences[0]]

    for i in range(1, len(sentences)):
        # è®¡ç®—å½“å‰å¥å­ä¸å‰ä¸€å¥çš„ç›¸ä¼¼åº¦
        sim = np.dot(embeddings[i], embeddings[i-1]) / (
            np.linalg.norm(embeddings[i]) * np.linalg.norm(embeddings[i-1])
        )

        current_text = "".join(current_chunk + [sentences[i]])

        # å¦‚æœç›¸ä¼¼åº¦ä½äºé˜ˆå€¼æˆ–è¶…è¿‡æœ€å¤§é•¿åº¦ï¼Œå¼€å§‹æ–°å—
        if sim < threshold or len(current_text) > max_chunk_size:
            chunks.append("".join(current_chunk))
            current_chunk = [sentences[i]]
        else:
            current_chunk.append(sentences[i])

    if current_chunk:
        chunks.append("".join(current_chunk))

    return chunks
```

### 4. Parent-Childï¼ˆçˆ¶å­æ–‡æ¡£ï¼‰

```python
class ParentChildChunker:
    """
    çˆ¶æ–‡æ¡£ç”¨äºä¸Šä¸‹æ–‡ï¼Œå­æ–‡æ¡£ç”¨äºç²¾ç¡®æ£€ç´¢
    """

    def __init__(self, parent_size: int = 2000, child_size: int = 400):
        self.parent_size = parent_size
        self.child_size = child_size

    def chunk(self, text: str) -> List[Dict]:
        results = []

        # å…ˆåˆ‡åˆ†çˆ¶æ–‡æ¡£
        parents = simple_chunk(text, self.parent_size, overlap=200)

        for parent_idx, parent in enumerate(parents):
            # å†åˆ‡åˆ†å­æ–‡æ¡£
            children = simple_chunk(parent, self.child_size, overlap=50)

            for child_idx, child in enumerate(children):
                results.append({
                    "parent_id": parent_idx,
                    "parent_content": parent,
                    "child_content": child,
                    "child_id": f"{parent_idx}_{child_idx}"
                })

        return results
```

---

## Embedding æ¨¡å‹é€‰æ‹©

```python
# ä¸åŒ Embedding æ¨¡å‹å¯¹æ¯”
models = {
    # å¼€æºæ¨¡å‹
    "all-MiniLM-L6-v2": {
        "dim": 384,
        "speed": "fast",
        "quality": "good",
        "multilingual": False
    },
    "all-mpnet-base-v2": {
        "dim": 768,
        "speed": "medium",
        "quality": "better",
        "multilingual": False
    },
    "multilingual-e5-large": {
        "dim": 1024,
        "speed": "slow",
        "quality": "best",
        "multilingual": True
    },

    # API æ¨¡å‹
    "text-embedding-3-small": {
        "dim": 1536,
        "provider": "OpenAI",
        "cost": "$0.02/1M tokens"
    },
    "text-embedding-3-large": {
        "dim": 3072,
        "provider": "OpenAI",
        "cost": "$0.13/1M tokens"
    }
}

# ä½¿ç”¨ OpenAI Embedding
def openai_embed(texts: List[str], model: str = "text-embedding-3-small"):
    response = client.embeddings.create(
        model=model,
        input=texts
    )
    return [item.embedding for item in response.data]
```

---

## å‘é‡æ•°æ®åº“é€‰æ‹©

```python
# 1. Faiss (æœ¬åœ°ï¼Œé€‚åˆä¸­å°è§„æ¨¡)
import faiss

index = faiss.IndexFlatIP(384)  # å†…ç§¯
index = faiss.IndexFlatL2(384)  # L2 è·ç¦»
index = faiss.IndexIVFFlat(quantizer, 384, nlist)  # åŠ é€Ÿ

# 2. ChromaDB (æœ¬åœ°ï¼Œæ˜“ç”¨)
import chromadb

client = chromadb.Client()
collection = client.create_collection("my_collection")
collection.add(
    documents=["doc1", "doc2"],
    embeddings=[[0.1, 0.2], [0.3, 0.4]],
    ids=["id1", "id2"]
)
results = collection.query(query_embeddings=[[0.1, 0.2]], n_results=5)

# 3. Milvus (åˆ†å¸ƒå¼ï¼Œç”Ÿäº§çº§)
from pymilvus import connections, Collection

connections.connect("default", host="localhost", port="19530")
# ... åˆ›å»º collection å’Œç´¢å¼•

# 4. Pinecone (äº‘æœåŠ¡)
import pinecone

pinecone.init(api_key="xxx")
index = pinecone.Index("my-index")
index.upsert(vectors=[("id1", [0.1, 0.2], {"meta": "data"})])
```

---

## ç»ƒä¹ é¢˜

### ç»ƒä¹  1ï¼šå®ç°æ™ºèƒ½åˆ‡åˆ†

```python
# ä»»åŠ¡ï¼šå®ç°ä¸€ä¸ªåˆ‡åˆ†å‡½æ•°ï¼Œèƒ½å¤Ÿï¼š
# 1. è¯†åˆ« Markdown æ ‡é¢˜ä½œä¸ºè‡ªç„¶è¾¹ç•Œ
# 2. ä¿æŒä»£ç å—å®Œæ•´
# 3. å°½é‡æŒ‰å¥å­è¾¹ç•Œåˆ‡åˆ†
```

### ç»ƒä¹  2ï¼šå¯¹æ¯” Embedding æ¨¡å‹

```python
# ä»»åŠ¡ï¼šå¯¹æ¯”ä»¥ä¸‹æ¨¡å‹åœ¨ä¸­æ–‡é—®ç­”åœºæ™¯çš„æ•ˆæœ
# - all-MiniLM-L6-v2
# - multilingual-e5-small
# - text-embedding-3-small
#
# æµ‹è¯•æ•°æ®ï¼šå‡†å¤‡ 10 ä¸ªä¸­æ–‡é—®é¢˜å’Œå¯¹åº”çš„æ–‡æ¡£
```

### ç»ƒä¹  3ï¼šæ„å»ºç®€å• RAG

```python
# ä»»åŠ¡ï¼šåŸºäºæä¾›çš„ä»£ç ï¼Œæ„å»ºä¸€ä¸ªèƒ½å¤Ÿï¼š
# 1. è¯»å– PDF æ–‡ä»¶
# 2. åˆ‡åˆ†å¹¶ç´¢å¼•
# 3. å›ç­”å…³äº PDF å†…å®¹çš„é—®é¢˜
```

---

## å°ç»“

```
æœ¬èŠ‚è¦ç‚¹ï¼š
1. RAG æµç¨‹ï¼šåˆ‡åˆ† â†’ Embedding â†’ æ£€ç´¢ â†’ ç”Ÿæˆ
2. Chunking ç­–ç•¥ï¼šå›ºå®šé•¿åº¦ã€å¥å­çº§ã€è¯­ä¹‰çº§ã€Parent-Child
3. Embedding é€‰æ‹©ï¼šå¼€æº vs APIï¼Œå¤šè¯­è¨€æ”¯æŒ
4. å‘é‡æ•°æ®åº“ï¼šFaissã€ChromaDBã€Milvusã€Pinecone
```

---

## â¡ï¸ ä¸‹ä¸€æ­¥

ç»§ç»­ [04-RAGè¿›é˜¶.md](./04-RAGè¿›é˜¶.md)

