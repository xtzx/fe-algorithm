# ğŸ–¼ï¸ å¤šæ¨¡æ€åº”ç”¨

> å›¾åƒç†è§£ã€è¯­éŸ³äº¤äº’ã€å¤šæ¨¡æ€ RAG

---

## å›¾åƒç†è§£

### GPT-4V / GPT-4o

```python
from openai import OpenAI
import base64

client = OpenAI()

def encode_image(image_path: str) -> str:
    """å°†å›¾ç‰‡ç¼–ç ä¸º base64"""
    with open(image_path, "rb") as f:
        return base64.b64encode(f.read()).decode("utf-8")

def analyze_image(image_path: str, question: str = "è¿™å¼ å›¾ç‰‡é‡Œæœ‰ä»€ä¹ˆï¼Ÿ"):
    """ä½¿ç”¨ GPT-4o åˆ†æå›¾ç‰‡"""
    base64_image = encode_image(image_path)

    response = client.chat.completions.create(
        model="gpt-4o",
        messages=[
            {
                "role": "user",
                "content": [
                    {"type": "text", "text": question},
                    {
                        "type": "image_url",
                        "image_url": {
                            "url": f"data:image/jpeg;base64,{base64_image}"
                        }
                    }
                ]
            }
        ],
        max_tokens=1000
    )

    return response.choices[0].message.content

# ä½¿ç”¨ URL
def analyze_image_url(image_url: str, question: str):
    response = client.chat.completions.create(
        model="gpt-4o",
        messages=[
            {
                "role": "user",
                "content": [
                    {"type": "text", "text": question},
                    {"type": "image_url", "image_url": {"url": image_url}}
                ]
            }
        ]
    )
    return response.choices[0].message.content

# æµ‹è¯•
result = analyze_image("photo.jpg", "è¯·æè¿°è¿™å¼ å›¾ç‰‡çš„å†…å®¹ï¼Œå¹¶è¯†åˆ«å…¶ä¸­çš„æ–‡å­—")
print(result)
```

### Claude 3 Vision

```python
import anthropic
import base64

client = anthropic.Anthropic()

def analyze_with_claude(image_path: str, question: str):
    """ä½¿ç”¨ Claude 3 åˆ†æå›¾ç‰‡"""
    with open(image_path, "rb") as f:
        image_data = base64.standard_b64encode(f.read()).decode("utf-8")

    # æ¨æ–­åª’ä½“ç±»å‹
    if image_path.endswith(".png"):
        media_type = "image/png"
    elif image_path.endswith(".gif"):
        media_type = "image/gif"
    elif image_path.endswith(".webp"):
        media_type = "image/webp"
    else:
        media_type = "image/jpeg"

    response = client.messages.create(
        model="claude-3-5-sonnet-20241022",
        max_tokens=1024,
        messages=[
            {
                "role": "user",
                "content": [
                    {
                        "type": "image",
                        "source": {
                            "type": "base64",
                            "media_type": media_type,
                            "data": image_data
                        }
                    },
                    {"type": "text", "text": question}
                ]
            }
        ]
    )

    return response.content[0].text

result = analyze_with_claude("chart.png", "è¯·åˆ†æè¿™ä¸ªå›¾è¡¨çš„è¶‹åŠ¿")
print(result)
```

### æœ¬åœ°å¤šæ¨¡æ€æ¨¡å‹ï¼ˆQwen-VLï¼‰

```python
from transformers import Qwen2VLForConditionalGeneration, AutoProcessor
from qwen_vl_utils import process_vision_info
import torch

# åŠ è½½æ¨¡å‹
model = Qwen2VLForConditionalGeneration.from_pretrained(
    "Qwen/Qwen2-VL-2B-Instruct",
    torch_dtype=torch.float16,
    device_map="auto"
)
processor = AutoProcessor.from_pretrained("Qwen/Qwen2-VL-2B-Instruct")

def analyze_local(image_path: str, question: str):
    """ä½¿ç”¨æœ¬åœ° Qwen-VL åˆ†æå›¾ç‰‡"""
    messages = [
        {
            "role": "user",
            "content": [
                {"type": "image", "image": image_path},
                {"type": "text", "text": question}
            ]
        }
    ]

    text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
    image_inputs, video_inputs = process_vision_info(messages)

    inputs = processor(
        text=[text],
        images=image_inputs,
        videos=video_inputs,
        padding=True,
        return_tensors="pt"
    ).to("cuda")

    outputs = model.generate(**inputs, max_new_tokens=256)
    response = processor.batch_decode(outputs, skip_special_tokens=True)[0]

    return response

result = analyze_local("document.png", "è¯·æå–è¿™ä¸ªæ–‡æ¡£ä¸­çš„å…³é”®ä¿¡æ¯")
print(result)
```

---

## è¯­éŸ³äº¤äº’

### Whisperï¼ˆè¯­éŸ³è½¬æ–‡å­—ï¼‰

```python
from openai import OpenAI

client = OpenAI()

def transcribe_audio(audio_path: str, language: str = "zh") -> str:
    """è¯­éŸ³è½¬æ–‡å­—"""
    with open(audio_path, "rb") as audio_file:
        transcript = client.audio.transcriptions.create(
            model="whisper-1",
            file=audio_file,
            language=language,
            response_format="text"
        )
    return transcript

# å¸¦æ—¶é—´æˆ³
def transcribe_with_timestamps(audio_path: str) -> dict:
    """è¯­éŸ³è½¬æ–‡å­—ï¼ˆå¸¦æ—¶é—´æˆ³ï¼‰"""
    with open(audio_path, "rb") as audio_file:
        transcript = client.audio.transcriptions.create(
            model="whisper-1",
            file=audio_file,
            response_format="verbose_json",
            timestamp_granularities=["segment"]
        )
    return transcript

# æœ¬åœ° Whisper
import whisper

def local_transcribe(audio_path: str):
    """ä½¿ç”¨æœ¬åœ° Whisper æ¨¡å‹"""
    model = whisper.load_model("base")  # tiny, base, small, medium, large
    result = model.transcribe(audio_path, language="zh")
    return result["text"]

# æµ‹è¯•
text = transcribe_audio("recording.mp3")
print(f"è½¬å½•ç»“æœ: {text}")
```

### TTSï¼ˆæ–‡å­—è½¬è¯­éŸ³ï¼‰

```python
from openai import OpenAI
from pathlib import Path

client = OpenAI()

def text_to_speech(text: str, output_path: str, voice: str = "alloy"):
    """æ–‡å­—è½¬è¯­éŸ³

    voice å¯é€‰: alloy, echo, fable, onyx, nova, shimmer
    """
    response = client.audio.speech.create(
        model="tts-1",  # æˆ– tts-1-hd
        voice=voice,
        input=text
    )

    response.stream_to_file(output_path)
    print(f"éŸ³é¢‘å·²ä¿å­˜åˆ° {output_path}")

# ä½¿ç”¨
text_to_speech("ä½ å¥½ï¼Œæ¬¢è¿ä½¿ç”¨è¯­éŸ³åŠ©æ‰‹ï¼", "output.mp3", voice="nova")

# æœ¬åœ° TTSï¼ˆä½¿ç”¨ edge-ttsï¼‰
import edge_tts
import asyncio

async def local_tts(text: str, output_path: str, voice: str = "zh-CN-XiaoxiaoNeural"):
    """ä½¿ç”¨ Edge TTSï¼ˆå…è´¹ï¼‰"""
    communicate = edge_tts.Communicate(text, voice)
    await communicate.save(output_path)

# è¿è¡Œ
asyncio.run(local_tts("ä½ å¥½ä¸–ç•Œ", "output.mp3"))
```

### è¯­éŸ³å¯¹è¯ç³»ç»Ÿ

```python
import sounddevice as sd
import numpy as np
from scipy.io import wavfile
import tempfile

class VoiceAssistant:
    """ç®€å•çš„è¯­éŸ³åŠ©æ‰‹"""

    def __init__(self):
        self.client = OpenAI()
        self.sample_rate = 16000

    def record_audio(self, duration: int = 5) -> str:
        """å½•åˆ¶éŸ³é¢‘"""
        print(f"å½•éŸ³ä¸­... ({duration}ç§’)")
        audio = sd.rec(
            int(duration * self.sample_rate),
            samplerate=self.sample_rate,
            channels=1,
            dtype=np.int16
        )
        sd.wait()

        # ä¿å­˜ä¸´æ—¶æ–‡ä»¶
        temp_file = tempfile.NamedTemporaryFile(suffix=".wav", delete=False)
        wavfile.write(temp_file.name, self.sample_rate, audio)
        return temp_file.name

    def transcribe(self, audio_path: str) -> str:
        """è¯­éŸ³è½¬æ–‡å­—"""
        with open(audio_path, "rb") as f:
            result = self.client.audio.transcriptions.create(
                model="whisper-1",
                file=f,
                language="zh"
            )
        return result.text

    def chat(self, user_message: str) -> str:
        """ä¸ LLM å¯¹è¯"""
        response = self.client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªå‹å¥½çš„è¯­éŸ³åŠ©æ‰‹ï¼Œå›ç­”è¦ç®€æ´ã€‚"},
                {"role": "user", "content": user_message}
            ]
        )
        return response.choices[0].message.content

    def speak(self, text: str):
        """æ–‡å­—è½¬è¯­éŸ³å¹¶æ’­æ”¾"""
        response = self.client.audio.speech.create(
            model="tts-1",
            voice="nova",
            input=text
        )

        # ä¿å­˜å¹¶æ’­æ”¾
        temp_file = tempfile.NamedTemporaryFile(suffix=".mp3", delete=False)
        response.stream_to_file(temp_file.name)

        # ä½¿ç”¨ playsound æ’­æ”¾ï¼ˆéœ€è¦ pip install playsoundï¼‰
        from playsound import playsound
        playsound(temp_file.name)

    def run(self):
        """è¿è¡Œè¯­éŸ³åŠ©æ‰‹"""
        print("è¯­éŸ³åŠ©æ‰‹å·²å¯åŠ¨ï¼è¯´ 'é€€å‡º' ç»“æŸå¯¹è¯ã€‚")

        while True:
            # å½•éŸ³
            audio_path = self.record_audio(5)

            # è½¬æ–‡å­—
            user_text = self.transcribe(audio_path)
            print(f"ä½ : {user_text}")

            if "é€€å‡º" in user_text:
                print("å†è§ï¼")
                break

            # è·å–å›å¤
            response = self.chat(user_text)
            print(f"åŠ©æ‰‹: {response}")

            # è¯­éŸ³å›å¤
            self.speak(response)

# è¿è¡Œ
assistant = VoiceAssistant()
assistant.run()
```

---

## å¤šæ¨¡æ€ RAG

### å›¾ç‰‡ + æ–‡æœ¬æ··åˆæ£€ç´¢

```python
from sentence_transformers import SentenceTransformer
import torch
from PIL import Image
from transformers import CLIPProcessor, CLIPModel

class MultimodalRAG:
    """å¤šæ¨¡æ€ RAGï¼šæ”¯æŒå›¾ç‰‡å’Œæ–‡æœ¬"""

    def __init__(self):
        # æ–‡æœ¬ embedding
        self.text_model = SentenceTransformer('all-MiniLM-L6-v2')

        # CLIP æ¨¡å‹ï¼ˆå›¾æ–‡å¯¹é½ï¼‰
        self.clip_model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32")
        self.clip_processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")

        self.documents = []  # å­˜å‚¨æ–‡æ¡£
        self.images = []     # å­˜å‚¨å›¾ç‰‡

    def add_text(self, text: str, metadata: dict = None):
        """æ·»åŠ æ–‡æœ¬æ–‡æ¡£"""
        embedding = self.text_model.encode(text)
        self.documents.append({
            "type": "text",
            "content": text,
            "embedding": embedding,
            "metadata": metadata or {}
        })

    def add_image(self, image_path: str, caption: str = "", metadata: dict = None):
        """æ·»åŠ å›¾ç‰‡"""
        image = Image.open(image_path)

        # ä½¿ç”¨ CLIP è·å–å›¾ç‰‡ embedding
        inputs = self.clip_processor(images=image, return_tensors="pt")
        with torch.no_grad():
            image_features = self.clip_model.get_image_features(**inputs)

        embedding = image_features.squeeze().numpy()

        self.images.append({
            "type": "image",
            "path": image_path,
            "caption": caption,
            "embedding": embedding,
            "metadata": metadata or {}
        })

    def search(self, query: str, top_k: int = 5, search_type: str = "all"):
        """æœç´¢

        search_type: "text", "image", "all"
        """
        results = []

        if search_type in ["text", "all"]:
            # æ–‡æœ¬æœç´¢
            query_emb = self.text_model.encode(query)
            for doc in self.documents:
                score = np.dot(query_emb, doc["embedding"]) / (
                    np.linalg.norm(query_emb) * np.linalg.norm(doc["embedding"])
                )
                results.append({**doc, "score": float(score)})

        if search_type in ["image", "all"]:
            # å›¾ç‰‡æœç´¢ï¼ˆä½¿ç”¨ CLIP æ–‡æœ¬ç¼–ç ï¼‰
            inputs = self.clip_processor(text=[query], return_tensors="pt", padding=True)
            with torch.no_grad():
                text_features = self.clip_model.get_text_features(**inputs)
            query_emb = text_features.squeeze().numpy()

            for img in self.images:
                score = np.dot(query_emb, img["embedding"]) / (
                    np.linalg.norm(query_emb) * np.linalg.norm(img["embedding"])
                )
                results.append({**img, "score": float(score)})

        # æ’åº
        results.sort(key=lambda x: x["score"], reverse=True)
        return results[:top_k]

    def query(self, question: str, top_k: int = 5):
        """RAG æŸ¥è¯¢"""
        # æ£€ç´¢
        results = self.search(question, top_k)

        # æ„é€ ä¸Šä¸‹æ–‡
        context_parts = []
        for r in results:
            if r["type"] == "text":
                context_parts.append(f"[æ–‡æ¡£] {r['content']}")
            else:
                context_parts.append(f"[å›¾ç‰‡] {r['caption']} (è·¯å¾„: {r['path']})")

        context = "\n\n".join(context_parts)

        # å¦‚æœæœ‰å›¾ç‰‡ï¼Œä½¿ç”¨å¤šæ¨¡æ€æ¨¡å‹
        image_paths = [r["path"] for r in results if r["type"] == "image"]

        if image_paths:
            # ä½¿ç”¨ GPT-4o åŒæ—¶ç†è§£æ–‡æœ¬å’Œå›¾ç‰‡
            return self._multimodal_generate(question, context, image_paths)
        else:
            return self._text_generate(question, context)

    def _text_generate(self, question: str, context: str) -> str:
        """çº¯æ–‡æœ¬ç”Ÿæˆ"""
        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": "åŸºäºæä¾›çš„ä¸Šä¸‹æ–‡å›ç­”é—®é¢˜ã€‚"},
                {"role": "user", "content": f"ä¸Šä¸‹æ–‡:\n{context}\n\né—®é¢˜: {question}"}
            ]
        )
        return response.choices[0].message.content

    def _multimodal_generate(self, question: str, context: str, image_paths: list) -> str:
        """å¤šæ¨¡æ€ç”Ÿæˆ"""
        content = [{"type": "text", "text": f"ä¸Šä¸‹æ–‡:\n{context}\n\né—®é¢˜: {question}"}]

        for path in image_paths[:4]:  # æœ€å¤š 4 å¼ å›¾
            base64_image = encode_image(path)
            content.append({
                "type": "image_url",
                "image_url": {"url": f"data:image/jpeg;base64,{base64_image}"}
            })

        response = client.chat.completions.create(
            model="gpt-4o",
            messages=[
                {"role": "system", "content": "åŸºäºæä¾›çš„æ–‡æœ¬å’Œå›¾ç‰‡å›ç­”é—®é¢˜ã€‚"},
                {"role": "user", "content": content}
            ]
        )
        return response.choices[0].message.content


# ä½¿ç”¨ç¤ºä¾‹
rag = MultimodalRAG()

# æ·»åŠ æ–‡æ¡£
rag.add_text("Python æ˜¯ä¸€ç§é«˜çº§ç¼–ç¨‹è¯­è¨€...")
rag.add_text("æœºå™¨å­¦ä¹ æ˜¯ AI çš„ä¸€ä¸ªåˆ†æ”¯...")

# æ·»åŠ å›¾ç‰‡
rag.add_image("architecture.png", caption="ç³»ç»Ÿæ¶æ„å›¾")
rag.add_image("chart.png", caption="æ€§èƒ½å¯¹æ¯”å›¾è¡¨")

# æŸ¥è¯¢
result = rag.query("è¯·è§£é‡Šç³»ç»Ÿæ¶æ„")
print(result)
```

---

## ç»ƒä¹ é¢˜

### ç»ƒä¹  1ï¼šå›¾ç‰‡é—®ç­”æœºå™¨äºº

```python
# ä»»åŠ¡ï¼šåˆ›å»ºä¸€ä¸ªèƒ½å¤Ÿï¼š
# 1. æ¥æ”¶ç”¨æˆ·ä¸Šä¼ çš„å›¾ç‰‡
# 2. å›ç­”å…³äºå›¾ç‰‡çš„é—®é¢˜
# 3. æ”¯æŒå¤šè½®å¯¹è¯
```

### ç»ƒä¹  2ï¼šè¯­éŸ³è½¬æ–‡å­—åº”ç”¨

```python
# ä»»åŠ¡ï¼šåˆ›å»ºä¸€ä¸ªä¼šè®®è®°å½•å·¥å…·ï¼š
# 1. å½•åˆ¶ä¼šè®®éŸ³é¢‘
# 2. è½¬å½•ä¸ºæ–‡å­—
# 3. è‡ªåŠ¨ç”Ÿæˆä¼šè®®æ‘˜è¦
```

### ç»ƒä¹  3ï¼šå¤šæ¨¡æ€çŸ¥è¯†åº“

```python
# ä»»åŠ¡ï¼šæ„å»ºä¸€ä¸ªäº§å“è¯´æ˜ä¹¦çŸ¥è¯†åº“ï¼š
# 1. æ”¯æŒ PDFï¼ˆåŒ…å«å›¾ç‰‡ï¼‰
# 2. æ”¯æŒç‹¬ç«‹å›¾ç‰‡
# 3. èƒ½å¤Ÿå›ç­”å…³äºäº§å“çš„é—®é¢˜
```

---

## å°ç»“

```
æœ¬èŠ‚è¦ç‚¹ï¼š
1. å›¾åƒç†è§£ï¼šGPT-4o / Claude 3 / Qwen-VL
2. è¯­éŸ³äº¤äº’ï¼šWhisperï¼ˆASRï¼‰+ TTS
3. å¤šæ¨¡æ€ RAGï¼šCLIP + æ··åˆæ£€ç´¢
4. åº”ç”¨åœºæ™¯ï¼šé—®ç­”ã€ä¼šè®®è®°å½•ã€çŸ¥è¯†åº“
```

---

## â¡ï¸ ä¸‹ä¸€æ­¥

ç»§ç»­ [10-é¡¹ç›®-RAGçŸ¥è¯†åº“.md](./10-é¡¹ç›®-RAGçŸ¥è¯†åº“.md)

