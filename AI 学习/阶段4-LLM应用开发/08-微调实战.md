# ğŸ”§ å¾®è°ƒå®æˆ˜

> ä½¿ç”¨ LLaMA-Factory å’Œ Unsloth å®Œæˆå¾®è°ƒ

---

## LLaMA-Factory

### æ¦‚è¿°

```
LLaMA-Factory æ˜¯ä¸€ä¸ªä¸€ç«™å¼å¤§æ¨¡å‹å¾®è°ƒå¹³å°ï¼š
- æ”¯æŒ 100+ æ¨¡å‹
- å¤šç§å¾®è°ƒæ–¹æ³•ï¼ˆLoRAã€QLoRAã€å…¨é‡ï¼‰
- Web UI å’Œå‘½ä»¤è¡Œ
- ä¸°å¯Œçš„æ•°æ®é›†æ”¯æŒ
- å®Œæ•´çš„è®­ç»ƒç›‘æ§

GitHub: https://github.com/hiyouga/LLaMA-Factory
```

### å®‰è£…

```bash
# å…‹éš†ä»“åº“
git clone https://github.com/hiyouga/LLaMA-Factory.git
cd LLaMA-Factory

# å®‰è£…ä¾èµ–
pip install -e ".[torch,metrics]"

# éªŒè¯å®‰è£…
llamafactory-cli version
```

### Web UI ä½¿ç”¨

```bash
# å¯åŠ¨ Web UI
llamafactory-cli webui

# æµè§ˆå™¨è®¿é—® http://localhost:7860
```

### å‘½ä»¤è¡Œè®­ç»ƒ

```bash
# ä½¿ç”¨é…ç½®æ–‡ä»¶è®­ç»ƒ
llamafactory-cli train examples/lora_single_gpu/llama3_lora_sft.yaml

# æˆ–ç›´æ¥æŒ‡å®šå‚æ•°
llamafactory-cli train \
    --model_name_or_path Qwen/Qwen2.5-1.5B \
    --stage sft \
    --do_train \
    --finetuning_type lora \
    --lora_target q_proj,v_proj \
    --dataset alpaca_zh \
    --template qwen \
    --output_dir ./output \
    --per_device_train_batch_size 4 \
    --gradient_accumulation_steps 4 \
    --learning_rate 1e-4 \
    --num_train_epochs 3 \
    --logging_steps 10 \
    --save_steps 100 \
    --fp16
```

### é…ç½®æ–‡ä»¶ç¤ºä¾‹

```yaml
# lora_qwen_sft.yaml
### model
model_name_or_path: Qwen/Qwen2.5-1.5B

### method
stage: sft
do_train: true
finetuning_type: lora
lora_target: all

### dataset
dataset: alpaca_zh,custom_data
template: qwen
cutoff_len: 1024
max_samples: 10000
overwrite_cache: true
preprocessing_num_workers: 16

### output
output_dir: saves/qwen2.5-1.5b/lora/sft
logging_steps: 10
save_steps: 500
plot_loss: true
overwrite_output_dir: true

### train
per_device_train_batch_size: 4
gradient_accumulation_steps: 4
learning_rate: 1.0e-4
num_train_epochs: 3.0
lr_scheduler_type: cosine
warmup_ratio: 0.1
fp16: true

### eval
val_size: 0.1
per_device_eval_batch_size: 1
eval_strategy: steps
eval_steps: 500
```

### è‡ªå®šä¹‰æ•°æ®é›†

```json
// data/custom_data.json
[
    {
        "instruction": "è¯·ç”¨ä¸€å¥è¯æ€»ç»“ä»¥ä¸‹å†…å®¹",
        "input": "äººå·¥æ™ºèƒ½æ­£åœ¨æ”¹å˜å„è¡Œå„ä¸š...",
        "output": "AI æ­£åœ¨é©æ–°å¤šä¸ªè¡Œä¸š"
    },
    {
        "instruction": "ç¿»è¯‘æˆè‹±æ–‡",
        "input": "æ·±åº¦å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„ä¸€ä¸ªåˆ†æ”¯",
        "output": "Deep learning is a branch of machine learning"
    }
]

// data/dataset_info.jsonï¼ˆæ³¨å†Œæ•°æ®é›†ï¼‰
{
    "custom_data": {
        "file_name": "custom_data.json",
        "formatting": "alpaca"
    }
}
```

### è¯„ä¼°å’Œæ¨ç†

```bash
# è¯„ä¼°
llamafactory-cli eval \
    --model_name_or_path Qwen/Qwen2.5-1.5B \
    --adapter_name_or_path ./output \
    --template qwen \
    --task ceval_validation \
    --lang zh

# æ¨ç†
llamafactory-cli chat \
    --model_name_or_path Qwen/Qwen2.5-1.5B \
    --adapter_name_or_path ./output \
    --template qwen

# å¯¼å‡ºåˆå¹¶æ¨¡å‹
llamafactory-cli export \
    --model_name_or_path Qwen/Qwen2.5-1.5B \
    --adapter_name_or_path ./output \
    --template qwen \
    --export_dir ./merged_model
```

---

## Unsloth

### æ¦‚è¿°

```
Unsloth æ˜¯ä¸€ä¸ªé«˜æ€§èƒ½å¾®è°ƒåº“ï¼š
- 2-5 å€è®­ç»ƒé€Ÿåº¦æå‡
- 70% æ˜¾å­˜å‡å°‘
- å…¼å®¹ Hugging Face ç”Ÿæ€
- æ”¯æŒ LoRAã€QLoRA
- è‡ªåŠ¨ä¼˜åŒ–

GitHub: https://github.com/unslothai/unsloth
```

### å®‰è£…

```bash
pip install "unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git"
```

### åŸºç¡€ä½¿ç”¨

```python
from unsloth import FastLanguageModel
import torch

# åŠ è½½æ¨¡å‹ï¼ˆè‡ªåŠ¨ä¼˜åŒ–ï¼‰
model, tokenizer = FastLanguageModel.from_pretrained(
    model_name="unsloth/Qwen2.5-1.5B",
    max_seq_length=2048,
    dtype=None,  # è‡ªåŠ¨æ£€æµ‹
    load_in_4bit=True,  # 4-bit é‡åŒ–
)

# æ·»åŠ  LoRA
model = FastLanguageModel.get_peft_model(
    model,
    r=16,
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj",
                    "gate_proj", "up_proj", "down_proj"],
    lora_alpha=16,
    lora_dropout=0,
    bias="none",
    use_gradient_checkpointing="unsloth",  # ä¼˜åŒ–æ˜¾å­˜
    random_state=3407,
    use_rslora=False,  # Rank Stabilized LoRA
)
```

### æ•°æ®å‡†å¤‡

```python
from datasets import load_dataset

# åŠ è½½æ•°æ®
dataset = load_dataset("json", data_files="data.json", split="train")

# æ ¼å¼åŒ–å‡½æ•°
alpaca_prompt = """Below is an instruction that describes a task. Write a response that appropriately completes the request.

### Instruction:
{}

### Input:
{}

### Response:
{}"""

def formatting_prompts_func(examples):
    instructions = examples["instruction"]
    inputs = examples["input"]
    outputs = examples["output"]
    texts = []
    for instruction, input, output in zip(instructions, inputs, outputs):
        text = alpaca_prompt.format(instruction, input, output)
        texts.append(text)
    return {"text": texts}

dataset = dataset.map(formatting_prompts_func, batched=True)
```

### è®­ç»ƒ

```python
from trl import SFTTrainer
from transformers import TrainingArguments

trainer = SFTTrainer(
    model=model,
    tokenizer=tokenizer,
    train_dataset=dataset,
    dataset_text_field="text",
    max_seq_length=2048,
    dataset_num_proc=2,
    packing=False,
    args=TrainingArguments(
        per_device_train_batch_size=2,
        gradient_accumulation_steps=4,
        warmup_steps=5,
        max_steps=60,
        learning_rate=2e-4,
        fp16=not torch.cuda.is_bf16_supported(),
        bf16=torch.cuda.is_bf16_supported(),
        logging_steps=1,
        optim="adamw_8bit",
        weight_decay=0.01,
        lr_scheduler_type="linear",
        seed=3407,
        output_dir="outputs",
    ),
)

# å¼€å§‹è®­ç»ƒ
trainer_stats = trainer.train()
```

### æ¨ç†

```python
# å¯ç”¨æ¨ç†æ¨¡å¼
FastLanguageModel.for_inference(model)

# ç”Ÿæˆ
inputs = tokenizer(
    [alpaca_prompt.format(
        "å†™ä¸€é¦–å…³äºæ˜¥å¤©çš„è¯—",  # instruction
        "",                   # input
        ""                    # output (ç•™ç©ºè®©æ¨¡å‹ç”Ÿæˆ)
    )],
    return_tensors="pt"
).to("cuda")

outputs = model.generate(**inputs, max_new_tokens=128, use_cache=True)
print(tokenizer.batch_decode(outputs))
```

### ä¿å­˜å’Œå¯¼å‡º

```python
# ä¿å­˜ LoRA
model.save_pretrained("lora_model")
tokenizer.save_pretrained("lora_model")

# åˆå¹¶ä¿å­˜
model.save_pretrained_merged("merged_model", tokenizer, save_method="merged_16bit")

# å¯¼å‡ºä¸º GGUFï¼ˆç”¨äº llama.cppï¼‰
model.save_pretrained_gguf("model", tokenizer, quantization_method="q4_k_m")

# æ¨é€åˆ° Hugging Face
model.push_to_hub_merged(
    "your-username/your-model-name",
    tokenizer,
    save_method="merged_16bit",
    token="hf_xxx"
)
```

---

## å®Œæ•´å¾®è°ƒæµç¨‹

### 1. å‡†å¤‡æ•°æ®

```python
# data_preparation.py
import json

def prepare_custom_dataset(raw_data_path: str, output_path: str):
    """å‡†å¤‡è‡ªå®šä¹‰æ•°æ®é›†"""
    with open(raw_data_path, 'r', encoding='utf-8') as f:
        raw_data = json.load(f)

    formatted_data = []
    for item in raw_data:
        formatted_data.append({
            "instruction": item.get("question", ""),
            "input": item.get("context", ""),
            "output": item.get("answer", "")
        })

    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(formatted_data, f, ensure_ascii=False, indent=2)

    print(f"è½¬æ¢å®Œæˆï¼Œå…± {len(formatted_data)} æ¡æ•°æ®")

# æ‰§è¡Œ
prepare_custom_dataset("raw_qa.json", "alpaca_qa.json")
```

### 2. é…ç½®è®­ç»ƒ

```python
# train_config.py
config = {
    # æ¨¡å‹é…ç½®
    "model_name": "Qwen/Qwen2.5-1.5B",
    "max_seq_length": 2048,
    "load_in_4bit": True,

    # LoRA é…ç½®
    "lora_r": 16,
    "lora_alpha": 32,
    "lora_dropout": 0.05,
    "target_modules": ["q_proj", "k_proj", "v_proj", "o_proj"],

    # è®­ç»ƒé…ç½®
    "batch_size": 4,
    "gradient_accumulation": 4,
    "learning_rate": 2e-4,
    "num_epochs": 3,
    "warmup_ratio": 0.1,

    # è¾“å‡º
    "output_dir": "./output",
    "logging_steps": 10,
    "save_steps": 100,
}
```

### 3. è®­ç»ƒè„šæœ¬

```python
# train.py
from unsloth import FastLanguageModel
from datasets import load_dataset
from trl import SFTTrainer
from transformers import TrainingArguments
import torch

def main():
    # åŠ è½½é…ç½®
    from train_config import config

    # åŠ è½½æ¨¡å‹
    model, tokenizer = FastLanguageModel.from_pretrained(
        model_name=config["model_name"],
        max_seq_length=config["max_seq_length"],
        load_in_4bit=config["load_in_4bit"],
    )

    # æ·»åŠ  LoRA
    model = FastLanguageModel.get_peft_model(
        model,
        r=config["lora_r"],
        target_modules=config["target_modules"],
        lora_alpha=config["lora_alpha"],
        lora_dropout=config["lora_dropout"],
    )

    # åŠ è½½æ•°æ®
    dataset = load_dataset("json", data_files="alpaca_qa.json", split="train")

    # æ ¼å¼åŒ–
    def format_func(examples):
        texts = []
        for i in range(len(examples["instruction"])):
            text = f"""<|im_start|>user
{examples["instruction"][i]}
{examples["input"][i]}<|im_end|>
<|im_start|>assistant
{examples["output"][i]}<|im_end|>"""
            texts.append(text)
        return {"text": texts}

    dataset = dataset.map(format_func, batched=True)

    # è®­ç»ƒ
    trainer = SFTTrainer(
        model=model,
        tokenizer=tokenizer,
        train_dataset=dataset,
        dataset_text_field="text",
        max_seq_length=config["max_seq_length"],
        args=TrainingArguments(
            per_device_train_batch_size=config["batch_size"],
            gradient_accumulation_steps=config["gradient_accumulation"],
            learning_rate=config["learning_rate"],
            num_train_epochs=config["num_epochs"],
            warmup_ratio=config["warmup_ratio"],
            logging_steps=config["logging_steps"],
            save_steps=config["save_steps"],
            output_dir=config["output_dir"],
            fp16=True,
        ),
    )

    trainer.train()

    # ä¿å­˜
    model.save_pretrained_merged("merged_model", tokenizer)
    print("è®­ç»ƒå®Œæˆï¼")

if __name__ == "__main__":
    main()
```

### 4. è¯„ä¼°è„šæœ¬

```python
# evaluate.py
from unsloth import FastLanguageModel
import json

def evaluate_model(model_path: str, test_data_path: str):
    """è¯„ä¼°æ¨¡å‹"""
    # åŠ è½½æ¨¡å‹
    model, tokenizer = FastLanguageModel.from_pretrained(
        model_name=model_path,
        max_seq_length=2048,
    )
    FastLanguageModel.for_inference(model)

    # åŠ è½½æµ‹è¯•æ•°æ®
    with open(test_data_path, 'r', encoding='utf-8') as f:
        test_data = json.load(f)

    results = []
    for item in test_data:
        prompt = f"""<|im_start|>user
{item["instruction"]}
{item["input"]}<|im_end|>
<|im_start|>assistant
"""
        inputs = tokenizer(prompt, return_tensors="pt").to("cuda")
        outputs = model.generate(**inputs, max_new_tokens=256)
        response = tokenizer.decode(outputs[0], skip_special_tokens=True)

        # æå–å›ç­”
        response = response.split("<|im_start|>assistant")[-1].strip()

        results.append({
            "question": item["instruction"],
            "expected": item["output"],
            "generated": response
        })

    # ä¿å­˜ç»“æœ
    with open("eval_results.json", 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)

    print(f"è¯„ä¼°å®Œæˆï¼Œç»“æœä¿å­˜åˆ° eval_results.json")

evaluate_model("merged_model", "test_data.json")
```

---

## å¸¸è§é—®é¢˜

### æ˜¾å­˜ä¸è¶³

```python
# 1. ä½¿ç”¨ 4-bit é‡åŒ–
load_in_4bit=True

# 2. å‡å°‘ batch size
per_device_train_batch_size=1
gradient_accumulation_steps=16

# 3. ä½¿ç”¨ gradient checkpointing
use_gradient_checkpointing=True

# 4. å‡å°‘ max_seq_length
max_seq_length=512
```

### è®­ç»ƒä¸ç¨³å®š

```python
# 1. é™ä½å­¦ä¹ ç‡
learning_rate=1e-5

# 2. å¢åŠ  warmup
warmup_ratio=0.1

# 3. ä½¿ç”¨ gradient clipping
max_grad_norm=1.0
```

### æ•ˆæœä¸å¥½

```python
# 1. å¢åŠ æ•°æ®é‡
# 2. è°ƒæ•´ LoRA rank
r=32  # æˆ–æ›´é«˜

# 3. æ‰©å¤§ target modules
target_modules=["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]

# 4. å¢åŠ è®­ç»ƒè½®æ•°
num_train_epochs=5
```

---

## å°ç»“

```
æœ¬èŠ‚è¦ç‚¹ï¼š
1. LLaMA-Factoryï¼šä¸€ç«™å¼å¾®è°ƒå¹³å°ï¼ŒWeb UI å‹å¥½
2. Unslothï¼šé«˜æ€§èƒ½å¾®è°ƒåº“ï¼Œé€Ÿåº¦å¿«ã€æ˜¾å­˜çœ
3. å®Œæ•´æµç¨‹ï¼šæ•°æ®å‡†å¤‡ â†’ é…ç½® â†’ è®­ç»ƒ â†’ è¯„ä¼° â†’ éƒ¨ç½²
4. å¸¸è§é—®é¢˜ï¼šæ˜¾å­˜ã€ç¨³å®šæ€§ã€æ•ˆæœä¼˜åŒ–
```

---

## â¡ï¸ ä¸‹ä¸€æ­¥

ç»§ç»­ [09-å¤šæ¨¡æ€åº”ç”¨.md](./09-å¤šæ¨¡æ€åº”ç”¨.md)

