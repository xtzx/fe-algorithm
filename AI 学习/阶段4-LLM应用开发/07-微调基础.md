# 🎛️ 07 - 微调基础

> PEFT、LoRA、QLoRA 原理与实践

---

## 为什么需要微调

```
预训练模型 vs 微调：
- 预训练：通用能力，大规模数据
- 微调：专业能力，特定数据

微调的目的：
1. 领域适配：让模型更懂专业领域
2. 风格调整：改变输出风格
3. 指令遵循：提高指令执行能力
4. 减少幻觉：基于准确数据训练

微调类型：
- 全量微调（Full Fine-tuning）：更新所有参数
- 参数高效微调（PEFT）：只更新少量参数
```

---

## PEFT（参数高效微调）

### 概述

```
PEFT 的核心思想：
- 冻结预训练模型大部分参数
- 只训练少量新增参数
- 大幅降低计算和存储需求

常见方法：
1. LoRA：低秩适配
2. Prefix Tuning：前缀调优
3. Prompt Tuning：提示调优
4. Adapter：适配器
```

### 对比

| 方法 | 参数量 | 训练速度 | 效果 | 推理开销 |
|------|--------|----------|------|----------|
| 全量微调 | 100% | 慢 | 最好 | 无 |
| LoRA | ~0.1% | 快 | 很好 | 可合并 |
| Prefix Tuning | ~0.1% | 快 | 好 | 有 |
| Adapter | ~1% | 中 | 很好 | 有 |

---

## LoRA（Low-Rank Adaptation）

### 数学原理

```
LoRA 的核心思想：
- 原始权重矩阵 W (d×k)
- 权重更新 ΔW 可以用低秩矩阵表示：ΔW = BA
  - B: (d×r)
  - A: (r×k)
  - r << min(d, k)，通常 r = 8, 16, 32

前向传播：
h = Wx + BAx = (W + BA)x

参数量对比：
- 原始：d × k
- LoRA：d × r + r × k = r(d + k)
- 如 d=4096, k=4096, r=8：
  - 原始：16M 参数
  - LoRA：65K 参数（约 0.4%）
```

### 可视化

```
原始矩阵        LoRA 分解
┌─────────┐    ┌───┐   ┌─────────┐
│         │    │   │   │         │
│    W    │  ≈ │ B │ × │    A    │
│  (d×k)  │    │(d×r)  │  (r×k)  │
│         │    │   │   │         │
└─────────┘    └───┘   └─────────┘
  16M 参数       32K  +  32K = 64K 参数
```

### 代码实现

```python
import torch
import torch.nn as nn

class LoRALayer(nn.Module):
    """LoRA 层实现"""

    def __init__(self, original_layer: nn.Linear, r: int = 8, alpha: int = 16):
        super().__init__()
        self.original = original_layer
        self.r = r
        self.alpha = alpha
        self.scaling = alpha / r

        # 冻结原始权重
        self.original.weight.requires_grad = False
        if self.original.bias is not None:
            self.original.bias.requires_grad = False

        # 创建 LoRA 矩阵
        d, k = original_layer.weight.shape
        self.lora_A = nn.Parameter(torch.randn(r, k) * 0.01)
        self.lora_B = nn.Parameter(torch.zeros(d, r))

    def forward(self, x):
        # 原始输出 + LoRA 输出
        original_output = self.original(x)
        lora_output = (x @ self.lora_A.T @ self.lora_B.T) * self.scaling
        return original_output + lora_output

    def merge_weights(self):
        """合并 LoRA 权重到原始权重（推理时用）"""
        self.original.weight.data += (self.lora_B @ self.lora_A).T * self.scaling


def apply_lora(model, r: int = 8, alpha: int = 16, target_modules: list = None):
    """给模型应用 LoRA"""
    if target_modules is None:
        target_modules = ['q_proj', 'v_proj']  # 常见配置

    for name, module in model.named_modules():
        if any(target in name for target in target_modules):
            if isinstance(module, nn.Linear):
                # 替换为 LoRA 层
                parent_name = '.'.join(name.split('.')[:-1])
                child_name = name.split('.')[-1]
                parent = model.get_submodule(parent_name) if parent_name else model
                setattr(parent, child_name, LoRALayer(module, r, alpha))

    return model
```

---

## QLoRA（Quantized LoRA）

### 原理

```
QLoRA = 量化 + LoRA：
1. 将基础模型量化到 4-bit（NF4）
2. 在量化模型上应用 LoRA
3. 只训练 LoRA 参数（FP16）

优势：
- 大幅降低显存需求（约 4 倍）
- 保持接近全精度的效果
- 可以在消费级 GPU 上微调大模型

关键技术：
- NF4：Normal Float 4-bit 量化
- Double Quantization：对量化常数也量化
- Paged Optimizer：分页优化器
```

### 显存对比

```
模型大小：7B 参数

全量微调：
- 模型权重（FP16）：14 GB
- 梯度：14 GB
- 优化器状态：28 GB
- 总计：~60 GB

QLoRA 微调：
- 模型权重（4-bit）：3.5 GB
- LoRA 参数（FP16）：~100 MB
- 梯度 + 优化器：~1 GB
- 总计：~5 GB
```

---

## DoRA（Weight-Decomposed LoRA）

### 原理

```
DoRA 将权重分解为方向和幅度：
W = m × (W₀ + BA) / ||W₀ + BA||

其中：
- m：幅度（magnitude）
- W₀ + BA：方向向量

优势：
- 更接近全量微调的效果
- 训练更稳定
- 可以和 LoRA 无缝切换
```

---

## 使用 PEFT 库

### 基础用法

```python
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import LoraConfig, get_peft_model, TaskType

# 加载模型
model_name = "Qwen/Qwen2.5-0.5B"
model = AutoModelForCausalLM.from_pretrained(model_name)
tokenizer = AutoTokenizer.from_pretrained(model_name)

# 配置 LoRA
lora_config = LoraConfig(
    task_type=TaskType.CAUSAL_LM,
    r=8,                      # 秩
    lora_alpha=16,            # 缩放因子
    lora_dropout=0.1,         # Dropout
    target_modules=["q_proj", "v_proj"],  # 目标模块
    bias="none"               # 是否训练 bias
)

# 应用 LoRA
model = get_peft_model(model, lora_config)

# 查看可训练参数
model.print_trainable_parameters()
# 输出：trainable params: 262,144 || all params: 500,262,144 || trainable%: 0.0524
```

### QLoRA 配置

```python
from transformers import BitsAndBytesConfig
import torch

# 量化配置
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,                    # 4-bit 量化
    bnb_4bit_quant_type="nf4",            # NF4 量化
    bnb_4bit_compute_dtype=torch.float16, # 计算类型
    bnb_4bit_use_double_quant=True        # 双重量化
)

# 加载量化模型
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    quantization_config=bnb_config,
    device_map="auto"
)

# 准备模型用于 k-bit 训练
from peft import prepare_model_for_kbit_training
model = prepare_model_for_kbit_training(model)

# 应用 LoRA
model = get_peft_model(model, lora_config)
```

---

## 数据集准备

### Alpaca 格式

```python
# Alpaca 格式：instruction, input, output
alpaca_data = [
    {
        "instruction": "将以下句子翻译成英文",
        "input": "今天天气真好",
        "output": "The weather is really nice today."
    },
    {
        "instruction": "总结以下文章的主要观点",
        "input": "...(文章内容)...",
        "output": "...(摘要)..."
    }
]

# 转换为训练格式
def format_alpaca(example):
    if example["input"]:
        prompt = f"""### Instruction:
{example["instruction"]}

### Input:
{example["input"]}

### Response:
{example["output"]}"""
    else:
        prompt = f"""### Instruction:
{example["instruction"]}

### Response:
{example["output"]}"""
    return {"text": prompt}
```

### ShareGPT 格式

```python
# ShareGPT 格式：多轮对话
sharegpt_data = [
    {
        "conversations": [
            {"role": "user", "content": "什么是机器学习？"},
            {"role": "assistant", "content": "机器学习是..."},
            {"role": "user", "content": "能举个例子吗？"},
            {"role": "assistant", "content": "比如垃圾邮件分类..."}
        ]
    }
]

# 转换为训练格式
def format_sharegpt(example, tokenizer):
    messages = example["conversations"]
    # 使用 tokenizer 的 chat template
    text = tokenizer.apply_chat_template(messages, tokenize=False)
    return {"text": text}
```

### 数据集加载

```python
from datasets import load_dataset, Dataset

# 从 Hugging Face 加载
dataset = load_dataset("tatsu-lab/alpaca")

# 从本地 JSON 加载
dataset = load_dataset("json", data_files="data.json")

# 从列表创建
dataset = Dataset.from_list(alpaca_data)

# 预处理
def preprocess(examples):
    return tokenizer(
        examples["text"],
        truncation=True,
        max_length=512,
        padding="max_length"
    )

tokenized_dataset = dataset.map(preprocess, batched=True)
```

---

## 训练流程

```python
from transformers import TrainingArguments, Trainer, DataCollatorForLanguageModeling

# 训练参数
training_args = TrainingArguments(
    output_dir="./output",
    num_train_epochs=3,
    per_device_train_batch_size=4,
    gradient_accumulation_steps=4,
    learning_rate=2e-4,
    warmup_ratio=0.03,
    logging_steps=10,
    save_steps=100,
    evaluation_strategy="steps",
    eval_steps=100,
    fp16=True,
    optim="adamw_torch",
    lr_scheduler_type="cosine",
)

# 数据整理器
data_collator = DataCollatorForLanguageModeling(
    tokenizer=tokenizer,
    mlm=False  # 因果语言模型
)

# 创建 Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset["train"],
    eval_dataset=tokenized_dataset["validation"],
    data_collator=data_collator,
)

# 训练
trainer.train()

# 保存
model.save_pretrained("./lora_model")
tokenizer.save_pretrained("./lora_model")
```

---

## 模型合并与推理

```python
from peft import PeftModel

# 方法 1：分开加载
base_model = AutoModelForCausalLM.from_pretrained(model_name)
model = PeftModel.from_pretrained(base_model, "./lora_model")

# 方法 2：合并权重（推荐用于部署）
model = model.merge_and_unload()
model.save_pretrained("./merged_model")

# 推理
def generate(prompt, max_length=100):
    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
    outputs = model.generate(
        **inputs,
        max_new_tokens=max_length,
        temperature=0.7,
        do_sample=True
    )
    return tokenizer.decode(outputs[0], skip_special_tokens=True)

result = generate("什么是人工智能？")
print(result)
```

---

## 练习题

### 练习 1：理解 LoRA

```python
# 任务：计算以下情况的参数量减少比例
# 原始矩阵：4096 × 4096
# LoRA rank：8, 16, 32, 64
```

### 练习 2：数据准备

```python
# 任务：将以下对话数据转换为 Alpaca 格式
data = [
    {"user": "北京的天气怎么样？", "assistant": "北京今天晴，温度 22°C"},
    {"user": "推荐一本书", "assistant": "推荐《深度学习》"}
]
```

### 练习 3：配置 LoRA

```python
# 任务：为不同场景选择合适的 LoRA 配置
# 场景 1：有限显存（8GB）
# 场景 2：需要高质量结果
# 场景 3：快速实验
```

---

## 小结

```
本节要点：
1. PEFT：参数高效微调，只训练少量参数
2. LoRA：低秩分解，最流行的 PEFT 方法
3. QLoRA：量化 + LoRA，显存友好
4. DoRA：方向-幅度分解，效果更好
5. 数据格式：Alpaca、ShareGPT
```

---

## ➡️ 下一步

继续 [08-微调实战.md](./08-微调实战.md)

