# ğŸ”§ 14 - é¡¹ç›®ï¼šLoRA å¾®è°ƒå®æˆ˜

> ä½¿ç”¨ LoRA å¾®è°ƒå¼€æºæ¨¡å‹ï¼Œä½¿å…¶æ“…é•¿ç‰¹å®šé¢†åŸŸ

---

## é¡¹ç›®ç›®æ ‡

```
ç›®æ ‡ï¼šå¾®è°ƒ Qwen æ¨¡å‹ï¼Œä½¿å…¶æˆä¸ºä¸€ä¸ªä¸“ä¸šçš„ä»£ç åŠ©æ‰‹

å…·ä½“è¦æ±‚ï¼š
1. å‡†å¤‡ä»£ç é—®ç­”æ•°æ®é›†
2. ä½¿ç”¨ QLoRA è¿›è¡Œå¾®è°ƒ
3. è¯„ä¼°å¾®è°ƒæ•ˆæœ
4. éƒ¨ç½²å¾®è°ƒåçš„æ¨¡å‹
```

---

## å®Œæ•´æµç¨‹

### 1. ç¯å¢ƒå‡†å¤‡

```bash
# åˆ›å»ºç¯å¢ƒ
conda create -n lora-finetune python=3.11 -y
conda activate lora-finetune

# å®‰è£…ä¾èµ–
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
pip install transformers datasets peft accelerate bitsandbytes
pip install trl wandb
```

### 2. æ•°æ®å‡†å¤‡

```python
"""prepare_data.py - å‡†å¤‡è®­ç»ƒæ•°æ®"""
import json
from datasets import Dataset, load_dataset

def create_code_qa_dataset():
    """åˆ›å»ºä»£ç é—®ç­”æ•°æ®é›†"""

    # æ–¹æ³• 1ï¼šä»ç°æœ‰æ•°æ®é›†åŠ è½½
    # dataset = load_dataset("sahil2801/CodeAlpaca-20k")

    # æ–¹æ³• 2ï¼šè‡ªå®šä¹‰æ•°æ®
    data = [
        {
            "instruction": "å†™ä¸€ä¸ª Python å‡½æ•°è®¡ç®—æ–æ³¢é‚£å¥‘æ•°åˆ—",
            "input": "",
            "output": """def fibonacci(n):
    if n <= 0:
        return []
    elif n == 1:
        return [0]
    elif n == 2:
        return [0, 1]

    fib = [0, 1]
    for i in range(2, n):
        fib.append(fib[i-1] + fib[i-2])
    return fib

# ä½¿ç”¨ç¤ºä¾‹
print(fibonacci(10))  # [0, 1, 1, 2, 3, 5, 8, 13, 21, 34]"""
        },
        {
            "instruction": "è§£é‡Š Python ä¸­çš„è£…é¥°å™¨",
            "input": "",
            "output": """è£…é¥°å™¨æ˜¯ä¸€ç§ç”¨äºä¿®æ”¹å‡½æ•°æˆ–ç±»è¡Œä¸ºçš„è®¾è®¡æ¨¡å¼ã€‚

åŸºæœ¬æ¦‚å¿µï¼š
1. è£…é¥°å™¨æ˜¯ä¸€ä¸ªæ¥å—å‡½æ•°ä½œä¸ºå‚æ•°çš„å‡½æ•°
2. è¿”å›ä¸€ä¸ªæ–°çš„å‡½æ•°ï¼Œé€šå¸¸åŒ…å«åŸå‡½æ•°çš„åŠŸèƒ½åŠ ä¸Šé¢å¤–çš„è¡Œä¸º
3. ä½¿ç”¨ @ è¯­æ³•ç³–æ¥åº”ç”¨è£…é¥°å™¨

ç¤ºä¾‹ï¼š
```python
def timer(func):
    import time
    def wrapper(*args, **kwargs):
        start = time.time()
        result = func(*args, **kwargs)
        print(f"æ‰§è¡Œæ—¶é—´: {time.time() - start:.2f}ç§’")
        return result
    return wrapper

@timer
def slow_function():
    time.sleep(1)
    return "å®Œæˆ"

slow_function()  # è¾“å‡º: æ‰§è¡Œæ—¶é—´: 1.00ç§’
```

å¸¸è§ç”¨é€”ï¼š
- æ—¥å¿—è®°å½•
- æ€§èƒ½è®¡æ—¶
- è®¿é—®æ§åˆ¶
- ç¼“å­˜"""
        },
        {
            "instruction": "å®ç°ä¸€ä¸ª Python å•ä¾‹æ¨¡å¼",
            "input": "",
            "output": """å•ä¾‹æ¨¡å¼ç¡®ä¿ä¸€ä¸ªç±»åªæœ‰ä¸€ä¸ªå®ä¾‹ã€‚

æ–¹æ³• 1ï¼šä½¿ç”¨è£…é¥°å™¨
```python
def singleton(cls):
    instances = {}
    def get_instance(*args, **kwargs):
        if cls not in instances:
            instances[cls] = cls(*args, **kwargs)
        return instances[cls]
    return get_instance

@singleton
class Database:
    def __init__(self, url):
        self.url = url

db1 = Database("localhost")
db2 = Database("remote")
print(db1 is db2)  # True
```

æ–¹æ³• 2ï¼šä½¿ç”¨å…ƒç±»
```python
class SingletonMeta(type):
    _instances = {}

    def __call__(cls, *args, **kwargs):
        if cls not in cls._instances:
            cls._instances[cls] = super().__call__(*args, **kwargs)
        return cls._instances[cls]

class Logger(metaclass=SingletonMeta):
    def __init__(self):
        self.logs = []

log1 = Logger()
log2 = Logger()
print(log1 is log2)  # True
```"""
        },
        # ... æ›´å¤šæ•°æ®
    ]

    # æ‰©å±•æ•°æ®é›†ï¼ˆå®é™…åº”ç”¨ä¸­éœ€è¦æ›´å¤šæ•°æ®ï¼‰
    extended_data = data * 100  # ç®€å•å¤åˆ¶ï¼Œå®é™…éœ€è¦å¤šæ ·åŒ–æ•°æ®

    return Dataset.from_list(extended_data)


def format_instruction(example):
    """æ ¼å¼åŒ–ä¸º Qwen å¯¹è¯æ ¼å¼"""
    if example["input"]:
        prompt = f"""<|im_start|>user
{example["instruction"]}

{example["input"]}<|im_end|>
<|im_start|>assistant
{example["output"]}<|im_end|>"""
    else:
        prompt = f"""<|im_start|>user
{example["instruction"]}<|im_end|>
<|im_start|>assistant
{example["output"]}<|im_end|>"""

    return {"text": prompt}


if __name__ == "__main__":
    # åˆ›å»ºæ•°æ®é›†
    dataset = create_code_qa_dataset()

    # æ ¼å¼åŒ–
    dataset = dataset.map(format_instruction)

    # åˆ†å‰²
    dataset = dataset.train_test_split(test_size=0.1, seed=42)

    # ä¿å­˜
    dataset.save_to_disk("./code_qa_dataset")

    print(f"è®­ç»ƒé›†: {len(dataset['train'])} æ¡")
    print(f"æµ‹è¯•é›†: {len(dataset['test'])} æ¡")
    print(f"\nç¤ºä¾‹:\n{dataset['train'][0]['text'][:500]}...")
```

### 3. è®­ç»ƒè„šæœ¬

```python
"""train.py - LoRA å¾®è°ƒè®­ç»ƒ"""
import torch
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    BitsAndBytesConfig,
    TrainingArguments
)
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training
from trl import SFTTrainer
from datasets import load_from_disk
import wandb

# ========== é…ç½® ==========
MODEL_NAME = "Qwen/Qwen2.5-1.5B-Instruct"
OUTPUT_DIR = "./qwen-code-assistant"
MAX_SEQ_LENGTH = 2048

# LoRA é…ç½®
LORA_R = 16
LORA_ALPHA = 32
LORA_DROPOUT = 0.05
TARGET_MODULES = ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]

# è®­ç»ƒé…ç½®
BATCH_SIZE = 4
GRADIENT_ACCUMULATION = 4
LEARNING_RATE = 2e-4
NUM_EPOCHS = 3
WARMUP_RATIO = 0.1

# ========== åŠ è½½æ¨¡å‹ ==========
print("åŠ è½½æ¨¡å‹...")

# 4-bit é‡åŒ–é…ç½®
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.float16,
    bnb_4bit_use_double_quant=True
)

# åŠ è½½æ¨¡å‹
model = AutoModelForCausalLM.from_pretrained(
    MODEL_NAME,
    quantization_config=bnb_config,
    device_map="auto",
    trust_remote_code=True
)

# åŠ è½½ tokenizer
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)
tokenizer.pad_token = tokenizer.eos_token
tokenizer.padding_side = "right"

# å‡†å¤‡æ¨¡å‹ç”¨äº k-bit è®­ç»ƒ
model = prepare_model_for_kbit_training(model)

# ========== é…ç½® LoRA ==========
print("é…ç½® LoRA...")

lora_config = LoraConfig(
    r=LORA_R,
    lora_alpha=LORA_ALPHA,
    lora_dropout=LORA_DROPOUT,
    target_modules=TARGET_MODULES,
    bias="none",
    task_type="CAUSAL_LM"
)

model = get_peft_model(model, lora_config)
model.print_trainable_parameters()

# ========== åŠ è½½æ•°æ® ==========
print("åŠ è½½æ•°æ®...")
dataset = load_from_disk("./code_qa_dataset")

# ========== è®­ç»ƒå‚æ•° ==========
training_args = TrainingArguments(
    output_dir=OUTPUT_DIR,
    num_train_epochs=NUM_EPOCHS,
    per_device_train_batch_size=BATCH_SIZE,
    gradient_accumulation_steps=GRADIENT_ACCUMULATION,
    learning_rate=LEARNING_RATE,
    warmup_ratio=WARMUP_RATIO,
    lr_scheduler_type="cosine",
    logging_steps=10,
    save_steps=100,
    save_total_limit=3,
    evaluation_strategy="steps",
    eval_steps=100,
    fp16=True,
    optim="paged_adamw_8bit",
    report_to="wandb",  # æˆ– "none"
    run_name="qwen-code-assistant",
)

# ========== åˆ›å»º Trainer ==========
trainer = SFTTrainer(
    model=model,
    args=training_args,
    train_dataset=dataset["train"],
    eval_dataset=dataset["test"],
    tokenizer=tokenizer,
    dataset_text_field="text",
    max_seq_length=MAX_SEQ_LENGTH,
    packing=False,
)

# ========== è®­ç»ƒ ==========
print("å¼€å§‹è®­ç»ƒ...")
trainer.train()

# ========== ä¿å­˜æ¨¡å‹ ==========
print("ä¿å­˜æ¨¡å‹...")
trainer.save_model(OUTPUT_DIR)
tokenizer.save_pretrained(OUTPUT_DIR)

print(f"è®­ç»ƒå®Œæˆï¼æ¨¡å‹ä¿å­˜åˆ° {OUTPUT_DIR}")
```

### 4. åˆå¹¶å’Œå¯¼å‡º

```python
"""merge_model.py - åˆå¹¶ LoRA æƒé‡"""
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel
import torch

BASE_MODEL = "Qwen/Qwen2.5-1.5B-Instruct"
LORA_PATH = "./qwen-code-assistant"
OUTPUT_PATH = "./qwen-code-assistant-merged"

print("åŠ è½½åŸºç¡€æ¨¡å‹...")
base_model = AutoModelForCausalLM.from_pretrained(
    BASE_MODEL,
    torch_dtype=torch.float16,
    device_map="auto",
    trust_remote_code=True
)

print("åŠ è½½ LoRA æƒé‡...")
model = PeftModel.from_pretrained(base_model, LORA_PATH)

print("åˆå¹¶æƒé‡...")
model = model.merge_and_unload()

print("ä¿å­˜åˆå¹¶åçš„æ¨¡å‹...")
model.save_pretrained(OUTPUT_PATH)

tokenizer = AutoTokenizer.from_pretrained(LORA_PATH)
tokenizer.save_pretrained(OUTPUT_PATH)

print(f"å®Œæˆï¼æ¨¡å‹ä¿å­˜åˆ° {OUTPUT_PATH}")
```

### 5. æ¨ç†æµ‹è¯•

```python
"""inference.py - æ¨ç†æµ‹è¯•"""
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

MODEL_PATH = "./qwen-code-assistant-merged"

# åŠ è½½æ¨¡å‹
print("åŠ è½½æ¨¡å‹...")
model = AutoModelForCausalLM.from_pretrained(
    MODEL_PATH,
    torch_dtype=torch.float16,
    device_map="auto"
)
tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)

def generate_response(instruction: str, max_length: int = 512):
    """ç”Ÿæˆå›ç­”"""
    prompt = f"""<|im_start|>user
{instruction}<|im_end|>
<|im_start|>assistant
"""

    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)

    outputs = model.generate(
        **inputs,
        max_new_tokens=max_length,
        temperature=0.7,
        top_p=0.9,
        do_sample=True,
        pad_token_id=tokenizer.eos_token_id
    )

    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    # æå–åŠ©æ‰‹å›ç­”
    response = response.split("<|im_start|>assistant")[-1].strip()

    return response


# æµ‹è¯•
test_questions = [
    "å†™ä¸€ä¸ª Python å¿«é€Ÿæ’åºå‡½æ•°",
    "å¦‚ä½•åœ¨ Python ä¸­å®ç°å¤šçº¿ç¨‹ï¼Ÿ",
    "è§£é‡Š Python çš„ GIL æ˜¯ä»€ä¹ˆ",
    "å†™ä¸€ä¸ªç®€å•çš„ REST API ä½¿ç”¨ FastAPI"
]

print("\n" + "="*60)
print("æ¨ç†æµ‹è¯•")
print("="*60)

for q in test_questions:
    print(f"\né—®é¢˜: {q}")
    print("-" * 40)
    response = generate_response(q)
    print(f"å›ç­”:\n{response}")
    print("="*60)
```

### 6. è¯„ä¼°è„šæœ¬

```python
"""evaluate.py - è¯„ä¼°å¾®è°ƒæ•ˆæœ"""
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch
import json

def evaluate_model(model_path: str, test_data: list):
    """è¯„ä¼°æ¨¡å‹"""
    model = AutoModelForCausalLM.from_pretrained(
        model_path,
        torch_dtype=torch.float16,
        device_map="auto"
    )
    tokenizer = AutoTokenizer.from_pretrained(model_path)

    results = []

    for item in test_data:
        prompt = f"""<|im_start|>user
{item['instruction']}<|im_end|>
<|im_start|>assistant
"""

        inputs = tokenizer(prompt, return_tensors="pt").to(model.device)

        outputs = model.generate(
            **inputs,
            max_new_tokens=256,
            temperature=0.1,  # ä½æ¸©åº¦ç”¨äºè¯„ä¼°
            do_sample=False
        )

        response = tokenizer.decode(outputs[0], skip_special_tokens=True)
        response = response.split("<|im_start|>assistant")[-1].strip()

        results.append({
            "instruction": item["instruction"],
            "expected": item["output"][:200],
            "generated": response[:200]
        })

    return results


# ç®€å•çš„è¯„ä¼°æŒ‡æ ‡
def calculate_metrics(results):
    """è®¡ç®—è¯„ä¼°æŒ‡æ ‡"""
    # è¿™é‡Œç®€åŒ–å¤„ç†ï¼Œå®é™…å¯ä»¥ä½¿ç”¨æ›´å¤æ‚çš„æŒ‡æ ‡
    # å¦‚ BLEUã€ROUGEã€ä»£ç æ‰§è¡Œæ­£ç¡®ç‡ç­‰

    total = len(results)

    # è®¡ç®—å“åº”é•¿åº¦
    avg_length = sum(len(r["generated"]) for r in results) / total

    # è®¡ç®—åŒ…å«ä»£ç çš„æ¯”ä¾‹
    code_ratio = sum(1 for r in results if "```" in r["generated"] or "def " in r["generated"]) / total

    return {
        "total_samples": total,
        "avg_response_length": avg_length,
        "code_ratio": code_ratio
    }


if __name__ == "__main__":
    # æµ‹è¯•æ•°æ®
    test_data = [
        {"instruction": "å†™ä¸€ä¸ªè®¡ç®—é˜¶ä¹˜çš„å‡½æ•°", "output": "def factorial(n)..."},
        {"instruction": "å¦‚ä½•è¯»å– JSON æ–‡ä»¶", "output": "import json..."},
        {"instruction": "å®ç°äºŒåˆ†æŸ¥æ‰¾", "output": "def binary_search..."},
    ]

    # è¯„ä¼°åŸå§‹æ¨¡å‹
    print("è¯„ä¼°åŸå§‹æ¨¡å‹...")
    base_results = evaluate_model("Qwen/Qwen2.5-1.5B-Instruct", test_data)
    base_metrics = calculate_metrics(base_results)

    # è¯„ä¼°å¾®è°ƒæ¨¡å‹
    print("è¯„ä¼°å¾®è°ƒæ¨¡å‹...")
    finetuned_results = evaluate_model("./qwen-code-assistant-merged", test_data)
    finetuned_metrics = calculate_metrics(finetuned_results)

    # å¯¹æ¯”
    print("\nè¯„ä¼°ç»“æœå¯¹æ¯”:")
    print(f"{'æŒ‡æ ‡':<20} {'åŸå§‹æ¨¡å‹':<15} {'å¾®è°ƒæ¨¡å‹':<15}")
    print("-" * 50)
    for key in base_metrics:
        print(f"{key:<20} {base_metrics[key]:<15.2f} {finetuned_metrics[key]:<15.2f}")
```

---

## ä½¿ç”¨ LLaMA-Factory

```yaml
# llama_factory_config.yaml
### Model
model_name_or_path: Qwen/Qwen2.5-1.5B-Instruct

### Method
stage: sft
do_train: true
finetuning_type: lora
lora_target: all
lora_rank: 16
lora_alpha: 32

### Dataset
dataset: code_alpaca  # æˆ–è‡ªå®šä¹‰æ•°æ®é›†
template: qwen
cutoff_len: 2048

### Output
output_dir: saves/qwen-code-lora
logging_steps: 10
save_steps: 100

### Train
per_device_train_batch_size: 4
gradient_accumulation_steps: 4
learning_rate: 2.0e-4
num_train_epochs: 3
lr_scheduler_type: cosine
warmup_ratio: 0.1
fp16: true

### Eval
val_size: 0.1
per_device_eval_batch_size: 4
eval_strategy: steps
eval_steps: 100
```

```bash
# è®­ç»ƒ
llamafactory-cli train llama_factory_config.yaml

# æ¨ç†æµ‹è¯•
llamafactory-cli chat \
    --model_name_or_path Qwen/Qwen2.5-1.5B-Instruct \
    --adapter_name_or_path saves/qwen-code-lora \
    --template qwen

# å¯¼å‡º
llamafactory-cli export \
    --model_name_or_path Qwen/Qwen2.5-1.5B-Instruct \
    --adapter_name_or_path saves/qwen-code-lora \
    --template qwen \
    --export_dir qwen-code-merged
```

---

## æ³¨æ„äº‹é¡¹

```
1. æ•°æ®è´¨é‡
   - æ•°æ®è¦å¤šæ ·åŒ–ï¼Œé¿å…è¿‡æ‹Ÿåˆ
   - è‡³å°‘ 1000-10000 æ¡é«˜è´¨é‡æ•°æ®
   - æ ¼å¼è¦ç»Ÿä¸€

2. è¶…å‚æ•°é€‰æ‹©
   - LoRA rank: 8-64ï¼Œè¶Šå¤§å®¹é‡è¶Šå¤§
   - learning rate: 1e-5 ~ 5e-4
   - batch size * gradient_accumulation >= 16

3. ç›‘æ§è®­ç»ƒ
   - ä½¿ç”¨ wandb æˆ– tensorboard
   - å…³æ³¨ loss æ›²çº¿
   - å®šæœŸè¯„ä¼°éªŒè¯é›†

4. å¸¸è§é—®é¢˜
   - OOM: å‡å°‘ batch_sizeï¼Œä½¿ç”¨ 4-bit é‡åŒ–
   - è¿‡æ‹Ÿåˆ: å‡å°‘ epochsï¼Œå¢åŠ  dropout
   - æ•ˆæœå·®: å¢åŠ æ•°æ®ï¼Œæé«˜ rank
```

---

## â¡ï¸ ä¸‹ä¸€æ­¥

ç»§ç»­ [15-è‡ªæµ‹æ¸…å•.md](./15-è‡ªæµ‹æ¸…å•.md)

