# ✅ 14 - 阶段5 自测清单

> 检验 LLM 部署与工程化能力

---

## 概念理解（10 题）

### 部署基础

```
Q1: 本地部署和云端 API 各有什么优缺点？什么场景选择哪种？

答案：
本地部署：
- 优点：数据隐私、低延迟、无调用限制、长期成本低
- 缺点：硬件投入、运维复杂

云端 API：
- 优点：快速上线、无运维、自动扩缩容
- 缺点：隐私风险、按量付费、网络依赖

选择：
- 敏感数据、高频调用 → 本地
- 快速验证、小规模 → 云端
```

```
Q2: FP16、INT8、INT4 量化各有什么特点？

答案：
FP16：
- 16 位浮点
- 精度损失很小
- 显存减半（相比 FP32）

INT8：
- 8 位整数
- 精度损失小（通常 <1%）
- 显存减少 4 倍

INT4：
- 4 位整数
- 有一定精度损失
- 显存减少 8 倍
- 可在消费级 GPU 运行大模型
```

```
Q3: GGUF、GPTQ、AWQ 格式有什么区别？怎么选？

答案：
GGUF：
- llama.cpp 格式
- 支持 CPU + GPU 混合
- 适合本地/消费级 GPU

GPTQ：
- 基于校准数据
- GPU 专用
- 精度保持好

AWQ：
- 感知激活值
- GPU 专用
- 速度快、精度好

选择：
- CPU/本地 → GGUF
- GPU 生产 → AWQ/GPTQ
```

### 推理优化

```
Q4: Continuous Batching 是什么？为什么能提高吞吐？

答案：
传统 Static Batching：
- 等待 batch 填满
- 所有请求同时开始同时结束
- 短请求被长请求拖累

Continuous Batching：
- 请求随到随处理
- 完成的立即返回
- 新请求可以加入进行中的 batch

效果：吞吐提升 3-5 倍
```

```
Q5: PagedAttention 解决了什么问题？

答案：
问题：
- KV Cache 显存碎片化严重
- 预分配浪费显存
- 限制并发数量

PagedAttention 解决方案：
- 将 KV Cache 分成固定大小的块
- 按需分配，不预先占用
- 使用块表记录映射

效果：
- 显存利用率接近 100%
- 支持更大并发
- vLLM 核心技术
```

### 服务化

```
Q6: 流式输出（SSE）相比普通 HTTP 有什么优势？

答案：
普通 HTTP：
- 等待完整响应
- 用户等待时间长
- 服务器内存占用高

SSE（Server-Sent Events）：
- 逐步返回内容
- 用户体验好（打字机效果）
- 降低首 token 延迟
- 服务器资源利用更好
```

```
Q7: 如何实现 API 限流？令牌桶算法的原理？

答案：
令牌桶算法：
1. 桶有固定容量（如 10）
2. 以固定速率添加令牌（如 1/秒）
3. 请求消耗令牌
4. 桶空则拒绝请求

优势：
- 允许突发流量（桶内积累的令牌）
- 长期控制平均速率

实现方式：
- 内存实现（单机）
- Redis 实现（分布式）
- 使用 SlowAPI 等库
```

### Docker

```
Q8: Docker 部署 GPU 应用需要什么配置？

答案：
1. 安装 NVIDIA Container Toolkit
2. Docker 运行时添加 --gpus 参数
3. 使用 CUDA 基础镜像

命令：
docker run --gpus all my-image

Docker Compose：
deploy:
  resources:
    reservations:
      devices:
        - driver: nvidia
          count: 1
          capabilities: [gpu]
```

### 监控与安全

```
Q9: RAG 系统的 Ragas 评估指标有哪些？

答案：
1. Faithfulness（忠实度）：答案是否忠实于上下文
2. Answer Relevancy（答案相关性）：答案是否切题
3. Context Precision（上下文精确度）：检索是否精确
4. Context Recall（上下文召回）：检索是否完整
5. Answer Correctness（答案正确性）：答案是否正确
```

```
Q10: 如何防护 Prompt 注入攻击？

答案：
1. 输入检测：关键词过滤、模式匹配
2. 系统提示隔离：使用分隔符、强化角色
3. LLM 检测：用另一个 LLM 检查输入
4. 输出过滤：敏感信息遮盖
5. 访问控制：限制高危操作
6. 日志监控：异常检测
```

---

## 实操检查

### 推理引擎

```
□ 能够使用 Ollama 下载和运行模型
□ 能够通过 REST API 调用 Ollama
□ 能够使用 OpenAI SDK 调用 Ollama
□ 了解 vLLM 的基本使用
```

### 服务化

```
□ 能够用 FastAPI 封装 LLM 接口
□ 能够实现流式输出（SSE）
□ 能够实现 API Key 认证
□ 能够实现请求限流
```

### 容器化

```
□ 能够编写 Dockerfile
□ 能够使用 Docker Compose 编排多服务
□ 了解 GPU 容器的配置方法
□ 了解基本的 CI/CD 流程
```

### 监控安全

```
□ 能够记录结构化日志
□ 了解 Prometheus 指标的使用
□ 能够使用 Ragas 评估 RAG
□ 了解 Prompt 注入防护方法
```

---

## 动手任务

### 任务1：部署 Ollama 并创建 API

```bash
# 1. 安装 Ollama
# 2. 下载模型 qwen2.5:7b
# 3. 用 Python 调用 API 获取回答
# 4. 用 OpenAI SDK 调用
```

### 任务2：编写 Dockerfile

```dockerfile
# 为一个 FastAPI + LLM 应用编写 Dockerfile
# 要求：
# 1. 多阶段构建
# 2. 非 root 用户
# 3. 健康检查
```

### 任务3：实现限流中间件

```python
# 实现一个简单的限流中间件
# 要求：
# 1. 每个 IP 每分钟 10 次
# 2. 超过限制返回 429
# 3. 响应头包含剩余次数
```

---

## 达标标准

```
□ 理解本地 vs 云端部署的权衡
□ 了解模型量化的基本概念
□ 能使用 Ollama 或 vLLM 部署模型
□ 能用 FastAPI 封装 LLM 接口
□ 能实现流式输出
□ 能用 Docker 打包服务
□ 了解基本的监控和日志
□ 了解 RAG 评估方法
□ 了解 Prompt 注入防护
□ 完成部署项目
```

---

## 🎉 恭喜完成阶段5！

你已经掌握了 LLM 部署与工程化的核心技术，接下来进入：

**阶段6：职业发展与进阶**
- 技术专精方向选择
- 作品集设计
- 面试准备

继续加油！🚀

