# ⚡ 推理优化技术

> Continuous Batching、PagedAttention、Speculative Decoding

---

## 推理性能指标

```
关键指标：
- 吞吐量（Throughput）：单位时间处理的请求数
- 延迟（Latency）：单个请求的响应时间
  - TTFT（Time To First Token）：首 token 延迟
  - TPOT（Time Per Output Token）：每 token 生成时间
- 显存占用（Memory）：运行时显存使用

优化目标：
- 高吞吐、低延迟
- 显存使用可控
- 稳定的 P99 延迟
```

---

## Continuous Batching（连续批处理）

### 原理

```
传统 Static Batching：
- 等待 batch 填满或超时才处理
- 所有请求同时开始、同时结束
- 短请求被长请求拖累

┌────────────────────────────┐
│ Request 1: ████████████    │
│ Request 2: ████            │ ← 等待 R1 完成
│ Request 3: ██████████      │ ← 等待 R1 完成
└────────────────────────────┘

Continuous Batching：
- 请求随到随处理
- 完成的请求立即返回
- 新请求可以加入正在处理的 batch

┌────────────────────────────┐
│ Request 1: ████████████    │
│ Request 2: ████→ done      │
│ Request 4:     ████████    │ ← R2 完成后加入
│ Request 3: ██████████      │
└────────────────────────────┘
```

### 实现示意

```python
class ContinuousBatcher:
    """连续批处理示意"""

    def __init__(self, max_batch_size: int = 32):
        self.max_batch_size = max_batch_size
        self.active_requests = []
        self.pending_requests = []

    def add_request(self, request):
        """添加新请求"""
        if len(self.active_requests) < self.max_batch_size:
            self.active_requests.append(request)
        else:
            self.pending_requests.append(request)

    def step(self):
        """执行一步推理"""
        # 1. 对所有活跃请求生成下一个 token
        outputs = self.model.forward_batch(self.active_requests)

        # 2. 检查完成的请求
        completed = []
        for req, output in zip(self.active_requests, outputs):
            req.append_token(output)
            if req.is_done():
                completed.append(req)

        # 3. 移除完成的请求，加入等待的请求
        for req in completed:
            self.active_requests.remove(req)
            req.finish()

        while (len(self.active_requests) < self.max_batch_size
               and self.pending_requests):
            self.active_requests.append(self.pending_requests.pop(0))
```

---

## PagedAttention（分页注意力）

### 问题背景

```
KV Cache 的挑战：
- 每个请求需要存储 Key-Value 缓存
- 显存碎片化严重
- 不同请求的 KV Cache 长度不同
- 预分配会浪费显存

传统方式：
┌──────────────────────────────────────┐
│ Request 1 KV: ████████░░░░░░░░       │ ← 预分配 max_len
│ Request 2 KV: ██░░░░░░░░░░░░░░       │ ← 大量浪费
│ Request 3 KV: ████████████░░░░       │
└──────────────────────────────────────┘
```

### PagedAttention 解决方案

```
核心思想：像操作系统的虚拟内存一样管理 KV Cache

1. 将 KV Cache 分成固定大小的块（Block）
2. 使用块表（Block Table）记录映射关系
3. 按需分配，不预先占用

PagedAttention：
┌──────────────────────────────────────┐
│ Block Pool: [B0][B1][B2][B3][B4]...  │
│                                      │
│ Request 1: [B0]->[B2]->[B4]          │
│ Request 2: [B1]                      │
│ Request 3: [B3]->[B5]->[B6]          │
└──────────────────────────────────────┘

优势：
- 显存利用率接近 100%
- 支持更大的并发
- 减少显存碎片
```

### vLLM 中的实现

```python
# vLLM 自动使用 PagedAttention
from vllm import LLM, SamplingParams

llm = LLM(
    model="Qwen/Qwen2.5-7B-Instruct",
    gpu_memory_utilization=0.9,  # 可以设置更高
    max_num_seqs=256,            # 支持更多并发
)

# 内部自动管理 KV Cache 的分页
```

---

## Speculative Decoding（推测解码）

### 原理

```
观察：大模型生成很慢，但验证很快

思路：
1. 用小模型快速生成 K 个候选 token
2. 用大模型一次性验证这 K 个 token
3. 如果都正确，相当于一次生成了 K 个 token
4. 如果有错误，从错误位置重新生成

┌─────────────────────────────────────────┐
│ Draft Model (小): 快速生成 "The cat sat" │
│ Target Model (大): 验证 ✓✓✗             │
│ 结果: 接受 "The cat"，从 "sat" 重新生成  │
└─────────────────────────────────────────┘

加速效果：
- 取决于小模型的准确率
- 通常 2-3 倍加速
- 不影响输出质量
```

### 代码示意

```python
def speculative_decode(
    draft_model,
    target_model,
    prompt,
    k: int = 5,  # 每次推测的 token 数
    max_tokens: int = 100
):
    """推测解码示意"""
    tokens = prompt_to_tokens(prompt)

    while len(tokens) < max_tokens:
        # 1. Draft model 生成 k 个候选 token
        draft_tokens = []
        draft_probs = []
        temp_tokens = tokens.copy()

        for _ in range(k):
            logits = draft_model(temp_tokens)
            next_token = sample(logits)
            draft_tokens.append(next_token)
            draft_probs.append(get_prob(logits, next_token))
            temp_tokens.append(next_token)

        # 2. Target model 一次验证所有 token
        target_logits = target_model(tokens + draft_tokens)

        # 3. 逐个验证
        accepted = 0
        for i, (draft_tok, draft_p) in enumerate(zip(draft_tokens, draft_probs)):
            target_p = get_prob(target_logits[i], draft_tok)

            # 接受概率
            if random() < min(1, target_p / draft_p):
                tokens.append(draft_tok)
                accepted += 1
            else:
                # 拒绝，使用 target 分布采样
                tokens.append(sample(target_logits[i]))
                break

        if is_eos(tokens[-1]):
            break

    return tokens
```

### 实际使用

```python
# vLLM 支持 Speculative Decoding
from vllm import LLM, SamplingParams

llm = LLM(
    model="Qwen/Qwen2.5-7B-Instruct",
    speculative_model="Qwen/Qwen2.5-0.5B-Instruct",  # Draft model
    num_speculative_tokens=5,
)
```

---

## 其他优化技术

### Flash Attention

```
优化注意力计算的内存访问：
- 减少 HBM 访问
- 分块计算
- 融合 kernel

效果：
- 2-4 倍速度提升
- 显存占用降低
- 支持更长上下文
```

### KV Cache 量化

```python
# vLLM 支持 KV Cache 量化
llm = LLM(
    model="Qwen/Qwen2.5-7B-Instruct",
    kv_cache_dtype="fp8",  # 或 "auto"
)

# 效果：
# - 减少约 50% KV Cache 显存
# - 精度损失很小
```

### Tensor Parallel

```
将模型分布到多个 GPU：

单 GPU：
┌──────────────────┐
│   Full Model     │
│   (Too Large)    │
└──────────────────┘

Tensor Parallel（2 GPU）：
┌────────┐  ┌────────┐
│ GPU 0  │  │ GPU 1  │
│ Half   │  │ Half   │
└────────┘  └────────┘

# vLLM 使用
llm = LLM(
    model="Qwen/Qwen2.5-72B-Instruct",
    tensor_parallel_size=4,  # 4 个 GPU
)
```

---

## 优化效果总结

| 技术 | 吞吐提升 | 延迟优化 | 显存优化 |
|------|----------|----------|----------|
| Continuous Batching | 3-5x | - | - |
| PagedAttention | 2-4x | - | 90%+ 利用率 |
| Speculative Decoding | 2-3x | ✓ | - |
| Flash Attention | 2-4x | ✓ | ✓ |
| KV Cache 量化 | - | - | 50% |
| INT4 量化 | - | ✓ | 75% |

---

## ➡️ 下一步

继续 [04-Ollama入门.md](./04-Ollama入门.md)

