# 📦 02 - 模型量化

> 精度格式与量化方案详解

---

## 精度格式

### 常见数据类型

```
FP32（单精度浮点）：
- 32 位存储
- 范围大、精度高
- 训练默认格式
- 1B 参数 ≈ 4GB

FP16（半精度浮点）：
- 16 位存储
- 范围和精度适中
- 推理常用格式
- 1B 参数 ≈ 2GB

BF16（Brain Float 16）：
- 16 位存储
- 范围同 FP32，精度较低
- 训练和推理都适用
- 需要硬件支持（A100+）
- 1B 参数 ≈ 2GB

INT8（8位整数）：
- 8 位存储
- 精度损失小（通常 <1%）
- 显存减少 2-4 倍
- 1B 参数 ≈ 1GB

INT4（4位整数）：
- 4 位存储
- 有一定精度损失
- 显存减少 4-8 倍
- 1B 参数 ≈ 0.5GB
```

### 显存占用对比

```python
# 7B 模型显存估算

formats = {
    "FP32": 7 * 4,      # 28 GB
    "FP16": 7 * 2,      # 14 GB
    "BF16": 7 * 2,      # 14 GB
    "INT8": 7 * 1,      # 7 GB
    "INT4": 7 * 0.5,    # 3.5 GB
}

# 加上 KV Cache 和运行时开销，实际需要 1.2-1.5 倍
```

---

## 量化格式

### GGUF（GPT-Generated Unified Format）

```
特点：
- llama.cpp 的标准格式
- 支持 CPU + GPU 混合推理
- 支持多种量化级别
- 单文件包含所有信息

常见量化级别：
- Q2_K：极限压缩，质量损失大
- Q4_0：基础 4-bit
- Q4_K_M：4-bit 优化版（推荐）
- Q5_K_M：5-bit 优化版（平衡）
- Q6_K：6-bit，接近原始质量
- Q8_0：8-bit，几乎无损

使用场景：
- 本地 CPU 推理
- 消费级 GPU
- Ollama 默认格式
```

### GPTQ（Post-Training Quantization）

```
特点：
- 基于校准数据的量化
- 主要用于 GPU
- 4-bit 量化效果好
- 需要预处理时间

优势：
- 精度损失小
- 推理速度快
- 显存占用低

使用场景：
- 生产级 GPU 部署
- 需要高精度的场景
```

### AWQ（Activation-aware Weight Quantization）

```
特点：
- 感知激活值的量化
- 保护重要权重
- 比 GPTQ 更快的量化过程

优势：
- 精度保持更好
- 推理速度快
- 支持更多硬件

使用场景：
- 生产环境
- 质量要求高的场景
```

### EXL2（ExLlama2 格式）

```
特点：
- ExLlama2 引擎专用
- 支持混合精度
- 非常高效

优势：
- 极快的推理速度
- 灵活的量化配置
- 针对消费级 GPU 优化

使用场景：
- 本地高性能推理
- 消费级 GPU
```

---

## 格式选择指南

| 场景 | 推荐格式 | 原因 |
|------|----------|------|
| CPU 推理 | GGUF Q4_K_M | CPU 优化好 |
| 消费级 GPU | GGUF/EXL2 | 显存友好 |
| 数据中心 GPU | AWQ/GPTQ | 高吞吐 |
| Ollama | GGUF | 原生支持 |
| vLLM | AWQ/GPTQ | 官方支持 |
| 最高质量 | GGUF Q8_0 | 接近无损 |
| 最小体积 | GGUF Q4_0 | 压缩率高 |

---

## 量化实操

### 下载量化模型

```bash
# 从 Hugging Face 下载 GGUF
# 例如：Qwen2.5-7B-Instruct-GGUF

# 方法 1：使用 huggingface-cli
pip install huggingface_hub
huggingface-cli download Qwen/Qwen2.5-7B-Instruct-GGUF \
    qwen2.5-7b-instruct-q4_k_m.gguf \
    --local-dir ./models

# 方法 2：直接下载
wget https://huggingface.co/Qwen/Qwen2.5-7B-Instruct-GGUF/resolve/main/qwen2.5-7b-instruct-q4_k_m.gguf
```

### 使用 llama.cpp 量化

```bash
# 编译 llama.cpp
git clone https://github.com/ggerganov/llama.cpp
cd llama.cpp
make -j

# 转换 HF 模型到 GGUF
python convert_hf_to_gguf.py /path/to/hf_model --outtype f16

# 量化
./llama-quantize model-f16.gguf model-q4_k_m.gguf Q4_K_M
```

### 使用 AutoGPTQ 量化

```python
from transformers import AutoModelForCausalLM, AutoTokenizer
from auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig
import torch

model_name = "Qwen/Qwen2.5-7B-Instruct"
output_dir = "./qwen-7b-gptq"

# 加载模型
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.float16,
    device_map="auto"
)

# 准备校准数据
calibration_data = [
    "什么是人工智能？",
    "请解释机器学习的基本原理。",
    # ... 更多样本
]

# 量化配置
quantize_config = BaseQuantizeConfig(
    bits=4,
    group_size=128,
    desc_act=True
)

# 执行量化
model = AutoGPTQForCausalLM.from_pretrained(
    model_name,
    quantize_config=quantize_config
)

model.quantize(
    tokenizer=tokenizer,
    examples=calibration_data
)

# 保存
model.save_quantized(output_dir)
tokenizer.save_pretrained(output_dir)
```

### 使用 AutoAWQ 量化

```python
from awq import AutoAWQForCausalLM
from transformers import AutoTokenizer

model_name = "Qwen/Qwen2.5-7B-Instruct"
output_dir = "./qwen-7b-awq"

# 加载模型
model = AutoAWQForCausalLM.from_pretrained(model_name)
tokenizer = AutoTokenizer.from_pretrained(model_name)

# 量化配置
quant_config = {
    "zero_point": True,
    "q_group_size": 128,
    "w_bit": 4,
    "version": "GEMM"
}

# 执行量化
model.quantize(tokenizer, quant_config=quant_config)

# 保存
model.save_quantized(output_dir)
tokenizer.save_pretrained(output_dir)
```

---

## 量化效果对比

```
Qwen2.5-7B 在不同量化下的表现：

| 格式 | 大小 | 显存 | C-Eval | 速度 |
|------|------|------|--------|------|
| FP16 | 14GB | 16GB | 80.5   | 1.0x |
| Q8_0 | 7.5GB| 9GB  | 80.2   | 1.1x |
| Q4_K_M|4.3GB| 6GB  | 79.1   | 1.3x |
| Q4_0 | 4GB  | 5GB  | 77.8   | 1.4x |

结论：Q4_K_M 是质量和效率的最佳平衡点
```

---

## 最佳实践

```
1. 生产环境选择
   - 有 GPU：AWQ 4-bit
   - 纯 CPU：GGUF Q4_K_M
   - 质量优先：GGUF Q6_K 或 Q8_0

2. 量化建议
   - 总是在量化后测试关键指标
   - 保留原始模型以便对比
   - 考虑使用混合精度（关键层保持高精度）

3. 显存优化
   - 结合 KV Cache 量化
   - 使用 Flash Attention
   - 适当调整 batch size
```

---

## ➡️ 下一步

继续 [03-推理优化.md](./03-推理优化.md)

