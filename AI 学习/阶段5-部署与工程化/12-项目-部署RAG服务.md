# ğŸ¯ é¡¹ç›®ï¼šéƒ¨ç½² RAG æœåŠ¡

> å°† RAG ç³»ç»Ÿå®Œæ•´éƒ¨ç½²åˆ°ç”Ÿäº§ç¯å¢ƒ

---

## é¡¹ç›®ç›®æ ‡

```
å°†é˜¶æ®µ4å¼€å‘çš„ RAG çŸ¥è¯†åº“ç³»ç»Ÿï¼š
1. ä½¿ç”¨æ¨ç†å¼•æ“ï¼ˆOllama/vLLMï¼‰éƒ¨ç½²æœ¬åœ°æ¨¡å‹
2. ç”¨ FastAPI æš´éœ² OpenAI å…¼å®¹æ¥å£
3. ç”¨ Docker æ‰“åŒ…æ•´ä¸ªç³»ç»Ÿ
4. æ·»åŠ æ—¥å¿—è®°å½•å’ŒåŸºç¡€ç›‘æ§
5. å¯é€‰ï¼šä½¿ç”¨ Ragas è¯„ä¼°è´¨é‡
```

---

## æ¶æ„è®¾è®¡

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      ç”Ÿäº§æ¶æ„                                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                              â”‚
â”‚   â”‚  Nginx   â”‚ â† åå‘ä»£ç† + SSL + é™æµ                       â”‚
â”‚   â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜                                              â”‚
â”‚        â”‚                                                    â”‚
â”‚        â†“                                                    â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚   â”‚ FastAPI  â”‚â”€â”€â”€â”€â†’â”‚  Ollama  â”‚     â”‚ ChromaDB â”‚          â”‚
â”‚   â”‚  (API)   â”‚     â”‚  (LLM)   â”‚     â”‚ (å‘é‡DB) â”‚          â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
â”‚        â”‚                                                    â”‚
â”‚        â†“                                                    â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                            â”‚
â”‚   â”‚   æ—¥å¿—   â”‚     â”‚  ç›‘æ§    â”‚                            â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                            â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## é¡¹ç›®ç»“æ„

```
rag-production/
â”œâ”€â”€ api/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ main.py              # FastAPI åº”ç”¨
â”‚   â”œâ”€â”€ config.py            # é…ç½®
â”‚   â”œâ”€â”€ rag_engine.py        # RAG å¼•æ“
â”‚   â”œâ”€â”€ middleware.py        # ä¸­é—´ä»¶
â”‚   â””â”€â”€ security.py          # å®‰å…¨
â”œâ”€â”€ docker/
â”‚   â”œâ”€â”€ Dockerfile.api       # API é•œåƒ
â”‚   â”œâ”€â”€ Dockerfile.ollama    # Ollama é•œåƒ
â”‚   â””â”€â”€ nginx.conf           # Nginx é…ç½®
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ init_db.py           # åˆå§‹åŒ–æ•°æ®åº“
â”‚   â””â”€â”€ evaluate.py          # è¯„ä¼°è„šæœ¬
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .env.example
â””â”€â”€ README.md
```

---

## æ ¸å¿ƒä»£ç 

### config.py

```python
"""é…ç½®ç®¡ç†"""
from pydantic_settings import BaseSettings
from functools import lru_cache

class Settings(BaseSettings):
    # åº”ç”¨
    app_name: str = "RAG Production API"
    debug: bool = False

    # LLM
    llm_backend: str = "http://ollama:11434"
    llm_model: str = "qwen2.5:7b"
    embedding_model: str = "nomic-embed-text"

    # å‘é‡æ•°æ®åº“
    chroma_host: str = "chromadb"
    chroma_port: int = 8000

    # å®‰å…¨
    api_key: str = ""

    # æ—¥å¿—
    log_level: str = "INFO"
    log_file: str = "/var/log/rag/app.log"

    class Config:
        env_file = ".env"

@lru_cache()
def get_settings():
    return Settings()
```

### main.py

```python
"""FastAPI ä¸»åº”ç”¨"""
from fastapi import FastAPI, HTTPException, Depends, Request
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import StreamingResponse
from pydantic import BaseModel
from typing import List, Optional
import time
import uuid
import structlog

from api.config import get_settings, Settings
from api.rag_engine import RAGEngine
from api.middleware import LoggingMiddleware, RateLimitMiddleware
from api.security import verify_api_key

# é…ç½®æ—¥å¿—
structlog.configure(
    processors=[
        structlog.processors.TimeStamper(fmt="iso"),
        structlog.processors.JSONRenderer()
    ]
)
logger = structlog.get_logger()

# åˆ›å»ºåº”ç”¨
app = FastAPI(title="RAG Production API", version="1.0.0")

# ä¸­é—´ä»¶
app.add_middleware(CORSMiddleware, allow_origins=["*"], allow_methods=["*"], allow_headers=["*"])
app.add_middleware(LoggingMiddleware)
app.add_middleware(RateLimitMiddleware, requests_per_minute=60)

# RAG å¼•æ“ï¼ˆå»¶è¿Ÿåˆå§‹åŒ–ï¼‰
rag_engine: Optional[RAGEngine] = None

@app.on_event("startup")
async def startup():
    global rag_engine
    settings = get_settings()
    rag_engine = RAGEngine(settings)
    logger.info("RAG engine initialized")

# ========== æ•°æ®æ¨¡å‹ ==========
class Message(BaseModel):
    role: str
    content: str

class ChatRequest(BaseModel):
    messages: List[Message]
    model: str = "rag"
    stream: bool = False
    temperature: float = 0.7

class DocumentUpload(BaseModel):
    content: str
    source: str

# ========== API ç«¯ç‚¹ ==========
@app.get("/health")
async def health():
    return {"status": "healthy", "timestamp": time.time()}

@app.post("/v1/chat/completions")
async def chat_completions(
    request: ChatRequest,
    api_key: str = Depends(verify_api_key)
):
    """RAG é—®ç­”æ¥å£"""
    request_id = str(uuid.uuid4())[:8]
    start_time = time.time()

    # è·å–æœ€åä¸€æ¡ç”¨æˆ·æ¶ˆæ¯
    user_message = next(
        (m.content for m in reversed(request.messages) if m.role == "user"),
        ""
    )

    if not user_message:
        raise HTTPException(status_code=400, detail="No user message found")

    logger.info("chat_request", request_id=request_id, user_message=user_message[:100])

    if request.stream:
        return StreamingResponse(
            rag_engine.stream_query(user_message, request_id),
            media_type="text/event-stream"
        )
    else:
        result = await rag_engine.query(user_message)

        latency = time.time() - start_time
        logger.info("chat_response", request_id=request_id, latency_ms=latency*1000)

        return {
            "id": f"chatcmpl-{request_id}",
            "object": "chat.completion",
            "created": int(time.time()),
            "model": request.model,
            "choices": [{
                "index": 0,
                "message": {"role": "assistant", "content": result["answer"]},
                "finish_reason": "stop"
            }],
            "usage": {
                "prompt_tokens": result.get("prompt_tokens", 0),
                "completion_tokens": result.get("completion_tokens", 0),
                "total_tokens": result.get("total_tokens", 0)
            }
        }

@app.post("/documents")
async def add_document(
    doc: DocumentUpload,
    api_key: str = Depends(verify_api_key)
):
    """æ·»åŠ æ–‡æ¡£åˆ°çŸ¥è¯†åº“"""
    num_chunks = await rag_engine.add_document(doc.content, doc.source)
    return {"message": "Document added", "chunks": num_chunks}

@app.get("/stats")
async def get_stats(api_key: str = Depends(verify_api_key)):
    """è·å–ç»Ÿè®¡ä¿¡æ¯"""
    return await rag_engine.get_stats()

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
```

### rag_engine.py

```python
"""RAG å¼•æ“"""
import httpx
import chromadb
from sentence_transformers import SentenceTransformer
from typing import List, Dict, AsyncGenerator
import json

class RAGEngine:
    def __init__(self, settings):
        self.settings = settings

        # åˆå§‹åŒ– Embedding æ¨¡å‹
        self.embedder = SentenceTransformer('all-MiniLM-L6-v2')

        # åˆå§‹åŒ– ChromaDB
        self.chroma = chromadb.HttpClient(
            host=settings.chroma_host,
            port=settings.chroma_port
        )
        self.collection = self.chroma.get_or_create_collection("knowledge_base")

    async def add_document(self, content: str, source: str) -> int:
        """æ·»åŠ æ–‡æ¡£"""
        # åˆ‡åˆ†
        chunks = self._chunk_text(content)

        # Embedding
        embeddings = self.embedder.encode(chunks).tolist()

        # å­˜å‚¨
        ids = [f"{source}_{i}" for i in range(len(chunks))]
        self.collection.add(
            ids=ids,
            embeddings=embeddings,
            documents=chunks,
            metadatas=[{"source": source}] * len(chunks)
        )

        return len(chunks)

    async def query(self, question: str) -> Dict:
        """RAG æŸ¥è¯¢"""
        # æ£€ç´¢
        query_embedding = self.embedder.encode([question]).tolist()
        results = self.collection.query(
            query_embeddings=query_embedding,
            n_results=5
        )

        contexts = results["documents"][0] if results["documents"] else []

        # ç”Ÿæˆ
        answer = await self._generate(question, contexts)

        return {
            "answer": answer,
            "sources": results.get("metadatas", [[]])[0]
        }

    async def stream_query(self, question: str, request_id: str) -> AsyncGenerator:
        """æµå¼ RAG æŸ¥è¯¢"""
        # æ£€ç´¢
        query_embedding = self.embedder.encode([question]).tolist()
        results = self.collection.query(
            query_embeddings=query_embedding,
            n_results=5
        )

        contexts = results["documents"][0] if results["documents"] else []

        # æµå¼ç”Ÿæˆ
        async for chunk in self._stream_generate(question, contexts):
            yield f"data: {json.dumps({'choices': [{'delta': {'content': chunk}}]})}\n\n"

        yield "data: [DONE]\n\n"

    async def _generate(self, question: str, contexts: List[str]) -> str:
        """è°ƒç”¨ LLM ç”Ÿæˆ"""
        context_text = "\n\n".join(contexts)

        prompt = f"""åŸºäºä»¥ä¸‹å‚è€ƒèµ„æ–™å›ç­”é—®é¢˜ã€‚

å‚è€ƒèµ„æ–™ï¼š
{context_text}

é—®é¢˜ï¼š{question}

å›ç­”ï¼š"""

        async with httpx.AsyncClient() as client:
            response = await client.post(
                f"{self.settings.llm_backend}/api/generate",
                json={
                    "model": self.settings.llm_model,
                    "prompt": prompt,
                    "stream": False
                },
                timeout=60.0
            )

            return response.json()["response"]

    async def _stream_generate(self, question: str, contexts: List[str]) -> AsyncGenerator:
        """æµå¼ç”Ÿæˆ"""
        context_text = "\n\n".join(contexts)

        prompt = f"""åŸºäºä»¥ä¸‹å‚è€ƒèµ„æ–™å›ç­”é—®é¢˜ã€‚

å‚è€ƒèµ„æ–™ï¼š
{context_text}

é—®é¢˜ï¼š{question}

å›ç­”ï¼š"""

        async with httpx.AsyncClient() as client:
            async with client.stream(
                "POST",
                f"{self.settings.llm_backend}/api/generate",
                json={
                    "model": self.settings.llm_model,
                    "prompt": prompt,
                    "stream": True
                },
                timeout=60.0
            ) as response:
                async for line in response.aiter_lines():
                    if line:
                        data = json.loads(line)
                        if not data.get("done"):
                            yield data["response"]

    def _chunk_text(self, text: str, chunk_size: int = 500) -> List[str]:
        """åˆ‡åˆ†æ–‡æœ¬"""
        chunks = []
        for i in range(0, len(text), chunk_size):
            chunks.append(text[i:i + chunk_size])
        return chunks

    async def get_stats(self) -> Dict:
        """è·å–ç»Ÿè®¡"""
        return {
            "total_documents": self.collection.count(),
            "model": self.settings.llm_model
        }
```

### docker-compose.yml

```yaml
version: '3.8'

services:
  nginx:
    image: nginx:alpine
    ports:
      - "80:80"
    volumes:
      - ./docker/nginx.conf:/etc/nginx/nginx.conf:ro
    depends_on:
      - api
    restart: unless-stopped

  api:
    build:
      context: .
      dockerfile: docker/Dockerfile.api
    environment:
      - LLM_BACKEND=http://ollama:11434
      - CHROMA_HOST=chromadb
      - API_KEY=${API_KEY:-secret123}
    depends_on:
      - ollama
      - chromadb
    restart: unless-stopped
    volumes:
      - ./logs:/var/log/rag

  ollama:
    image: ollama/ollama:latest
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama
    restart: unless-stopped
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]

  chromadb:
    image: chromadb/chroma:latest
    ports:
      - "8001:8000"
    volumes:
      - chroma_data:/chroma/chroma
    restart: unless-stopped

volumes:
  ollama_data:
  chroma_data:
```

### Dockerfile.api

```dockerfile
FROM python:3.11-slim

WORKDIR /app

RUN apt-get update && apt-get install -y \
    build-essential \
    && rm -rf /var/lib/apt/lists/*

COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

COPY api/ ./api/

RUN mkdir -p /var/log/rag

EXPOSE 8000

CMD ["uvicorn", "api.main:app", "--host", "0.0.0.0", "--port", "8000"]
```

---

## éƒ¨ç½²æ­¥éª¤

```bash
# 1. é…ç½®ç¯å¢ƒå˜é‡
cp .env.example .env
# ç¼–è¾‘ .env è®¾ç½® API_KEY ç­‰

# 2. å¯åŠ¨æœåŠ¡
docker compose up -d

# 3. ä¸‹è½½æ¨¡å‹ï¼ˆé¦–æ¬¡ï¼‰
docker compose exec ollama ollama pull qwen2.5:7b
docker compose exec ollama ollama pull nomic-embed-text

# 4. éªŒè¯æœåŠ¡
curl http://localhost/health

# 5. æµ‹è¯• API
curl http://localhost/v1/chat/completions \
    -H "Authorization: Bearer secret123" \
    -H "Content-Type: application/json" \
    -d '{"messages": [{"role": "user", "content": "ä½ å¥½"}]}'

# 6. æŸ¥çœ‹æ—¥å¿—
docker compose logs -f api
```

---

## å®ç° Checklist

```
â–¡ Ollama/vLLM éƒ¨ç½²å¹¶å¯è®¿é—®
â–¡ FastAPI æœåŠ¡æ­£å¸¸è¿è¡Œ
â–¡ OpenAI å…¼å®¹æ¥å£å¯ç”¨
â–¡ æµå¼è¾“å‡ºæ­£å¸¸
â–¡ Docker æ‰“åŒ…å®Œæˆ
â–¡ Docker Compose ç¼–æ’å®Œæˆ
â–¡ æ—¥å¿—è®°å½•æ­£å¸¸
â–¡ API è®¤è¯å·²å®ç°
â–¡ å¥åº·æ£€æŸ¥ç«¯ç‚¹å¯ç”¨
â–¡ å¯é€‰ï¼šRagas è¯„ä¼°è„šæœ¬
```

---

## è¿ç»´å‘½ä»¤

```bash
# æŸ¥çœ‹çŠ¶æ€
docker compose ps

# æŸ¥çœ‹æ—¥å¿—
docker compose logs -f

# é‡å¯æœåŠ¡
docker compose restart api

# æ›´æ–°éƒ¨ç½²
docker compose pull
docker compose up -d --build

# å¤‡ä»½æ•°æ®
docker compose exec chromadb tar -czvf /backup/chroma.tar.gz /chroma/chroma
```

---

## â¡ï¸ ä¸‹ä¸€æ­¥

ç»§ç»­ [13-è‡ªæµ‹æ¸…å•.md](./13-è‡ªæµ‹æ¸…å•.md)

