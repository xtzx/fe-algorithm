# 🏗️ 01 - 部署基础

> 本地部署 vs 云端 API 的选择与权衡

---

## 部署方案对比

### 云端 API

```
优点：
✅ 无需硬件投入
✅ 开箱即用，快速上线
✅ 自动扩缩容
✅ 持续更新最新模型
✅ 无运维负担

缺点：
❌ 数据隐私风险
❌ 长期成本可能较高
❌ 依赖网络，有延迟
❌ 受 API 限制（速率、功能）
❌ 供应商锁定

适用场景：
- 快速原型验证
- 非敏感数据应用
- 小规模使用
- 需要最新模型能力
```

### 本地/私有化部署

```
优点：
✅ 数据完全可控
✅ 长期成本可优化
✅ 无网络延迟
✅ 可深度定制
✅ 无 API 调用限制

缺点：
❌ 需要硬件投入
❌ 运维复杂度高
❌ 模型能力可能不如最新闭源模型
❌ 需要持续关注更新

适用场景：
- 敏感数据处理
- 大规模高频调用
- 需要低延迟
- 合规要求严格
```

---

## 成本对比

### 云端 API 成本示例

```python
# OpenAI GPT-4o-mini 定价（2024）
# 输入：$0.15 / 1M tokens
# 输出：$0.60 / 1M tokens

# 假设场景：RAG 问答
# - 每次请求：输入 2000 tokens，输出 500 tokens
# - 日请求量：10000 次

daily_input_tokens = 2000 * 10000  # 20M tokens
daily_output_tokens = 500 * 10000  # 5M tokens

daily_cost = (20 * 0.15) + (5 * 0.60)  # $6
monthly_cost = daily_cost * 30  # $180/月
yearly_cost = monthly_cost * 12  # $2160/年
```

### 本地部署成本示例

```python
# 方案：RTX 4090 + 开源 7B 模型

# 硬件成本
gpu_cost = 13000  # RTX 4090 约 ¥13000
server_cost = 10000  # 其他硬件约 ¥10000
total_hardware = 23000  # 约 $3200

# 运营成本（电费 + 维护）
monthly_power = 0.5 * 24 * 30 * 1  # 500W * 24h * 30d * ¥1/kWh = ¥360
yearly_operation = monthly_power * 12  # ¥4320 ≈ $600

# 第一年总成本：$3800
# 第二年起：$600/年

# 结论：日请求量 > 5000 时，本地部署更划算
```

### 决策矩阵

| 因素 | 云端 API | 本地部署 |
|------|----------|----------|
| 初始成本 | 低 | 高 |
| 日常成本 | 按量付费 | 固定成本 |
| 数据隐私 | 较低 | 高 |
| 延迟 | 100-500ms | 10-100ms |
| 可扩展性 | 高 | 受硬件限制 |
| 运维复杂度 | 低 | 高 |
| 模型更新 | 自动 | 手动 |

---

## 模型选择

### 开源模型推荐

```
文本生成：
- Qwen2.5 系列（0.5B-72B）：中文最强
- LLaMA 3.1（8B-405B）：英文最强
- Mistral（7B-8x22B）：性价比高
- DeepSeek-V2：超长上下文

代码生成：
- Qwen2.5-Coder
- DeepSeek-Coder
- CodeLlama

Embedding：
- BGE 系列（中英文）
- E5 系列
- GTE 系列
```

### 模型大小与硬件需求

```
粗略估算（FP16）：
- 7B 模型 → 14GB 显存
- 13B 模型 → 26GB 显存
- 70B 模型 → 140GB 显存

量化后（INT4）：
- 7B 模型 → 4GB 显存
- 13B 模型 → 8GB 显存
- 70B 模型 → 40GB 显存

推荐配置：
- 消费级（RTX 3090/4090）：7B-13B 模型
- 专业级（A100 40GB）：70B 模型
- 企业级（多卡）：更大模型
```

---

## 部署架构选择

### 单机部署

```
适用场景：
- 开发测试
- 小规模生产
- 单一模型服务

架构：
┌──────────────────┐
│     Nginx        │
├──────────────────┤
│     FastAPI      │
├──────────────────┤
│  Ollama / vLLM   │
├──────────────────┤
│   GPU (单卡)     │
└──────────────────┘
```

### 分布式部署

```
适用场景：
- 高并发生产环境
- 多模型服务
- 高可用要求

架构：
┌─────────────────────────────────────────┐
│            Load Balancer                │
├─────────────────────────────────────────┤
│  ┌──────────┐  ┌──────────┐  ┌──────────┐
│  │ API 节点1 │  │ API 节点2 │  │ API 节点N │
│  └──────────┘  └──────────┘  └──────────┘
├─────────────────────────────────────────┤
│  ┌──────────┐  ┌──────────┐  ┌──────────┐
│  │推理节点1  │  │推理节点2  │  │推理节点N  │
│  │ (GPU)    │  │ (GPU)    │  │ (GPU)    │
│  └──────────┘  └──────────┘  └──────────┘
└─────────────────────────────────────────┘
```

### 混合架构

```
适用场景：
- 成本优化
- 灵活扩展
- 多模型混合

策略：
- 高频简单任务 → 本地小模型
- 低频复杂任务 → 云端大模型
- 敏感数据 → 本地处理
- 非敏感数据 → 云端处理
```

---

## 部署 Checklist

```
□ 确定业务需求（QPS、延迟、成本预算）
□ 选择模型（能力 vs 资源需求）
□ 选择部署方案（云端/本地/混合）
□ 准备硬件/云资源
□ 选择推理引擎
□ 设计 API 接口
□ 实现监控告警
□ 制定运维计划
□ 安全审计
□ 灰度发布策略
```

---

## ➡️ 下一步

继续 [02-模型量化.md](./02-模型量化.md)

