# ğŸ³ Docker éƒ¨ç½²

> å®¹å™¨åŒ– LLM æœåŠ¡

---

## Docker åŸºç¡€

### Dockerfile ç¼–å†™

```dockerfile
# Dockerfile - FastAPI æœåŠ¡
FROM python:3.11-slim

# è®¾ç½®å·¥ä½œç›®å½•
WORKDIR /app

# å®‰è£…ç³»ç»Ÿä¾èµ–
RUN apt-get update && apt-get install -y \
    build-essential \
    curl \
    && rm -rf /var/lib/apt/lists/*

# å¤åˆ¶ä¾èµ–æ–‡ä»¶
COPY requirements.txt .

# å®‰è£… Python ä¾èµ–
RUN pip install --no-cache-dir -r requirements.txt

# å¤åˆ¶ä»£ç 
COPY . .

# æš´éœ²ç«¯å£
EXPOSE 8000

# å¯åŠ¨å‘½ä»¤
CMD ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8000"]
```

### ä¼˜åŒ–çš„ Dockerfileï¼ˆå¤šé˜¶æ®µæ„å»ºï¼‰

```dockerfile
# æ„å»ºé˜¶æ®µ
FROM python:3.11-slim as builder

WORKDIR /app

# åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
RUN python -m venv /opt/venv
ENV PATH="/opt/venv/bin:$PATH"

# å®‰è£…ä¾èµ–
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# è¿è¡Œé˜¶æ®µ
FROM python:3.11-slim

WORKDIR /app

# ä»æ„å»ºé˜¶æ®µå¤åˆ¶è™šæ‹Ÿç¯å¢ƒ
COPY --from=builder /opt/venv /opt/venv
ENV PATH="/opt/venv/bin:$PATH"

# å¤åˆ¶ä»£ç 
COPY . .

# é root ç”¨æˆ·
RUN useradd -m appuser && chown -R appuser:appuser /app
USER appuser

EXPOSE 8000

CMD ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8000"]
```

### .dockerignore

```
# .dockerignore
__pycache__
*.pyc
*.pyo
.git
.gitignore
.env
.venv
venv
*.md
tests/
.pytest_cache
.coverage
htmlcov
.mypy_cache
*.egg-info
dist
build
```

---

## GPU å®¹å™¨æ”¯æŒ

### NVIDIA Container Toolkit å®‰è£…

```bash
# Ubuntu/Debian
distribution=$(. /etc/os-release;echo $ID$VERSION_ID)
curl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | sudo apt-key add -
curl -s -L https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.list | \
    sudo tee /etc/apt/sources.list.d/nvidia-docker.list

sudo apt-get update
sudo apt-get install -y nvidia-container-toolkit
sudo systemctl restart docker

# éªŒè¯
docker run --rm --gpus all nvidia/cuda:12.0-base nvidia-smi
```

### GPU Dockerfile

```dockerfile
# GPU ç‰ˆæœ¬ - vLLM æœåŠ¡
FROM nvidia/cuda:12.1.0-runtime-ubuntu22.04

WORKDIR /app

# å®‰è£… Python
RUN apt-get update && apt-get install -y \
    python3.11 \
    python3-pip \
    && rm -rf /var/lib/apt/lists/*

# å®‰è£…ä¾èµ–
COPY requirements.txt .
RUN pip3 install --no-cache-dir -r requirements.txt

# å®‰è£… vLLM
RUN pip3 install vllm

# å¤åˆ¶ä»£ç 
COPY . .

EXPOSE 8000

# å¯åŠ¨ vLLM æœåŠ¡
CMD ["python3", "-m", "vllm.entrypoints.openai.api_server", \
     "--model", "Qwen/Qwen2.5-7B-Instruct", \
     "--host", "0.0.0.0", "--port", "8000"]
```

### è¿è¡Œ GPU å®¹å™¨

```bash
# ä½¿ç”¨æ‰€æœ‰ GPU
docker run --gpus all -p 8000:8000 my-vllm-service

# æŒ‡å®š GPU
docker run --gpus '"device=0,1"' -p 8000:8000 my-vllm-service

# è®¾ç½®æ˜¾å­˜é™åˆ¶ï¼ˆé€šè¿‡ç¯å¢ƒå˜é‡ï¼‰
docker run --gpus all \
    -e CUDA_VISIBLE_DEVICES=0 \
    -p 8000:8000 my-vllm-service
```

---

## Docker Compose

### åŸºç¡€é…ç½®

```yaml
# docker-compose.yml
version: '3.8'

services:
  api:
    build: .
    ports:
      - "8000:8000"
    environment:
      - OLLAMA_HOST=ollama:11434
    depends_on:
      - ollama
    restart: unless-stopped

  ollama:
    image: ollama/ollama:latest
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama
    restart: unless-stopped

volumes:
  ollama_data:
```

### å®Œæ•´ç”Ÿäº§é…ç½®

```yaml
# docker-compose.prod.yml
version: '3.8'

services:
  # Nginx åå‘ä»£ç†
  nginx:
    image: nginx:alpine
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./nginx.conf:/etc/nginx/nginx.conf:ro
      - ./certs:/etc/nginx/certs:ro
    depends_on:
      - api
    restart: unless-stopped

  # API æœåŠ¡
  api:
    build:
      context: .
      dockerfile: Dockerfile
    environment:
      - LLM_BACKEND=http://vllm:8000/v1
      - LOG_LEVEL=INFO
    depends_on:
      - vllm
    deploy:
      replicas: 2
      resources:
        limits:
          memory: 4G
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3

  # vLLM æ¨ç†æœåŠ¡
  vllm:
    image: vllm/vllm-openai:latest
    command: >
      --model Qwen/Qwen2.5-7B-Instruct
      --host 0.0.0.0
      --port 8000
      --gpu-memory-utilization 0.9
    volumes:
      - model_cache:/root/.cache/huggingface
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    restart: unless-stopped

  # å‘é‡æ•°æ®åº“
  chromadb:
    image: chromadb/chroma:latest
    ports:
      - "8001:8000"
    volumes:
      - chroma_data:/chroma/chroma
    restart: unless-stopped

  # ç›‘æ§
  prometheus:
    image: prom/prometheus:latest
    ports:
      - "9090:9090"
    volumes:
      - ./prometheus.yml:/etc/prometheus/prometheus.yml:ro
    restart: unless-stopped

  grafana:
    image: grafana/grafana:latest
    ports:
      - "3000:3000"
    volumes:
      - grafana_data:/var/lib/grafana
    restart: unless-stopped

volumes:
  model_cache:
  chroma_data:
  grafana_data:
```

### Nginx é…ç½®

```nginx
# nginx.conf
events {
    worker_connections 1024;
}

http {
    upstream api_backend {
        server api:8000;
    }

    # é™æµé…ç½®
    limit_req_zone $binary_remote_addr zone=api_limit:10m rate=10r/s;

    server {
        listen 80;
        server_name api.example.com;

        # é™æµ
        limit_req zone=api_limit burst=20 nodelay;

        location / {
            proxy_pass http://api_backend;
            proxy_http_version 1.1;
            proxy_set_header Upgrade $http_upgrade;
            proxy_set_header Connection "upgrade";
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;

            # SSE æ”¯æŒ
            proxy_buffering off;
            proxy_cache off;
            proxy_read_timeout 300s;
        }

        location /health {
            proxy_pass http://api_backend/health;
        }
    }
}
```

---

## å¸¸ç”¨å‘½ä»¤

```bash
# æ„å»º
docker build -t my-llm-api .

# è¿è¡Œ
docker run -d -p 8000:8000 --name llm-api my-llm-api

# æŸ¥çœ‹æ—¥å¿—
docker logs -f llm-api

# è¿›å…¥å®¹å™¨
docker exec -it llm-api /bin/bash

# Docker Compose
docker compose up -d
docker compose logs -f
docker compose down

# é‡æ–°æ„å»º
docker compose up -d --build

# æŸ¥çœ‹èµ„æºä½¿ç”¨
docker stats
```

---

## CI/CD ç¤ºä¾‹

### GitHub Actions

```yaml
# .github/workflows/deploy.yml
name: Build and Deploy

on:
  push:
    branches: [main]

jobs:
  build:
    runs-on: ubuntu-latest

    steps:
      - uses: actions/checkout@v4

      - name: Login to Docker Hub
        uses: docker/login-action@v3
        with:
          username: ${{ secrets.DOCKER_USERNAME }}
          password: ${{ secrets.DOCKER_PASSWORD }}

      - name: Build and push
        uses: docker/build-push-action@v5
        with:
          context: .
          push: true
          tags: |
            myuser/llm-api:latest
            myuser/llm-api:${{ github.sha }}

      - name: Deploy to server
        uses: appleboy/ssh-action@v1.0.0
        with:
          host: ${{ secrets.SERVER_HOST }}
          username: ${{ secrets.SERVER_USER }}
          key: ${{ secrets.SERVER_SSH_KEY }}
          script: |
            cd /app
            docker compose pull
            docker compose up -d
```

### GitLab CI

```yaml
# .gitlab-ci.yml
stages:
  - build
  - deploy

build:
  stage: build
  image: docker:latest
  services:
    - docker:dind
  script:
    - docker build -t $CI_REGISTRY_IMAGE:$CI_COMMIT_SHA .
    - docker push $CI_REGISTRY_IMAGE:$CI_COMMIT_SHA
  only:
    - main

deploy:
  stage: deploy
  script:
    - ssh user@server "cd /app && docker compose pull && docker compose up -d"
  only:
    - main
```

---

## æœ€ä½³å®è·µ

```
1. é•œåƒä¼˜åŒ–
   - ä½¿ç”¨å¤šé˜¶æ®µæ„å»º
   - é€‰æ‹©åˆé€‚çš„åŸºç¡€é•œåƒ
   - æ¸…ç†ä¸å¿…è¦çš„æ–‡ä»¶

2. å®‰å…¨
   - ä¸ä½¿ç”¨ root ç”¨æˆ·
   - ä¸åœ¨é•œåƒä¸­å­˜å‚¨æ•æ„Ÿä¿¡æ¯
   - ä½¿ç”¨ .dockerignore

3. èµ„æºç®¡ç†
   - è®¾ç½®èµ„æºé™åˆ¶
   - é…ç½®å¥åº·æ£€æŸ¥
   - ä½¿ç”¨æ•°æ®å·æŒä¹…åŒ–

4. æ—¥å¿—
   - è¾“å‡ºåˆ° stdout/stderr
   - ä½¿ç”¨æ—¥å¿—é©±åŠ¨æ”¶é›†
```

---

## â¡ï¸ ä¸‹ä¸€æ­¥

ç»§ç»­ [09-ç›‘æ§ä¸æ—¥å¿—.md](./09-ç›‘æ§ä¸æ—¥å¿—.md)

