# ğŸ’° 12 - æˆæœ¬ä¼˜åŒ–

> LLM åº”ç”¨çš„æˆæœ¬æ§åˆ¶ä¸ä¼˜åŒ–ç­–ç•¥

---

## ç›®å½•

1. [ä¸ºä»€ä¹ˆéœ€è¦æˆæœ¬ä¼˜åŒ–](#1-ä¸ºä»€ä¹ˆéœ€è¦æˆæœ¬ä¼˜åŒ–)
2. [Token ä½¿ç”¨ç›‘æ§](#2-token-ä½¿ç”¨ç›‘æ§)
3. [è¯­ä¹‰ç¼“å­˜](#3-è¯­ä¹‰ç¼“å­˜)
4. [æ¨¡å‹åˆ†å±‚ç­–ç•¥](#4-æ¨¡å‹åˆ†å±‚ç­–ç•¥)
5. [Prompt ä¼˜åŒ–](#5-prompt-ä¼˜åŒ–)
6. [æ‰¹é‡å¤„ç†](#6-æ‰¹é‡å¤„ç†)
7. [æˆæœ¬ç›‘æ§ä»ªè¡¨æ¿](#7-æˆæœ¬ç›‘æ§ä»ªè¡¨æ¿)
8. [ç»ƒä¹ é¢˜](#8-ç»ƒä¹ é¢˜)

---

## 1. ä¸ºä»€ä¹ˆéœ€è¦æˆæœ¬ä¼˜åŒ–

### 1.1 æˆæœ¬æ„æˆ

```
LLM åº”ç”¨æˆæœ¬æ„æˆï¼š

1. æ¨ç†æˆæœ¬ï¼ˆæœ€å¤§ï¼‰
   â”œâ”€â”€ è¾“å…¥ Token è´¹ç”¨
   â”œâ”€â”€ è¾“å‡º Token è´¹ç”¨
   â””â”€â”€ æ¨¡å‹é€‰æ‹©å½±å“

2. åŸºç¡€è®¾æ–½æˆæœ¬
   â”œâ”€â”€ GPU æœåŠ¡å™¨
   â”œâ”€â”€ å‘é‡æ•°æ®åº“
   â””â”€â”€ å­˜å‚¨å’Œå¸¦å®½

3. å¼€å‘è¿ç»´æˆæœ¬
   â”œâ”€â”€ å¼€å‘äººåŠ›
   â”œâ”€â”€ è¿ç»´ç›‘æ§
   â””â”€â”€ å®‰å…¨åˆè§„
```

### 1.2 æˆæœ¬å¯¹æ¯”

```python
# ä¸»æµæ¨¡å‹ä»·æ ¼å¯¹æ¯” (2024å¹´ï¼Œ$/1M tokens)
MODEL_PRICING = {
    # OpenAI
    "gpt-4o": {"input": 5.0, "output": 15.0},
    "gpt-4o-mini": {"input": 0.15, "output": 0.6},
    "gpt-3.5-turbo": {"input": 0.5, "output": 1.5},

    # Anthropic
    "claude-3-opus": {"input": 15.0, "output": 75.0},
    "claude-3-sonnet": {"input": 3.0, "output": 15.0},
    "claude-3-haiku": {"input": 0.25, "output": 1.25},

    # å›½å†…æ¨¡å‹
    "deepseek-chat": {"input": 0.14, "output": 0.28},
    "qwen-turbo": {"input": 0.3, "output": 0.6},
}

def estimate_cost(model: str, input_tokens: int, output_tokens: int) -> float:
    """ä¼°ç®—å•æ¬¡è°ƒç”¨æˆæœ¬"""
    pricing = MODEL_PRICING.get(model, {"input": 1.0, "output": 2.0})
    input_cost = (input_tokens / 1_000_000) * pricing["input"]
    output_cost = (output_tokens / 1_000_000) * pricing["output"]
    return input_cost + output_cost

# ç¤ºä¾‹ï¼š1000æ¬¡è°ƒç”¨ï¼Œå¹³å‡ 500 è¾“å…¥ + 200 è¾“å‡º
calls = 1000
input_tokens = 500
output_tokens = 200

print("1000æ¬¡è°ƒç”¨æˆæœ¬ä¼°ç®—ï¼š")
for model in ["gpt-4o", "gpt-4o-mini", "claude-3-haiku", "deepseek-chat"]:
    cost = estimate_cost(model, input_tokens, output_tokens) * calls
    print(f"  {model}: ${cost:.2f}")
```

---

## 2. Token ä½¿ç”¨ç›‘æ§

### 2.1 Token è®¡æ•°å™¨

```python
import tiktoken
from dataclasses import dataclass, field
from datetime import datetime
from typing import Dict, List
import json

@dataclass
class TokenUsage:
    """Token ä½¿ç”¨è®°å½•"""
    timestamp: datetime
    model: str
    input_tokens: int
    output_tokens: int
    cost_usd: float
    request_id: str = ""
    user_id: str = ""
    endpoint: str = ""

class TokenTracker:
    """Token ä½¿ç”¨è¿½è¸ªå™¨"""

    def __init__(self, pricing: Dict[str, Dict[str, float]] = None):
        self.pricing = pricing or MODEL_PRICING
        self.usage_log: List[TokenUsage] = []
        self.encoders = {}

    def get_encoder(self, model: str):
        """è·å– tokenizer"""
        if model not in self.encoders:
            try:
                self.encoders[model] = tiktoken.encoding_for_model(model)
            except:
                self.encoders[model] = tiktoken.get_encoding("cl100k_base")
        return self.encoders[model]

    def count_tokens(self, text: str, model: str = "gpt-4o") -> int:
        """è®¡ç®— token æ•°é‡"""
        encoder = self.get_encoder(model)
        return len(encoder.encode(text))

    def record_usage(self, model: str, input_tokens: int, output_tokens: int,
                    request_id: str = "", user_id: str = "", endpoint: str = ""):
        """è®°å½•ä½¿ç”¨"""
        pricing = self.pricing.get(model, {"input": 1.0, "output": 2.0})
        cost = (input_tokens / 1_000_000) * pricing["input"] + \
               (output_tokens / 1_000_000) * pricing["output"]

        usage = TokenUsage(
            timestamp=datetime.now(),
            model=model,
            input_tokens=input_tokens,
            output_tokens=output_tokens,
            cost_usd=cost,
            request_id=request_id,
            user_id=user_id,
            endpoint=endpoint
        )
        self.usage_log.append(usage)
        return usage

    def get_summary(self, hours: int = 24) -> dict:
        """è·å–ä½¿ç”¨æ‘˜è¦"""
        cutoff = datetime.now().timestamp() - hours * 3600
        recent = [u for u in self.usage_log
                  if u.timestamp.timestamp() > cutoff]

        if not recent:
            return {"message": "No data"}

        total_input = sum(u.input_tokens for u in recent)
        total_output = sum(u.output_tokens for u in recent)
        total_cost = sum(u.cost_usd for u in recent)

        # æŒ‰æ¨¡å‹åˆ†ç»„
        by_model = {}
        for u in recent:
            if u.model not in by_model:
                by_model[u.model] = {"input": 0, "output": 0, "cost": 0, "count": 0}
            by_model[u.model]["input"] += u.input_tokens
            by_model[u.model]["output"] += u.output_tokens
            by_model[u.model]["cost"] += u.cost_usd
            by_model[u.model]["count"] += 1

        return {
            "period_hours": hours,
            "total_requests": len(recent),
            "total_input_tokens": total_input,
            "total_output_tokens": total_output,
            "total_cost_usd": total_cost,
            "by_model": by_model
        }

# ä½¿ç”¨
tracker = TokenTracker()

# è®°å½•ä¸€æ¬¡è°ƒç”¨
tracker.record_usage(
    model="gpt-4o-mini",
    input_tokens=500,
    output_tokens=200,
    user_id="user_123",
    endpoint="/chat"
)

print(tracker.get_summary())
```

### 2.2 FastAPI ä¸­é—´ä»¶

```python
from fastapi import FastAPI, Request
from starlette.middleware.base import BaseHTTPMiddleware
import time

app = FastAPI()
tracker = TokenTracker()

class TokenTrackingMiddleware(BaseHTTPMiddleware):
    """Token è¿½è¸ªä¸­é—´ä»¶"""

    async def dispatch(self, request: Request, call_next):
        # è®°å½•è¯·æ±‚å¼€å§‹
        start_time = time.time()

        # å¤„ç†è¯·æ±‚
        response = await call_next(request)

        # ä»å“åº”å¤´è·å– token ä½¿ç”¨ï¼ˆéœ€è¦åœ¨å“åº”ä¸­æ·»åŠ ï¼‰
        input_tokens = int(response.headers.get("X-Input-Tokens", 0))
        output_tokens = int(response.headers.get("X-Output-Tokens", 0))
        model = response.headers.get("X-Model", "unknown")

        if input_tokens > 0 or output_tokens > 0:
            tracker.record_usage(
                model=model,
                input_tokens=input_tokens,
                output_tokens=output_tokens,
                endpoint=request.url.path,
                user_id=request.headers.get("X-User-ID", "anonymous")
            )

        return response

app.add_middleware(TokenTrackingMiddleware)

@app.get("/usage/summary")
async def get_usage_summary(hours: int = 24):
    """è·å–ä½¿ç”¨æ‘˜è¦"""
    return tracker.get_summary(hours)
```

---

## 3. è¯­ä¹‰ç¼“å­˜

### 3.1 ç²¾ç¡®åŒ¹é…ç¼“å­˜

```python
import hashlib
import json
from typing import Optional
from datetime import datetime, timedelta
import diskcache

class ExactCache:
    """ç²¾ç¡®åŒ¹é…ç¼“å­˜"""

    def __init__(self, cache_dir: str = ".cache/llm"):
        self.cache = diskcache.Cache(cache_dir)
        self.ttl = 3600 * 24  # 24å°æ—¶è¿‡æœŸ
        self.hits = 0
        self.misses = 0

    def _make_key(self, prompt: str, model: str, **kwargs) -> str:
        """ç”Ÿæˆç¼“å­˜ key"""
        data = {
            "prompt": prompt,
            "model": model,
            **kwargs
        }
        content = json.dumps(data, sort_keys=True)
        return hashlib.sha256(content.encode()).hexdigest()

    def get(self, prompt: str, model: str, **kwargs) -> Optional[str]:
        """è·å–ç¼“å­˜"""
        key = self._make_key(prompt, model, **kwargs)
        result = self.cache.get(key)

        if result is not None:
            self.hits += 1
            return result

        self.misses += 1
        return None

    def set(self, prompt: str, model: str, response: str, **kwargs):
        """è®¾ç½®ç¼“å­˜"""
        key = self._make_key(prompt, model, **kwargs)
        self.cache.set(key, response, expire=self.ttl)

    def stats(self) -> dict:
        """ç¼“å­˜ç»Ÿè®¡"""
        total = self.hits + self.misses
        hit_rate = self.hits / total if total > 0 else 0
        return {
            "hits": self.hits,
            "misses": self.misses,
            "hit_rate": hit_rate,
            "cache_size": len(self.cache)
        }

# ä½¿ç”¨
cache = ExactCache()

def chat_with_cache(prompt: str, model: str = "gpt-4o-mini") -> str:
    # å…ˆæŸ¥ç¼“å­˜
    cached = cache.get(prompt, model)
    if cached:
        print("Cache hit!")
        return cached

    # è°ƒç”¨ LLM
    response = call_llm(prompt, model)

    # å­˜å…¥ç¼“å­˜
    cache.set(prompt, model, response)

    return response
```

### 3.2 è¯­ä¹‰ç›¸ä¼¼ç¼“å­˜

```python
import numpy as np
from sentence_transformers import SentenceTransformer
from typing import List, Tuple, Optional

class SemanticCache:
    """è¯­ä¹‰ç›¸ä¼¼ç¼“å­˜"""

    def __init__(self, threshold: float = 0.95, max_size: int = 10000):
        self.encoder = SentenceTransformer('all-MiniLM-L6-v2')
        self.threshold = threshold
        self.max_size = max_size

        # å­˜å‚¨
        self.prompts: List[str] = []
        self.embeddings: List[np.ndarray] = []
        self.responses: List[str] = []
        self.models: List[str] = []

        self.hits = 0
        self.misses = 0

    def _encode(self, text: str) -> np.ndarray:
        """ç¼–ç æ–‡æœ¬"""
        return self.encoder.encode(text, normalize_embeddings=True)

    def _find_similar(self, query_embedding: np.ndarray, model: str) -> Optional[Tuple[int, float]]:
        """æŸ¥æ‰¾ç›¸ä¼¼ç¼“å­˜"""
        if not self.embeddings:
            return None

        # è¿‡æ»¤ç›¸åŒæ¨¡å‹
        valid_indices = [i for i, m in enumerate(self.models) if m == model]
        if not valid_indices:
            return None

        # è®¡ç®—ç›¸ä¼¼åº¦
        embeddings_matrix = np.stack([self.embeddings[i] for i in valid_indices])
        similarities = np.dot(embeddings_matrix, query_embedding)

        max_idx = np.argmax(similarities)
        max_sim = similarities[max_idx]

        if max_sim >= self.threshold:
            return valid_indices[max_idx], max_sim

        return None

    def get(self, prompt: str, model: str) -> Optional[str]:
        """è·å–ç¼“å­˜"""
        query_embedding = self._encode(prompt)
        result = self._find_similar(query_embedding, model)

        if result:
            idx, similarity = result
            self.hits += 1
            print(f"Semantic cache hit! Similarity: {similarity:.3f}")
            return self.responses[idx]

        self.misses += 1
        return None

    def set(self, prompt: str, model: str, response: str):
        """è®¾ç½®ç¼“å­˜"""
        # LRU æ·˜æ±°
        if len(self.prompts) >= self.max_size:
            self.prompts.pop(0)
            self.embeddings.pop(0)
            self.responses.pop(0)
            self.models.pop(0)

        embedding = self._encode(prompt)

        self.prompts.append(prompt)
        self.embeddings.append(embedding)
        self.responses.append(response)
        self.models.append(model)

    def stats(self) -> dict:
        """ç»Ÿè®¡ä¿¡æ¯"""
        total = self.hits + self.misses
        return {
            "hits": self.hits,
            "misses": self.misses,
            "hit_rate": self.hits / total if total > 0 else 0,
            "cache_size": len(self.prompts),
            "threshold": self.threshold
        }

# ä½¿ç”¨
semantic_cache = SemanticCache(threshold=0.92)

def chat_with_semantic_cache(prompt: str, model: str = "gpt-4o-mini") -> str:
    # æŸ¥æ‰¾è¯­ä¹‰ç›¸ä¼¼ç¼“å­˜
    cached = semantic_cache.get(prompt, model)
    if cached:
        return cached

    # è°ƒç”¨ LLM
    response = call_llm(prompt, model)

    # å­˜å…¥ç¼“å­˜
    semantic_cache.set(prompt, model, response)

    return response
```

---

## 4. æ¨¡å‹åˆ†å±‚ç­–ç•¥

### 4.1 æ„å›¾è·¯ç”±

```python
from enum import Enum
from openai import OpenAI

class Complexity(Enum):
    SIMPLE = "simple"      # ç®€å•é—®é¢˜ï¼Œç”¨å°æ¨¡å‹
    MEDIUM = "medium"      # ä¸­ç­‰å¤æ‚åº¦
    COMPLEX = "complex"    # å¤æ‚é—®é¢˜ï¼Œç”¨å¤§æ¨¡å‹

# æ¨¡å‹æ˜ å°„
MODEL_TIERS = {
    Complexity.SIMPLE: "gpt-4o-mini",
    Complexity.MEDIUM: "gpt-4o-mini",
    Complexity.COMPLEX: "gpt-4o",
}

client = OpenAI()

def classify_complexity(query: str) -> Complexity:
    """åˆ†ç±»é—®é¢˜å¤æ‚åº¦"""

    response = client.chat.completions.create(
        model="gpt-4o-mini",  # ç”¨å°æ¨¡å‹åˆ†ç±»
        messages=[{
            "role": "user",
            "content": f"""
åˆ†æä»¥ä¸‹é—®é¢˜çš„å¤æ‚åº¦ï¼Œåªè¿”å›ä¸€ä¸ªå•è¯ï¼šsimple/medium/complex

åˆ¤æ–­æ ‡å‡†ï¼š
- simple: äº‹å®æ€§é—®é¢˜ã€ç®€å•é—®ç­”ã€ç¿»è¯‘ã€æ ¼å¼è½¬æ¢
- medium: éœ€è¦ä¸€å®šæ¨ç†ã€å¤šæ­¥éª¤ä»»åŠ¡
- complex: å¤æ‚æ¨ç†ã€åˆ›ä½œã€ä»£ç ç”Ÿæˆã€ä¸“ä¸šåˆ†æ

é—®é¢˜ï¼š{query}

å¤æ‚åº¦ï¼š"""
        }],
        max_tokens=10,
        temperature=0
    )

    result = response.choices[0].message.content.strip().lower()

    if "simple" in result:
        return Complexity.SIMPLE
    elif "complex" in result:
        return Complexity.COMPLEX
    else:
        return Complexity.MEDIUM

def smart_chat(query: str) -> dict:
    """æ™ºèƒ½è·¯ç”±åˆ°åˆé€‚æ¨¡å‹"""

    # 1. åˆ†ç±»å¤æ‚åº¦
    complexity = classify_complexity(query)

    # 2. é€‰æ‹©æ¨¡å‹
    model = MODEL_TIERS[complexity]

    # 3. è°ƒç”¨
    response = client.chat.completions.create(
        model=model,
        messages=[{"role": "user", "content": query}]
    )

    return {
        "answer": response.choices[0].message.content,
        "complexity": complexity.value,
        "model": model,
        "input_tokens": response.usage.prompt_tokens,
        "output_tokens": response.usage.completion_tokens
    }

# æµ‹è¯•
queries = [
    "ä»Šå¤©æ˜ŸæœŸå‡ ï¼Ÿ",  # simple
    "è§£é‡Šé‡å­è®¡ç®—çš„åŸºæœ¬åŸç†",  # medium
    "è®¾è®¡ä¸€ä¸ªåˆ†å¸ƒå¼ç³»ç»Ÿæ¥å¤„ç†æ¯ç§’ç™¾ä¸‡çº§è¯·æ±‚",  # complex
]

for q in queries:
    result = smart_chat(q)
    print(f"é—®é¢˜: {q[:30]}...")
    print(f"  å¤æ‚åº¦: {result['complexity']}, æ¨¡å‹: {result['model']}")
    print(f"  Tokens: {result['input_tokens']} + {result['output_tokens']}")
```

### 4.2 çº§è”è°ƒç”¨

```python
def cascade_chat(query: str, max_attempts: int = 2) -> dict:
    """çº§è”è°ƒç”¨ï¼šå…ˆç”¨å°æ¨¡å‹ï¼Œå¤±è´¥å†ç”¨å¤§æ¨¡å‹"""

    models = ["gpt-4o-mini", "gpt-4o"]

    for i, model in enumerate(models[:max_attempts]):
        try:
            response = client.chat.completions.create(
                model=model,
                messages=[{"role": "user", "content": query}],
                temperature=0.7
            )

            answer = response.choices[0].message.content

            # ç®€å•çš„è´¨é‡æ£€æŸ¥
            if len(answer) > 50 and "I don't know" not in answer:
                return {
                    "answer": answer,
                    "model": model,
                    "attempts": i + 1
                }
        except Exception as e:
            print(f"Model {model} failed: {e}")
            continue

    return {
        "answer": "æ— æ³•å›ç­”",
        "model": None,
        "attempts": max_attempts
    }
```

---

## 5. Prompt ä¼˜åŒ–

### 5.1 Prompt å‹ç¼©

```python
def compress_prompt(prompt: str, max_tokens: int = 500) -> str:
    """å‹ç¼© Prompt"""

    # ä½¿ç”¨ LLM å‹ç¼©
    response = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[{
            "role": "user",
            "content": f"""
è¯·å‹ç¼©ä»¥ä¸‹æ–‡æœ¬ï¼Œä¿ç•™æ ¸å¿ƒä¿¡æ¯ï¼Œç›®æ ‡æ§åˆ¶åœ¨ {max_tokens} tokens ä»¥å†…ï¼š

{prompt}

å‹ç¼©åçš„æ–‡æœ¬ï¼š"""
        }]
    )

    return response.choices[0].message.content

def optimize_system_prompt(system_prompt: str) -> str:
    """ä¼˜åŒ–ç³»ç»Ÿæç¤ºè¯"""

    # ç§»é™¤å¤šä½™ç©ºæ ¼
    lines = [line.strip() for line in system_prompt.split('\n')]
    lines = [line for line in lines if line]

    # åˆå¹¶çŸ­è¡Œ
    optimized = ' '.join(lines)

    return optimized
```

### 5.2 ä¸Šä¸‹æ–‡çª—å£ç®¡ç†

```python
from typing import List, Dict

def manage_context(
    messages: List[Dict],
    max_tokens: int = 4000,
    model: str = "gpt-4o-mini"
) -> List[Dict]:
    """ç®¡ç†ä¸Šä¸‹æ–‡çª—å£"""

    tracker = TokenTracker()

    # è®¡ç®—æ€» token
    total_tokens = 0
    for msg in messages:
        total_tokens += tracker.count_tokens(msg.get("content", ""), model)

    if total_tokens <= max_tokens:
        return messages

    # ä¿ç•™ç³»ç»Ÿæ¶ˆæ¯å’Œæœ€è¿‘æ¶ˆæ¯
    result = []
    system_msgs = [m for m in messages if m.get("role") == "system"]
    other_msgs = [m for m in messages if m.get("role") != "system"]

    # ç³»ç»Ÿæ¶ˆæ¯å¿…é¡»ä¿ç•™
    result.extend(system_msgs)
    current_tokens = sum(tracker.count_tokens(m.get("content", ""), model)
                         for m in system_msgs)

    # ä»æœ€è¿‘çš„æ¶ˆæ¯å¼€å§‹æ·»åŠ 
    for msg in reversed(other_msgs):
        msg_tokens = tracker.count_tokens(msg.get("content", ""), model)
        if current_tokens + msg_tokens <= max_tokens:
            result.insert(len(system_msgs), msg)
            current_tokens += msg_tokens
        else:
            break

    return result
```

---

## 6. æ‰¹é‡å¤„ç†

### 6.1 æ‰¹é‡ API è°ƒç”¨

```python
import asyncio
from typing import List
from openai import AsyncOpenAI

async_client = AsyncOpenAI()

async def batch_chat(
    prompts: List[str],
    model: str = "gpt-4o-mini",
    max_concurrent: int = 10
) -> List[str]:
    """æ‰¹é‡å¹¶å‘è°ƒç”¨"""

    semaphore = asyncio.Semaphore(max_concurrent)

    async def call_with_semaphore(prompt: str) -> str:
        async with semaphore:
            response = await async_client.chat.completions.create(
                model=model,
                messages=[{"role": "user", "content": prompt}]
            )
            return response.choices[0].message.content

    tasks = [call_with_semaphore(p) for p in prompts]
    results = await asyncio.gather(*tasks, return_exceptions=True)

    return [r if isinstance(r, str) else f"Error: {r}" for r in results]

# ä½¿ç”¨
async def main():
    prompts = [f"What is {i} + {i}?" for i in range(20)]
    results = await batch_chat(prompts)
    for p, r in zip(prompts, results):
        print(f"{p} -> {r[:50]}")

# asyncio.run(main())
```

### 6.2 OpenAI Batch API

```python
import json
from openai import OpenAI

client = OpenAI()

def create_batch_file(requests: List[dict], filename: str = "batch_input.jsonl"):
    """åˆ›å»ºæ‰¹å¤„ç†è¾“å…¥æ–‡ä»¶"""
    with open(filename, 'w') as f:
        for i, req in enumerate(requests):
            batch_request = {
                "custom_id": f"request-{i}",
                "method": "POST",
                "url": "/v1/chat/completions",
                "body": {
                    "model": req.get("model", "gpt-4o-mini"),
                    "messages": req["messages"],
                    "max_tokens": req.get("max_tokens", 1000)
                }
            }
            f.write(json.dumps(batch_request) + '\n')
    return filename

def submit_batch(input_file: str) -> str:
    """æäº¤æ‰¹å¤„ç†ä»»åŠ¡"""
    # ä¸Šä¼ æ–‡ä»¶
    with open(input_file, 'rb') as f:
        file = client.files.create(file=f, purpose="batch")

    # åˆ›å»ºæ‰¹å¤„ç†ä»»åŠ¡
    batch = client.batches.create(
        input_file_id=file.id,
        endpoint="/v1/chat/completions",
        completion_window="24h"
    )

    return batch.id

def check_batch_status(batch_id: str) -> dict:
    """æ£€æŸ¥æ‰¹å¤„ç†çŠ¶æ€"""
    batch = client.batches.retrieve(batch_id)
    return {
        "status": batch.status,
        "completed": batch.request_counts.completed,
        "failed": batch.request_counts.failed,
        "total": batch.request_counts.total
    }

# æ‰¹å¤„ç†å¯ä»¥èŠ‚çœçº¦ 50% æˆæœ¬
```

---

## 7. æˆæœ¬ç›‘æ§ä»ªè¡¨æ¿

### 7.1 Streamlit ä»ªè¡¨æ¿

```python
import streamlit as st
import pandas as pd
import plotly.express as px
from datetime import datetime, timedelta

def create_cost_dashboard():
    st.title("ğŸ’° LLM æˆæœ¬ç›‘æ§ä»ªè¡¨æ¿")

    # å‡è®¾æˆ‘ä»¬æœ‰ä½¿ç”¨æ•°æ®
    tracker = TokenTracker()  # å®é™…åº”ç”¨ä¸­ä»æŒä¹…åŒ–å­˜å‚¨åŠ è½½
    summary = tracker.get_summary(24)

    # KPI å¡ç‰‡
    col1, col2, col3, col4 = st.columns(4)

    with col1:
        st.metric("æ€»è¯·æ±‚æ•°", summary.get("total_requests", 0))

    with col2:
        st.metric("æ€»æˆæœ¬", f"${summary.get('total_cost_usd', 0):.2f}")

    with col3:
        total_tokens = summary.get("total_input_tokens", 0) + summary.get("total_output_tokens", 0)
        st.metric("æ€» Tokens", f"{total_tokens:,}")

    with col4:
        avg_cost = summary.get("total_cost_usd", 0) / max(summary.get("total_requests", 1), 1)
        st.metric("å¹³å‡æˆæœ¬/è¯·æ±‚", f"${avg_cost:.4f}")

    # æŒ‰æ¨¡å‹åˆ†å¸ƒ
    st.subheader("ğŸ“Š æ¨¡å‹ä½¿ç”¨åˆ†å¸ƒ")

    by_model = summary.get("by_model", {})
    if by_model:
        df = pd.DataFrame([
            {"model": k, "cost": v["cost"], "count": v["count"]}
            for k, v in by_model.items()
        ])

        col1, col2 = st.columns(2)

        with col1:
            fig = px.pie(df, values='cost', names='model', title='æˆæœ¬åˆ†å¸ƒ')
            st.plotly_chart(fig)

        with col2:
            fig = px.bar(df, x='model', y='count', title='è°ƒç”¨æ¬¡æ•°')
            st.plotly_chart(fig)

    # ç¼“å­˜ç»Ÿè®¡
    st.subheader("ğŸ—„ï¸ ç¼“å­˜æ•ˆæœ")

    cache_stats = {"hits": 150, "misses": 50, "hit_rate": 0.75}  # ç¤ºä¾‹æ•°æ®

    col1, col2 = st.columns(2)
    with col1:
        st.metric("ç¼“å­˜å‘½ä¸­ç‡", f"{cache_stats['hit_rate']:.1%}")
    with col2:
        saved_cost = cache_stats['hits'] * 0.001  # å‡è®¾æ¯æ¬¡å‘½ä¸­èŠ‚çœ $0.001
        st.metric("ç¼“å­˜èŠ‚çœæˆæœ¬", f"${saved_cost:.2f}")

# è¿è¡Œï¼šstreamlit run dashboard.py
```

---

## 8. ç»ƒä¹ é¢˜

### åŸºç¡€ç»ƒä¹ 

1. å®ç°ä¸€ä¸ª Token è®¡æ•°å’Œæˆæœ¬ä¼°ç®—å‡½æ•°
2. å®ç°ç²¾ç¡®åŒ¹é…ç¼“å­˜
3. è®¡ç®—ä¸åŒæ¨¡å‹çš„æˆæœ¬å¯¹æ¯”

### è¿›é˜¶ç»ƒä¹ 

4. å®ç°è¯­ä¹‰ç¼“å­˜ï¼Œæµ‹è¯•å‘½ä¸­ç‡
5. å®ç°æ¨¡å‹åˆ†å±‚è·¯ç”±ç­–ç•¥

### å‚è€ƒç­”æ¡ˆ

<details>
<summary>ç»ƒä¹  1 å‚è€ƒç­”æ¡ˆ</summary>

```python
import tiktoken

def count_and_estimate(
    text: str,
    model: str = "gpt-4o-mini",
    is_input: bool = True
) -> dict:
    """è®¡ç®— token æ•°é‡å’Œæˆæœ¬"""

    # Token è®¡æ•°
    try:
        encoder = tiktoken.encoding_for_model(model)
    except:
        encoder = tiktoken.get_encoding("cl100k_base")

    tokens = len(encoder.encode(text))

    # æˆæœ¬ä¼°ç®—
    pricing = {
        "gpt-4o": {"input": 5.0, "output": 15.0},
        "gpt-4o-mini": {"input": 0.15, "output": 0.6},
    }

    rate = pricing.get(model, {"input": 1.0, "output": 2.0})
    price_per_token = rate["input"] if is_input else rate["output"]
    cost = (tokens / 1_000_000) * price_per_token

    return {
        "tokens": tokens,
        "cost_usd": cost,
        "model": model,
        "type": "input" if is_input else "output"
    }

# æµ‹è¯•
text = "è¿™æ˜¯ä¸€æ®µæµ‹è¯•æ–‡æœ¬ï¼Œç”¨äºè®¡ç®— token æ•°é‡å’Œæˆæœ¬ä¼°ç®—ã€‚"
result = count_and_estimate(text, "gpt-4o-mini", is_input=True)
print(f"Tokens: {result['tokens']}, Cost: ${result['cost_usd']:.6f}")
```

</details>

---

## â¡ï¸ ä¸‹ä¸€æ­¥

å­¦å®Œæœ¬èŠ‚åï¼Œç»§ç»­å®æˆ˜é¡¹ç›® [13-é¡¹ç›®-éƒ¨ç½²RAGæœåŠ¡.md](./13-é¡¹ç›®-éƒ¨ç½²RAGæœåŠ¡.md)

