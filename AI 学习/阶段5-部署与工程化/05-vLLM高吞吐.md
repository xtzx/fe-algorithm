# ğŸš€ 05 - vLLM é«˜ååæ¨ç†

> ç”Ÿäº§çº§ LLM æ¨ç†å¼•æ“

---

## vLLM ç®€ä»‹

```
vLLM æ˜¯ä»€ä¹ˆï¼š
- é«˜æ€§èƒ½ LLM æ¨ç†å’ŒæœåŠ¡å¼•æ“
- ç”± UC Berkeley å¼€å‘
- ç”Ÿäº§ç¯å¢ƒé¦–é€‰

æ ¸å¿ƒç‰¹æ€§ï¼š
âœ… PagedAttention é«˜æ•ˆå†…å­˜ç®¡ç†
âœ… Continuous Batching é«˜åå
âœ… æ”¯æŒå¤šç§é‡åŒ–æ ¼å¼
âœ… OpenAI å…¼å®¹ API
âœ… æ”¯æŒå¤š GPU å¹¶è¡Œ
âœ… æ”¯æŒ LoRA çƒ­åŠ è½½
```

---

## å®‰è£…

```bash
# åŸºç¡€å®‰è£…ï¼ˆéœ€è¦ CUDAï¼‰
pip install vllm

# ç‰¹å®š CUDA ç‰ˆæœ¬
pip install vllm --extra-index-url https://download.pytorch.org/whl/cu118

# éªŒè¯å®‰è£…
python -c "import vllm; print(vllm.__version__)"
```

---

## å¿«é€Ÿå¼€å§‹

### Python API

```python
from vllm import LLM, SamplingParams

# åŠ è½½æ¨¡å‹
llm = LLM(
    model="Qwen/Qwen2.5-7B-Instruct",
    trust_remote_code=True
)

# é‡‡æ ·å‚æ•°
sampling_params = SamplingParams(
    temperature=0.7,
    top_p=0.9,
    max_tokens=256
)

# ç”Ÿæˆ
prompts = [
    "ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Ÿ",
    "Python æœ‰ä»€ä¹ˆç‰¹ç‚¹ï¼Ÿ",
    "è§£é‡Šæœºå™¨å­¦ä¹ çš„åŸºæœ¬åŸç†"
]

outputs = llm.generate(prompts, sampling_params)

for output in outputs:
    prompt = output.prompt
    generated_text = output.outputs[0].text
    print(f"Prompt: {prompt}")
    print(f"Output: {generated_text}")
    print("-" * 50)
```

### å¯¹è¯æ ¼å¼

```python
from vllm import LLM, SamplingParams

llm = LLM(model="Qwen/Qwen2.5-7B-Instruct")

def chat(messages: list, **kwargs) -> str:
    """å¯¹è¯å‡½æ•°"""
    # è½¬æ¢ä¸ºæ¨¡å‹æ ¼å¼
    prompt = llm.get_tokenizer().apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True
    )

    sampling_params = SamplingParams(
        temperature=kwargs.get("temperature", 0.7),
        max_tokens=kwargs.get("max_tokens", 512)
    )

    outputs = llm.generate([prompt], sampling_params)
    return outputs[0].outputs[0].text

# ä½¿ç”¨
response = chat([
    {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªæœ‰å¸®åŠ©çš„åŠ©æ‰‹"},
    {"role": "user", "content": "ä½ å¥½"}
])
print(response)
```

---

## å¯åŠ¨ API æœåŠ¡

### åŸºæœ¬å¯åŠ¨

```bash
# å¯åŠ¨ OpenAI å…¼å®¹æœåŠ¡å™¨
python -m vllm.entrypoints.openai.api_server \
    --model Qwen/Qwen2.5-7B-Instruct \
    --host 0.0.0.0 \
    --port 8000 \
    --trust-remote-code

# ç®€åŒ–å‘½ä»¤
vllm serve Qwen/Qwen2.5-7B-Instruct --host 0.0.0.0 --port 8000
```

### é«˜çº§é…ç½®

```bash
vllm serve Qwen/Qwen2.5-7B-Instruct \
    --host 0.0.0.0 \
    --port 8000 \
    --tensor-parallel-size 1 \
    --gpu-memory-utilization 0.9 \
    --max-model-len 8192 \
    --max-num-seqs 256 \
    --enable-prefix-caching \
    --api-key "your-secret-key"
```

### å‚æ•°è¯´æ˜

```
å¸¸ç”¨å‚æ•°ï¼š
--model                    æ¨¡å‹è·¯å¾„æˆ–åç§°
--host                     ç›‘å¬åœ°å€
--port                     ç›‘å¬ç«¯å£
--tensor-parallel-size     GPU å¹¶è¡Œæ•°
--gpu-memory-utilization   æ˜¾å­˜ä½¿ç”¨æ¯”ä¾‹ï¼ˆ0-1ï¼‰
--max-model-len            æœ€å¤§åºåˆ—é•¿åº¦
--max-num-seqs             æœ€å¤§å¹¶å‘è¯·æ±‚æ•°
--dtype                    æ•°æ®ç±»å‹ï¼ˆauto/float16/bfloat16ï¼‰
--quantization             é‡åŒ–æ–¹å¼ï¼ˆawq/gptq/squeezellmï¼‰
--enable-prefix-caching    å¯ç”¨å‰ç¼€ç¼“å­˜
--api-key                  API å¯†é’¥
```

---

## API è°ƒç”¨

### ä½¿ç”¨ OpenAI SDK

```python
from openai import OpenAI

client = OpenAI(
    base_url="http://localhost:8000/v1",
    api_key="your-secret-key"
)

# Chat Completions
response = client.chat.completions.create(
    model="Qwen/Qwen2.5-7B-Instruct",
    messages=[
        {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªæœ‰å¸®åŠ©çš„åŠ©æ‰‹"},
        {"role": "user", "content": "ä»‹ç»ä¸€ä¸‹Python"}
    ],
    temperature=0.7,
    max_tokens=512
)

print(response.choices[0].message.content)

# æµå¼è¾“å‡º
stream = client.chat.completions.create(
    model="Qwen/Qwen2.5-7B-Instruct",
    messages=[{"role": "user", "content": "å†™ä¸€é¦–è¯—"}],
    stream=True
)

for chunk in stream:
    if chunk.choices[0].delta.content:
        print(chunk.choices[0].delta.content, end="")
```

### ç›´æ¥ HTTP è°ƒç”¨

```bash
# Chat
curl http://localhost:8000/v1/chat/completions \
    -H "Content-Type: application/json" \
    -H "Authorization: Bearer your-secret-key" \
    -d '{
        "model": "Qwen/Qwen2.5-7B-Instruct",
        "messages": [{"role": "user", "content": "ä½ å¥½"}],
        "temperature": 0.7
    }'

# è·å–æ¨¡å‹åˆ—è¡¨
curl http://localhost:8000/v1/models
```

---

## é«˜çº§åŠŸèƒ½

### é‡åŒ–æ¨¡å‹

```bash
# AWQ é‡åŒ–
vllm serve TheBloke/Qwen-7B-AWQ \
    --quantization awq \
    --dtype float16

# GPTQ é‡åŒ–
vllm serve TheBloke/Qwen-7B-GPTQ \
    --quantization gptq \
    --dtype float16
```

### å¤š GPU éƒ¨ç½²

```bash
# å¼ é‡å¹¶è¡Œï¼ˆéœ€è¦å¤š GPUï¼‰
vllm serve Qwen/Qwen2.5-72B-Instruct \
    --tensor-parallel-size 4 \
    --gpu-memory-utilization 0.9

# æµæ°´çº¿å¹¶è¡Œï¼ˆå¤§æ¨¡å‹ï¼‰
vllm serve Qwen/Qwen2.5-72B-Instruct \
    --pipeline-parallel-size 2 \
    --tensor-parallel-size 2
```

### LoRA çƒ­åŠ è½½

```bash
# å¯åŠ¨æ—¶æŒ‡å®š LoRA
vllm serve Qwen/Qwen2.5-7B-Instruct \
    --enable-lora \
    --lora-modules my-lora=./lora-weights

# åŠ¨æ€åŠ è½½ï¼ˆé€šè¿‡ APIï¼‰
```

```python
# è°ƒç”¨ç‰¹å®š LoRA
response = client.chat.completions.create(
    model="my-lora",  # ä½¿ç”¨ LoRA åç§°
    messages=[{"role": "user", "content": "ä½ å¥½"}]
)
```

### å‰ç¼€ç¼“å­˜

```bash
# å¯ç”¨å‰ç¼€ç¼“å­˜ï¼ˆé€‚åˆå›ºå®šå‰ç¼€åœºæ™¯ï¼‰
vllm serve Qwen/Qwen2.5-7B-Instruct \
    --enable-prefix-caching
```

---

## æ€§èƒ½ä¼˜åŒ–

### æ˜¾å­˜ä¼˜åŒ–

```python
from vllm import LLM

llm = LLM(
    model="Qwen/Qwen2.5-7B-Instruct",
    gpu_memory_utilization=0.95,  # æé«˜åˆ©ç”¨ç‡
    max_num_seqs=128,             # å‡å°‘å¹¶å‘
    max_model_len=4096,           # é™åˆ¶é•¿åº¦
    enforce_eager=True,           # ç¦ç”¨ CUDA Graphï¼ˆçœæ˜¾å­˜ï¼‰
)
```

### ååä¼˜åŒ–

```python
llm = LLM(
    model="Qwen/Qwen2.5-7B-Instruct",
    gpu_memory_utilization=0.9,
    max_num_seqs=256,             # å¢åŠ å¹¶å‘
    enable_chunked_prefill=True,  # åˆ†å—é¢„å¡«å……
    max_num_batched_tokens=4096,  # æ‰¹å¤„ç† token æ•°
)
```

### å»¶è¿Ÿä¼˜åŒ–

```python
llm = LLM(
    model="Qwen/Qwen2.5-7B-Instruct",
    max_num_seqs=32,              # å‡å°‘å¹¶å‘
    speculative_model="Qwen/Qwen2.5-0.5B-Instruct",  # æ¨æµ‹è§£ç 
    num_speculative_tokens=5,
)
```

---

## ç›‘æ§

### å†…ç½®æŒ‡æ ‡

```bash
# vLLM æš´éœ² Prometheus æŒ‡æ ‡
curl http://localhost:8000/metrics

# å¸¸è§æŒ‡æ ‡ï¼š
# vllm:num_requests_running - è¿è¡Œä¸­çš„è¯·æ±‚
# vllm:num_requests_waiting - ç­‰å¾…ä¸­çš„è¯·æ±‚
# vllm:gpu_cache_usage_perc - GPU ç¼“å­˜ä½¿ç”¨ç‡
# vllm:cpu_cache_usage_perc - CPU ç¼“å­˜ä½¿ç”¨ç‡
```

### Prometheus é…ç½®

```yaml
# prometheus.yml
scrape_configs:
  - job_name: 'vllm'
    static_configs:
      - targets: ['localhost:8000']
```

---

## ä¸ Ollama å¯¹æ¯”

| ç‰¹æ€§ | Ollama | vLLM |
|------|--------|------|
| å®‰è£…éš¾åº¦ | ç®€å• | éœ€è¦ CUDA |
| CPU æ”¯æŒ | âœ… | âŒ |
| ååé‡ | ä¸€èˆ¬ | å¾ˆé«˜ |
| æ˜¾å­˜æ•ˆç‡ | ä¸€èˆ¬ | å¾ˆé«˜ |
| å¹¶å‘æ”¯æŒ | æœ‰é™ | å¾ˆå¼º |
| é‡åŒ–æ”¯æŒ | GGUF | AWQ/GPTQ |
| é€‚ç”¨åœºæ™¯ | æœ¬åœ°å¼€å‘ | ç”Ÿäº§éƒ¨ç½² |

---

## â¡ï¸ ä¸‹ä¸€æ­¥

ç»§ç»­ [06-FastAPIæœåŠ¡.md](./06-FastAPIæœåŠ¡.md)

