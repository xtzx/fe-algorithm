# ğŸ¦™ Ollama å…¥é—¨

> æœ€ç®€å•çš„æœ¬åœ° LLM éƒ¨ç½²æ–¹æ¡ˆ

---

## Ollama ç®€ä»‹

```
Ollama æ˜¯ä»€ä¹ˆï¼š
- æœ¬åœ°è¿è¡Œå¤§æ¨¡å‹çš„å·¥å…·
- ç±»ä¼¼ Docker çš„æ¨¡å‹ç®¡ç†
- å¼€ç®±å³ç”¨ï¼Œæ— éœ€å¤æ‚é…ç½®
- æ”¯æŒ macOS / Linux / Windows

ç‰¹ç‚¹ï¼š
âœ… å®‰è£…ç®€å•ï¼ˆä¸€è¡Œå‘½ä»¤ï¼‰
âœ… æ¨¡å‹åº“ä¸°å¯Œ
âœ… è‡ªåŠ¨ç®¡ç†æ¨¡å‹æ–‡ä»¶
âœ… æä¾› REST API
âœ… æ”¯æŒ GPU åŠ é€Ÿ
```

---

## å®‰è£…ä¸é…ç½®

### å®‰è£…

```bash
# macOS / Linux
curl -fsSL https://ollama.com/install.sh | sh

# macOS (Homebrew)
brew install ollama

# Windows
# ä» https://ollama.com/download ä¸‹è½½å®‰è£…åŒ…

# éªŒè¯å®‰è£…
ollama --version
```

### å¯åŠ¨æœåŠ¡

```bash
# å‰å°è¿è¡Œ
ollama serve

# åå°è¿è¡Œï¼ˆLinuxï¼‰
sudo systemctl start ollama
sudo systemctl enable ollama  # å¼€æœºè‡ªå¯

# æ£€æŸ¥çŠ¶æ€
ollama list
```

### é…ç½®ï¼ˆå¯é€‰ï¼‰

```bash
# ç¯å¢ƒå˜é‡é…ç½®
export OLLAMA_HOST=0.0.0.0:11434  # ç›‘å¬åœ°å€
export OLLAMA_MODELS=/path/to/models  # æ¨¡å‹å­˜å‚¨è·¯å¾„
export OLLAMA_NUM_PARALLEL=4  # å¹¶è¡Œè¯·æ±‚æ•°
export OLLAMA_MAX_LOADED_MODELS=2  # æœ€å¤§åŠ è½½æ¨¡å‹æ•°
```

---

## æ¨¡å‹ç®¡ç†

### ä¸‹è½½æ¨¡å‹

```bash
# ä¸‹è½½æ¨¡å‹ï¼ˆè‡ªåŠ¨é€‰æ‹©åˆé€‚çš„é‡åŒ–ç‰ˆæœ¬ï¼‰
ollama pull llama3.1:8b
ollama pull qwen2.5:7b
ollama pull mistral:7b

# æŒ‡å®šç‰ˆæœ¬
ollama pull qwen2.5:7b-instruct-q4_K_M
ollama pull llama3.1:70b-instruct-q4_K_M

# å¸¸ç”¨æ¨¡å‹
ollama pull llama3.1       # Meta Llama 3.1
ollama pull qwen2.5        # é€šä¹‰åƒé—®
ollama pull mistral        # Mistral AI
ollama pull codellama      # ä»£ç ç”Ÿæˆ
ollama pull nomic-embed-text  # Embedding
```

### ç®¡ç†å‘½ä»¤

```bash
# æŸ¥çœ‹å·²ä¸‹è½½çš„æ¨¡å‹
ollama list

# æŸ¥çœ‹æ¨¡å‹è¯¦æƒ…
ollama show qwen2.5:7b

# åˆ é™¤æ¨¡å‹
ollama rm qwen2.5:7b

# å¤åˆ¶æ¨¡å‹ï¼ˆåˆ›å»ºåˆ«åï¼‰
ollama cp qwen2.5:7b my-model
```

### æ¨¡å‹åº“

```
å®˜æ–¹æ¨¡å‹åº“ï¼šhttps://ollama.com/library

çƒ­é—¨æ¨¡å‹ï¼š
- llama3.1: 8b/70b/405b
- qwen2.5: 0.5b/1.5b/3b/7b/14b/32b/72b
- mistral: 7b
- phi3: 3.8b/14b
- gemma2: 2b/9b/27b
- codellama: 7b/13b/34b
- deepseek-coder: 1.3b/6.7b/33b
```

---

## å‘½ä»¤è¡Œä½¿ç”¨

### äº¤äº’å¯¹è¯

```bash
# å¯åŠ¨å¯¹è¯
ollama run qwen2.5:7b

>>> ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹è‡ªå·±
æˆ‘æ˜¯é€šä¹‰åƒé—®ï¼Œä¸€ä¸ªç”±é˜¿é‡Œäº‘å¼€å‘çš„AIåŠ©æ‰‹...

>>> /bye  # é€€å‡º
```

### å•æ¬¡è°ƒç”¨

```bash
# ç›´æ¥è·å–å›ç­”
ollama run qwen2.5:7b "ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ"

# ä»æ–‡ä»¶è¯»å–
ollama run qwen2.5:7b < prompt.txt

# è¾“å‡ºåˆ°æ–‡ä»¶
ollama run qwen2.5:7b "å†™ä¸€é¦–è¯—" > output.txt
```

### å‚æ•°è®¾ç½®

```bash
# è®¾ç½®æ¸©åº¦å’Œ token æ•°
ollama run qwen2.5:7b --temperature 0.7 --num-predict 200 "è®²ä¸ªæ•…äº‹"

# è®¾ç½®ç³»ç»Ÿæç¤º
ollama run qwen2.5:7b --system "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„Pythonæ•™å¸ˆ"
```

---

## REST API

### åŸºæœ¬è°ƒç”¨

```bash
# ç”Ÿæˆï¼ˆéæµå¼ï¼‰
curl http://localhost:11434/api/generate -d '{
  "model": "qwen2.5:7b",
  "prompt": "ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Ÿ",
  "stream": false
}'

# ç”Ÿæˆï¼ˆæµå¼ï¼‰
curl http://localhost:11434/api/generate -d '{
  "model": "qwen2.5:7b",
  "prompt": "ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Ÿ",
  "stream": true
}'

# å¯¹è¯
curl http://localhost:11434/api/chat -d '{
  "model": "qwen2.5:7b",
  "messages": [
    {"role": "user", "content": "ä½ å¥½"}
  ],
  "stream": false
}'
```

### Python è°ƒç”¨

```python
import requests
import json

OLLAMA_URL = "http://localhost:11434"

def generate(prompt: str, model: str = "qwen2.5:7b") -> str:
    """éæµå¼ç”Ÿæˆ"""
    response = requests.post(
        f"{OLLAMA_URL}/api/generate",
        json={
            "model": model,
            "prompt": prompt,
            "stream": False
        }
    )
    return response.json()["response"]

def generate_stream(prompt: str, model: str = "qwen2.5:7b"):
    """æµå¼ç”Ÿæˆ"""
    response = requests.post(
        f"{OLLAMA_URL}/api/generate",
        json={
            "model": model,
            "prompt": prompt,
            "stream": True
        },
        stream=True
    )

    for line in response.iter_lines():
        if line:
            data = json.loads(line)
            if not data.get("done"):
                yield data["response"]

def chat(messages: list, model: str = "qwen2.5:7b") -> str:
    """å¯¹è¯"""
    response = requests.post(
        f"{OLLAMA_URL}/api/chat",
        json={
            "model": model,
            "messages": messages,
            "stream": False
        }
    )
    return response.json()["message"]["content"]

# ä½¿ç”¨
print(generate("1+1ç­‰äºå‡ ï¼Ÿ"))

for chunk in generate_stream("å†™ä¸€é¦–çŸ­è¯—"):
    print(chunk, end="", flush=True)

result = chat([
    {"role": "user", "content": "ä½ å¥½"},
    {"role": "assistant", "content": "ä½ å¥½ï¼æœ‰ä»€ä¹ˆå¯ä»¥å¸®ä½ çš„ï¼Ÿ"},
    {"role": "user", "content": "ä»Šå¤©å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿ"}
])
print(result)
```

### Embedding

```python
def get_embedding(text: str, model: str = "nomic-embed-text") -> list:
    """è·å–æ–‡æœ¬ embedding"""
    response = requests.post(
        f"{OLLAMA_URL}/api/embeddings",
        json={
            "model": model,
            "prompt": text
        }
    )
    return response.json()["embedding"]

embedding = get_embedding("Hello, world!")
print(f"ç»´åº¦: {len(embedding)}")  # 768
```

---

## OpenAI å…¼å®¹æ¥å£

```
Ollama æä¾› OpenAI å…¼å®¹çš„ APIï¼š
- /v1/chat/completions
- /v1/completions
- /v1/embeddings

å¯ä»¥ç›´æ¥ä½¿ç”¨ openai SDKï¼
```

```python
from openai import OpenAI

# æŒ‡å‘ Ollama
client = OpenAI(
    base_url="http://localhost:11434/v1",
    api_key="ollama"  # ä»»æ„å€¼å³å¯
)

# å¯¹è¯
response = client.chat.completions.create(
    model="qwen2.5:7b",
    messages=[
        {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªæœ‰å¸®åŠ©çš„åŠ©æ‰‹"},
        {"role": "user", "content": "ä½ å¥½"}
    ],
    temperature=0.7
)

print(response.choices[0].message.content)

# æµå¼
stream = client.chat.completions.create(
    model="qwen2.5:7b",
    messages=[{"role": "user", "content": "è®²ä¸ªæ•…äº‹"}],
    stream=True
)

for chunk in stream:
    if chunk.choices[0].delta.content:
        print(chunk.choices[0].delta.content, end="")
```

---

## è‡ªå®šä¹‰ Modelfile

### åˆ›å»ºè‡ªå®šä¹‰æ¨¡å‹

```dockerfile
# Modelfile
FROM qwen2.5:7b

# è®¾ç½®å‚æ•°
PARAMETER temperature 0.7
PARAMETER top_p 0.9
PARAMETER num_ctx 4096

# è®¾ç½®ç³»ç»Ÿæç¤º
SYSTEM """
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„Pythonç¼–ç¨‹åŠ©æ‰‹ã€‚
- å›ç­”è¦ç®€æ´å‡†ç¡®
- ä»£ç è¦æœ‰æ³¨é‡Š
- é‡åˆ°ä¸ç¡®å®šçš„é—®é¢˜è¦è¯´æ˜
"""

# æ·»åŠ æ¨¡æ¿ï¼ˆå¯é€‰ï¼‰
TEMPLATE """{{ if .System }}<|im_start|>system
{{ .System }}<|im_end|>
{{ end }}{{ if .Prompt }}<|im_start|>user
{{ .Prompt }}<|im_end|>
{{ end }}<|im_start|>assistant
"""
```

```bash
# æ„å»ºæ¨¡å‹
ollama create python-assistant -f Modelfile

# è¿è¡Œ
ollama run python-assistant "å¦‚ä½•è¯»å–CSVæ–‡ä»¶ï¼Ÿ"

# æŸ¥çœ‹
ollama show python-assistant
```

### å¯¼å…¥ GGUF æ¨¡å‹

```dockerfile
# ä»æœ¬åœ° GGUF æ–‡ä»¶åˆ›å»º
FROM ./my-model.gguf

PARAMETER temperature 0.7
SYSTEM "ä½ æ˜¯ä¸€ä¸ªAIåŠ©æ‰‹"
```

```bash
ollama create my-model -f Modelfile
ollama run my-model
```

---

## æœ€ä½³å®è·µ

```
1. æ¨¡å‹é€‰æ‹©
   - å¼€å‘æµ‹è¯•ï¼šå°æ¨¡å‹ï¼ˆ1.5B-7Bï¼‰
   - ç”Ÿäº§ï¼šæ ¹æ®éœ€æ±‚é€‰æ‹©ï¼ˆ7B-72Bï¼‰
   - Embeddingï¼šnomic-embed-text

2. æ€§èƒ½ä¼˜åŒ–
   - ä½¿ç”¨ SSD å­˜å‚¨æ¨¡å‹
   - é¢„çƒ­æ¨¡å‹ï¼ˆé¦–æ¬¡åŠ è½½è¾ƒæ…¢ï¼‰
   - åˆç†è®¾ç½® num_ctx

3. èµ„æºç®¡ç†
   - é™åˆ¶åŠ è½½çš„æ¨¡å‹æ•°é‡
   - ç›‘æ§æ˜¾å­˜ä½¿ç”¨
   - å®šæœŸæ¸…ç†ä¸ç”¨çš„æ¨¡å‹

4. ç”Ÿäº§éƒ¨ç½²
   - é…ç½®ä¸ºç³»ç»ŸæœåŠ¡
   - è®¾ç½®åˆé€‚çš„ç›‘å¬åœ°å€
   - æ·»åŠ åå‘ä»£ç†ï¼ˆNginxï¼‰
```

---

## â¡ï¸ ä¸‹ä¸€æ­¥

ç»§ç»­ [05-vLLMé«˜åå.md](./05-vLLMé«˜åå.md)

