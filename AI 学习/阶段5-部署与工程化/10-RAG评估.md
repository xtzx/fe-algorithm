# ğŸ“ˆ RAG è¯„ä¼°

> ä½¿ç”¨ Ragas è¯„ä¼° RAG ç³»ç»Ÿè´¨é‡

---

## RAG è¯„ä¼°æŒ‡æ ‡

### æ ¸å¿ƒæŒ‡æ ‡

```
1. Faithfulnessï¼ˆå¿ å®åº¦ï¼‰
   - ç­”æ¡ˆæ˜¯å¦å¿ å®äºæ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡
   - æ˜¯å¦å­˜åœ¨å¹»è§‰

2. Answer Relevanceï¼ˆç­”æ¡ˆç›¸å…³æ€§ï¼‰
   - ç­”æ¡ˆæ˜¯å¦ç›¸å…³äºé—®é¢˜
   - æ˜¯å¦å›ç­”äº†ç”¨æˆ·çš„å®é™…é—®é¢˜

3. Context Precisionï¼ˆä¸Šä¸‹æ–‡ç²¾ç¡®åº¦ï¼‰
   - æ£€ç´¢çš„ä¸Šä¸‹æ–‡æ˜¯å¦ç²¾ç¡®
   - æ˜¯å¦åŒ…å«æ— å…³ä¿¡æ¯

4. Context Recallï¼ˆä¸Šä¸‹æ–‡å¬å›ç‡ï¼‰
   - æ£€ç´¢æ˜¯å¦è¦†ç›–äº†å›ç­”é—®é¢˜æ‰€éœ€çš„ä¿¡æ¯
   - æ˜¯å¦é—æ¼é‡è¦å†…å®¹

5. Answer Correctnessï¼ˆç­”æ¡ˆæ­£ç¡®æ€§ï¼‰
   - ç­”æ¡ˆæ˜¯å¦æ­£ç¡®
   - ä¸æ ‡å‡†ç­”æ¡ˆçš„ä¸€è‡´æ€§
```

---

## Ragas æ¡†æ¶

### å®‰è£…

```bash
pip install ragas datasets
```

### åŸºç¡€ä½¿ç”¨

```python
from ragas import evaluate
from ragas.metrics import (
    faithfulness,
    answer_relevancy,
    context_precision,
    context_recall,
    answer_correctness
)
from datasets import Dataset

# å‡†å¤‡è¯„ä¼°æ•°æ®
eval_data = {
    "question": [
        "ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ",
        "Python æœ‰ä»€ä¹ˆç‰¹ç‚¹ï¼Ÿ"
    ],
    "answer": [
        "æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œé€šè¿‡æ•°æ®è®­ç»ƒæ¨¡å‹æ¥åšé¢„æµ‹ã€‚",
        "Python æ˜¯ä¸€ç§ç®€æ´æ˜“è¯»çš„ç¼–ç¨‹è¯­è¨€ï¼Œå¹¿æ³›ç”¨äºæ•°æ®ç§‘å­¦å’Œ AIã€‚"
    ],
    "contexts": [
        ["æœºå™¨å­¦ä¹ æ˜¯AIçš„å­é¢†åŸŸï¼Œä¸“æ³¨äºè®©è®¡ç®—æœºä»æ•°æ®ä¸­å­¦ä¹ ã€‚"],
        ["Python ä»¥ç®€æ´çš„è¯­æ³•è‘—ç§°ï¼Œæ˜¯æ•°æ®ç§‘å­¦é¦–é€‰è¯­è¨€ã€‚"]
    ],
    "ground_truth": [
        "æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œä½¿è®¡ç®—æœºèƒ½å¤Ÿä»æ•°æ®ä¸­å­¦ä¹ å¹¶æ”¹è¿›ã€‚",
        "Python æ˜¯ä¸€ç§é«˜çº§ç¼–ç¨‹è¯­è¨€ï¼Œä»¥ç®€æ´å’Œæ˜“è¯»æ€§è‘—ç§°ã€‚"
    ]
}

dataset = Dataset.from_dict(eval_data)

# è¿è¡Œè¯„ä¼°
results = evaluate(
    dataset,
    metrics=[
        faithfulness,
        answer_relevancy,
        context_precision,
        context_recall,
        answer_correctness
    ]
)

print(results)
# {'faithfulness': 0.95, 'answer_relevancy': 0.88, ...}
```

### è¯„ä¼°å•ä¸ªæ ·æœ¬

```python
from ragas.metrics import faithfulness, answer_relevancy

# å•æ ·æœ¬è¯„ä¼°
sample = {
    "question": "ä»€ä¹ˆæ˜¯æ·±åº¦å­¦ä¹ ï¼Ÿ",
    "answer": "æ·±åº¦å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„å­é›†ï¼Œä½¿ç”¨å¤šå±‚ç¥ç»ç½‘ç»œã€‚",
    "contexts": ["æ·±åº¦å­¦ä¹ ä½¿ç”¨å¤šå±‚ç¥ç»ç½‘ç»œå¤„ç†å¤æ‚ä»»åŠ¡ã€‚"]
}

# è®¡ç®—å¿ å®åº¦
faith_score = faithfulness.score(sample)
print(f"Faithfulness: {faith_score}")

# è®¡ç®—ç­”æ¡ˆç›¸å…³æ€§
relevancy_score = answer_relevancy.score(sample)
print(f"Answer Relevancy: {relevancy_score}")
```

---

## å®Œæ•´è¯„ä¼°æµç¨‹

### è¯„ä¼°è„šæœ¬

```python
"""rag_evaluation.py - RAG ç³»ç»Ÿè¯„ä¼°"""
from ragas import evaluate
from ragas.metrics import (
    faithfulness,
    answer_relevancy,
    context_precision,
    context_recall
)
from datasets import Dataset
import json
from typing import List, Dict
from datetime import datetime

class RAGEvaluator:
    """RAG è¯„ä¼°å™¨"""

    def __init__(self, rag_system):
        self.rag = rag_system
        self.metrics = [
            faithfulness,
            answer_relevancy,
            context_precision,
            context_recall
        ]

    def prepare_dataset(self, test_cases: List[Dict]) -> Dataset:
        """å‡†å¤‡è¯„ä¼°æ•°æ®é›†"""
        questions = []
        answers = []
        contexts = []
        ground_truths = []

        for case in test_cases:
            # è°ƒç”¨ RAG ç³»ç»Ÿ
            result = self.rag.query(case["question"])

            questions.append(case["question"])
            answers.append(result["answer"])
            contexts.append([s["content"] for s in result["sources"]])
            ground_truths.append(case.get("ground_truth", ""))

        return Dataset.from_dict({
            "question": questions,
            "answer": answers,
            "contexts": contexts,
            "ground_truth": ground_truths
        })

    def evaluate(self, test_cases: List[Dict]) -> Dict:
        """è¿è¡Œè¯„ä¼°"""
        print(f"è¯„ä¼° {len(test_cases)} ä¸ªæµ‹è¯•ç”¨ä¾‹...")

        dataset = self.prepare_dataset(test_cases)
        results = evaluate(dataset, metrics=self.metrics)

        return {
            "timestamp": datetime.now().isoformat(),
            "num_samples": len(test_cases),
            "scores": dict(results),
            "per_sample": results.to_pandas().to_dict("records")
        }

    def save_results(self, results: Dict, path: str):
        """ä¿å­˜è¯„ä¼°ç»“æœ"""
        with open(path, "w", encoding="utf-8") as f:
            json.dump(results, f, ensure_ascii=False, indent=2)


# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    from rag_engine import RAGEngine

    # æµ‹è¯•ç”¨ä¾‹
    test_cases = [
        {
            "question": "ä»€ä¹ˆæ˜¯ RAGï¼Ÿ",
            "ground_truth": "RAG æ˜¯æ£€ç´¢å¢å¼ºç”Ÿæˆï¼Œé€šè¿‡æ£€ç´¢å¤–éƒ¨çŸ¥è¯†æ¥å¢å¼º LLM çš„å›ç­”ã€‚"
        },
        {
            "question": "LangChain æ˜¯ä»€ä¹ˆï¼Ÿ",
            "ground_truth": "LangChain æ˜¯ä¸€ä¸ªç”¨äºæ„å»º LLM åº”ç”¨çš„å¼€å‘æ¡†æ¶ã€‚"
        },
        # ... æ›´å¤šæµ‹è¯•ç”¨ä¾‹
    ]

    # è¯„ä¼°
    rag = RAGEngine()
    evaluator = RAGEvaluator(rag)
    results = evaluator.evaluate(test_cases)

    # è¾“å‡ºç»“æœ
    print("\nè¯„ä¼°ç»“æœï¼š")
    for metric, score in results["scores"].items():
        print(f"  {metric}: {score:.4f}")

    # ä¿å­˜
    evaluator.save_results(results, "eval_results.json")
```

### æ‰¹é‡è¯„ä¼°

```python
from concurrent.futures import ThreadPoolExecutor
import pandas as pd

def batch_evaluate(rag, test_cases: List[Dict], batch_size: int = 10):
    """æ‰¹é‡è¯„ä¼°"""
    results = []

    for i in range(0, len(test_cases), batch_size):
        batch = test_cases[i:i+batch_size]

        with ThreadPoolExecutor(max_workers=5) as executor:
            batch_results = list(executor.map(
                lambda case: {
                    "question": case["question"],
                    **rag.query(case["question"])
                },
                batch
            ))

        results.extend(batch_results)
        print(f"è¿›åº¦: {min(i+batch_size, len(test_cases))}/{len(test_cases)}")

    return results
```

---

## è‡ªå®šä¹‰è¯„ä¼°æŒ‡æ ‡

### è‡ªå®šä¹‰æŒ‡æ ‡

```python
from ragas.metrics.base import MetricWithLLM
from dataclasses import dataclass

@dataclass
class CustomMetric(MetricWithLLM):
    name: str = "custom_metric"

    def score(self, sample: Dict) -> float:
        """è‡ªå®šä¹‰è¯„åˆ†é€»è¾‘"""
        question = sample["question"]
        answer = sample["answer"]

        # ç¤ºä¾‹ï¼šæ£€æŸ¥ç­”æ¡ˆé•¿åº¦
        if len(answer) < 10:
            return 0.0
        elif len(answer) > 500:
            return 0.5
        else:
            return 1.0

# ç®€å•çš„è¯„ä¼°å‡½æ•°
def evaluate_answer_length(answer: str) -> float:
    """è¯„ä¼°ç­”æ¡ˆé•¿åº¦æ˜¯å¦åˆé€‚"""
    if len(answer) < 20:
        return 0.3
    elif len(answer) < 50:
        return 0.7
    elif len(answer) < 500:
        return 1.0
    else:
        return 0.8

def evaluate_has_source(answer: str, sources: list) -> float:
    """è¯„ä¼°æ˜¯å¦å¼•ç”¨äº†æ¥æº"""
    return 1.0 if sources else 0.0
```

### åŸºäº LLM çš„è¯„ä¼°

```python
from openai import OpenAI

client = OpenAI()

def llm_evaluate(question: str, answer: str, criteria: str) -> float:
    """ä½¿ç”¨ LLM è¿›è¡Œè¯„ä¼°"""

    prompt = f"""è¯·è¯„ä¼°ä»¥ä¸‹å›ç­”çš„è´¨é‡ã€‚

é—®é¢˜ï¼š{question}
å›ç­”ï¼š{answer}

è¯„ä¼°æ ‡å‡†ï¼š{criteria}

è¯·ç»™å‡º 1-10 çš„åˆ†æ•°ï¼Œåªè¾“å‡ºæ•°å­—ã€‚
"""

    response = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[{"role": "user", "content": prompt}],
        temperature=0
    )

    try:
        score = float(response.choices[0].message.content.strip())
        return score / 10
    except:
        return 0.5

# ä½¿ç”¨
score = llm_evaluate(
    question="ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ",
    answer="æœºå™¨å­¦ä¹ æ˜¯ AI çš„ä¸€ä¸ªåˆ†æ”¯...",
    criteria="å›ç­”æ˜¯å¦å‡†ç¡®ã€å®Œæ•´ã€æ˜“æ‡‚"
)
```

---

## è¯„ä¼°æŠ¥å‘Š

```python
import pandas as pd
import matplotlib.pyplot as plt

def generate_report(results: Dict, output_path: str = "eval_report.html"):
    """ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š"""

    # æ€»ä½“åˆ†æ•°
    overall_scores = results["scores"]

    # æ¯ä¸ªæ ·æœ¬çš„åˆ†æ•°
    per_sample = pd.DataFrame(results["per_sample"])

    # ç”Ÿæˆ HTML æŠ¥å‘Š
    html = f"""
    <html>
    <head>
        <title>RAG è¯„ä¼°æŠ¥å‘Š</title>
        <style>
            body {{ font-family: Arial, sans-serif; margin: 40px; }}
            h1 {{ color: #333; }}
            .metric {{
                display: inline-block;
                margin: 10px;
                padding: 20px;
                background: #f5f5f5;
                border-radius: 8px;
            }}
            .score {{ font-size: 24px; font-weight: bold; color: #2196F3; }}
            table {{ border-collapse: collapse; width: 100%; }}
            th, td {{ border: 1px solid #ddd; padding: 8px; text-align: left; }}
            th {{ background-color: #4CAF50; color: white; }}
        </style>
    </head>
    <body>
        <h1>RAG è¯„ä¼°æŠ¥å‘Š</h1>
        <p>æ—¶é—´: {results['timestamp']}</p>
        <p>æ ·æœ¬æ•°: {results['num_samples']}</p>

        <h2>æ€»ä½“åˆ†æ•°</h2>
        <div>
    """

    for metric, score in overall_scores.items():
        html += f"""
            <div class="metric">
                <div>{metric}</div>
                <div class="score">{score:.2%}</div>
            </div>
        """

    html += """
        </div>

        <h2>è¯¦ç»†ç»“æœ</h2>
        <table>
            <tr>
                <th>é—®é¢˜</th>
                <th>Faithfulness</th>
                <th>Relevancy</th>
                <th>Precision</th>
                <th>Recall</th>
            </tr>
    """

    for _, row in per_sample.iterrows():
        html += f"""
            <tr>
                <td>{row.get('question', '')[:50]}...</td>
                <td>{row.get('faithfulness', 0):.2f}</td>
                <td>{row.get('answer_relevancy', 0):.2f}</td>
                <td>{row.get('context_precision', 0):.2f}</td>
                <td>{row.get('context_recall', 0):.2f}</td>
            </tr>
        """

    html += """
        </table>
    </body>
    </html>
    """

    with open(output_path, "w", encoding="utf-8") as f:
        f.write(html)

    print(f"æŠ¥å‘Šå·²ä¿å­˜åˆ° {output_path}")
```

---

## æŒç»­è¯„ä¼°

```python
"""å®šæœŸè¯„ä¼° RAG ç³»ç»Ÿ"""
import schedule
import time

def daily_evaluation():
    """æ¯æ—¥è¯„ä¼°ä»»åŠ¡"""
    # åŠ è½½æµ‹è¯•é›†
    test_cases = load_test_cases("test_cases.json")

    # è¿è¡Œè¯„ä¼°
    evaluator = RAGEvaluator(rag_system)
    results = evaluator.evaluate(test_cases)

    # ä¿å­˜ç»“æœ
    date_str = datetime.now().strftime("%Y%m%d")
    evaluator.save_results(results, f"eval_{date_str}.json")

    # æ£€æŸ¥è´¨é‡ä¸‹é™
    for metric, score in results["scores"].items():
        if score < 0.7:
            send_alert(f"è­¦å‘Š: {metric} åˆ†æ•°è¿‡ä½: {score:.2f}")

# æ¯å¤©å‡Œæ™¨ 2 ç‚¹è¿è¡Œ
schedule.every().day.at("02:00").do(daily_evaluation)

while True:
    schedule.run_pending()
    time.sleep(60)
```

---

## â¡ï¸ ä¸‹ä¸€æ­¥

ç»§ç»­ [11-å®‰å…¨é˜²æŠ¤.md](./11-å®‰å…¨é˜²æŠ¤.md)

