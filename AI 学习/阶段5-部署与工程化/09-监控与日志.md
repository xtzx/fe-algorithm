# ğŸ“Š 09 - ç›‘æ§ä¸æ—¥å¿—

> æ—¥å¿—è®°å½•ã€æŒ‡æ ‡ç›‘æ§ä¸è°ƒç”¨é“¾è¿½è¸ª

---

## æ—¥å¿—è®°å½•

### ç»“æ„åŒ–æ—¥å¿—

```python
"""ä½¿ç”¨ structlog è¿›è¡Œç»“æ„åŒ–æ—¥å¿—"""
import structlog
from datetime import datetime
import json

# é…ç½® structlog
structlog.configure(
    processors=[
        structlog.stdlib.filter_by_level,
        structlog.stdlib.add_logger_name,
        structlog.stdlib.add_log_level,
        structlog.processors.TimeStamper(fmt="iso"),
        structlog.processors.JSONRenderer()
    ],
    wrapper_class=structlog.stdlib.BoundLogger,
    context_class=dict,
    logger_factory=structlog.stdlib.LoggerFactory(),
)

logger = structlog.get_logger()

# ä½¿ç”¨
logger.info("è¯·æ±‚å¼€å§‹",
    request_id="req-123",
    user="test_user",
    model="qwen2.5:7b"
)

logger.info("ç”Ÿæˆå®Œæˆ",
    request_id="req-123",
    prompt_tokens=100,
    completion_tokens=50,
    latency_ms=1500
)
```

### FastAPI è¯·æ±‚æ—¥å¿—ä¸­é—´ä»¶

```python
from fastapi import FastAPI, Request
import time
import uuid
import structlog

app = FastAPI()
logger = structlog.get_logger()

@app.middleware("http")
async def log_requests(request: Request, call_next):
    """è®°å½•æ‰€æœ‰è¯·æ±‚"""
    request_id = str(uuid.uuid4())[:8]
    start_time = time.time()

    # ç»‘å®šè¯·æ±‚ä¸Šä¸‹æ–‡
    log = logger.bind(
        request_id=request_id,
        method=request.method,
        path=request.url.path,
        client_ip=request.client.host
    )

    log.info("request_started")

    # å¤„ç†è¯·æ±‚
    response = await call_next(request)

    # è®¡ç®—å»¶è¿Ÿ
    latency_ms = (time.time() - start_time) * 1000

    log.info(
        "request_completed",
        status_code=response.status_code,
        latency_ms=round(latency_ms, 2)
    )

    # æ·»åŠ è¯·æ±‚ ID åˆ°å“åº”å¤´
    response.headers["X-Request-ID"] = request_id

    return response
```

### LLM è°ƒç”¨æ—¥å¿—

```python
from dataclasses import dataclass, asdict
from datetime import datetime
import json

@dataclass
class LLMCallLog:
    request_id: str
    timestamp: str
    model: str
    prompt_tokens: int
    completion_tokens: int
    total_tokens: int
    latency_ms: float
    user: str
    success: bool
    error: str = None

class LLMLogger:
    def __init__(self, log_file: str = "llm_calls.jsonl"):
        self.log_file = log_file

    def log(self, call_log: LLMCallLog):
        """è®°å½• LLM è°ƒç”¨"""
        with open(self.log_file, "a") as f:
            f.write(json.dumps(asdict(call_log), ensure_ascii=False) + "\n")

    def get_stats(self, hours: int = 24):
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        # è¯»å–æ—¥å¿—å¹¶ç»Ÿè®¡
        total_calls = 0
        total_tokens = 0
        total_latency = 0
        errors = 0

        # ... å®ç°ç»Ÿè®¡é€»è¾‘

        return {
            "total_calls": total_calls,
            "total_tokens": total_tokens,
            "avg_latency_ms": total_latency / total_calls if total_calls else 0,
            "error_rate": errors / total_calls if total_calls else 0
        }

# ä½¿ç”¨
llm_logger = LLMLogger()

async def chat_with_logging(request, user):
    start = time.time()
    request_id = str(uuid.uuid4())[:8]

    try:
        response = await call_llm(request)

        llm_logger.log(LLMCallLog(
            request_id=request_id,
            timestamp=datetime.now().isoformat(),
            model=request.model,
            prompt_tokens=response.usage.prompt_tokens,
            completion_tokens=response.usage.completion_tokens,
            total_tokens=response.usage.total_tokens,
            latency_ms=(time.time() - start) * 1000,
            user=user,
            success=True
        ))

        return response
    except Exception as e:
        llm_logger.log(LLMCallLog(
            request_id=request_id,
            timestamp=datetime.now().isoformat(),
            model=request.model,
            prompt_tokens=0,
            completion_tokens=0,
            total_tokens=0,
            latency_ms=(time.time() - start) * 1000,
            user=user,
            success=False,
            error=str(e)
        ))
        raise
```

---

## Prometheus æŒ‡æ ‡

### å®šä¹‰æŒ‡æ ‡

```python
from prometheus_client import Counter, Histogram, Gauge, generate_latest
from fastapi import FastAPI, Response

app = FastAPI()

# å®šä¹‰æŒ‡æ ‡
REQUEST_COUNT = Counter(
    "llm_requests_total",
    "Total LLM requests",
    ["model", "status"]
)

REQUEST_LATENCY = Histogram(
    "llm_request_latency_seconds",
    "LLM request latency",
    ["model"],
    buckets=[0.1, 0.5, 1.0, 2.0, 5.0, 10.0, 30.0]
)

TOKENS_USED = Counter(
    "llm_tokens_total",
    "Total tokens used",
    ["model", "type"]  # type: prompt/completion
)

ACTIVE_REQUESTS = Gauge(
    "llm_active_requests",
    "Number of active requests"
)

# æš´éœ²æŒ‡æ ‡ç«¯ç‚¹
@app.get("/metrics")
async def metrics():
    return Response(
        content=generate_latest(),
        media_type="text/plain"
    )
```

### è®°å½•æŒ‡æ ‡

```python
import time

async def chat_completions(request: ChatRequest):
    ACTIVE_REQUESTS.inc()
    start_time = time.time()

    try:
        response = await call_llm(request)

        # è®°å½•æˆåŠŸ
        REQUEST_COUNT.labels(model=request.model, status="success").inc()
        REQUEST_LATENCY.labels(model=request.model).observe(time.time() - start_time)
        TOKENS_USED.labels(model=request.model, type="prompt").inc(response.usage.prompt_tokens)
        TOKENS_USED.labels(model=request.model, type="completion").inc(response.usage.completion_tokens)

        return response
    except Exception as e:
        REQUEST_COUNT.labels(model=request.model, status="error").inc()
        raise
    finally:
        ACTIVE_REQUESTS.dec()
```

### Prometheus é…ç½®

```yaml
# prometheus.yml
global:
  scrape_interval: 15s

scrape_configs:
  - job_name: 'llm-api'
    static_configs:
      - targets: ['api:8000']
    metrics_path: /metrics
```

---

## LangSmith è¿½è¸ª

### é…ç½®

```bash
# ç¯å¢ƒå˜é‡
export LANGCHAIN_TRACING_V2=true
export LANGCHAIN_API_KEY=lsv2_xxx
export LANGCHAIN_PROJECT=my-llm-project
```

### ä½¿ç”¨

```python
from langchain_openai import ChatOpenAI
from langchain.callbacks import tracing_enabled

llm = ChatOpenAI(model="gpt-4o-mini")

# è‡ªåŠ¨è¿½è¸ªæ‰€æœ‰ LangChain è°ƒç”¨
with tracing_enabled():
    response = llm.invoke("Hello")

# æˆ–åœ¨åº”ç”¨å¯åŠ¨æ—¶å…¨å±€å¯ç”¨
import langchain
langchain.debug = True
```

### æ‰‹åŠ¨è¿½è¸ª

```python
from langsmith import Client
from langsmith.run_helpers import traceable

client = Client()

@traceable(name="chat_completion")
def chat_with_trace(messages: list, model: str):
    """å¸¦è¿½è¸ªçš„èŠå¤©å‡½æ•°"""
    response = client.chat.completions.create(
        model=model,
        messages=messages
    )
    return response.choices[0].message.content

# ä½¿ç”¨
result = chat_with_trace(
    messages=[{"role": "user", "content": "Hello"}],
    model="gpt-4o-mini"
)
```

---

## LangFuse è¿½è¸ª

### é…ç½®

```python
from langfuse import Langfuse

langfuse = Langfuse(
    public_key="pk-xxx",
    secret_key="sk-xxx",
    host="https://cloud.langfuse.com"  # æˆ–è‡ªæ‰˜ç®¡åœ°å€
)
```

### è¿½è¸ª LLM è°ƒç”¨

```python
from langfuse.decorators import observe, langfuse_context

@observe()
def chat_completion(messages: list, model: str):
    """å¸¦è¿½è¸ªçš„èŠå¤©"""
    # è®°å½•è¾“å…¥
    langfuse_context.update_current_observation(
        input=messages,
        model=model
    )

    response = call_llm(messages, model)

    # è®°å½•è¾“å‡ºå’Œ token ä½¿ç”¨
    langfuse_context.update_current_observation(
        output=response.content,
        usage={
            "input": response.usage.prompt_tokens,
            "output": response.usage.completion_tokens
        }
    )

    return response.content

@observe()
def rag_query(question: str):
    """è¿½è¸ªæ•´ä¸ª RAG æµç¨‹"""
    # æ£€ç´¢
    with langfuse_context.observation(name="retrieval") as obs:
        docs = retrieve(question)
        obs.set_output(docs)

    # ç”Ÿæˆ
    with langfuse_context.observation(name="generation") as obs:
        answer = generate(question, docs)
        obs.set_output(answer)

    return answer
```

### è¯„ä¼°é›†æˆ

```python
# åˆ›å»ºè¯„ä¼°æ•°æ®é›†
langfuse.create_dataset(
    name="qa_eval",
    description="QA evaluation dataset"
)

# æ·»åŠ æµ‹è¯•ç”¨ä¾‹
langfuse.create_dataset_item(
    dataset_name="qa_eval",
    input={"question": "ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ"},
    expected_output="æœºå™¨å­¦ä¹ æ˜¯..."
)

# è¿è¡Œè¯„ä¼°
from langfuse import Langfuse

def run_evaluation():
    dataset = langfuse.get_dataset("qa_eval")

    for item in dataset.items:
        result = chat_completion(item.input["question"])

        # è®°å½•è¯„ä¼°ç»“æœ
        item.create_observation(
            output=result,
            scores=[
                {"name": "relevance", "value": 0.9}
            ]
        )
```

---

## ç®€å•ç›‘æ§é¢æ¿

```python
"""ç®€å•çš„ç›‘æ§ API"""
from fastapi import FastAPI
from collections import defaultdict
from datetime import datetime, timedelta

app = FastAPI()

# å†…å­˜ä¸­çš„ç»Ÿè®¡æ•°æ®
stats = {
    "requests": defaultdict(int),
    "tokens": defaultdict(int),
    "latencies": [],
    "errors": []
}

@app.get("/admin/stats")
async def get_stats():
    """è·å–ç»Ÿè®¡ä¿¡æ¯"""
    recent_latencies = [l for l in stats["latencies"] if l["time"] > datetime.now() - timedelta(hours=1)]

    return {
        "total_requests": sum(stats["requests"].values()),
        "total_tokens": sum(stats["tokens"].values()),
        "avg_latency_ms": sum(l["value"] for l in recent_latencies) / len(recent_latencies) if recent_latencies else 0,
        "requests_by_model": dict(stats["requests"]),
        "recent_errors": stats["errors"][-10:]
    }

@app.get("/admin/health")
async def health_check():
    """å¥åº·æ£€æŸ¥"""
    return {
        "status": "healthy",
        "timestamp": datetime.now().isoformat(),
        "active_requests": ACTIVE_REQUESTS._value.get()
    }
```

---

## â¡ï¸ ä¸‹ä¸€æ­¥

ç»§ç»­ [10-RAGè¯„ä¼°.md](./10-RAGè¯„ä¼°.md)

