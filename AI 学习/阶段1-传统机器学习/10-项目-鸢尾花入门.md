# ğŸŒ¸ 10 - é¡¹ç›®ï¼šé¸¢å°¾èŠ±åˆ†ç±»å…¥é—¨

> æœºå™¨å­¦ä¹ çš„ "Hello World"ï¼Œç”¨ç»å…¸æ•°æ®é›†ç†Ÿæ‚‰å®Œæ•´çš„ ML æµç¨‹

---

## é¡¹ç›®ç›®æ ‡

é€šè¿‡é¸¢å°¾èŠ±ï¼ˆIrisï¼‰æ•°æ®é›†ï¼Œå®Œæˆä»¥ä¸‹ç›®æ ‡ï¼š

1. âœ… åŠ è½½å’Œæ¢ç´¢æ•°æ®
2. âœ… æ•°æ®å¯è§†åŒ–
3. âœ… åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†
4. âœ… è®­ç»ƒå¤šä¸ªæ¨¡å‹å¹¶å¯¹æ¯”
5. âœ… è¯„ä¼°æ¨¡å‹æ€§èƒ½
6. âœ… ç†è§£åˆ†ç±»é—®é¢˜çš„å®Œæ•´æµç¨‹

**éš¾åº¦**ï¼šâ­ï¼ˆå…¥é—¨çº§ï¼‰

---

## å®Œæ•´ä»£ç 

```python
"""
é¸¢å°¾èŠ±åˆ†ç±»é¡¹ç›®
ç›®æ ‡ï¼šç†Ÿæ‚‰æœºå™¨å­¦ä¹ çš„å®Œæ•´æµç¨‹
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

# è®¾ç½®æ˜¾ç¤º
plt.rcParams['figure.figsize'] = (10, 6)
plt.rcParams['font.size'] = 12
np.random.seed(42)

print("=" * 60)
print("ğŸŒ¸ é¸¢å°¾èŠ±åˆ†ç±»é¡¹ç›®")
print("=" * 60)

# ============================================================
# 1. åŠ è½½æ•°æ®
# ============================================================
print("\n" + "=" * 60)
print("1. åŠ è½½æ•°æ®")
print("=" * 60)

# åŠ è½½é¸¢å°¾èŠ±æ•°æ®é›†
iris = load_iris()

# è½¬æ¢ä¸º DataFrame æ–¹ä¾¿æ“ä½œ
df = pd.DataFrame(iris.data, columns=iris.feature_names)
df['species'] = iris.target
df['species_name'] = df['species'].map({
    0: 'setosa',
    1: 'versicolor',
    2: 'virginica'
})

print(f"æ•°æ®é›†å½¢çŠ¶: {df.shape}")
print(f"æ ·æœ¬æ•°é‡: {len(df)}")
print(f"ç‰¹å¾æ•°é‡: {len(iris.feature_names)}")
print(f"ç±»åˆ«æ•°é‡: {len(iris.target_names)}")

print(f"\nç‰¹å¾åç§°:")
for i, name in enumerate(iris.feature_names):
    print(f"  {i+1}. {name}")

print(f"\nç±»åˆ«åç§°:")
for i, name in enumerate(iris.target_names):
    print(f"  {i}. {name}")

print(f"\nå‰ 5 è¡Œæ•°æ®:")
print(df.head())

# ============================================================
# 2. æ•°æ®æ¢ç´¢
# ============================================================
print("\n" + "=" * 60)
print("2. æ•°æ®æ¢ç´¢")
print("=" * 60)

print("\nåŸºæœ¬ç»Ÿè®¡ä¿¡æ¯:")
print(df.describe())

print("\nå„ç±»åˆ«æ ·æœ¬æ•°:")
print(df['species_name'].value_counts())

print("\nç¼ºå¤±å€¼æ£€æŸ¥:")
print(df.isnull().sum())

# ============================================================
# 3. æ•°æ®å¯è§†åŒ–
# ============================================================
print("\n" + "=" * 60)
print("3. æ•°æ®å¯è§†åŒ–")
print("=" * 60)

# 3.1 ç‰¹å¾åˆ†å¸ƒ
fig, axes = plt.subplots(2, 2, figsize=(12, 10))

for idx, col in enumerate(iris.feature_names):
    ax = axes[idx // 2, idx % 2]
    for species in [0, 1, 2]:
        subset = df[df['species'] == species]
        ax.hist(subset[col], bins=15, alpha=0.6,
                label=iris.target_names[species])
    ax.set_xlabel(col)
    ax.set_ylabel('Frequency')
    ax.legend()

plt.suptitle('Feature Distributions by Species', fontsize=14)
plt.tight_layout()
plt.savefig('iris_feature_distribution.png', dpi=150)
plt.show()

# 3.2 æ•£ç‚¹çŸ©é˜µ
print("\nç»˜åˆ¶æ•£ç‚¹çŸ©é˜µ...")
sns.pairplot(df, hue='species_name', diag_kind='hist',
             palette='viridis', corner=True)
plt.suptitle('Iris Pairplot', y=1.02)
plt.savefig('iris_pairplot.png', dpi=150)
plt.show()

# 3.3 ç›¸å…³æ€§çƒ­åŠ›å›¾
plt.figure(figsize=(8, 6))
correlation = df[iris.feature_names].corr()
sns.heatmap(correlation, annot=True, cmap='coolwarm', center=0,
            fmt='.2f', square=True, linewidths=0.5)
plt.title('Feature Correlation Matrix')
plt.tight_layout()
plt.savefig('iris_correlation.png', dpi=150)
plt.show()

# 3.4 ç®±çº¿å›¾
fig, axes = plt.subplots(1, 4, figsize=(16, 4))

for idx, col in enumerate(iris.feature_names):
    df.boxplot(column=col, by='species_name', ax=axes[idx])
    axes[idx].set_xlabel('Species')
    axes[idx].set_title(col)

plt.suptitle('Feature Distributions by Species (Boxplot)', fontsize=14, y=1.02)
plt.tight_layout()
plt.savefig('iris_boxplot.png', dpi=150)
plt.show()

# ============================================================
# 4. æ•°æ®å‡†å¤‡
# ============================================================
print("\n" + "=" * 60)
print("4. æ•°æ®å‡†å¤‡")
print("=" * 60)

# åˆ†ç¦»ç‰¹å¾å’Œæ ‡ç­¾
X = df[iris.feature_names].values
y = df['species'].values

print(f"ç‰¹å¾çŸ©é˜µå½¢çŠ¶: {X.shape}")
print(f"æ ‡ç­¾å‘é‡å½¢çŠ¶: {y.shape}")

# åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†
X_train, X_test, y_train, y_test = train_test_split(
    X, y,
    test_size=0.2,      # 20% ä½œä¸ºæµ‹è¯•é›†
    random_state=42,    # éšæœºç§å­ï¼Œä¿è¯å¯å¤ç°
    stratify=y          # åˆ†å±‚æŠ½æ ·ï¼Œä¿æŒç±»åˆ«æ¯”ä¾‹
)

print(f"\nè®­ç»ƒé›†å¤§å°: {len(X_train)} ({len(X_train)/len(X)*100:.0f}%)")
print(f"æµ‹è¯•é›†å¤§å°: {len(X_test)} ({len(X_test)/len(X)*100:.0f}%)")

# éªŒè¯åˆ†å±‚æŠ½æ ·
print(f"\nç±»åˆ«åˆ†å¸ƒ:")
print(f"  åŸå§‹æ•°æ®: {np.bincount(y)}")
print(f"  è®­ç»ƒé›†:   {np.bincount(y_train)}")
print(f"  æµ‹è¯•é›†:   {np.bincount(y_test)}")

# ç‰¹å¾æ ‡å‡†åŒ–ï¼ˆå¯é€‰ï¼Œä½†å¯¹æŸäº›ç®—æ³•æœ‰å¸®åŠ©ï¼‰
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

print(f"\nç‰¹å¾æ ‡å‡†åŒ–å:")
print(f"  å‡å€¼: {X_train_scaled.mean(axis=0).round(2)}")
print(f"  æ ‡å‡†å·®: {X_train_scaled.std(axis=0).round(2)}")

# ============================================================
# 5. è®­ç»ƒæ¨¡å‹
# ============================================================
print("\n" + "=" * 60)
print("5. è®­ç»ƒæ¨¡å‹")
print("=" * 60)

# å®šä¹‰å¤šä¸ªæ¨¡å‹
models = {
    'Logistic Regression': LogisticRegression(max_iter=200),
    'Decision Tree': DecisionTreeClassifier(max_depth=3, random_state=42),
    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),
    'K-Nearest Neighbors': KNeighborsClassifier(n_neighbors=5),
    'SVM': SVC(kernel='rbf', random_state=42)
}

# å­˜å‚¨ç»“æœ
results = []

print("\nè®­ç»ƒå’Œè¯„ä¼°å„æ¨¡å‹...")
print("-" * 50)

for name, model in models.items():
    # è®­ç»ƒï¼ˆSVM å’Œ KNN éœ€è¦æ ‡å‡†åŒ–æ•°æ®ï¼‰
    if name in ['K-Nearest Neighbors', 'SVM']:
        model.fit(X_train_scaled, y_train)
        y_pred = model.predict(X_test_scaled)
    else:
        model.fit(X_train, y_train)
        y_pred = model.predict(X_test)

    # è¯„ä¼°
    accuracy = accuracy_score(y_test, y_pred)
    results.append({
        'Model': name,
        'Accuracy': accuracy
    })

    print(f"{name}: Accuracy = {accuracy:.4f}")

# æŒ‰å‡†ç¡®ç‡æ’åº
results_df = pd.DataFrame(results).sort_values('Accuracy', ascending=False)
print("\næ¨¡å‹æ’å:")
print(results_df.to_string(index=False))

# ============================================================
# 6. è¯¦ç»†è¯„ä¼°æœ€ä½³æ¨¡å‹
# ============================================================
print("\n" + "=" * 60)
print("6. è¯¦ç»†è¯„ä¼°æœ€ä½³æ¨¡å‹")
print("=" * 60)

# ä½¿ç”¨éšæœºæ£®æ—ï¼ˆé€šå¸¸è¡¨ç°æœ€å¥½ï¼‰
best_model = RandomForestClassifier(n_estimators=100, random_state=42)
best_model.fit(X_train, y_train)
y_pred = best_model.predict(X_test)

print(f"\næœ€ä½³æ¨¡å‹: Random Forest")
print(f"å‡†ç¡®ç‡: {accuracy_score(y_test, y_pred):.4f}")

print(f"\nåˆ†ç±»æŠ¥å‘Š:")
print(classification_report(y_test, y_pred, target_names=iris.target_names))

# æ··æ·†çŸ©é˜µ
cm = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=iris.target_names,
            yticklabels=iris.target_names)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix - Random Forest')
plt.tight_layout()
plt.savefig('iris_confusion_matrix.png', dpi=150)
plt.show()

# ç‰¹å¾é‡è¦æ€§
print("\nç‰¹å¾é‡è¦æ€§:")
importance_df = pd.DataFrame({
    'Feature': iris.feature_names,
    'Importance': best_model.feature_importances_
}).sort_values('Importance', ascending=False)

for idx, row in importance_df.iterrows():
    print(f"  {row['Feature']}: {row['Importance']:.4f}")

# å¯è§†åŒ–ç‰¹å¾é‡è¦æ€§
plt.figure(figsize=(10, 5))
plt.barh(importance_df['Feature'], importance_df['Importance'])
plt.xlabel('Importance')
plt.title('Feature Importances in Random Forest')
plt.gca().invert_yaxis()
plt.tight_layout()
plt.savefig('iris_feature_importance.png', dpi=150)
plt.show()

# ============================================================
# 7. æ¨¡å‹å¯¹æ¯”å¯è§†åŒ–
# ============================================================
print("\n" + "=" * 60)
print("7. æ¨¡å‹å¯¹æ¯”å¯è§†åŒ–")
print("=" * 60)

plt.figure(figsize=(10, 6))
colors = plt.cm.viridis(np.linspace(0, 0.8, len(results_df)))
bars = plt.barh(results_df['Model'], results_df['Accuracy'], color=colors)
plt.xlabel('Accuracy')
plt.title('Model Comparison')
plt.xlim(0.8, 1.0)  # è°ƒæ•´ x è½´èŒƒå›´ä»¥ä¾¿è§‚å¯Ÿå·®å¼‚

# æ·»åŠ æ•°å€¼æ ‡ç­¾
for bar, acc in zip(bars, results_df['Accuracy']):
    plt.text(acc + 0.005, bar.get_y() + bar.get_height()/2,
             f'{acc:.2%}', va='center')

plt.tight_layout()
plt.savefig('iris_model_comparison.png', dpi=150)
plt.show()

# ============================================================
# 8. ä½¿ç”¨æ¨¡å‹é¢„æµ‹æ–°æ ·æœ¬
# ============================================================
print("\n" + "=" * 60)
print("8. ä½¿ç”¨æ¨¡å‹é¢„æµ‹æ–°æ ·æœ¬")
print("=" * 60)

# æ¨¡æ‹Ÿä¸€äº›æ–°æ ·æœ¬
new_samples = np.array([
    [5.1, 3.5, 1.4, 0.2],   # ç±»ä¼¼ setosa
    [6.2, 2.9, 4.3, 1.3],   # ç±»ä¼¼ versicolor
    [6.5, 3.0, 5.5, 2.0],   # ç±»ä¼¼ virginica
])

print("æ–°æ ·æœ¬:")
for i, sample in enumerate(new_samples):
    print(f"  æ ·æœ¬ {i+1}: {sample}")

predictions = best_model.predict(new_samples)
probabilities = best_model.predict_proba(new_samples)

print("\né¢„æµ‹ç»“æœ:")
for i, (pred, prob) in enumerate(zip(predictions, probabilities)):
    species = iris.target_names[pred]
    confidence = prob[pred] * 100
    print(f"  æ ·æœ¬ {i+1}: {species} (ç½®ä¿¡åº¦: {confidence:.1f}%)")
    print(f"    å„ç±»åˆ«æ¦‚ç‡: {dict(zip(iris.target_names, prob.round(3)))}")

# ============================================================
# 9. é¡¹ç›®æ€»ç»“
# ============================================================
print("\n" + "=" * 60)
print("9. é¡¹ç›®æ€»ç»“")
print("=" * 60)

print("""
ğŸ¯ æœ¬é¡¹ç›®å®Œæˆçš„ä»»åŠ¡ï¼š

1. âœ… åŠ è½½å’Œæ¢ç´¢ Iris æ•°æ®é›†
   - 150 ä¸ªæ ·æœ¬ï¼Œ4 ä¸ªç‰¹å¾ï¼Œ3 ä¸ªç±»åˆ«

2. âœ… æ•°æ®å¯è§†åŒ–
   - ç‰¹å¾åˆ†å¸ƒã€æ•£ç‚¹çŸ©é˜µã€ç›¸å…³æ€§åˆ†æã€ç®±çº¿å›¾

3. âœ… æ•°æ®å‡†å¤‡
   - è®­ç»ƒé›†/æµ‹è¯•é›†åˆ’åˆ† (80%/20%)
   - ç‰¹å¾æ ‡å‡†åŒ–

4. âœ… è®­ç»ƒå¤šä¸ªæ¨¡å‹
   - Logistic Regression
   - Decision Tree
   - Random Forest
   - K-Nearest Neighbors
   - SVM

5. âœ… æ¨¡å‹è¯„ä¼°
   - å‡†ç¡®ç‡ã€åˆ†ç±»æŠ¥å‘Šã€æ··æ·†çŸ©é˜µ

6. âœ… ç‰¹å¾é‡è¦æ€§åˆ†æ
   - petal length å’Œ petal width æœ€é‡è¦

7. âœ… æ–°æ ·æœ¬é¢„æµ‹

ğŸ“ å­¦åˆ°çš„çŸ¥è¯†ç‚¹ï¼š

- åˆ†ç±»é—®é¢˜çš„å®Œæ•´æµç¨‹
- æ•°æ®æ¢ç´¢å’Œå¯è§†åŒ–æŠ€å·§
- è®­ç»ƒé›†/æµ‹è¯•é›†åˆ’åˆ†çš„é‡è¦æ€§
- å¤šä¸ªæ¨¡å‹çš„å¯¹æ¯”æ–¹æ³•
- è¯„ä¼°æŒ‡æ ‡çš„ä½¿ç”¨
""")

print("=" * 60)
print("âœ… é¡¹ç›®å®Œæˆï¼")
print("=" * 60)
```

---

## è¿è¡Œç»“æœ

è¿è¡Œä¸Šè¿°ä»£ç åï¼Œä½ å°†å¾—åˆ°ï¼š

### 1. æ•°æ®æ¦‚è§ˆ

```
æ•°æ®é›†å½¢çŠ¶: (150, 6)
æ ·æœ¬æ•°é‡: 150
ç‰¹å¾æ•°é‡: 4
ç±»åˆ«æ•°é‡: 3

å„ç±»åˆ«æ ·æœ¬æ•°:
  setosa        50
  versicolor    50
  virginica     50
```

### 2. æ¨¡å‹å¯¹æ¯”

```
æ¨¡å‹æ’å:
               Model  Accuracy
       Random Forest    1.0000
Logistic Regression    1.0000
       Decision Tree    0.9667
                 SVM    0.9667
K-Nearest Neighbors    0.9667
```

### 3. ç”Ÿæˆçš„å›¾è¡¨

- `iris_feature_distribution.png` - ç‰¹å¾åˆ†å¸ƒå›¾
- `iris_pairplot.png` - æ•£ç‚¹çŸ©é˜µ
- `iris_correlation.png` - ç›¸å…³æ€§çƒ­åŠ›å›¾
- `iris_boxplot.png` - ç®±çº¿å›¾
- `iris_confusion_matrix.png` - æ··æ·†çŸ©é˜µ
- `iris_feature_importance.png` - ç‰¹å¾é‡è¦æ€§
- `iris_model_comparison.png` - æ¨¡å‹å¯¹æ¯”

---

## å…³é”®çŸ¥è¯†ç‚¹

### 1. æ•°æ®é›†ä»‹ç»

| ç‰¹å¾ | å«ä¹‰ |
|------|------|
| sepal length | èŠ±è¼é•¿åº¦ (cm) |
| sepal width | èŠ±è¼å®½åº¦ (cm) |
| petal length | èŠ±ç“£é•¿åº¦ (cm) |
| petal width | èŠ±ç“£å®½åº¦ (cm) |

| ç±»åˆ« | ä¸­æ–‡å |
|------|--------|
| setosa | å±±é¸¢å°¾ |
| versicolor | å˜è‰²é¸¢å°¾ |
| virginica | ç»´å‰å°¼äºšé¸¢å°¾ |

### 2. ä¸ºä»€ä¹ˆ Iris æ˜¯å…¥é—¨é¦–é€‰

- âœ… æ•°æ®å¹²å‡€ï¼šæ— ç¼ºå¤±å€¼ã€æ— å¼‚å¸¸å€¼
- âœ… è§„æ¨¡é€‚ä¸­ï¼š150 æ ·æœ¬ï¼Œä¸å¤§ä¸å°
- âœ… ç±»åˆ«å¹³è¡¡ï¼šæ¯ç±» 50 ä¸ªæ ·æœ¬
- âœ… å¯è§†åŒ–ï¼š4 ä¸ªç‰¹å¾å¯ä»¥è½»æ¾å¯è§†åŒ–
- âœ… ç»å…¸ä»»åŠ¡ï¼šå¤šåˆ†ç±»é—®é¢˜

### 3. åˆ†å±‚æŠ½æ ·

```python
train_test_split(X, y, stratify=y)
```

- ç¡®ä¿è®­ç»ƒé›†å’Œæµ‹è¯•é›†ä¸­å„ç±»åˆ«æ¯”ä¾‹ä¸€è‡´
- åœ¨ç±»åˆ«ä¸å¹³è¡¡æ—¶å°¤å…¶é‡è¦

---

## æ‰©å±•ç»ƒä¹ 

1. **è°ƒå‚**ï¼šå°è¯•è°ƒæ•´å„æ¨¡å‹çš„è¶…å‚æ•°ï¼Œçœ‹æ˜¯å¦èƒ½æé«˜å‡†ç¡®ç‡
2. **ç‰¹å¾é€‰æ‹©**ï¼šåªç”¨ 2 ä¸ªæœ€é‡è¦çš„ç‰¹å¾è®­ç»ƒï¼Œè§‚å¯Ÿæ•ˆæœå˜åŒ–
3. **å¯è§†åŒ–å†³ç­–è¾¹ç•Œ**ï¼šç”¨ 2 ä¸ªç‰¹å¾ç»˜åˆ¶ä¸åŒæ¨¡å‹çš„å†³ç­–è¾¹ç•Œ
4. **äº¤å‰éªŒè¯**ï¼šä½¿ç”¨ 5-Fold äº¤å‰éªŒè¯æ›¿ä»£ç®€å•çš„ train_test_split

---

## â¡ï¸ ä¸‹ä¸€æ­¥

å®Œæˆæœ¬é¡¹ç›®åï¼Œç»§ç»­æŒ‘æˆ˜ [11-é¡¹ç›®-æˆ¿ä»·é¢„æµ‹.md](./11-é¡¹ç›®-æˆ¿ä»·é¢„æµ‹.md)

