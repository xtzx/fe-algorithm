# ğŸ“Š é˜¶æ®µ 1 è‡ªæµ‹æ¸…å•

> å­¦å®Œé˜¶æ®µ 1 åï¼Œæ£€éªŒä½ çš„å­¦ä¹ æˆæœ

---

## ğŸ“‹ çŸ¥è¯†ç‚¹è‡ªæµ‹

### æœºå™¨å­¦ä¹ æµç¨‹

- [ ] 1. æœºå™¨å­¦ä¹ çš„å®Œæ•´æµç¨‹æ˜¯ä»€ä¹ˆï¼Ÿä»æ•°æ®åˆ°æ¨¡å‹ä¸Šçº¿æœ‰å“ªäº›æ­¥éª¤ï¼Ÿ
- [ ] 2. ä¸ºä»€ä¹ˆè¦åœ¨åˆ’åˆ†æ•°æ®ä¹‹åå†åšç‰¹å¾ç¼©æ”¾ï¼Ÿ
- [ ] 3. One-Hot Encoding å’Œ Label Encoding åˆ†åˆ«é€‚ç”¨äºä»€ä¹ˆåœºæ™¯ï¼Ÿ
- [ ] 4. è®­ç»ƒé›†ã€éªŒè¯é›†ã€æµ‹è¯•é›†åˆ†åˆ«æœ‰ä»€ä¹ˆä½œç”¨ï¼Ÿ

### æ¨¡å‹è¯„ä¼°

- [ ] 5. æ··æ·†çŸ©é˜µä¸­çš„ TPã€TNã€FPã€FN åˆ†åˆ«ä»£è¡¨ä»€ä¹ˆï¼Ÿ
- [ ] 6. Precision å’Œ Recall çš„åŒºåˆ«æ˜¯ä»€ä¹ˆï¼Ÿä»€ä¹ˆåœºæ™¯æ›´å…³æ³¨å“ªä¸ªï¼Ÿ
- [ ] 7. AUC-ROC æ›²çº¿è¡¨ç¤ºä»€ä¹ˆï¼ŸAUC=0.5 å’Œ AUC=1 åˆ†åˆ«æ„å‘³ç€ä»€ä¹ˆï¼Ÿ
- [ ] 8. MSEã€RMSEã€MAEã€RÂ² åˆ†åˆ«æ˜¯ä»€ä¹ˆï¼Ÿä»€ä¹ˆæ—¶å€™ç”¨å“ªä¸ªï¼Ÿ

### è¿‡æ‹Ÿåˆä¸æ­£åˆ™åŒ–

- [ ] 9. ä»€ä¹ˆæ˜¯è¿‡æ‹Ÿåˆï¼Ÿå¦‚ä½•æ£€æµ‹å’Œè§£å†³ï¼Ÿ
- [ ] 10. L1 å’Œ L2 æ­£åˆ™åŒ–çš„åŒºåˆ«æ˜¯ä»€ä¹ˆï¼Ÿ
- [ ] 11. äº¤å‰éªŒè¯çš„ä½œç”¨æ˜¯ä»€ä¹ˆï¼ŸK-Fold CV æ˜¯æ€ä¹ˆå·¥ä½œçš„ï¼Ÿ

### æ¨¡å‹ç†è§£

- [ ] 12. çº¿æ€§å›å½’å’Œé€»è¾‘å›å½’çš„åŒºåˆ«æ˜¯ä»€ä¹ˆï¼Ÿ
- [ ] 13. å†³ç­–æ ‘æ˜¯å¦‚ä½•åšå†³ç­–çš„ï¼Ÿ
- [ ] 14. éšæœºæ£®æ—çš„"éšæœº"ä½“ç°åœ¨å“ªé‡Œï¼Ÿ
- [ ] 15. Bagging å’Œ Boosting çš„åŒºåˆ«æ˜¯ä»€ä¹ˆï¼Ÿ
- [ ] 16. XGBoost ç›¸æ¯”éšæœºæ£®æ—æœ‰ä»€ä¹ˆä¼˜åŠ¿ï¼Ÿ
- [ ] 17. K-Means èšç±»çš„æ ¸å¿ƒæ€æƒ³æ˜¯ä»€ä¹ˆï¼Ÿå¦‚ä½•é€‰æ‹© Kï¼Ÿ
- [ ] 18. PCA çš„ä½œç”¨æ˜¯ä»€ä¹ˆï¼Ÿå¦‚ä½•é€‰æ‹©ä¿ç•™å¤šå°‘ä¸»æˆåˆ†ï¼Ÿ

### è¶…å‚æ•°è°ƒä¼˜

- [ ] 19. GridSearchCV å’Œ RandomizedSearchCV çš„åŒºåˆ«æ˜¯ä»€ä¹ˆï¼Ÿ
- [ ] 20. ä»€ä¹ˆæ˜¯åµŒå¥—äº¤å‰éªŒè¯ï¼Ÿ

---

## âœ… å‚è€ƒç­”æ¡ˆ

<details>
<summary>ç‚¹å‡»æŸ¥çœ‹ç­”æ¡ˆ</summary>

### 1. æœºå™¨å­¦ä¹ å®Œæ•´æµç¨‹
æ•°æ®æ”¶é›† â†’ æ•°æ®æ¸…æ´— â†’ ç‰¹å¾å·¥ç¨‹ â†’ æ•°æ®åˆ’åˆ† â†’ æ¨¡å‹è®­ç»ƒ â†’ æ¨¡å‹è¯„ä¼° â†’ è°ƒå‚ä¼˜åŒ– â†’ éƒ¨ç½²ä¸Šçº¿

### 2. åˆ’åˆ†åå†åšç‰¹å¾ç¼©æ”¾
é˜²æ­¢æ•°æ®æ³„éœ²ã€‚å¦‚æœå…ˆç¼©æ”¾å†åˆ’åˆ†ï¼Œæµ‹è¯•é›†çš„ä¿¡æ¯ï¼ˆå‡å€¼ã€æ ‡å‡†å·®ï¼‰ä¼šæ³„éœ²åˆ°è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œå¯¼è‡´è¯„ä¼°ç»“æœè¿‡äºä¹è§‚ã€‚

### 3. One-Hot vs Label Encoding
- **Label Encoding**ï¼šæœ‰åºç±»åˆ«ï¼ˆå¦‚ S < M < Lï¼‰
- **One-Hot Encoding**ï¼šæ— åºç±»åˆ«ï¼ˆå¦‚é¢œè‰²ã€åŸå¸‚ï¼‰

### 4. ä¸‰ä¸ªæ•°æ®é›†çš„ä½œç”¨
- **è®­ç»ƒé›†**ï¼šè®­ç»ƒæ¨¡å‹å‚æ•°
- **éªŒè¯é›†**ï¼šè°ƒæ•´è¶…å‚æ•°
- **æµ‹è¯•é›†**ï¼šæœ€ç»ˆè¯„ä¼°ï¼Œæ¨¡æ‹ŸçœŸå®åœºæ™¯

### 5. æ··æ·†çŸ©é˜µ
- TNï¼šçœŸå®ä¸ºè´Ÿï¼Œé¢„æµ‹ä¸ºè´Ÿï¼ˆé¢„æµ‹æ­£ç¡®ï¼‰
- FPï¼šçœŸå®ä¸ºè´Ÿï¼Œé¢„æµ‹ä¸ºæ­£ï¼ˆè¯¯æŠ¥ï¼‰
- FNï¼šçœŸå®ä¸ºæ­£ï¼Œé¢„æµ‹ä¸ºè´Ÿï¼ˆæ¼æŠ¥ï¼‰
- TPï¼šçœŸå®ä¸ºæ­£ï¼Œé¢„æµ‹ä¸ºæ­£ï¼ˆé¢„æµ‹æ­£ç¡®ï¼‰

### 6. Precision vs Recall
- **Precision**ï¼šé¢„æµ‹ä¸ºæ­£çš„é‡Œé¢æœ‰å¤šå°‘æ˜¯çœŸæ­£ä¸ºæ­£
- **Recall**ï¼šçœŸæ­£ä¸ºæ­£çš„é‡Œé¢æœ‰å¤šå°‘è¢«é¢„æµ‹å‡ºæ¥
- å…³æ³¨è¯¯æŠ¥æˆæœ¬æ—¶çœ‹ Precisionï¼ˆå¦‚åƒåœ¾é‚®ä»¶æ£€æµ‹ï¼‰
- å…³æ³¨æ¼æŠ¥æˆæœ¬æ—¶çœ‹ Recallï¼ˆå¦‚ç–¾ç—…æ£€æµ‹ï¼‰

### 7. AUC-ROC
- ROC æ›²çº¿æ˜¯ TPR vs FPR
- AUC = 0.5ï¼šéšæœºçŒœæµ‹
- AUC = 1ï¼šå®Œç¾åˆ†ç±»
- AUC è¡¨ç¤ºéšæœºé€‰ä¸€ä¸ªæ­£æ ·æœ¬å’Œè´Ÿæ ·æœ¬ï¼Œæ¨¡å‹å°†æ­£æ ·æœ¬æ’åœ¨å‰é¢çš„æ¦‚ç‡

### 8. å›å½’æŒ‡æ ‡
- **MSE**ï¼šå‡æ–¹è¯¯å·®ï¼Œå¯¹å¤§è¯¯å·®æ•æ„Ÿ
- **RMSE**ï¼šMSE å¼€æ ¹å·ï¼Œå’ŒåŸæ•°æ®åŒå•ä½
- **MAE**ï¼šå¹³å‡ç»å¯¹è¯¯å·®ï¼Œå¯¹å¼‚å¸¸å€¼é²æ£’
- **RÂ²**ï¼šå†³å®šç³»æ•°ï¼Œè¶Šæ¥è¿‘ 1 è¶Šå¥½

### 9. è¿‡æ‹Ÿåˆ
- **å®šä¹‰**ï¼šæ¨¡å‹åœ¨è®­ç»ƒé›†ä¸Šè¡¨ç°å¥½ä½†æµ‹è¯•é›†ä¸Šå·®
- **æ£€æµ‹**ï¼šæ¯”è¾ƒè®­ç»ƒ/æµ‹è¯•è¯¯å·®
- **è§£å†³**ï¼šæ­£åˆ™åŒ–ã€å¢åŠ æ•°æ®ã€å‡å°‘æ¨¡å‹å¤æ‚åº¦ã€æ—©åœ

### 10. L1 vs L2 æ­£åˆ™åŒ–
- **L1 (Lasso)**ï¼šäº§ç”Ÿç¨€ç–è§£ï¼Œæœ‰ç‰¹å¾é€‰æ‹©æ•ˆæœ
- **L2 (Ridge)**ï¼šç³»æ•°å˜å°ä½†ä¸ä¼šä¸º 0ï¼Œæ›´ç¨³å®š

### 11. äº¤å‰éªŒè¯
- **ä½œç”¨**ï¼šæ›´å¯é åœ°è¯„ä¼°æ¨¡å‹æ€§èƒ½
- **K-Fold CV**ï¼šæ•°æ®åˆ†æˆ K ä»½ï¼Œè½®æµç”¨ 1 ä»½åšéªŒè¯ï¼ŒK-1 ä»½åšè®­ç»ƒï¼Œå–å¹³å‡åˆ†æ•°

### 12. çº¿æ€§å›å½’ vs é€»è¾‘å›å½’
- **çº¿æ€§å›å½’**ï¼šé¢„æµ‹è¿ç»­å€¼ï¼Œç”¨äºå›å½’ä»»åŠ¡
- **é€»è¾‘å›å½’**ï¼šé¢„æµ‹æ¦‚ç‡ï¼Œç”¨äºåˆ†ç±»ä»»åŠ¡ï¼ˆè™½ç„¶åå­—æœ‰"å›å½’"ï¼‰

### 13. å†³ç­–æ ‘å†³ç­–è¿‡ç¨‹
é€šè¿‡ä¸€ç³»åˆ— if-else è§„åˆ™ï¼Œæ ¹æ®ç‰¹å¾å€¼é€æ­¥åˆ’åˆ†æ•°æ®ï¼Œæœ€ç»ˆåˆ°è¾¾å¶å­èŠ‚ç‚¹ç»™å‡ºé¢„æµ‹ã€‚

### 14. éšæœºæ£®æ—çš„éšæœºæ€§
1. **æ ·æœ¬éšæœº**ï¼šBootstrap æœ‰æ”¾å›æŠ½æ ·
2. **ç‰¹å¾éšæœº**ï¼šæ¯æ¬¡åˆ†è£‚åªè€ƒè™‘éƒ¨åˆ†ç‰¹å¾

### 15. Bagging vs Boosting
- **Bagging**ï¼šå¹¶è¡Œè®­ç»ƒå¤šä¸ªæ¨¡å‹ï¼ŒæŠ•ç¥¨/å¹³å‡
- **Boosting**ï¼šä¸²è¡Œè®­ç»ƒï¼Œæ¯ä¸ªæ¨¡å‹ä¿®æ­£å‰é¢çš„é”™è¯¯

### 16. XGBoost ä¼˜åŠ¿
- æœ‰æ­£åˆ™åŒ–é˜²æ­¢è¿‡æ‹Ÿåˆ
- æ”¯æŒè‡ªå®šä¹‰æŸå¤±å‡½æ•°
- å¯ä»¥å¤„ç†ç¼ºå¤±å€¼
- æœ‰å†…ç½®çš„äº¤å‰éªŒè¯
- æ”¯æŒå¹¶è¡Œè®­ç»ƒ

### 17. K-Means
- **æ€æƒ³**ï¼šæœ€å°åŒ–æ ·æœ¬åˆ°èšç±»ä¸­å¿ƒçš„è·ç¦»
- **é€‰ K**ï¼šè‚˜éƒ¨æ³•åˆ™ï¼ˆInertia ä¸‹é™å˜ç¼“ï¼‰+ è½®å»“ç³»æ•°

### 18. PCA
- **ä½œç”¨**ï¼šé™ç»´ï¼Œä¿ç•™ä¸»è¦ä¿¡æ¯ï¼Œå»é™¤å†—ä½™
- **é€‰æ‹©ä¸»æˆåˆ†æ•°**ï¼šç´¯è®¡è§£é‡Šæ–¹å·®è¾¾åˆ° 95%ï¼ˆæˆ–å…¶ä»–é˜ˆå€¼ï¼‰

### 19. GridSearchCV vs RandomizedSearchCV
- **GridSearchCV**ï¼šéå†æ‰€æœ‰å‚æ•°ç»„åˆï¼Œå…¨é¢ä½†æ…¢
- **RandomizedSearchCV**ï¼šéšæœºé‡‡æ ·ï¼Œæ›´å¿«ï¼Œé€‚åˆå¤§å‚æ•°ç©ºé—´

### 20. åµŒå¥—äº¤å‰éªŒè¯
- å¤–å±‚ CVï¼šè¯„ä¼°æ¨¡å‹æ³›åŒ–æ€§èƒ½
- å†…å±‚ CVï¼šè¶…å‚æ•°è°ƒä¼˜
- é¿å…è°ƒå‚å¯¼è‡´çš„è¯„ä¼°åå·®

</details>

---

## ğŸ’» ä»£ç èƒ½åŠ›è‡ªæµ‹

### 1. å®Œæˆä¸€ä¸ªåˆ†ç±»ä»»åŠ¡çš„å®Œæ•´ä»£ç 

```python
# TODO: ç”¨ sklearn å®Œæˆä»¥ä¸‹æ­¥éª¤
# 1. åŠ è½½æ•°æ®
# 2. æ•°æ®æ¸…æ´—å’Œç‰¹å¾å·¥ç¨‹
# 3. åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†
# 4. è®­ç»ƒéšæœºæ£®æ—æ¨¡å‹
# 5. è¯„ä¼°æ¨¡å‹ï¼ˆå‡†ç¡®ç‡ã€åˆ†ç±»æŠ¥å‘Šã€æ··æ·†çŸ©é˜µï¼‰
# 6. ç”» ROC æ›²çº¿
```

<details>
<summary>å‚è€ƒä»£ç </summary>

```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import (
    accuracy_score, classification_report,
    confusion_matrix, roc_curve, roc_auc_score
)
import seaborn as sns

# 1. åŠ è½½æ•°æ®
X, y = make_classification(n_samples=1000, n_features=20, random_state=42)

# 2. åˆ’åˆ†æ•°æ®
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# 3. ç‰¹å¾ç¼©æ”¾
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# 4. è®­ç»ƒæ¨¡å‹
model = RandomForestClassifier(n_estimators=100, random_state=42)
model.fit(X_train_scaled, y_train)

# 5. è¯„ä¼°
y_pred = model.predict(X_test_scaled)
y_prob = model.predict_proba(X_test_scaled)[:, 1]

print(f"å‡†ç¡®ç‡: {accuracy_score(y_test, y_pred):.4f}")
print(f"\nåˆ†ç±»æŠ¥å‘Š:\n{classification_report(y_test, y_pred)}")

# æ··æ·†çŸ©é˜µ
plt.figure(figsize=(6, 5))
cm = confusion_matrix(y_test, y_pred)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()

# 6. ROC æ›²çº¿
fpr, tpr, _ = roc_curve(y_test, y_prob)
auc = roc_auc_score(y_test, y_prob)

plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, label=f'AUC = {auc:.4f}')
plt.plot([0, 1], [0, 1], 'k--')
plt.xlabel('FPR')
plt.ylabel('TPR')
plt.title('ROC Curve')
plt.legend()
plt.show()
```

</details>

### 2. å®Œæˆä¸€ä¸ªå›å½’ä»»åŠ¡çš„å®Œæ•´ä»£ç 

```python
# TODO: ç”¨ sklearn å®Œæˆä»¥ä¸‹æ­¥éª¤
# 1. åŠ è½½åŠ å·æˆ¿ä»·æ•°æ®é›†
# 2. åˆ’åˆ†æ•°æ®
# 3. è®­ç»ƒ XGBoost å›å½’æ¨¡å‹
# 4. è¯„ä¼°ï¼ˆRMSEã€RÂ²ï¼‰
# 5. ç”»é¢„æµ‹ vs çœŸå®å›¾
```

<details>
<summary>å‚è€ƒä»£ç </summary>

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import fetch_california_housing
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score
from xgboost import XGBRegressor

# 1. åŠ è½½æ•°æ®
housing = fetch_california_housing()
X, y = housing.data, housing.target

# 2. åˆ’åˆ†æ•°æ®
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 3. è®­ç»ƒæ¨¡å‹
model = XGBRegressor(n_estimators=100, max_depth=5, learning_rate=0.1, random_state=42)
model.fit(X_train, y_train)

# 4. è¯„ä¼°
y_pred = model.predict(X_test)
rmse = np.sqrt(mean_squared_error(y_test, y_pred))
r2 = r2_score(y_test, y_pred)

print(f"RMSE: {rmse:.4f}")
print(f"RÂ²: {r2:.4f}")

# 5. é¢„æµ‹ vs çœŸå®
plt.figure(figsize=(8, 6))
plt.scatter(y_test, y_pred, alpha=0.5)
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')
plt.xlabel('True Values')
plt.ylabel('Predicted Values')
plt.title(f'Predicted vs True (RÂ² = {r2:.4f})')
plt.show()
```

</details>

### 3. å®Œæˆä¸€ä¸ªèšç±»ä»»åŠ¡çš„å®Œæ•´ä»£ç 

```python
# TODO: ç”¨ sklearn å®Œæˆä»¥ä¸‹æ­¥éª¤
# 1. ç”Ÿæˆèšç±»æ•°æ®
# 2. ç”¨è‚˜éƒ¨æ³•åˆ™é€‰æ‹© K
# 3. K-Means èšç±»
# 4. å¯è§†åŒ–ç»“æœ
```

<details>
<summary>å‚è€ƒä»£ç </summary>

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import make_blobs
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score

# 1. ç”Ÿæˆæ•°æ®
X, y_true = make_blobs(n_samples=300, centers=4, cluster_std=0.8, random_state=42)

# 2. è‚˜éƒ¨æ³•åˆ™
inertias = []
K_range = range(1, 11)
for k in K_range:
    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
    kmeans.fit(X)
    inertias.append(kmeans.inertia_)

plt.figure(figsize=(8, 5))
plt.plot(K_range, inertias, 'bo-')
plt.xlabel('K')
plt.ylabel('Inertia')
plt.title('Elbow Method')
plt.show()

# 3. K-Means èšç±»ï¼ˆé€‰æ‹© K=4ï¼‰
kmeans = KMeans(n_clusters=4, random_state=42, n_init=10)
labels = kmeans.fit_predict(X)

print(f"è½®å»“ç³»æ•°: {silhouette_score(X, labels):.4f}")

# 4. å¯è§†åŒ–
plt.figure(figsize=(8, 6))
plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis', s=50)
plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1],
            c='red', marker='X', s=200, label='Centers')
plt.legend()
plt.title('K-Means Clustering')
plt.show()
```

</details>

---

## ğŸ¯ ä¸‹ä¸€æ­¥è¡ŒåŠ¨

å®Œæˆè‡ªæµ‹åï¼š

1. **åˆ†æ•° < 60%**ï¼šé‡æ–°å­¦ä¹ æœªæŒæ¡çš„ç« èŠ‚
2. **åˆ†æ•° 60-80%**ï¼šé’ˆå¯¹æ€§å¤ä¹ è–„å¼±ç‚¹
3. **åˆ†æ•° > 80%**ï¼šè¿›å…¥ [é˜¶æ®µ 2ï¼šæ·±åº¦å­¦ä¹  & PyTorch](../é˜¶æ®µ2-æ·±åº¦å­¦ä¹ /)

---

## ğŸ“š æ¨èå·©å›ºèµ„æº

### Kaggle å®æˆ˜
- [Titanic - Machine Learning from Disaster](https://www.kaggle.com/c/titanic)ï¼ˆå…¥é—¨ï¼‰
- [House Prices](https://www.kaggle.com/c/house-prices-advanced-regression-techniques)ï¼ˆå›å½’ï¼‰
- [Digit Recognizer](https://www.kaggle.com/c/digit-recognizer)ï¼ˆåˆ†ç±»ï¼‰

### è¿›ä¸€æ­¥å­¦ä¹ 
- scikit-learn å®˜æ–¹æ–‡æ¡£
- ã€ŠHands-On Machine Learningã€‹
- æå®æ¯…æœºå™¨å­¦ä¹ è¯¾ç¨‹

> ğŸ’ª **è®°ä½ï¼šå¤šåšé¡¹ç›®ï¼Œå¤šå†™ä»£ç ï¼Œæ‰èƒ½çœŸæ­£æŒæ¡ï¼**

