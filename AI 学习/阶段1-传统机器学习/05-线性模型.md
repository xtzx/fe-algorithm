# ğŸ“Š 05 - çº¿æ€§æ¨¡å‹

> æœ¬æ–‡è¯¦ç»†ä»‹ç»çº¿æ€§å›å½’ã€é€»è¾‘å›å½’åŠå…¶å˜ä½“

---

## ç›®å½•

1. [çº¿æ€§å›å½’](#1-çº¿æ€§å›å½’)
2. [å¤šé¡¹å¼å›å½’](#2-å¤šé¡¹å¼å›å½’)
3. [é€»è¾‘å›å½’](#3-é€»è¾‘å›å½’)
4. [å¤šåˆ†ç±»é€»è¾‘å›å½’](#4-å¤šåˆ†ç±»é€»è¾‘å›å½’)
5. [çº¿æ€§æ¨¡å‹çš„ä¼˜ç¼ºç‚¹](#5-çº¿æ€§æ¨¡å‹çš„ä¼˜ç¼ºç‚¹)
6. [ç»ƒä¹ é¢˜](#6-ç»ƒä¹ é¢˜)

---

## 1. çº¿æ€§å›å½’

### 1.1 åŸºæœ¬åŸç†

**ç›®æ ‡**ï¼šæ‰¾åˆ°ä¸€æ¡ç›´çº¿ï¼ˆæˆ–è¶…å¹³é¢ï¼‰ï¼Œä½¿é¢„æµ‹å€¼ä¸çœŸå®å€¼çš„å·®è·æœ€å°

**æ¨¡å‹**ï¼š
$$\hat{y} = w_0 + w_1 x_1 + w_2 x_2 + ... + w_n x_n = \mathbf{w}^T \mathbf{x} + b$$

**æŸå¤±å‡½æ•°ï¼ˆå‡æ–¹è¯¯å·®ï¼‰**ï¼š
$$L = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2$$

### 1.2 ç®€å•çº¿æ€§å›å½’ï¼ˆå•å˜é‡ï¼‰

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score

# ç”Ÿæˆæ•°æ®
np.random.seed(42)
X = 2 * np.random.rand(100, 1)
y = 4 + 3 * X.ravel() + np.random.randn(100)  # y = 4 + 3x + noise

# åˆ’åˆ†æ•°æ®
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# è®­ç»ƒæ¨¡å‹
model = LinearRegression()
model.fit(X_train, y_train)

# æ¨¡å‹å‚æ•°
print(f"æˆªè· (w0): {model.intercept_:.4f}")
print(f"ç³»æ•° (w1): {model.coef_[0]:.4f}")
print(f"å­¦åˆ°çš„æ–¹ç¨‹: y = {model.intercept_:.2f} + {model.coef_[0]:.2f} * x")

# é¢„æµ‹
y_pred = model.predict(X_test)

# è¯„ä¼°
print(f"\nRÂ² Score: {r2_score(y_test, y_pred):.4f}")
print(f"RMSE: {np.sqrt(mean_squared_error(y_test, y_pred)):.4f}")

# å¯è§†åŒ–
plt.figure(figsize=(10, 6))
plt.scatter(X_train, y_train, color='blue', alpha=0.6, label='Training data')
plt.scatter(X_test, y_test, color='green', alpha=0.6, label='Test data')
plt.plot(X, model.predict(X), color='red', linewidth=2, label='Fitted line')
plt.xlabel('X')
plt.ylabel('y')
plt.title('Simple Linear Regression')
plt.legend()
plt.grid(True, alpha=0.3)
plt.show()
```

### 1.3 å¤šå…ƒçº¿æ€§å›å½’

```python
import numpy as np
import pandas as pd
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.datasets import fetch_california_housing
import matplotlib.pyplot as plt

# åŠ è½½åŠ å·æˆ¿ä»·æ•°æ®é›†
housing = fetch_california_housing()
X = pd.DataFrame(housing.data, columns=housing.feature_names)
y = housing.target

print("ç‰¹å¾åˆ—:")
print(X.columns.tolist())
print(f"\næ•°æ®å½¢çŠ¶: {X.shape}")

# åˆ’åˆ†æ•°æ®
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# è®­ç»ƒæ¨¡å‹
model = LinearRegression()
model.fit(X_train, y_train)

# ç³»æ•°
print("\nå„ç‰¹å¾ç³»æ•°:")
coef_df = pd.DataFrame({
    'Feature': X.columns,
    'Coefficient': model.coef_
}).sort_values('Coefficient', key=abs, ascending=False)
print(coef_df.to_string(index=False))

print(f"\næˆªè·: {model.intercept_:.4f}")

# è¯„ä¼°
y_pred = model.predict(X_test)
print(f"\nRÂ² Score: {r2_score(y_test, y_pred):.4f}")
print(f"RMSE: {np.sqrt(mean_squared_error(y_test, y_pred)):.4f}")

# å¯è§†åŒ–ï¼šé¢„æµ‹ vs çœŸå®
plt.figure(figsize=(8, 6))
plt.scatter(y_test, y_pred, alpha=0.5)
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)
plt.xlabel('True Values')
plt.ylabel('Predicted Values')
plt.title('Multiple Linear Regression: Predicted vs True')
plt.grid(True, alpha=0.3)
plt.show()

# æ®‹å·®åˆ†å¸ƒ
residuals = y_test - y_pred
plt.figure(figsize=(8, 6))
plt.hist(residuals, bins=50, edgecolor='black', alpha=0.7)
plt.xlabel('Residuals')
plt.ylabel('Frequency')
plt.title('Residual Distribution')
plt.axvline(0, color='red', linestyle='--')
plt.show()
```

### 1.4 å¸¦æ­£åˆ™åŒ–çš„çº¿æ€§å›å½’

```python
from sklearn.linear_model import Ridge, Lasso, ElasticNet
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import make_pipeline
import numpy as np

# ä½¿ç”¨ç›¸åŒçš„æ•°æ®
# æ³¨æ„ï¼šæ­£åˆ™åŒ–æ¨¡å‹éœ€è¦å…ˆå¯¹ç‰¹å¾è¿›è¡Œæ ‡å‡†åŒ–

models = {
    'Linear Regression': LinearRegression(),
    'Ridge (Î±=1)': make_pipeline(StandardScaler(), Ridge(alpha=1)),
    'Ridge (Î±=10)': make_pipeline(StandardScaler(), Ridge(alpha=10)),
    'Lasso (Î±=0.01)': make_pipeline(StandardScaler(), Lasso(alpha=0.01)),
    'ElasticNet': make_pipeline(StandardScaler(), ElasticNet(alpha=0.01, l1_ratio=0.5))
}

print("æ¨¡å‹å¯¹æ¯”:")
print("-" * 50)
for name, model in models.items():
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    r2 = r2_score(y_test, y_pred)
    rmse = np.sqrt(mean_squared_error(y_test, y_pred))
    print(f"{name}: RÂ² = {r2:.4f}, RMSE = {rmse:.4f}")
```

### 1.5 æ‰‹åŠ¨å®ç°çº¿æ€§å›å½’ï¼ˆæœ€å°äºŒä¹˜æ³•ï¼‰

```python
import numpy as np

class SimpleLinearRegression:
    """æ‰‹åŠ¨å®ç°çº¿æ€§å›å½’"""

    def __init__(self):
        self.coef_ = None
        self.intercept_ = None

    def fit(self, X, y):
        """ä½¿ç”¨æ­£è§„æ–¹ç¨‹æ±‚è§£: w = (X^T X)^(-1) X^T y"""
        X = np.array(X)
        y = np.array(y)

        # æ·»åŠ åç½®é¡¹
        X_b = np.c_[np.ones((X.shape[0], 1)), X]

        # æ­£è§„æ–¹ç¨‹
        theta = np.linalg.inv(X_b.T @ X_b) @ X_b.T @ y

        self.intercept_ = theta[0]
        self.coef_ = theta[1:]

        return self

    def predict(self, X):
        X = np.array(X)
        return X @ self.coef_ + self.intercept_

    def score(self, X, y):
        """è®¡ç®— RÂ² åˆ†æ•°"""
        y_pred = self.predict(X)
        ss_res = np.sum((y - y_pred) ** 2)
        ss_tot = np.sum((y - y.mean()) ** 2)
        return 1 - ss_res / ss_tot

# æµ‹è¯•
np.random.seed(42)
X = 2 * np.random.rand(100, 1)
y = 4 + 3 * X.ravel() + np.random.randn(100)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# æ‰‹åŠ¨å®ç°
my_model = SimpleLinearRegression()
my_model.fit(X_train, y_train)
print(f"æ‰‹åŠ¨å®ç° - æˆªè·: {my_model.intercept_:.4f}, ç³»æ•°: {my_model.coef_[0]:.4f}")
print(f"æ‰‹åŠ¨å®ç° - RÂ²: {my_model.score(X_test, y_test):.4f}")

# sklearn
sk_model = LinearRegression()
sk_model.fit(X_train, y_train)
print(f"sklearn   - æˆªè·: {sk_model.intercept_:.4f}, ç³»æ•°: {sk_model.coef_[0]:.4f}")
print(f"sklearn   - RÂ²: {sk_model.score(X_test, y_test):.4f}")
```

---

## 2. å¤šé¡¹å¼å›å½’

### 2.1 åŸºæœ¬æ¦‚å¿µ

å¤šé¡¹å¼å›å½’ç”¨äºæ‹Ÿåˆéçº¿æ€§æ•°æ®ï¼Œé€šè¿‡æ„é€ å¤šé¡¹å¼ç‰¹å¾æ¥æ‰©å±•çº¿æ€§æ¨¡å‹

$$\hat{y} = w_0 + w_1 x + w_2 x^2 + ... + w_d x^d$$

### 2.2 å®ç°

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LinearRegression
from sklearn.pipeline import make_pipeline
from sklearn.metrics import mean_squared_error

# ç”Ÿæˆéçº¿æ€§æ•°æ®
np.random.seed(42)
X = np.sort(np.random.rand(100) * 10).reshape(-1, 1)
y = np.sin(X).ravel() + np.random.randn(100) * 0.3

# ä¸åŒé˜¶æ•°çš„å¤šé¡¹å¼
fig, axes = plt.subplots(2, 2, figsize=(12, 10))
axes = axes.flatten()

degrees = [1, 3, 6, 15]

for ax, degree in zip(axes, degrees):
    # åˆ›å»ºå¤šé¡¹å¼å›å½’æ¨¡å‹
    model = make_pipeline(
        PolynomialFeatures(degree),
        LinearRegression()
    )
    model.fit(X, y)

    # é¢„æµ‹
    X_plot = np.linspace(0, 10, 100).reshape(-1, 1)
    y_pred = model.predict(X_plot)
    y_train_pred = model.predict(X)

    # è®¡ç®— MSE
    mse = mean_squared_error(y, y_train_pred)

    # ç»˜å›¾
    ax.scatter(X, y, color='blue', alpha=0.5, s=20)
    ax.plot(X_plot, y_pred, color='red', linewidth=2)
    ax.plot(X_plot, np.sin(X_plot), color='green', linewidth=2, linestyle='--', label='True function')
    ax.set_xlabel('X')
    ax.set_ylabel('y')
    ax.set_title(f'Degree = {degree}, MSE = {mse:.4f}')
    ax.set_ylim(-2, 2)
    ax.legend()

plt.tight_layout()
plt.show()
```

### 2.3 å¤šé¡¹å¼ç‰¹å¾

```python
from sklearn.preprocessing import PolynomialFeatures
import numpy as np

# åŸå§‹ç‰¹å¾
X = np.array([[2, 3], [4, 5]])
print("åŸå§‹ç‰¹å¾:")
print(X)

# äºŒæ¬¡å¤šé¡¹å¼ç‰¹å¾
poly = PolynomialFeatures(degree=2, include_bias=False)
X_poly = poly.fit_transform(X)

print("\näºŒæ¬¡å¤šé¡¹å¼ç‰¹å¾:")
print(X_poly)
print(f"\nç‰¹å¾å: {poly.get_feature_names_out()}")

# include_bias=True ä¼šæ·»åŠ å¸¸æ•°é¡¹ 1
poly_with_bias = PolynomialFeatures(degree=2, include_bias=True)
X_poly_bias = poly_with_bias.fit_transform(X)
print(f"\nå¸¦åç½®çš„ç‰¹å¾å: {poly_with_bias.get_feature_names_out()}")
```

### 2.4 é˜²æ­¢å¤šé¡¹å¼å›å½’è¿‡æ‹Ÿåˆ

```python
from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import Ridge
from sklearn.pipeline import make_pipeline
from sklearn.model_selection import cross_val_score
import numpy as np
import matplotlib.pyplot as plt

# ç”Ÿæˆæ•°æ®
np.random.seed(42)
X = np.sort(np.random.rand(50) * 10).reshape(-1, 1)
y = np.sin(X).ravel() + np.random.randn(50) * 0.3

# æ¯”è¾ƒï¼šæ— æ­£åˆ™åŒ– vs æœ‰æ­£åˆ™åŒ–
fig, axes = plt.subplots(1, 2, figsize=(14, 5))

# æ— æ­£åˆ™åŒ–çš„é«˜é˜¶å¤šé¡¹å¼
model_no_reg = make_pipeline(
    PolynomialFeatures(degree=15),
    LinearRegression()
)
model_no_reg.fit(X, y)

# æœ‰æ­£åˆ™åŒ–çš„é«˜é˜¶å¤šé¡¹å¼
model_with_reg = make_pipeline(
    PolynomialFeatures(degree=15),
    Ridge(alpha=0.1)
)
model_with_reg.fit(X, y)

X_plot = np.linspace(0, 10, 100).reshape(-1, 1)

for ax, model, title in zip(axes,
                            [model_no_reg, model_with_reg],
                            ['Without Regularization', 'With Ridge Regularization']):
    y_pred = model.predict(X_plot)

    ax.scatter(X, y, color='blue', alpha=0.5, s=30)
    ax.plot(X_plot, y_pred, color='red', linewidth=2)
    ax.plot(X_plot, np.sin(X_plot), color='green', linewidth=2, linestyle='--', label='True')
    ax.set_xlabel('X')
    ax.set_ylabel('y')
    ax.set_title(f'{title} (degree=15)')
    ax.set_ylim(-3, 3)
    ax.legend()

plt.tight_layout()
plt.show()
```

---

## 3. é€»è¾‘å›å½’

### 3.1 åŸºæœ¬åŸç†

**ç”¨é€”**ï¼šäºŒåˆ†ç±»é—®é¢˜

**æ¨¡å‹**ï¼š
$$P(y=1|x) = \sigma(w^T x + b) = \frac{1}{1 + e^{-(w^T x + b)}}$$

å…¶ä¸­ Ïƒ æ˜¯ Sigmoid å‡½æ•°ï¼Œå°†ä»»æ„å®æ•°æ˜ å°„åˆ° (0, 1)

**æŸå¤±å‡½æ•°ï¼ˆäº¤å‰ç†µï¼‰**ï¼š
$$L = -\frac{1}{n}\sum_{i=1}^{n}[y_i \log(\hat{y}_i) + (1-y_i)\log(1-\hat{y}_i)]$$

### 3.2 Sigmoid å‡½æ•°

```python
import numpy as np
import matplotlib.pyplot as plt

def sigmoid(x):
    return 1 / (1 + np.exp(-x))

x = np.linspace(-10, 10, 100)
y = sigmoid(x)

plt.figure(figsize=(10, 6))
plt.plot(x, y, 'b-', linewidth=2)
plt.axhline(0.5, color='red', linestyle='--', alpha=0.7, label='y = 0.5')
plt.axvline(0, color='gray', linestyle='--', alpha=0.5)
plt.xlabel('x')
plt.ylabel('Ïƒ(x)')
plt.title('Sigmoid Function')
plt.grid(True, alpha=0.3)
plt.legend()
plt.ylim(-0.1, 1.1)
plt.show()

print("Sigmoid çš„ç‰¹æ€§:")
print(f"Ïƒ(0) = {sigmoid(0):.4f}")
print(f"Ïƒ(-âˆ) â†’ 0")
print(f"Ïƒ(+âˆ) â†’ 1")
```

### 3.3 äºŒåˆ†ç±»é€»è¾‘å›å½’

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LogisticRegression
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report, roc_auc_score

# ç”ŸæˆäºŒåˆ†ç±»æ•°æ®
X, y = make_classification(
    n_samples=500, n_features=2, n_redundant=0,
    n_informative=2, random_state=42, n_clusters_per_class=1
)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# è®­ç»ƒæ¨¡å‹
model = LogisticRegression()
model.fit(X_train, y_train)

# é¢„æµ‹
y_pred = model.predict(X_test)
y_prob = model.predict_proba(X_test)[:, 1]

# è¯„ä¼°
print("åˆ†ç±»æŠ¥å‘Š:")
print(classification_report(y_test, y_pred))
print(f"AUC: {roc_auc_score(y_test, y_prob):.4f}")

# æ¨¡å‹å‚æ•°
print(f"\nç³»æ•°: {model.coef_[0]}")
print(f"æˆªè·: {model.intercept_[0]:.4f}")

# å¯è§†åŒ–å†³ç­–è¾¹ç•Œ
def plot_decision_boundary(model, X, y, title):
    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 200),
                         np.linspace(y_min, y_max, 200))

    # é¢„æµ‹æ¦‚ç‡
    Z = model.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]
    Z = Z.reshape(xx.shape)

    plt.figure(figsize=(10, 8))

    # ç»˜åˆ¶æ¦‚ç‡ç­‰é«˜çº¿
    contour = plt.contourf(xx, yy, Z, levels=np.linspace(0, 1, 11), cmap='RdYlBu', alpha=0.8)
    plt.colorbar(contour, label='P(y=1)')

    # ç»˜åˆ¶å†³ç­–è¾¹ç•Œï¼ˆP=0.5ï¼‰
    plt.contour(xx, yy, Z, levels=[0.5], colors='black', linewidths=2)

    # ç»˜åˆ¶æ•°æ®ç‚¹
    scatter = plt.scatter(X[:, 0], X[:, 1], c=y, cmap='RdYlBu', edgecolors='black', s=50)

    plt.xlabel('Feature 1')
    plt.ylabel('Feature 2')
    plt.title(title)
    plt.show()

plot_decision_boundary(model, X, y, 'Logistic Regression Decision Boundary')
```

### 3.4 æ¦‚ç‡é¢„æµ‹

```python
# é¢„æµ‹æ¦‚ç‡ vs é¢„æµ‹ç±»åˆ«
print("é¢„æµ‹ç¤ºä¾‹:")
sample_indices = [0, 1, 2, 3, 4]
for i in sample_indices:
    prob = model.predict_proba(X_test[i:i+1])[0]
    pred = model.predict(X_test[i:i+1])[0]
    true = y_test[i]
    print(f"æ ·æœ¬ {i}: P(0)={prob[0]:.3f}, P(1)={prob[1]:.3f} â†’ é¢„æµ‹={pred}, çœŸå®={true}")

# è°ƒæ•´é˜ˆå€¼
from sklearn.metrics import precision_recall_curve
import matplotlib.pyplot as plt

precisions, recalls, thresholds = precision_recall_curve(y_test, y_prob)

plt.figure(figsize=(10, 6))
plt.plot(thresholds, precisions[:-1], 'b-', label='Precision')
plt.plot(thresholds, recalls[:-1], 'r-', label='Recall')
plt.xlabel('Threshold')
plt.ylabel('Score')
plt.title('Precision and Recall vs Threshold')
plt.legend()
plt.grid(True)
plt.show()

# ä½¿ç”¨è‡ªå®šä¹‰é˜ˆå€¼
custom_threshold = 0.7
y_pred_custom = (y_prob >= custom_threshold).astype(int)
print(f"\nä½¿ç”¨é˜ˆå€¼ {custom_threshold}:")
print(classification_report(y_test, y_pred_custom))
```

### 3.5 æ­£åˆ™åŒ–

```python
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import cross_val_score

# ä¸åŒæ­£åˆ™åŒ–è®¾ç½®
regularizations = {
    'No Regularization': LogisticRegression(penalty=None, max_iter=1000),
    'L1 (C=1)': LogisticRegression(penalty='l1', C=1, solver='saga', max_iter=1000),
    'L2 (C=1)': LogisticRegression(penalty='l2', C=1, max_iter=1000),
    'L2 (C=0.1)': LogisticRegression(penalty='l2', C=0.1, max_iter=1000),
    'L2 (C=10)': LogisticRegression(penalty='l2', C=10, max_iter=1000),
}

# æ³¨æ„ï¼šC æ˜¯æ­£åˆ™åŒ–å¼ºåº¦çš„å€’æ•°ï¼ŒC è¶Šå°æ­£åˆ™åŒ–è¶Šå¼º

print("æ­£åˆ™åŒ–å¯¹æ¯”:")
print("-" * 50)
for name, model in regularizations.items():
    scores = cross_val_score(model, X, y, cv=5, scoring='accuracy')
    print(f"{name}: {scores.mean():.4f} Â± {scores.std():.4f}")
```

---

## 4. å¤šåˆ†ç±»é€»è¾‘å›å½’

### 4.1 ç­–ç•¥

- **One-vs-Rest (OvR)**ï¼šä¸ºæ¯ä¸ªç±»åˆ«è®­ç»ƒä¸€ä¸ªäºŒåˆ†ç±»å™¨
- **Multinomial**ï¼šç›´æ¥ç”¨ Softmax è¿›è¡Œå¤šåˆ†ç±»

### 4.2 å®ç°

```python
from sklearn.linear_model import LogisticRegression
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

# åŠ è½½é¸¢å°¾èŠ±æ•°æ®é›†
iris = load_iris()
X, y = iris.data, iris.target

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# One-vs-Rest
model_ovr = LogisticRegression(multi_class='ovr', max_iter=1000)
model_ovr.fit(X_train, y_train)
y_pred_ovr = model_ovr.predict(X_test)

# Multinomial (Softmax)
model_softmax = LogisticRegression(multi_class='multinomial', max_iter=1000)
model_softmax.fit(X_train, y_train)
y_pred_softmax = model_softmax.predict(X_test)

print("One-vs-Rest:")
print(classification_report(y_test, y_pred_ovr, target_names=iris.target_names))

print("\nMultinomial (Softmax):")
print(classification_report(y_test, y_pred_softmax, target_names=iris.target_names))

# æ··æ·†çŸ©é˜µ
fig, axes = plt.subplots(1, 2, figsize=(12, 5))

for ax, y_pred, title in zip(axes,
                              [y_pred_ovr, y_pred_softmax],
                              ['One-vs-Rest', 'Multinomial']):
    cm = confusion_matrix(y_test, y_pred)
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax,
                xticklabels=iris.target_names, yticklabels=iris.target_names)
    ax.set_xlabel('Predicted')
    ax.set_ylabel('True')
    ax.set_title(title)

plt.tight_layout()
plt.show()
```

### 4.3 é¢„æµ‹æ¦‚ç‡ï¼ˆSoftmaxï¼‰

```python
# Softmax è¾“å‡ºå„ç±»åˆ«çš„æ¦‚ç‡
probs = model_softmax.predict_proba(X_test)

print("å‰ 5 ä¸ªæ ·æœ¬çš„é¢„æµ‹æ¦‚ç‡:")
print(f"{'Sample':<8} {'P(setosa)':<12} {'P(versicolor)':<15} {'P(virginica)':<14} {'Predicted':<12} {'True'}")
print("-" * 70)
for i in range(5):
    p = probs[i]
    pred = iris.target_names[y_pred_softmax[i]]
    true = iris.target_names[y_test[i]]
    print(f"{i:<8} {p[0]:<12.4f} {p[1]:<15.4f} {p[2]:<14.4f} {pred:<12} {true}")

# æ¦‚ç‡å’Œä¸º 1
print(f"\næ¦‚ç‡å’Œ: {probs[0].sum():.4f}")
```

---

## 5. çº¿æ€§æ¨¡å‹çš„ä¼˜ç¼ºç‚¹

### 5.1 ä¼˜ç‚¹

| ä¼˜ç‚¹ | è¯´æ˜ |
|------|------|
| **ç®€å•é«˜æ•ˆ** | è®­ç»ƒå¿«ï¼Œé€‚åˆå¤§æ•°æ®é›† |
| **å¯è§£é‡Šæ€§å¼º** | ç³»æ•°ç›´æ¥åæ˜ ç‰¹å¾é‡è¦æ€§ |
| **ä¸æ˜“è¿‡æ‹Ÿåˆ** | æ¨¡å‹ç®€å•ï¼Œé…åˆæ­£åˆ™åŒ–æ›´ç¨³å®š |
| **æ¦‚ç‡è¾“å‡º** | é€»è¾‘å›å½’è¾“å‡ºæ¦‚ç‡ï¼Œå¯è°ƒé˜ˆå€¼ |
| **ç†è®ºåŸºç¡€å¥½** | ç»Ÿè®¡å­¦æœ‰å®Œæ•´ç†è®ºæ”¯æŒ |

### 5.2 ç¼ºç‚¹

| ç¼ºç‚¹ | è¯´æ˜ |
|------|------|
| **åªèƒ½å­¦ä¹ çº¿æ€§å…³ç³»** | æ— æ³•å¤„ç†å¤æ‚éçº¿æ€§ |
| **ç‰¹å¾å·¥ç¨‹ä¾èµ–é«˜** | éœ€è¦æ‰‹åŠ¨æ„é€ éçº¿æ€§ç‰¹å¾ |
| **å¯¹å¼‚å¸¸å€¼æ•æ„Ÿ** | æœ€å°äºŒä¹˜æ³•å—å¼‚å¸¸å€¼å½±å“ |
| **ç‰¹å¾ç›¸å…³æ€§é—®é¢˜** | å¤šé‡å…±çº¿æ€§å½±å“ç³»æ•°è§£é‡Š |

### 5.3 ä»€ä¹ˆæ—¶å€™ç”¨çº¿æ€§æ¨¡å‹

```
âœ… é€‚åˆä½¿ç”¨çš„åœºæ™¯ï¼š
- ç‰¹å¾ä¸ç›®æ ‡æ¥è¿‘çº¿æ€§å…³ç³»
- éœ€è¦æ¨¡å‹å¯è§£é‡Šæ€§
- æ•°æ®é‡å¤§ä½†è®¡ç®—èµ„æºæœ‰é™
- ä½œä¸ºåŸºçº¿æ¨¡å‹
- ç¨€ç–é«˜ç»´æ•°æ®ï¼ˆé…åˆ Lassoï¼‰

âŒ ä¸é€‚åˆçš„åœºæ™¯ï¼š
- å¤æ‚éçº¿æ€§å…³ç³»
- å›¾åƒã€æ–‡æœ¬ç­‰éç»“æ„åŒ–æ•°æ®
- éœ€è¦è‡ªåŠ¨å­¦ä¹ ç‰¹å¾äº¤äº’
```

---

## 6. ç»ƒä¹ é¢˜

### åŸºç¡€ç»ƒä¹ 

1. ç”¨çº¿æ€§å›å½’æ‹Ÿåˆä¸€ç»„æ•°æ®ï¼Œç”»å‡ºå›å½’çº¿
2. ç”¨é€»è¾‘å›å½’åšäºŒåˆ†ç±»ï¼Œç”»å‡ºå†³ç­–è¾¹ç•Œ
3. æ¯”è¾ƒ Ridge å’Œ Lasso çš„ç³»æ•°å·®å¼‚

### è¿›é˜¶ç»ƒä¹ 

4. ç”¨å¤šé¡¹å¼å›å½’æ‹Ÿåˆéçº¿æ€§æ•°æ®ï¼Œæ‰¾åˆ°æœ€ä½³çš„ degree
5. ç”¨é€»è¾‘å›å½’åšé¸¢å°¾èŠ±ä¸‰åˆ†ç±»ï¼Œç”»å‡ºæ··æ·†çŸ©é˜µ

### å‚è€ƒç­”æ¡ˆ

<details>
<summary>ç»ƒä¹  4 å‚è€ƒç­”æ¡ˆ</summary>

```python
from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LinearRegression
from sklearn.pipeline import make_pipeline
from sklearn.model_selection import cross_val_score
import numpy as np

# ç”Ÿæˆéçº¿æ€§æ•°æ®
np.random.seed(42)
X = np.sort(np.random.rand(100) * 10).reshape(-1, 1)
y = np.sin(X).ravel() + np.random.randn(100) * 0.3

# æµ‹è¯•ä¸åŒ degree
degrees = range(1, 15)
cv_scores = []

for degree in degrees:
    model = make_pipeline(
        PolynomialFeatures(degree),
        LinearRegression()
    )
    scores = cross_val_score(model, X, y, cv=5, scoring='neg_mean_squared_error')
    cv_scores.append(-scores.mean())

# æ‰¾æœ€ä½³ degree
best_degree = degrees[np.argmin(cv_scores)]
print(f"æœ€ä½³ degree: {best_degree}")
print(f"æœ€ä½³ CV MSE: {min(cv_scores):.4f}")

# å¯è§†åŒ–
import matplotlib.pyplot as plt
plt.plot(degrees, cv_scores, 'bo-')
plt.xlabel('Polynomial Degree')
plt.ylabel('CV MSE')
plt.title('Finding Optimal Polynomial Degree')
plt.axvline(best_degree, color='red', linestyle='--')
plt.show()
```

</details>

---

## â¡ï¸ ä¸‹ä¸€æ­¥

å­¦å®Œæœ¬èŠ‚åï¼Œç»§ç»­å­¦ä¹  [06-æ ‘æ¨¡å‹ä¸é›†æˆå­¦ä¹ .md](./06-æ ‘æ¨¡å‹ä¸é›†æˆå­¦ä¹ .md)

