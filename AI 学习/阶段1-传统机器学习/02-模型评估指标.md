# ğŸ“Š 02 - æ¨¡å‹è¯„ä¼°æŒ‡æ ‡

> æœ¬æ–‡è¯¦ç»†ä»‹ç»åˆ†ç±»å’Œå›å½’ä»»åŠ¡çš„å„ç§è¯„ä¼°æŒ‡æ ‡

---

## ç›®å½•

1. [ä¸ºä»€ä¹ˆéœ€è¦è¯„ä¼°æŒ‡æ ‡](#1-ä¸ºä»€ä¹ˆéœ€è¦è¯„ä¼°æŒ‡æ ‡)
2. [åˆ†ç±»æŒ‡æ ‡](#2-åˆ†ç±»æŒ‡æ ‡)
3. [å›å½’æŒ‡æ ‡](#3-å›å½’æŒ‡æ ‡)
4. [æŒ‡æ ‡é€‰æ‹©æŒ‡å—](#4-æŒ‡æ ‡é€‰æ‹©æŒ‡å—)
5. [ç»ƒä¹ é¢˜](#5-ç»ƒä¹ é¢˜)

---

## 1. ä¸ºä»€ä¹ˆéœ€è¦è¯„ä¼°æŒ‡æ ‡

### ä¸åŒåœºæ™¯éœ€è¦ä¸åŒæŒ‡æ ‡

```
åœºæ™¯ 1ï¼šåƒåœ¾é‚®ä»¶æ£€æµ‹
  - æŠŠæ­£å¸¸é‚®ä»¶è¯¯åˆ¤ä¸ºåƒåœ¾ï¼ˆè¯¯æŠ¥ï¼‰â†’ ç”¨æˆ·é”™è¿‡é‡è¦é‚®ä»¶
  - æŠŠåƒåœ¾é‚®ä»¶æ¼åˆ¤ä¸ºæ­£å¸¸ï¼ˆæ¼æŠ¥ï¼‰â†’ ç”¨æˆ·æ”¶åˆ°åƒåœ¾é‚®ä»¶
  â†’ æ›´å…³æ³¨ï¼šPrecisionï¼ˆå‡å°‘è¯¯æŠ¥ï¼‰

åœºæ™¯ 2ï¼šç™Œç—‡æ£€æµ‹
  - æŠŠæ‚£è€…è¯¯åˆ¤ä¸ºå¥åº·ï¼ˆæ¼æŠ¥ï¼‰â†’ å»¶è¯¯æ²»ç–—ï¼Œåæœä¸¥é‡
  - æŠŠå¥åº·äººè¯¯åˆ¤ä¸ºæ‚£è€…ï¼ˆè¯¯æŠ¥ï¼‰â†’ éœ€è¦å¤æŸ¥ï¼Œæˆæœ¬è¾ƒä½
  â†’ æ›´å…³æ³¨ï¼šRecallï¼ˆå‡å°‘æ¼æŠ¥ï¼‰

åœºæ™¯ 3ï¼šæˆ¿ä»·é¢„æµ‹
  - é¢„æµ‹å€¼ä¸çœŸå®å€¼çš„å·®è·
  â†’ å…³æ³¨ï¼šMSEã€MAEã€RÂ²
```

---

## 2. åˆ†ç±»æŒ‡æ ‡

### 2.1 æ··æ·†çŸ©é˜µï¼ˆConfusion Matrixï¼‰

#### åŸºæœ¬æ¦‚å¿µ

```
                    é¢„æµ‹å€¼
                  0        1
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”
çœŸ      0     â”‚   TN   â”‚   FP   â”‚  â† çœŸå®ä¸ºè´Ÿ
å®            â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¤
å€¼      1     â”‚   FN   â”‚   TP   â”‚  â† çœŸå®ä¸ºæ­£
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜

TN (True Negative)ï¼šçœŸå®ä¸º 0ï¼Œé¢„æµ‹ä¸º 0 âœ“
FP (False Positive)ï¼šçœŸå®ä¸º 0ï¼Œé¢„æµ‹ä¸º 1 âœ—ï¼ˆè¯¯æŠ¥/å‡é˜³æ€§ï¼‰
FN (False Negative)ï¼šçœŸå®ä¸º 1ï¼Œé¢„æµ‹ä¸º 0 âœ—ï¼ˆæ¼æŠ¥/å‡é˜´æ€§ï¼‰
TP (True Positive)ï¼šçœŸå®ä¸º 1ï¼Œé¢„æµ‹ä¸º 1 âœ“
```

#### ä»£ç å®ç°

```python
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
import matplotlib.pyplot as plt
import numpy as np

# ç¤ºä¾‹æ•°æ®
y_true = [1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1]
y_pred = [1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1]

# è®¡ç®—æ··æ·†çŸ©é˜µ
cm = confusion_matrix(y_true, y_pred)
print("æ··æ·†çŸ©é˜µ:")
print(cm)

# æå–å„ä¸ªå€¼
TN, FP, FN, TP = cm.ravel()
print(f"\nTN={TN}, FP={FP}, FN={FN}, TP={TP}")

# å¯è§†åŒ–
fig, ax = plt.subplots(figsize=(6, 5))
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Negative', 'Positive'])
disp.plot(ax=ax, cmap='Blues')
plt.title('Confusion Matrix')
plt.show()

# æ‰‹åŠ¨ç»˜åˆ¶æ›´ç¾è§‚çš„æ··æ·†çŸ©é˜µ
import seaborn as sns

plt.figure(figsize=(6, 5))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=['Pred 0', 'Pred 1'],
            yticklabels=['True 0', 'True 1'])
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()
```

#### å¤šåˆ†ç±»æ··æ·†çŸ©é˜µ

```python
from sklearn.metrics import confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

# ä¸‰åˆ†ç±»ç¤ºä¾‹
y_true = [0, 1, 2, 0, 1, 2, 0, 1, 2, 0]
y_pred = [0, 1, 1, 0, 2, 2, 0, 1, 2, 1]
labels = ['Cat', 'Dog', 'Bird']

cm = confusion_matrix(y_true, y_pred)

plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=labels, yticklabels=labels)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Multi-class Confusion Matrix')
plt.show()

# å½’ä¸€åŒ–æ··æ·†çŸ©é˜µï¼ˆæŒ‰è¡Œå½’ä¸€åŒ–ï¼Œæ˜¾ç¤ºå¬å›ç‡ï¼‰
cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]

plt.figure(figsize=(8, 6))
sns.heatmap(cm_normalized, annot=True, fmt='.2%', cmap='Blues',
            xticklabels=labels, yticklabels=labels)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Normalized Confusion Matrix (Recall)')
plt.show()
```

### 2.2 Accuracyï¼ˆå‡†ç¡®ç‡ï¼‰

```python
from sklearn.metrics import accuracy_score

y_true = [1, 0, 1, 1, 0, 1, 0, 0, 1, 0]
y_pred = [1, 0, 1, 0, 0, 1, 1, 0, 1, 0]

# æ–¹æ³• 1ï¼šsklearn
accuracy = accuracy_score(y_true, y_pred)
print(f"Accuracy: {accuracy:.4f}")

# æ–¹æ³• 2ï¼šæ‰‹åŠ¨è®¡ç®—
cm = confusion_matrix(y_true, y_pred)
TN, FP, FN, TP = cm.ravel()
accuracy_manual = (TP + TN) / (TP + TN + FP + FN)
print(f"Accuracy (manual): {accuracy_manual:.4f}")
```

**å…¬å¼**ï¼š
$$Accuracy = \frac{TP + TN}{TP + TN + FP + FN}$$

**ä¼˜ç‚¹**ï¼šç›´è§‚æ˜“æ‡‚
**ç¼ºç‚¹**ï¼šç±»åˆ«ä¸å¹³è¡¡æ—¶ä¼šè¯¯å¯¼

```python
# ç±»åˆ«ä¸å¹³è¡¡ç¤ºä¾‹
# 1000 ä¸ªæ ·æœ¬ï¼Œ990 ä¸ªè´Ÿæ ·æœ¬ï¼Œ10 ä¸ªæ­£æ ·æœ¬
y_true = [0] * 990 + [1] * 10
y_pred = [0] * 1000  # å…¨éƒ¨é¢„æµ‹ä¸ºè´Ÿ

accuracy = accuracy_score(y_true, y_pred)
print(f"Accuracy: {accuracy:.2%}")  # 99%ï¼ä½†å®Œå…¨æ²¡æœ‰é¢„æµ‹å‡ºæ­£æ ·æœ¬

# è¿™å°±æ˜¯ä¸ºä»€ä¹ˆä¸èƒ½åªçœ‹ Accuracy
```

### 2.3 Precisionï¼ˆç²¾ç¡®ç‡ï¼‰

```python
from sklearn.metrics import precision_score

y_true = [1, 0, 1, 1, 0, 1, 0, 0, 1, 0]
y_pred = [1, 0, 1, 0, 0, 1, 1, 0, 1, 0]

precision = precision_score(y_true, y_pred)
print(f"Precision: {precision:.4f}")

# æ‰‹åŠ¨è®¡ç®—
cm = confusion_matrix(y_true, y_pred)
TN, FP, FN, TP = cm.ravel()
precision_manual = TP / (TP + FP)
print(f"Precision (manual): {precision_manual:.4f}")
```

**å…¬å¼**ï¼š
$$Precision = \frac{TP}{TP + FP}$$

**å«ä¹‰**ï¼šé¢„æµ‹ä¸ºæ­£çš„æ ·æœ¬ä¸­ï¼Œæœ‰å¤šå°‘æ˜¯çœŸæ­£ä¸ºæ­£çš„
**å…³æ³¨**ï¼šå‡å°‘è¯¯æŠ¥ï¼ˆFalse Positiveï¼‰

**åº”ç”¨åœºæ™¯**ï¼š
- åƒåœ¾é‚®ä»¶æ£€æµ‹ï¼ˆä¸æƒ³è¯¯åˆ¤æ­£å¸¸é‚®ä»¶ï¼‰
- æ¨èç³»ç»Ÿï¼ˆæ¨èçš„ä¸œè¥¿è¦å‡†ï¼‰
- æœç´¢å¼•æ“ï¼ˆæœç´¢ç»“æœè¦ç›¸å…³ï¼‰

### 2.4 Recallï¼ˆå¬å›ç‡ï¼‰

```python
from sklearn.metrics import recall_score

y_true = [1, 0, 1, 1, 0, 1, 0, 0, 1, 0]
y_pred = [1, 0, 1, 0, 0, 1, 1, 0, 1, 0]

recall = recall_score(y_true, y_pred)
print(f"Recall: {recall:.4f}")

# æ‰‹åŠ¨è®¡ç®—
cm = confusion_matrix(y_true, y_pred)
TN, FP, FN, TP = cm.ravel()
recall_manual = TP / (TP + FN)
print(f"Recall (manual): {recall_manual:.4f}")
```

**å…¬å¼**ï¼š
$$Recall = \frac{TP}{TP + FN}$$

**å«ä¹‰**ï¼šçœŸå®ä¸ºæ­£çš„æ ·æœ¬ä¸­ï¼Œæœ‰å¤šå°‘è¢«æ­£ç¡®é¢„æµ‹å‡ºæ¥
**å…³æ³¨**ï¼šå‡å°‘æ¼æŠ¥ï¼ˆFalse Negativeï¼‰

**åº”ç”¨åœºæ™¯**ï¼š
- ç–¾ç—…æ£€æµ‹ï¼ˆä¸æƒ³æ¼è¯Šï¼‰
- æ¬ºè¯ˆæ£€æµ‹ï¼ˆä¸æƒ³æ¼æ‰æ¬ºè¯ˆäº¤æ˜“ï¼‰
- å®‰å…¨æ£€æµ‹ï¼ˆä¸æƒ³æ¼æ‰å¨èƒï¼‰

### 2.5 F1 Score

```python
from sklearn.metrics import f1_score

y_true = [1, 0, 1, 1, 0, 1, 0, 0, 1, 0]
y_pred = [1, 0, 1, 0, 0, 1, 1, 0, 1, 0]

f1 = f1_score(y_true, y_pred)
print(f"F1 Score: {f1:.4f}")

# æ‰‹åŠ¨è®¡ç®—
precision = precision_score(y_true, y_pred)
recall = recall_score(y_true, y_pred)
f1_manual = 2 * precision * recall / (precision + recall)
print(f"F1 Score (manual): {f1_manual:.4f}")
```

**å…¬å¼**ï¼š
$$F1 = \frac{2 \times Precision \times Recall}{Precision + Recall}$$

**å«ä¹‰**ï¼šPrecision å’Œ Recall çš„è°ƒå’Œå¹³å‡
**ç‰¹ç‚¹**ï¼šå½“ P å’Œ R å·®è·å¤§æ—¶ï¼ŒF1 ä¼šåä½

```python
# F1 vs ç®—æœ¯å¹³å‡
p, r = 0.9, 0.1
f1 = 2 * p * r / (p + r)
arithmetic_mean = (p + r) / 2

print(f"Precision: {p}, Recall: {r}")
print(f"F1 Score: {f1:.4f}")
print(f"ç®—æœ¯å¹³å‡: {arithmetic_mean:.4f}")
# F1 ä¼šæƒ©ç½šä¸å¹³è¡¡çš„ P å’Œ R
```

### 2.6 F-beta Score

```python
from sklearn.metrics import fbeta_score

y_true = [1, 0, 1, 1, 0, 1, 0, 0, 1, 0]
y_pred = [1, 0, 1, 0, 0, 1, 1, 0, 1, 0]

# F0.5ï¼šæ›´é‡è§† Precision
f05 = fbeta_score(y_true, y_pred, beta=0.5)
print(f"F0.5 Score: {f05:.4f}")

# F1ï¼šP å’Œ R åŒç­‰é‡è¦
f1 = fbeta_score(y_true, y_pred, beta=1)
print(f"F1 Score: {f1:.4f}")

# F2ï¼šæ›´é‡è§† Recall
f2 = fbeta_score(y_true, y_pred, beta=2)
print(f"F2 Score: {f2:.4f}")
```

**å…¬å¼**ï¼š
$$F_\beta = \frac{(1 + \beta^2) \times Precision \times Recall}{\beta^2 \times Precision + Recall}$$

- Î² < 1ï¼šæ›´é‡è§† Precision
- Î² = 1ï¼šåŒç­‰é‡è§†ï¼ˆå³ F1ï¼‰
- Î² > 1ï¼šæ›´é‡è§† Recall

### 2.7 Classification Report

```python
from sklearn.metrics import classification_report

y_true = [0, 0, 0, 1, 1, 1, 2, 2, 2, 2]
y_pred = [0, 0, 1, 1, 1, 0, 2, 2, 2, 1]

print(classification_report(y_true, y_pred, target_names=['Class 0', 'Class 1', 'Class 2']))
```

è¾“å‡ºè§£è¯»ï¼š

```
              precision    recall  f1-score   support

     Class 0       0.67      0.67      0.67         3
     Class 1       0.50      0.67      0.57         3
     Class 2       1.00      0.75      0.86         4

    accuracy                           0.70        10
   macro avg       0.72      0.69      0.70        10
weighted avg       0.74      0.70      0.71        10
```

- **support**ï¼šæ¯ä¸ªç±»åˆ«çš„çœŸå®æ ·æœ¬æ•°
- **macro avg**ï¼šå„ç±»åˆ«æŒ‡æ ‡çš„ç®€å•å¹³å‡
- **weighted avg**ï¼šå„ç±»åˆ«æŒ‡æ ‡çš„åŠ æƒå¹³å‡ï¼ˆæŒ‰ support åŠ æƒï¼‰

### 2.8 ROC æ›²çº¿ä¸ AUC

#### ROC æ›²çº¿

```python
from sklearn.metrics import roc_curve, roc_auc_score
import matplotlib.pyplot as plt
import numpy as np

# æ¨¡æ‹Ÿé¢„æµ‹æ¦‚ç‡
np.random.seed(42)
y_true = np.array([0, 0, 0, 0, 0, 1, 1, 1, 1, 1])
y_scores = np.array([0.1, 0.2, 0.3, 0.35, 0.6, 0.4, 0.65, 0.7, 0.8, 0.9])

# è®¡ç®— ROC æ›²çº¿
fpr, tpr, thresholds = roc_curve(y_true, y_scores)

print("é˜ˆå€¼\t\tFPR\t\tTPR")
for t, f, tp in zip(thresholds, fpr, tpr):
    print(f"{t:.2f}\t\t{f:.2f}\t\t{tp:.2f}")

# ç”» ROC æ›²çº¿
plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, 'b-', linewidth=2, label=f'ROC curve (AUC = {roc_auc_score(y_true, y_scores):.2f})')
plt.plot([0, 1], [0, 1], 'r--', linewidth=2, label='Random (AUC = 0.5)')
plt.fill_between(fpr, tpr, alpha=0.3)

plt.xlim([0, 1])
plt.ylim([0, 1.05])
plt.xlabel('False Positive Rate (FPR)', fontsize=12)
plt.ylabel('True Positive Rate (TPR)', fontsize=12)
plt.title('ROC Curve', fontsize=14)
plt.legend(loc='lower right')
plt.grid(True, alpha=0.3)
plt.show()
```

**FPRï¼ˆå‡é˜³æ€§ç‡ï¼‰**ï¼šè´Ÿæ ·æœ¬ä¸­è¢«é”™è¯¯é¢„æµ‹ä¸ºæ­£çš„æ¯”ä¾‹
$$FPR = \frac{FP}{FP + TN}$$

**TPRï¼ˆçœŸé˜³æ€§ç‡ï¼‰= Recall**ï¼šæ­£æ ·æœ¬ä¸­è¢«æ­£ç¡®é¢„æµ‹çš„æ¯”ä¾‹
$$TPR = \frac{TP}{TP + FN}$$

#### AUC è§£è¯»

```python
# AUC = æ›²çº¿ä¸‹é¢ç§¯
auc = roc_auc_score(y_true, y_scores)
print(f"AUC: {auc:.4f}")
```

| AUC å€¼ | å«ä¹‰ |
|--------|------|
| 0.5 | éšæœºçŒœæµ‹ |
| 0.5-0.7 | æ•ˆæœè¾ƒå·® |
| 0.7-0.8 | ä¸€èˆ¬ |
| 0.8-0.9 | è‰¯å¥½ |
| 0.9-1.0 | ä¼˜ç§€ |
| 1.0 | å®Œç¾åˆ†ç±» |

**AUC çš„æ¦‚ç‡è§£é‡Š**ï¼šéšæœºé€‰æ‹©ä¸€ä¸ªæ­£æ ·æœ¬å’Œä¸€ä¸ªè´Ÿæ ·æœ¬ï¼Œæ¨¡å‹å°†æ­£æ ·æœ¬æ’åœ¨è´Ÿæ ·æœ¬å‰é¢çš„æ¦‚ç‡ã€‚

#### å¤šåˆ†ç±» ROC

```python
from sklearn.metrics import roc_curve, auc
from sklearn.preprocessing import label_binarize
import matplotlib.pyplot as plt
import numpy as np

# ä¸‰åˆ†ç±»ç¤ºä¾‹
y_true = [0, 1, 2, 0, 1, 2, 0, 1, 2, 0]
y_score = np.array([
    [0.8, 0.1, 0.1],
    [0.2, 0.7, 0.1],
    [0.1, 0.2, 0.7],
    [0.7, 0.2, 0.1],
    [0.3, 0.6, 0.1],
    [0.1, 0.1, 0.8],
    [0.6, 0.3, 0.1],
    [0.2, 0.5, 0.3],
    [0.2, 0.3, 0.5],
    [0.9, 0.05, 0.05]
])

# äºŒå€¼åŒ–æ ‡ç­¾
y_bin = label_binarize(y_true, classes=[0, 1, 2])
n_classes = 3

# è®¡ç®—æ¯ä¸ªç±»åˆ«çš„ ROC
plt.figure(figsize=(8, 6))
colors = ['blue', 'green', 'red']

for i, color in enumerate(colors):
    fpr, tpr, _ = roc_curve(y_bin[:, i], y_score[:, i])
    roc_auc = auc(fpr, tpr)
    plt.plot(fpr, tpr, color=color, label=f'Class {i} (AUC = {roc_auc:.2f})')

plt.plot([0, 1], [0, 1], 'k--')
plt.xlabel('FPR')
plt.ylabel('TPR')
plt.title('Multi-class ROC Curves')
plt.legend()
plt.show()
```

### 2.9 Precision-Recall æ›²çº¿

```python
from sklearn.metrics import precision_recall_curve, average_precision_score
import matplotlib.pyplot as plt
import numpy as np

np.random.seed(42)
y_true = np.array([0, 0, 0, 0, 0, 1, 1, 1, 1, 1])
y_scores = np.array([0.1, 0.2, 0.3, 0.35, 0.6, 0.4, 0.65, 0.7, 0.8, 0.9])

# è®¡ç®— PR æ›²çº¿
precision, recall, thresholds = precision_recall_curve(y_true, y_scores)
ap = average_precision_score(y_true, y_scores)

# ç”» PR æ›²çº¿
plt.figure(figsize=(8, 6))
plt.plot(recall, precision, 'b-', linewidth=2, label=f'PR curve (AP = {ap:.2f})')
plt.fill_between(recall, precision, alpha=0.3)

plt.xlabel('Recall', fontsize=12)
plt.ylabel('Precision', fontsize=12)
plt.title('Precision-Recall Curve', fontsize=14)
plt.legend()
plt.grid(True, alpha=0.3)
plt.show()
```

**ROC vs PR æ›²çº¿**ï¼š
- ROCï¼šé€‚ç”¨äºç±»åˆ«å¹³è¡¡çš„æ•°æ®
- PRï¼šé€‚ç”¨äºç±»åˆ«ä¸å¹³è¡¡çš„æ•°æ®ï¼ˆæ›´å…³æ³¨æ­£ç±»ï¼‰

---

## 3. å›å½’æŒ‡æ ‡

### 3.1 MAEï¼ˆå¹³å‡ç»å¯¹è¯¯å·®ï¼‰

```python
from sklearn.metrics import mean_absolute_error
import numpy as np

y_true = np.array([3, 5, 2.5, 7, 4.2])
y_pred = np.array([2.8, 5.2, 2.3, 7.5, 4.0])

# sklearn
mae = mean_absolute_error(y_true, y_pred)
print(f"MAE: {mae:.4f}")

# æ‰‹åŠ¨è®¡ç®—
mae_manual = np.mean(np.abs(y_true - y_pred))
print(f"MAE (manual): {mae_manual:.4f}")
```

**å…¬å¼**ï¼š
$$MAE = \frac{1}{n}\sum_{i=1}^{n}|y_i - \hat{y}_i|$$

**ç‰¹ç‚¹**ï¼š
- å’ŒåŸå§‹æ•°æ®åŒå•ä½
- å¯¹å¼‚å¸¸å€¼æ¯”è¾ƒé²æ£’
- æ‰€æœ‰è¯¯å·®åŒç­‰æƒé‡

### 3.2 MSEï¼ˆå‡æ–¹è¯¯å·®ï¼‰

```python
from sklearn.metrics import mean_squared_error
import numpy as np

y_true = np.array([3, 5, 2.5, 7, 4.2])
y_pred = np.array([2.8, 5.2, 2.3, 7.5, 4.0])

# sklearn
mse = mean_squared_error(y_true, y_pred)
print(f"MSE: {mse:.4f}")

# æ‰‹åŠ¨è®¡ç®—
mse_manual = np.mean((y_true - y_pred) ** 2)
print(f"MSE (manual): {mse_manual:.4f}")
```

**å…¬å¼**ï¼š
$$MSE = \frac{1}{n}\sum_{i=1}^{n}(y_i - \hat{y}_i)^2$$

**ç‰¹ç‚¹**ï¼š
- å¯¹å¤§è¯¯å·®æ›´æ•æ„Ÿï¼ˆå¹³æ–¹æ”¾å¤§ï¼‰
- å•ä½æ˜¯åŸå§‹æ•°æ®å•ä½çš„å¹³æ–¹

### 3.3 RMSEï¼ˆå‡æ–¹æ ¹è¯¯å·®ï¼‰

```python
import numpy as np
from sklearn.metrics import mean_squared_error

y_true = np.array([3, 5, 2.5, 7, 4.2])
y_pred = np.array([2.8, 5.2, 2.3, 7.5, 4.0])

# RMSE = sqrt(MSE)
rmse = np.sqrt(mean_squared_error(y_true, y_pred))
print(f"RMSE: {rmse:.4f}")

# æˆ–è€…ä½¿ç”¨å‚æ•°
rmse = mean_squared_error(y_true, y_pred, squared=False)
print(f"RMSE: {rmse:.4f}")
```

**å…¬å¼**ï¼š
$$RMSE = \sqrt{\frac{1}{n}\sum_{i=1}^{n}(y_i - \hat{y}_i)^2}$$

**ç‰¹ç‚¹**ï¼š
- å’ŒåŸå§‹æ•°æ®åŒå•ä½
- ä¿ç•™å¯¹å¤§è¯¯å·®çš„æ•æ„Ÿæ€§

### 3.4 RÂ² Scoreï¼ˆå†³å®šç³»æ•°ï¼‰

```python
from sklearn.metrics import r2_score
import numpy as np

y_true = np.array([3, 5, 2.5, 7, 4.2])
y_pred = np.array([2.8, 5.2, 2.3, 7.5, 4.0])

# sklearn
r2 = r2_score(y_true, y_pred)
print(f"RÂ² Score: {r2:.4f}")

# æ‰‹åŠ¨è®¡ç®—
ss_res = np.sum((y_true - y_pred) ** 2)  # æ®‹å·®å¹³æ–¹å’Œ
ss_tot = np.sum((y_true - y_true.mean()) ** 2)  # æ€»å¹³æ–¹å’Œ
r2_manual = 1 - ss_res / ss_tot
print(f"RÂ² Score (manual): {r2_manual:.4f}")
```

**å…¬å¼**ï¼š
$$R^2 = 1 - \frac{SS_{res}}{SS_{tot}} = 1 - \frac{\sum(y_i - \hat{y}_i)^2}{\sum(y_i - \bar{y})^2}$$

**è§£è¯»**ï¼š
- RÂ² = 1ï¼šå®Œç¾é¢„æµ‹
- RÂ² = 0ï¼šå’Œé¢„æµ‹å‡å€¼ä¸€æ ·å¥½
- RÂ² < 0ï¼šæ¯”é¢„æµ‹å‡å€¼è¿˜å·®

### 3.5 MAPEï¼ˆå¹³å‡ç»å¯¹ç™¾åˆ†æ¯”è¯¯å·®ï¼‰

```python
import numpy as np

y_true = np.array([3, 5, 2.5, 7, 4.2])
y_pred = np.array([2.8, 5.2, 2.3, 7.5, 4.0])

# æ‰‹åŠ¨è®¡ç®—
mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100
print(f"MAPE: {mape:.2f}%")

# æ³¨æ„ï¼šå½“ y_true æœ‰ 0 æ—¶ä¼šå‡ºé—®é¢˜
# å¯ä»¥ç”¨ SMAPE ä»£æ›¿
```

**å…¬å¼**ï¼š
$$MAPE = \frac{100\%}{n}\sum_{i=1}^{n}\left|\frac{y_i - \hat{y}_i}{y_i}\right|$$

**ç‰¹ç‚¹**ï¼š
- ç™¾åˆ†æ¯”å½¢å¼ï¼Œæ˜“äºç†è§£
- å¯¹å°å€¼æ•æ„Ÿ

### 3.6 æŒ‡æ ‡å¯¹æ¯”å¯è§†åŒ–

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

# ç¤ºä¾‹æ•°æ®
np.random.seed(42)
y_true = np.linspace(1, 10, 50) + np.random.normal(0, 0.5, 50)
y_pred = np.linspace(1, 10, 50) + np.random.normal(0, 1, 50)

# è®¡ç®—å„æŒ‡æ ‡
mae = mean_absolute_error(y_true, y_pred)
mse = mean_squared_error(y_true, y_pred)
rmse = np.sqrt(mse)
r2 = r2_score(y_true, y_pred)

# å¯è§†åŒ–
fig, axes = plt.subplots(1, 2, figsize=(14, 5))

# é¢„æµ‹ vs çœŸå®
axes[0].scatter(y_true, y_pred, alpha=0.6)
axes[0].plot([y_true.min(), y_true.max()], [y_true.min(), y_true.max()], 'r--', lw=2)
axes[0].set_xlabel('True Values')
axes[0].set_ylabel('Predicted Values')
axes[0].set_title(f'Predicted vs True\nRÂ² = {r2:.4f}')

# æ®‹å·®åˆ†å¸ƒ
residuals = y_true - y_pred
axes[1].hist(residuals, bins=20, edgecolor='black', alpha=0.7)
axes[1].axvline(0, color='red', linestyle='--')
axes[1].set_xlabel('Residuals')
axes[1].set_ylabel('Frequency')
axes[1].set_title(f'Residual Distribution\nMAE = {mae:.4f}, RMSE = {rmse:.4f}')

plt.tight_layout()
plt.show()
```

---

## 4. æŒ‡æ ‡é€‰æ‹©æŒ‡å—

### åˆ†ç±»ä»»åŠ¡

| åœºæ™¯ | æ¨èæŒ‡æ ‡ | åŸå›  |
|------|---------|------|
| ç±»åˆ«å¹³è¡¡ | Accuracy, F1 | å„ç±»åˆ«åŒç­‰é‡è¦ |
| ç±»åˆ«ä¸å¹³è¡¡ | F1, AUC-PR | Accuracy ä¼šè¯¯å¯¼ |
| å…³æ³¨è¯¯æŠ¥æˆæœ¬ | Precision | å‡å°‘ False Positive |
| å…³æ³¨æ¼æŠ¥æˆæœ¬ | Recall | å‡å°‘ False Negative |
| éœ€è¦æ¦‚ç‡è¾“å‡º | AUC-ROC | è¯„ä¼°æ’åºèƒ½åŠ› |
| æ¯”è¾ƒå¤šä¸ªæ¨¡å‹ | AUC | é˜ˆå€¼æ— å…³ |

### å›å½’ä»»åŠ¡

| åœºæ™¯ | æ¨èæŒ‡æ ‡ | åŸå›  |
|------|---------|------|
| é€šç”¨åœºæ™¯ | RMSE, RÂ² | å¸¸ç”¨ä¸”æ˜“è§£é‡Š |
| å¯¹å¼‚å¸¸å€¼æ•æ„Ÿ | RMSE | å¹³æ–¹æ”¾å¤§å¤§è¯¯å·® |
| å¯¹å¼‚å¸¸å€¼é²æ£’ | MAE | ç»å¯¹å€¼ä¸æ”¾å¤§ |
| éœ€è¦ç™¾åˆ†æ¯”è¯¯å·® | MAPE | æ˜“äºç†è§£ |
| æ¯”è¾ƒä¸åŒé‡çº§æ•°æ® | RÂ² | æ— é‡çº² |

---

## 5. ç»ƒä¹ é¢˜

### åŸºç¡€ç»ƒä¹ 

1. ç»™å®šæ··æ·†çŸ©é˜µ `[[50, 10], [5, 35]]`ï¼Œè®¡ç®— Accuracyã€Precisionã€Recallã€F1
2. è§£é‡Šä¸ºä»€ä¹ˆåœ¨ç±»åˆ«ä¸å¹³è¡¡æ—¶ Accuracy ä¸æ˜¯å¥½æŒ‡æ ‡
3. ç”»ä¸€ä¸ª ROC æ›²çº¿ï¼Œå¹¶è®¡ç®— AUC

### è¿›é˜¶ç»ƒä¹ 

4. æ¯”è¾ƒ MAE å’Œ RMSE å¯¹å¼‚å¸¸å€¼çš„æ•æ„Ÿç¨‹åº¦
5. å®ç°ä¸€ä¸ªå‡½æ•°ï¼Œè¾“å…¥ y_true å’Œ y_predï¼Œè¾“å‡ºæ‰€æœ‰å›å½’æŒ‡æ ‡

### å‚è€ƒç­”æ¡ˆ

<details>
<summary>ç»ƒä¹  1 å‚è€ƒç­”æ¡ˆ</summary>

```python
import numpy as np

# æ··æ·†çŸ©é˜µ
cm = np.array([[50, 10], [5, 35]])
TN, FP, FN, TP = cm.ravel()

accuracy = (TP + TN) / (TP + TN + FP + FN)
precision = TP / (TP + FP)
recall = TP / (TP + FN)
f1 = 2 * precision * recall / (precision + recall)

print(f"TN={TN}, FP={FP}, FN={FN}, TP={TP}")
print(f"Accuracy: {accuracy:.4f}")
print(f"Precision: {precision:.4f}")
print(f"Recall: {recall:.4f}")
print(f"F1 Score: {f1:.4f}")
```

</details>

<details>
<summary>ç»ƒä¹  5 å‚è€ƒç­”æ¡ˆ</summary>

```python
import numpy as np
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

def regression_report(y_true, y_pred):
    """è®¡ç®—å¹¶è¿”å›æ‰€æœ‰å›å½’æŒ‡æ ‡"""
    y_true = np.array(y_true)
    y_pred = np.array(y_pred)

    mae = mean_absolute_error(y_true, y_pred)
    mse = mean_squared_error(y_true, y_pred)
    rmse = np.sqrt(mse)
    r2 = r2_score(y_true, y_pred)

    # MAPEï¼ˆé¿å…é™¤ä»¥0ï¼‰
    mask = y_true != 0
    mape = np.mean(np.abs((y_true[mask] - y_pred[mask]) / y_true[mask])) * 100

    print("=" * 40)
    print("å›å½’è¯„ä¼°æŠ¥å‘Š")
    print("=" * 40)
    print(f"MAE:  {mae:.4f}")
    print(f"MSE:  {mse:.4f}")
    print(f"RMSE: {rmse:.4f}")
    print(f"RÂ²:   {r2:.4f}")
    print(f"MAPE: {mape:.2f}%")
    print("=" * 40)

    return {'MAE': mae, 'MSE': mse, 'RMSE': rmse, 'R2': r2, 'MAPE': mape}

# æµ‹è¯•
y_true = [3, 5, 2.5, 7, 4.2]
y_pred = [2.8, 5.2, 2.3, 7.5, 4.0]
regression_report(y_true, y_pred)
```

</details>

---

## â¡ï¸ ä¸‹ä¸€æ­¥

å­¦å®Œæœ¬èŠ‚åï¼Œç»§ç»­å­¦ä¹  [03-è¿‡æ‹Ÿåˆä¸æ­£åˆ™åŒ–.md](./03-è¿‡æ‹Ÿåˆä¸æ­£åˆ™åŒ–.md)

