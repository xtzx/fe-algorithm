# ğŸ“Š é˜¶æ®µ 1ï¼šä¼ ç»Ÿæœºå™¨å­¦ä¹ ï¼ˆClassic MLï¼‰

> **ç›®æ ‡**ï¼šç†è§£æœºå™¨å­¦ä¹ æµç¨‹ï¼ŒæŒæ¡ scikit-learnï¼Œå®Œæˆå®æˆ˜é¡¹ç›®
> **å»ºè®®æ—¶é•¿**ï¼š2ï½3 å‘¨ï¼ˆæ¯å¤© 2ï½3 å°æ—¶ï¼‰
> **å‰ç½®è¦æ±‚**ï¼šPython + NumPy + Pandas åŸºç¡€ï¼ˆé˜¶æ®µ 0ï¼‰

---

## ğŸ¯ æœ¬é˜¶æ®µç›®æ ‡

å®Œæˆæœ¬é˜¶æ®µåï¼Œä½ å°†èƒ½å¤Ÿï¼š

1. **ç†è§£æœºå™¨å­¦ä¹ çš„å®Œæ•´æµç¨‹**ï¼šä»æ•°æ®æ¸…æ´—åˆ°æ¨¡å‹éƒ¨ç½²
2. **ç†Ÿç»ƒä½¿ç”¨ scikit-learn**ï¼šè®­ç»ƒã€è¯„ä¼°ã€è°ƒä¼˜å¸¸è§æ¨¡å‹
3. **æŒæ¡æ¨¡å‹è¯„ä¼°æ–¹æ³•**ï¼šçŸ¥é“ä»€ä¹ˆæ—¶å€™ç”¨ä»€ä¹ˆæŒ‡æ ‡
4. **å®Œæˆå®æˆ˜é¡¹ç›®**ï¼šæˆ¿ä»·é¢„æµ‹ã€æ³°å¦å°¼å…‹ç”Ÿå­˜é¢„æµ‹ã€ç”¨æˆ·èšç±»

---

## ğŸ“ æœ¬é˜¶æ®µæ–‡ä»¶ç»“æ„

```
é˜¶æ®µ1-ä¼ ç»Ÿæœºå™¨å­¦ä¹ /
â”œâ”€â”€ 00-æ¦‚è¿°.md                 # æœ¬æ–‡ä»¶
â”œâ”€â”€ 01-æœºå™¨å­¦ä¹ æµæ°´çº¿.md        # æ•°æ®æ¸…æ´—ã€ç‰¹å¾å·¥ç¨‹ã€æ•°æ®åˆ’åˆ†
â”œâ”€â”€ 02-æ¨¡å‹è¯„ä¼°æŒ‡æ ‡.md          # åˆ†ç±»æŒ‡æ ‡ã€å›å½’æŒ‡æ ‡ã€æ··æ·†çŸ©é˜µ
â”œâ”€â”€ 03-è¿‡æ‹Ÿåˆä¸æ­£åˆ™åŒ–.md        # è¿‡æ‹Ÿåˆã€æ¬ æ‹Ÿåˆã€L1/L2ã€äº¤å‰éªŒè¯
â”œâ”€â”€ 04-çº¿æ€§æ¨¡å‹.md              # çº¿æ€§å›å½’ã€é€»è¾‘å›å½’
â”œâ”€â”€ 05-æ ‘æ¨¡å‹ä¸é›†æˆå­¦ä¹ .md      # å†³ç­–æ ‘ã€éšæœºæ£®æ—ã€XGBoost/LightGBM
â”œâ”€â”€ 06-æ— ç›‘ç£å­¦ä¹ .md            # K-Meansã€PCAã€t-SNE/UMAP
â”œâ”€â”€ 07-è¶…å‚æ•°è°ƒä¼˜.md            # GridSearchCVã€RandomizedSearchCV
â”œâ”€â”€ 08-é¡¹ç›®-æˆ¿ä»·é¢„æµ‹.md         # å®Œæ•´å›å½’é¡¹ç›®
â”œâ”€â”€ 09-é¡¹ç›®-æ³°å¦å°¼å…‹.md         # å®Œæ•´åˆ†ç±»é¡¹ç›®
â”œâ”€â”€ 10-é¡¹ç›®-ç”¨æˆ·èšç±».md         # å®Œæ•´èšç±»é¡¹ç›®
â””â”€â”€ 11-è‡ªæµ‹æ¸…å•.md              # å­¦ä¹ æ£€éªŒ
```

---

## ğŸ—ºï¸ æœºå™¨å­¦ä¹ å…¨æ™¯å›¾

### æœºå™¨å­¦ä¹  vs ä¼ ç»Ÿç¼–ç¨‹

```
ä¼ ç»Ÿç¼–ç¨‹ï¼š
  è¾“å…¥æ•°æ® + è§„åˆ™ â†’ ç¨‹åº â†’ è¾“å‡ºç»“æœ
  ï¼ˆäººå·¥ç¼–å†™è§„åˆ™ï¼‰

æœºå™¨å­¦ä¹ ï¼š
  è¾“å…¥æ•°æ® + æœŸæœ›è¾“å‡º â†’ ç®—æ³• â†’ å­¦ä¹ å‡ºè§„åˆ™ï¼ˆæ¨¡å‹ï¼‰
  ï¼ˆè®©æœºå™¨ä»æ•°æ®ä¸­å­¦ä¹ è§„åˆ™ï¼‰
```

### æœºå™¨å­¦ä¹ ä»»åŠ¡åˆ†ç±»

```
æœºå™¨å­¦ä¹ 
â”œâ”€â”€ ç›‘ç£å­¦ä¹ ï¼ˆæœ‰æ ‡ç­¾ï¼‰
â”‚   â”œâ”€â”€ åˆ†ç±»ï¼ˆClassificationï¼‰
â”‚   â”‚   â”œâ”€â”€ äºŒåˆ†ç±»ï¼šåƒåœ¾é‚®ä»¶æ£€æµ‹ã€ç–¾ç—…è¯Šæ–­
â”‚   â”‚   â””â”€â”€ å¤šåˆ†ç±»ï¼šå›¾åƒè¯†åˆ«ã€æƒ…æ„Ÿåˆ†æ
â”‚   â””â”€â”€ å›å½’ï¼ˆRegressionï¼‰
â”‚       â””â”€â”€ é¢„æµ‹è¿ç»­å€¼ï¼šæˆ¿ä»·é¢„æµ‹ã€é”€é‡é¢„æµ‹
â”‚
â”œâ”€â”€ æ— ç›‘ç£å­¦ä¹ ï¼ˆæ— æ ‡ç­¾ï¼‰
â”‚   â”œâ”€â”€ èšç±»ï¼ˆClusteringï¼‰
â”‚   â”‚   â””â”€â”€ ç”¨æˆ·åˆ†ç¾¤ã€å¼‚å¸¸æ£€æµ‹
â”‚   â””â”€â”€ é™ç»´ï¼ˆDimensionality Reductionï¼‰
â”‚       â””â”€â”€ ç‰¹å¾å‹ç¼©ã€æ•°æ®å¯è§†åŒ–
â”‚
â””â”€â”€ å¼ºåŒ–å­¦ä¹ ï¼ˆå¥–åŠ±ä¿¡å·ï¼‰
    â””â”€â”€ æ¸¸æˆAIã€æœºå™¨äººæ§åˆ¶
```

### æœ¬é˜¶æ®µè¦†ç›–çš„ç®—æ³•

| ç±»å‹ | ç®—æ³• | åº”ç”¨åœºæ™¯ |
|------|------|---------|
| **çº¿æ€§æ¨¡å‹** | Linear Regression | å›å½’åŸºçº¿ |
| | Logistic Regression | äºŒåˆ†ç±»åŸºçº¿ |
| | Ridge / Lasso | å¸¦æ­£åˆ™åŒ–çš„å›å½’ |
| **æ ‘æ¨¡å‹** | Decision Tree | å¯è§£é‡Šæ€§è¦æ±‚é«˜ |
| | Random Forest | é€šç”¨æ€§å¼º |
| | XGBoost / LightGBM | ç«èµ›é¦–é€‰ |
| **æ— ç›‘ç£** | K-Means | èšç±» |
| | PCA | é™ç»´ |
| | t-SNE / UMAP | å¯è§†åŒ– |

---

## ğŸ“… å»ºè®®å­¦ä¹ è®¡åˆ’

### ç¬¬ 1 å‘¨ï¼šåŸºç¡€æ¦‚å¿µ + çº¿æ€§æ¨¡å‹

| å¤©æ•° | å†…å®¹ | æ–‡ä»¶ |
|------|------|------|
| Day 1-2 | æœºå™¨å­¦ä¹ æµæ°´çº¿ | `01-æœºå™¨å­¦ä¹ æµæ°´çº¿.md` |
| Day 3 | æ¨¡å‹è¯„ä¼°æŒ‡æ ‡ | `02-æ¨¡å‹è¯„ä¼°æŒ‡æ ‡.md` |
| Day 4 | è¿‡æ‹Ÿåˆä¸æ­£åˆ™åŒ– | `03-è¿‡æ‹Ÿåˆä¸æ­£åˆ™åŒ–.md` |
| Day 5-6 | çº¿æ€§æ¨¡å‹ | `04-çº¿æ€§æ¨¡å‹.md` |
| Day 7 | ç»ƒä¹  + å¤ä¹  | |

### ç¬¬ 2 å‘¨ï¼šæ ‘æ¨¡å‹ + æ— ç›‘ç£

| å¤©æ•° | å†…å®¹ | æ–‡ä»¶ |
|------|------|------|
| Day 8-10 | æ ‘æ¨¡å‹ä¸é›†æˆå­¦ä¹  | `05-æ ‘æ¨¡å‹ä¸é›†æˆå­¦ä¹ .md` |
| Day 11-12 | æ— ç›‘ç£å­¦ä¹  | `06-æ— ç›‘ç£å­¦ä¹ .md` |
| Day 13 | è¶…å‚æ•°è°ƒä¼˜ | `07-è¶…å‚æ•°è°ƒä¼˜.md` |
| Day 14 | ç»ƒä¹  + å¤ä¹  | |

### ç¬¬ 3 å‘¨ï¼šå®æˆ˜é¡¹ç›®

| å¤©æ•° | å†…å®¹ | æ–‡ä»¶ |
|------|------|------|
| Day 15-16 | æˆ¿ä»·é¢„æµ‹é¡¹ç›® | `08-é¡¹ç›®-æˆ¿ä»·é¢„æµ‹.md` |
| Day 17-18 | æ³°å¦å°¼å…‹é¡¹ç›® | `09-é¡¹ç›®-æ³°å¦å°¼å…‹.md` |
| Day 19-20 | ç”¨æˆ·èšç±»é¡¹ç›® | `10-é¡¹ç›®-ç”¨æˆ·èšç±».md` |
| Day 21 | è‡ªæµ‹ + æ€»ç»“ | `11-è‡ªæµ‹æ¸…å•.md` |

---

## ğŸ› ï¸ ç¯å¢ƒå‡†å¤‡

### å®‰è£…å¿…è¦çš„åº“

```bash
# åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
conda create -n ml_env python=3.11
conda activate ml_env

# å®‰è£…æ ¸å¿ƒåº“
pip install numpy pandas matplotlib seaborn scikit-learn

# å®‰è£…é¢å¤–åº“
pip install xgboost lightgbm catboost

# å®‰è£… Jupyter
pip install jupyter jupyterlab
```

### éªŒè¯å®‰è£…

```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import sklearn

print(f"NumPy: {np.__version__}")
print(f"Pandas: {pd.__version__}")
print(f"Scikit-learn: {sklearn.__version__}")

# æµ‹è¯• sklearn
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier

iris = load_iris()
X_train, X_test, y_train, y_test = train_test_split(
    iris.data, iris.target, test_size=0.2, random_state=42
)
model = RandomForestClassifier(n_estimators=10, random_state=42)
model.fit(X_train, y_train)
print(f"æµ‹è¯•å‡†ç¡®ç‡: {model.score(X_test, y_test):.2%}")
```

---

## ğŸ“š å‚è€ƒèµ„æº

### å®˜æ–¹æ–‡æ¡£
- [Scikit-learn å®˜æ–¹æ–‡æ¡£](https://scikit-learn.org/stable/)
- [XGBoost æ–‡æ¡£](https://xgboost.readthedocs.io/)
- [LightGBM æ–‡æ¡£](https://lightgbm.readthedocs.io/)

### æ¨èä¹¦ç±
- ã€ŠHands-On Machine Learning with Scikit-Learn, Keras, and TensorFlowã€‹
- ã€ŠPython Machine Learningã€‹

### å®æˆ˜å¹³å°
- [Kaggle](https://www.kaggle.com/) - å…¥é—¨æ¯”èµ›ï¼šTitanicã€House Prices

---

## â¡ï¸ ä¸‹ä¸€æ­¥

å‡†å¤‡å¥½äº†å—ï¼Ÿå¼€å§‹å­¦ä¹  [01-æœºå™¨å­¦ä¹ æµæ°´çº¿.md](./01-æœºå™¨å­¦ä¹ æµæ°´çº¿.md)ï¼

