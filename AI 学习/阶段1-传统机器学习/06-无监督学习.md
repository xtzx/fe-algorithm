# ğŸ“Š 06 - æ— ç›‘ç£å­¦ä¹ 

> æœ¬æ–‡è¯¦ç»†ä»‹ç»èšç±»ã€é™ç»´ç­‰æ— ç›‘ç£å­¦ä¹ æ–¹æ³•

---

## ç›®å½•

1. [æ— ç›‘ç£å­¦ä¹ æ¦‚è¿°](#1-æ— ç›‘ç£å­¦ä¹ æ¦‚è¿°)
2. [K-Means èšç±»](#2-k-means-èšç±»)
3. [å±‚æ¬¡èšç±»](#3-å±‚æ¬¡èšç±»)
4. [DBSCAN èšç±»](#4-dbscan-èšç±»)
5. [PCA é™ç»´](#5-pca-é™ç»´)
6. [t-SNE å’Œ UMAP](#6-t-sne-å’Œ-umap)
7. [ç»ƒä¹ é¢˜](#7-ç»ƒä¹ é¢˜)

---

## 1. æ— ç›‘ç£å­¦ä¹ æ¦‚è¿°

### 1.1 ä¸ç›‘ç£å­¦ä¹ çš„åŒºåˆ«

| | ç›‘ç£å­¦ä¹  | æ— ç›‘ç£å­¦ä¹  |
|---|---------|-----------|
| æ•°æ® | æœ‰æ ‡ç­¾ | æ— æ ‡ç­¾ |
| ç›®æ ‡ | é¢„æµ‹æ ‡ç­¾ | å‘ç°ç»“æ„ |
| è¯„ä¼° | ä¸çœŸå®æ ‡ç­¾æ¯”è¾ƒ | å†…éƒ¨æŒ‡æ ‡ |
| åº”ç”¨ | åˆ†ç±»ã€å›å½’ | èšç±»ã€é™ç»´ã€å¼‚å¸¸æ£€æµ‹ |

### 1.2 æ— ç›‘ç£å­¦ä¹ åˆ†ç±»

```
æ— ç›‘ç£å­¦ä¹ 
â”œâ”€â”€ èšç±»ï¼ˆClusteringï¼‰
â”‚   â”œâ”€â”€ K-Means
â”‚   â”œâ”€â”€ å±‚æ¬¡èšç±»
â”‚   â””â”€â”€ DBSCAN
â”‚
â”œâ”€â”€ é™ç»´ï¼ˆDimensionality Reductionï¼‰
â”‚   â”œâ”€â”€ PCAï¼ˆçº¿æ€§ï¼‰
â”‚   â”œâ”€â”€ t-SNEï¼ˆéçº¿æ€§ï¼‰
â”‚   â””â”€â”€ UMAPï¼ˆéçº¿æ€§ï¼‰
â”‚
â””â”€â”€ å¼‚å¸¸æ£€æµ‹ï¼ˆAnomaly Detectionï¼‰
    â”œâ”€â”€ Isolation Forest
    â””â”€â”€ One-Class SVM
```

---

## 2. K-Means èšç±»

### 2.1 åŸºæœ¬åŸç†

**ç›®æ ‡**ï¼šå°†æ•°æ®åˆ†æˆ K ä¸ªç°‡ï¼Œä½¿ç°‡å†…è·ç¦»æœ€å°

**ç®—æ³•æ­¥éª¤**ï¼š
1. éšæœºåˆå§‹åŒ– K ä¸ªä¸­å¿ƒç‚¹
2. å°†æ¯ä¸ªç‚¹åˆ†é…åˆ°æœ€è¿‘çš„ä¸­å¿ƒ
3. æ›´æ–°ä¸­å¿ƒä¸ºç°‡å†…ç‚¹çš„å‡å€¼
4. é‡å¤ 2-3 ç›´åˆ°æ”¶æ•›

### 2.2 åŸºæœ¬å®ç°

```python
from sklearn.cluster import KMeans
from sklearn.datasets import make_blobs
import matplotlib.pyplot as plt
import numpy as np

# ç”Ÿæˆèšç±»æ•°æ®
X, y_true = make_blobs(n_samples=300, centers=4, cluster_std=0.8, random_state=42)

# K-Means èšç±»
kmeans = KMeans(n_clusters=4, random_state=42, n_init=10)
y_pred = kmeans.fit_predict(X)

# å¯è§†åŒ–
fig, axes = plt.subplots(1, 2, figsize=(14, 5))

# çœŸå®æ ‡ç­¾
axes[0].scatter(X[:, 0], X[:, 1], c=y_true, cmap='viridis', s=50, alpha=0.7)
axes[0].set_title('True Labels')
axes[0].set_xlabel('Feature 1')
axes[0].set_ylabel('Feature 2')

# èšç±»ç»“æœ
axes[1].scatter(X[:, 0], X[:, 1], c=y_pred, cmap='viridis', s=50, alpha=0.7)
axes[1].scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1],
                c='red', marker='X', s=200, edgecolors='black', label='Centers')
axes[1].set_title('K-Means Clustering')
axes[1].set_xlabel('Feature 1')
axes[1].set_ylabel('Feature 2')
axes[1].legend()

plt.tight_layout()
plt.show()

# èšç±»ç»“æœä¿¡æ¯
print(f"èšç±»ä¸­å¿ƒ:\n{kmeans.cluster_centers_}")
print(f"æƒ¯æ€§ï¼ˆInertiaï¼‰: {kmeans.inertia_:.2f}")
print(f"è¿­ä»£æ¬¡æ•°: {kmeans.n_iter_}")
```

### 2.3 é€‰æ‹©æœ€ä½³ Kï¼šè‚˜éƒ¨æ³•åˆ™

```python
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt

# è®¡ç®—ä¸åŒ K çš„ inertia
K_range = range(1, 11)
inertias = []

for k in K_range:
    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
    kmeans.fit(X)
    inertias.append(kmeans.inertia_)

# å¯è§†åŒ–è‚˜éƒ¨å›¾
plt.figure(figsize=(10, 6))
plt.plot(K_range, inertias, 'bo-', linewidth=2, markersize=8)
plt.xlabel('Number of Clusters (K)', fontsize=12)
plt.ylabel('Inertia', fontsize=12)
plt.title('Elbow Method for Optimal K', fontsize=14)
plt.xticks(K_range)
plt.grid(True, alpha=0.3)

# æ ‡æ³¨è‚˜éƒ¨
plt.annotate('Elbow', xy=(4, inertias[3]), xytext=(5.5, inertias[3] + 200),
            arrowprops=dict(arrowstyle='->', color='red'),
            fontsize=12, color='red')
plt.show()
```

### 2.4 é€‰æ‹©æœ€ä½³ Kï¼šè½®å»“ç³»æ•°

```python
from sklearn.metrics import silhouette_score, silhouette_samples
import matplotlib.pyplot as plt
import numpy as np

K_range = range(2, 11)
silhouette_scores = []

for k in K_range:
    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
    labels = kmeans.fit_predict(X)
    score = silhouette_score(X, labels)
    silhouette_scores.append(score)
    print(f"K={k}: è½®å»“ç³»æ•° = {score:.4f}")

# å¯è§†åŒ–
plt.figure(figsize=(10, 6))
plt.plot(K_range, silhouette_scores, 'go-', linewidth=2, markersize=8)
plt.xlabel('Number of Clusters (K)', fontsize=12)
plt.ylabel('Silhouette Score', fontsize=12)
plt.title('Silhouette Score for Optimal K', fontsize=14)
plt.xticks(K_range)
plt.grid(True, alpha=0.3)

best_k = K_range[np.argmax(silhouette_scores)]
plt.axvline(best_k, color='red', linestyle='--', label=f'Best K = {best_k}')
plt.legend()
plt.show()
```

### 2.5 è½®å»“å›¾è¯¦ç»†åˆ†æ

```python
from sklearn.metrics import silhouette_samples
import matplotlib.pyplot as plt
import numpy as np

def plot_silhouette(X, n_clusters):
    """ç»˜åˆ¶è½®å»“å›¾"""
    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)
    labels = kmeans.fit_predict(X)

    silhouette_vals = silhouette_samples(X, labels)
    silhouette_avg = silhouette_vals.mean()

    fig, ax = plt.subplots(figsize=(8, 6))

    y_lower = 10
    for i in range(n_clusters):
        cluster_silhouette_vals = silhouette_vals[labels == i]
        cluster_silhouette_vals.sort()

        cluster_size = len(cluster_silhouette_vals)
        y_upper = y_lower + cluster_size

        color = plt.cm.viridis(i / n_clusters)
        ax.fill_betweenx(np.arange(y_lower, y_upper),
                         0, cluster_silhouette_vals,
                         facecolor=color, edgecolor=color, alpha=0.7)
        ax.text(-0.05, y_lower + 0.5 * cluster_size, str(i))
        y_lower = y_upper + 10

    ax.axvline(silhouette_avg, color='red', linestyle='--',
               label=f'Average: {silhouette_avg:.3f}')
    ax.set_xlabel('Silhouette Coefficient')
    ax.set_ylabel('Cluster')
    ax.set_title(f'Silhouette Plot (K={n_clusters})')
    ax.legend()
    plt.show()

# å¯¹æ¯”ä¸åŒ K çš„è½®å»“å›¾
for k in [3, 4, 5]:
    plot_silhouette(X, k)
```

### 2.6 K-Means çš„å±€é™æ€§

```python
from sklearn.datasets import make_moons, make_circles
import matplotlib.pyplot as plt

# ç”Ÿæˆéå‡¸æ•°æ®
X_moons, y_moons = make_moons(n_samples=200, noise=0.1, random_state=42)
X_circles, y_circles = make_circles(n_samples=200, noise=0.05, factor=0.5, random_state=42)

fig, axes = plt.subplots(2, 2, figsize=(12, 10))

# æœˆç‰™å½¢æ•°æ®
axes[0, 0].scatter(X_moons[:, 0], X_moons[:, 1], c=y_moons, cmap='viridis')
axes[0, 0].set_title('Moons - True Labels')

kmeans = KMeans(n_clusters=2, random_state=42, n_init=10)
labels_moons = kmeans.fit_predict(X_moons)
axes[0, 1].scatter(X_moons[:, 0], X_moons[:, 1], c=labels_moons, cmap='viridis')
axes[0, 1].set_title('Moons - K-Means (K=2)')

# åŒå¿ƒåœ†æ•°æ®
axes[1, 0].scatter(X_circles[:, 0], X_circles[:, 1], c=y_circles, cmap='viridis')
axes[1, 0].set_title('Circles - True Labels')

kmeans = KMeans(n_clusters=2, random_state=42, n_init=10)
labels_circles = kmeans.fit_predict(X_circles)
axes[1, 1].scatter(X_circles[:, 0], X_circles[:, 1], c=labels_circles, cmap='viridis')
axes[1, 1].set_title('Circles - K-Means (K=2)')

plt.tight_layout()
plt.show()

print("K-Means å¯¹éå‡¸å½¢çŠ¶çš„æ•°æ®æ•ˆæœè¾ƒå·®ï¼")
```

---

## 3. å±‚æ¬¡èšç±»

### 3.1 åŸºæœ¬åŸç†

**å‡èšå±‚æ¬¡èšç±»ï¼ˆè‡ªåº•å‘ä¸Šï¼‰**ï¼š
1. æ¯ä¸ªç‚¹ä½œä¸ºä¸€ä¸ªç°‡
2. åˆå¹¶æœ€è¿‘çš„ä¸¤ä¸ªç°‡
3. é‡å¤ç›´åˆ°åªå‰©ä¸€ä¸ªç°‡

### 3.2 å®ç°

```python
from sklearn.cluster import AgglomerativeClustering
from scipy.cluster.hierarchy import dendrogram, linkage
import matplotlib.pyplot as plt
import numpy as np

# ä½¿ç”¨ä¹‹å‰çš„æ•°æ®
X, y_true = make_blobs(n_samples=100, centers=4, cluster_std=0.8, random_state=42)

# å±‚æ¬¡èšç±»
agg = AgglomerativeClustering(n_clusters=4, linkage='ward')
labels = agg.fit_predict(X)

# å¯è§†åŒ–
plt.figure(figsize=(8, 6))
plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis', s=50)
plt.title('Agglomerative Clustering')
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.show()

# æ ‘çŠ¶å›¾
linked = linkage(X, method='ward')

plt.figure(figsize=(12, 6))
dendrogram(linked, truncate_mode='level', p=5)
plt.title('Dendrogram')
plt.xlabel('Sample Index')
plt.ylabel('Distance')
plt.show()
```

### 3.3 ä¸åŒé“¾æ¥æ–¹æ³•

```python
from sklearn.cluster import AgglomerativeClustering
import matplotlib.pyplot as plt

linkages = ['ward', 'complete', 'average', 'single']

fig, axes = plt.subplots(2, 2, figsize=(12, 10))
axes = axes.flatten()

for ax, linkage_method in zip(axes, linkages):
    agg = AgglomerativeClustering(n_clusters=4, linkage=linkage_method)
    labels = agg.fit_predict(X)

    ax.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis', s=50)
    ax.set_title(f'Linkage: {linkage_method}')
    ax.set_xlabel('Feature 1')
    ax.set_ylabel('Feature 2')

plt.tight_layout()
plt.show()
```

---

## 4. DBSCAN èšç±»

### 4.1 åŸºæœ¬åŸç†

**DBSCAN = Density-Based Spatial Clustering of Applications with Noise**

- **æ ¸å¿ƒç‚¹**ï¼šé‚»åŸŸå†…æœ‰è¶³å¤Ÿå¤šçš„ç‚¹
- **è¾¹ç•Œç‚¹**ï¼šåœ¨æ ¸å¿ƒç‚¹é‚»åŸŸå†…ï¼Œä½†è‡ªèº«ä¸æ˜¯æ ¸å¿ƒç‚¹
- **å™ªå£°ç‚¹**ï¼šæ—¢ä¸æ˜¯æ ¸å¿ƒç‚¹ä¹Ÿä¸æ˜¯è¾¹ç•Œç‚¹

**å‚æ•°**ï¼š
- `eps`ï¼šé‚»åŸŸåŠå¾„
- `min_samples`ï¼šæˆä¸ºæ ¸å¿ƒç‚¹æ‰€éœ€çš„æœ€å°é‚»å±…æ•°

### 4.2 å®ç°

```python
from sklearn.cluster import DBSCAN
import matplotlib.pyplot as plt

# ä½¿ç”¨æœˆç‰™å½¢æ•°æ®
X, y_true = make_moons(n_samples=300, noise=0.1, random_state=42)

# DBSCAN èšç±»
dbscan = DBSCAN(eps=0.2, min_samples=5)
labels = dbscan.fit_predict(X)

# å¯è§†åŒ–
plt.figure(figsize=(10, 6))
plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis', s=50)
plt.title(f'DBSCAN Clustering (eps=0.2, min_samples=5)\nNumber of clusters: {len(set(labels)) - (1 if -1 in labels else 0)}')
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.colorbar(label='Cluster')
plt.show()

# æŸ¥çœ‹å™ªå£°ç‚¹
n_noise = list(labels).count(-1)
print(f"å™ªå£°ç‚¹æ•°é‡: {n_noise}")
print(f"ç°‡æ•°é‡: {len(set(labels)) - (1 if -1 in labels else 0)}")
```

### 4.3 å‚æ•°é€‰æ‹©

```python
from sklearn.neighbors import NearestNeighbors
import numpy as np
import matplotlib.pyplot as plt

# ä½¿ç”¨ k-è·ç¦»å›¾é€‰æ‹© eps
k = 5
neighbors = NearestNeighbors(n_neighbors=k)
neighbors.fit(X)
distances, indices = neighbors.kneighbors(X)

# è®¡ç®—ç¬¬ k ä¸ªæœ€è¿‘é‚»çš„è·ç¦»
k_distances = distances[:, k-1]
k_distances = np.sort(k_distances)[::-1]

plt.figure(figsize=(10, 6))
plt.plot(k_distances)
plt.xlabel('Points sorted by distance')
plt.ylabel(f'{k}-th nearest neighbor distance')
plt.title('K-Distance Graph for Choosing eps')
plt.grid(True)
plt.show()

# ä¸åŒå‚æ•°çš„æ•ˆæœ
eps_values = [0.1, 0.2, 0.3, 0.5]
fig, axes = plt.subplots(2, 2, figsize=(12, 10))
axes = axes.flatten()

for ax, eps in zip(axes, eps_values):
    dbscan = DBSCAN(eps=eps, min_samples=5)
    labels = dbscan.fit_predict(X)
    n_clusters = len(set(labels)) - (1 if -1 in labels else 0)

    ax.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis', s=30)
    ax.set_title(f'eps={eps}, n_clusters={n_clusters}')

plt.tight_layout()
plt.show()
```

### 4.4 DBSCAN vs K-Means

```python
from sklearn.cluster import KMeans, DBSCAN
from sklearn.datasets import make_moons
import matplotlib.pyplot as plt

# æœˆç‰™å½¢æ•°æ®
X, y_true = make_moons(n_samples=300, noise=0.1, random_state=42)

fig, axes = plt.subplots(1, 3, figsize=(15, 4))

# çœŸå®æ ‡ç­¾
axes[0].scatter(X[:, 0], X[:, 1], c=y_true, cmap='viridis', s=30)
axes[0].set_title('True Labels')

# K-Means
kmeans = KMeans(n_clusters=2, random_state=42, n_init=10)
labels_kmeans = kmeans.fit_predict(X)
axes[1].scatter(X[:, 0], X[:, 1], c=labels_kmeans, cmap='viridis', s=30)
axes[1].set_title('K-Means')

# DBSCAN
dbscan = DBSCAN(eps=0.2, min_samples=5)
labels_dbscan = dbscan.fit_predict(X)
axes[2].scatter(X[:, 0], X[:, 1], c=labels_dbscan, cmap='viridis', s=30)
axes[2].set_title('DBSCAN')

plt.tight_layout()
plt.show()
```

---

## 5. PCA é™ç»´

### 5.1 åŸºæœ¬åŸç†

**PCA = Principal Component Analysisï¼ˆä¸»æˆåˆ†åˆ†æï¼‰**

- æ‰¾åˆ°æ•°æ®æ–¹å·®æœ€å¤§çš„æ–¹å‘
- å°†æ•°æ®æŠ•å½±åˆ°è¿™äº›æ–¹å‘ä¸Š
- ä¿ç•™æœ€é‡è¦çš„ä¿¡æ¯ï¼Œå»é™¤å†—ä½™

### 5.2 åŸºæœ¬å®ç°

```python
from sklearn.decomposition import PCA
from sklearn.datasets import load_iris
import matplotlib.pyplot as plt
import numpy as np

# åŠ è½½æ•°æ®
iris = load_iris()
X, y = iris.data, iris.target

print(f"åŸå§‹æ•°æ®ç»´åº¦: {X.shape}")

# PCA é™ç»´åˆ° 2D
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X)

print(f"é™ç»´åç»´åº¦: {X_pca.shape}")
print(f"å„ä¸»æˆåˆ†è§£é‡Šçš„æ–¹å·®æ¯”ä¾‹: {pca.explained_variance_ratio_}")
print(f"ç´¯è®¡è§£é‡Šæ–¹å·®: {pca.explained_variance_ratio_.sum():.2%}")

# å¯è§†åŒ–
plt.figure(figsize=(10, 8))
scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap='viridis', s=50, alpha=0.7)
plt.colorbar(scatter, label='Class')
plt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%})')
plt.ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%})')
plt.title('PCA on Iris Dataset')
plt.show()
```

### 5.3 é€‰æ‹©ä¸»æˆåˆ†æ•°é‡

```python
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt
import numpy as np

# ä¸æŒ‡å®š n_componentsï¼Œä¿ç•™æ‰€æœ‰
pca_full = PCA()
pca_full.fit(X)

# æ–¹å·®è§£é‡Šæ¯”ä¾‹
explained_variance = pca_full.explained_variance_ratio_
cumulative_variance = np.cumsum(explained_variance)

fig, axes = plt.subplots(1, 2, figsize=(14, 5))

# æ¯ä¸ªä¸»æˆåˆ†çš„æ–¹å·®
axes[0].bar(range(1, len(explained_variance) + 1), explained_variance)
axes[0].set_xlabel('Principal Component')
axes[0].set_ylabel('Explained Variance Ratio')
axes[0].set_title('Variance Explained by Each PC')

# ç´¯è®¡æ–¹å·®
axes[1].plot(range(1, len(cumulative_variance) + 1), cumulative_variance, 'bo-')
axes[1].axhline(0.95, color='red', linestyle='--', label='95% threshold')
axes[1].set_xlabel('Number of Components')
axes[1].set_ylabel('Cumulative Explained Variance')
axes[1].set_title('Cumulative Explained Variance')
axes[1].legend()
axes[1].grid(True)

plt.tight_layout()
plt.show()

# é€‰æ‹©ä¿ç•™ 95% æ–¹å·®æ‰€éœ€çš„ä¸»æˆåˆ†æ•°
n_components_95 = np.argmax(cumulative_variance >= 0.95) + 1
print(f"ä¿ç•™ 95% æ–¹å·®éœ€è¦ {n_components_95} ä¸ªä¸»æˆåˆ†")
```

### 5.4 ä¸»æˆåˆ†ç³»æ•°è§£è¯»

```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# è·å–ä¸»æˆåˆ†ç³»æ•°ï¼ˆloadingsï¼‰
pca = PCA(n_components=2)
pca.fit(X)

loadings = pd.DataFrame(
    pca.components_.T,
    columns=['PC1', 'PC2'],
    index=iris.feature_names
)

print("ä¸»æˆåˆ†ç³»æ•°:")
print(loadings)

# å¯è§†åŒ–
plt.figure(figsize=(8, 6))
sns.heatmap(loadings, annot=True, cmap='coolwarm', center=0)
plt.title('PCA Loadings')
plt.show()

# Biplotï¼ˆåŒæ ‡å›¾ï¼‰
plt.figure(figsize=(10, 8))

# æ•£ç‚¹å›¾
plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap='viridis', alpha=0.5)

# æ·»åŠ ç‰¹å¾å‘é‡
scale = 3
for i, (name, loading) in enumerate(zip(iris.feature_names, pca.components_.T)):
    plt.arrow(0, 0, loading[0]*scale, loading[1]*scale,
              head_width=0.1, head_length=0.05, fc='red', ec='red')
    plt.text(loading[0]*scale*1.1, loading[1]*scale*1.1, name, fontsize=10)

plt.xlabel('PC1')
plt.ylabel('PC2')
plt.title('PCA Biplot')
plt.grid(True, alpha=0.3)
plt.show()
```

### 5.5 PCA åœ¨é¢„å¤„ç†ä¸­çš„åº”ç”¨

```python
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import cross_val_score

# é«˜ç»´æ•°æ®
from sklearn.datasets import load_digits
digits = load_digits()
X, y = digits.data, digits.target

print(f"åŸå§‹ç»´åº¦: {X.shape}")

# æ¯”è¾ƒä¸åŒä¸»æˆåˆ†æ•°çš„æ•ˆæœ
n_components_list = [10, 20, 30, 50, 64]
results = []

for n in n_components_list:
    pipeline = Pipeline([
        ('scaler', StandardScaler()),
        ('pca', PCA(n_components=n)),
        ('clf', LogisticRegression(max_iter=1000))
    ])

    scores = cross_val_score(pipeline, X, y, cv=5)
    results.append({
        'n_components': n,
        'accuracy': scores.mean(),
        'std': scores.std()
    })
    print(f"n_components={n}: {scores.mean():.4f} Â± {scores.std():.4f}")

# ä¸ç”¨ PCA
pipeline_full = Pipeline([
    ('scaler', StandardScaler()),
    ('clf', LogisticRegression(max_iter=1000))
])
scores_full = cross_val_score(pipeline_full, X, y, cv=5)
print(f"Full features ({X.shape[1]}): {scores_full.mean():.4f} Â± {scores_full.std():.4f}")
```

---

## 6. t-SNE å’Œ UMAP

### 6.1 t-SNE

**t-SNE = t-Distributed Stochastic Neighbor Embedding**

- éçº¿æ€§é™ç»´ï¼Œä¿æŒå±€éƒ¨ç»“æ„
- ä¸»è¦ç”¨äºå¯è§†åŒ–
- è®¡ç®—è¾ƒæ…¢

```python
from sklearn.manifold import TSNE
from sklearn.datasets import load_digits
import matplotlib.pyplot as plt

# åŠ è½½æ‰‹å†™æ•°å­—æ•°æ®
digits = load_digits()
X, y = digits.data, digits.target

# t-SNE é™ç»´
tsne = TSNE(
    n_components=2,
    perplexity=30,      # æ§åˆ¶å±€éƒ¨/å…¨å±€æƒè¡¡
    learning_rate=200,
    n_iter=1000,
    random_state=42
)
X_tsne = tsne.fit_transform(X)

# å¯è§†åŒ–
plt.figure(figsize=(12, 10))
scatter = plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=y, cmap='tab10', s=10, alpha=0.7)
plt.colorbar(scatter, label='Digit')
plt.xlabel('t-SNE 1')
plt.ylabel('t-SNE 2')
plt.title('t-SNE on Digits Dataset')
plt.show()
```

### 6.2 t-SNE å‚æ•°è°ƒä¼˜

```python
from sklearn.manifold import TSNE
import matplotlib.pyplot as plt

# ä¸åŒ perplexity çš„æ•ˆæœ
perplexities = [5, 15, 30, 50]

fig, axes = plt.subplots(2, 2, figsize=(14, 12))
axes = axes.flatten()

for ax, perp in zip(axes, perplexities):
    tsne = TSNE(n_components=2, perplexity=perp, random_state=42)
    X_tsne = tsne.fit_transform(X)

    scatter = ax.scatter(X_tsne[:, 0], X_tsne[:, 1], c=y, cmap='tab10', s=10, alpha=0.7)
    ax.set_title(f'Perplexity = {perp}')
    ax.set_xlabel('t-SNE 1')
    ax.set_ylabel('t-SNE 2')

plt.tight_layout()
plt.show()
```

### 6.3 UMAP

**UMAP = Uniform Manifold Approximation and Projection**

- æ¯” t-SNE æ›´å¿«
- ä¿æŒå…¨å±€ç»“æ„æ›´å¥½
- å¯ç”¨äºé™ç»´åçš„æœºå™¨å­¦ä¹ 

```python
# å®‰è£…ï¼špip install umap-learn

import umap
import matplotlib.pyplot as plt

# UMAP é™ç»´
reducer = umap.UMAP(
    n_components=2,
    n_neighbors=15,     # æ§åˆ¶å±€éƒ¨ç»“æ„
    min_dist=0.1,       # æ§åˆ¶ç‚¹çš„ç´§å¯†ç¨‹åº¦
    random_state=42
)
X_umap = reducer.fit_transform(X)

# å¯è§†åŒ–
plt.figure(figsize=(12, 10))
scatter = plt.scatter(X_umap[:, 0], X_umap[:, 1], c=y, cmap='tab10', s=10, alpha=0.7)
plt.colorbar(scatter, label='Digit')
plt.xlabel('UMAP 1')
plt.ylabel('UMAP 2')
plt.title('UMAP on Digits Dataset')
plt.show()
```

### 6.4 PCA vs t-SNE vs UMAP

```python
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
# import umap  # å¦‚æœå·²å®‰è£…
import matplotlib.pyplot as plt
import time

fig, axes = plt.subplots(1, 3, figsize=(18, 5))

# PCA
start = time.time()
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X)
pca_time = time.time() - start

axes[0].scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap='tab10', s=10, alpha=0.7)
axes[0].set_title(f'PCA ({pca_time:.2f}s)')

# t-SNE
start = time.time()
tsne = TSNE(n_components=2, random_state=42)
X_tsne = tsne.fit_transform(X)
tsne_time = time.time() - start

axes[1].scatter(X_tsne[:, 0], X_tsne[:, 1], c=y, cmap='tab10', s=10, alpha=0.7)
axes[1].set_title(f't-SNE ({tsne_time:.2f}s)')

# UMAPï¼ˆå¦‚æœå®‰è£…äº†ï¼‰
try:
    import umap
    start = time.time()
    reducer = umap.UMAP(n_components=2, random_state=42)
    X_umap = reducer.fit_transform(X)
    umap_time = time.time() - start

    axes[2].scatter(X_umap[:, 0], X_umap[:, 1], c=y, cmap='tab10', s=10, alpha=0.7)
    axes[2].set_title(f'UMAP ({umap_time:.2f}s)')
except ImportError:
    axes[2].text(0.5, 0.5, 'UMAP not installed', ha='center', va='center')
    axes[2].set_title('UMAP (not installed)')

plt.tight_layout()
plt.show()
```

---

## 7. ç»ƒä¹ é¢˜

### åŸºç¡€ç»ƒä¹ 

1. ç”¨ K-Means å¯¹ä¸€ä¸ªæ•°æ®é›†è¿›è¡Œèšç±»ï¼Œä½¿ç”¨è‚˜éƒ¨æ³•åˆ™é€‰æ‹©æœ€ä½³ K
2. ç”¨ PCA å°†é«˜ç»´æ•°æ®é™åˆ° 2D å¹¶å¯è§†åŒ–
3. æ¯”è¾ƒ K-Means å’Œ DBSCAN åœ¨æœˆç‰™å½¢æ•°æ®ä¸Šçš„æ•ˆæœ

### è¿›é˜¶ç»ƒä¹ 

4. ç”¨ t-SNE å¯è§†åŒ–æ‰‹å†™æ•°å­—æ•°æ®é›†
5. ç»“åˆ PCA å’Œèšç±»ï¼šå…ˆé™ç»´å†èšç±»ï¼Œè§‚å¯Ÿæ•ˆæœ

### å‚è€ƒç­”æ¡ˆ

<details>
<summary>ç»ƒä¹  5 å‚è€ƒç­”æ¡ˆ</summary>

```python
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans
from sklearn.datasets import load_digits
from sklearn.metrics import silhouette_score
import matplotlib.pyplot as plt

digits = load_digits()
X, y = digits.data, digits.target

# å…ˆ PCA é™ç»´
pca = PCA(n_components=50)  # ä¿ç•™ 50 ä¸ªä¸»æˆåˆ†
X_pca = pca.fit_transform(X)

print(f"PCA åç´¯è®¡æ–¹å·®: {pca.explained_variance_ratio_.sum():.2%}")

# åœ¨é™ç»´åçš„æ•°æ®ä¸Šèšç±»
kmeans = KMeans(n_clusters=10, random_state=42, n_init=10)
labels = kmeans.fit_predict(X_pca)

print(f"è½®å»“ç³»æ•°: {silhouette_score(X_pca, labels):.4f}")

# è¿›ä¸€æ­¥é™åˆ° 2D å¯è§†åŒ–
pca_2d = PCA(n_components=2)
X_2d = pca_2d.fit_transform(X_pca)

fig, axes = plt.subplots(1, 2, figsize=(14, 5))

axes[0].scatter(X_2d[:, 0], X_2d[:, 1], c=y, cmap='tab10', s=10)
axes[0].set_title('True Labels')

axes[1].scatter(X_2d[:, 0], X_2d[:, 1], c=labels, cmap='tab10', s=10)
axes[1].set_title('K-Means on PCA Features')

plt.tight_layout()
plt.show()
```

</details>

---

## â¡ï¸ ä¸‹ä¸€æ­¥

å­¦å®Œæœ¬èŠ‚åï¼Œç»§ç»­å­¦ä¹  [07-è¶…å‚æ•°è°ƒä¼˜.md](./07-è¶…å‚æ•°è°ƒä¼˜.md)

