# ğŸš¢ 12 - é¡¹ç›®ï¼šæ³°å¦å°¼å…‹ç”Ÿå­˜é¢„æµ‹ï¼ˆåˆ†ç±»ï¼‰

> å®Œæ•´çš„åˆ†ç±»é¡¹ç›®å®æˆ˜ï¼ŒKaggle ç»å…¸å…¥é—¨ç«èµ›

---

## é¡¹ç›®æ¦‚è¿°

| é¡¹ç›® | è¯´æ˜ |
|------|------|
| **ç›®æ ‡** | é¢„æµ‹ä¹˜å®¢æ˜¯å¦ç”Ÿå­˜ï¼ˆäºŒåˆ†ç±»ï¼‰ |
| **ç±»å‹** | ç›‘ç£å­¦ä¹  - åˆ†ç±» |
| **æ•°æ®** | Titanic Dataset |
| **è¯„ä¼°** | Accuracyã€F1ã€AUC |

---

## å®Œæ•´ä»£ç 

```python
# ============================================================
# æ³°å¦å°¼å…‹ç”Ÿå­˜é¢„æµ‹é¡¹ç›® - å®Œæ•´ä»£ç 
# ============================================================

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.metrics import (
    accuracy_score, classification_report, confusion_matrix,
    roc_curve, roc_auc_score, precision_recall_curve
)
import warnings
warnings.filterwarnings('ignore')

plt.rcParams['figure.figsize'] = (10, 6)

print("=" * 60)
print("1. æ•°æ®åŠ è½½ä¸æ¢ç´¢")
print("=" * 60)

# åŠ è½½æ•°æ®ï¼ˆä»ç½‘ç»œæˆ–æœ¬åœ°ï¼‰
url = "https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv"
df = pd.read_csv(url)

print(f"æ•°æ®å½¢çŠ¶: {df.shape}")
print(f"\nåˆ—å: {df.columns.tolist()}")
print(f"\nå‰ 5 è¡Œ:")
print(df.head())

print(f"\næ•°æ®ä¿¡æ¯:")
print(df.info())

print(f"\næ•°å€¼ç‰¹å¾ç»Ÿè®¡:")
print(df.describe())

print(f"\nç¼ºå¤±å€¼:")
print(df.isnull().sum())
print(f"\nç¼ºå¤±å€¼æ¯”ä¾‹:")
print((df.isnull().sum() / len(df) * 100).round(2))

# ============================================================
print("\n" + "=" * 60)
print("2. æ¢ç´¢æ€§æ•°æ®åˆ†æ (EDA)")
print("=" * 60)

# 2.1 ç”Ÿå­˜ç‡
survived_rate = df['Survived'].mean()
print(f"\næ€»ä½“ç”Ÿå­˜ç‡: {survived_rate:.2%}")

# å¯è§†åŒ–
fig, axes = plt.subplots(2, 3, figsize=(15, 10))

# ç”Ÿå­˜åˆ†å¸ƒ
df['Survived'].value_counts().plot(kind='bar', ax=axes[0, 0], color=['salmon', 'lightgreen'])
axes[0, 0].set_title(f'Survival Distribution\n(Survived: {survived_rate:.1%})')
axes[0, 0].set_xticklabels(['Died', 'Survived'], rotation=0)

# æŒ‰æ€§åˆ«çš„ç”Ÿå­˜ç‡
df.groupby('Sex')['Survived'].mean().plot(kind='bar', ax=axes[0, 1], color=['lightblue', 'pink'])
axes[0, 1].set_title('Survival Rate by Gender')
axes[0, 1].set_xticklabels(['Female', 'Male'], rotation=0)
axes[0, 1].set_ylabel('Survival Rate')

# æŒ‰èˆ±ä½çš„ç”Ÿå­˜ç‡
df.groupby('Pclass')['Survived'].mean().plot(kind='bar', ax=axes[0, 2], color='steelblue')
axes[0, 2].set_title('Survival Rate by Class')
axes[0, 2].set_xticklabels(['1st', '2nd', '3rd'], rotation=0)
axes[0, 2].set_ylabel('Survival Rate')

# å¹´é¾„åˆ†å¸ƒ
df['Age'].hist(bins=30, ax=axes[1, 0], edgecolor='black', alpha=0.7)
axes[1, 0].set_title('Age Distribution')
axes[1, 0].set_xlabel('Age')

# æŒ‰å¹´é¾„æ®µçš„ç”Ÿå­˜ç‡
df['AgeGroup'] = pd.cut(df['Age'], bins=[0, 12, 18, 35, 60, 100],
                        labels=['Child', 'Teen', 'Adult', 'Middle', 'Senior'])
df.groupby('AgeGroup')['Survived'].mean().plot(kind='bar', ax=axes[1, 1], color='green')
axes[1, 1].set_title('Survival Rate by Age Group')
axes[1, 1].set_ylabel('Survival Rate')
axes[1, 1].tick_params(axis='x', rotation=45)

# ç¥¨ä»·åˆ†å¸ƒ
df['Fare'].hist(bins=50, ax=axes[1, 2], edgecolor='black', alpha=0.7)
axes[1, 2].set_title('Fare Distribution')
axes[1, 2].set_xlabel('Fare')

plt.tight_layout()
plt.savefig('titanic_eda.png', dpi=150)
plt.show()

# æ‰“å°ç»Ÿè®¡ä¿¡æ¯
print("\næŒ‰æ€§åˆ«çš„ç”Ÿå­˜ç‡:")
print(df.groupby('Sex')['Survived'].agg(['count', 'sum', 'mean']))

print("\næŒ‰èˆ±ä½çš„ç”Ÿå­˜ç‡:")
print(df.groupby('Pclass')['Survived'].agg(['count', 'sum', 'mean']))

print("\næŒ‰ç™»èˆ¹æ¸¯å£çš„ç”Ÿå­˜ç‡:")
print(df.groupby('Embarked')['Survived'].agg(['count', 'sum', 'mean']))

# ============================================================
print("\n" + "=" * 60)
print("3. ç‰¹å¾å·¥ç¨‹")
print("=" * 60)

# å¤åˆ¶æ•°æ®
data = df.copy()

# 3.1 å¤„ç†ç¼ºå¤±å€¼
print("å¤„ç†ç¼ºå¤±å€¼...")

# Ageï¼šç”¨ä¸­ä½æ•°å¡«å……
data['Age'] = data['Age'].fillna(data['Age'].median())

# Embarkedï¼šç”¨ä¼—æ•°å¡«å……
data['Embarked'] = data['Embarked'].fillna(data['Embarked'].mode()[0])

# Fareï¼šç”¨ä¸­ä½æ•°å¡«å……
data['Fare'] = data['Fare'].fillna(data['Fare'].median())

# Cabinï¼šç¼ºå¤±å¤ªå¤šï¼Œè½¬æ¢ä¸ºæ˜¯å¦æœ‰èˆ±ä½å·
data['HasCabin'] = data['Cabin'].notna().astype(int)

print(f"å¤„ç†åç¼ºå¤±å€¼: {data[['Age', 'Embarked', 'Fare']].isnull().sum().sum()}")

# 3.2 åˆ›å»ºæ–°ç‰¹å¾
print("\nåˆ›å»ºæ–°ç‰¹å¾...")

# å®¶åº­å¤§å°
data['FamilySize'] = data['SibSp'] + data['Parch'] + 1
print(f"  FamilySize: 1-{data['FamilySize'].max()}")

# æ˜¯å¦ç‹¬è‡ªä¸€äºº
data['IsAlone'] = (data['FamilySize'] == 1).astype(int)
print(f"  IsAlone: {data['IsAlone'].value_counts().to_dict()}")

# ä»åå­—æå–ç§°è°“
data['Title'] = data['Name'].str.extract(r' ([A-Za-z]+)\.', expand=False)
print(f"  åŸå§‹ Title åˆ†å¸ƒ: {data['Title'].value_counts().to_dict()}")

# åˆå¹¶ç¨€æœ‰ç§°è°“
title_mapping = {
    'Mr': 'Mr', 'Miss': 'Miss', 'Mrs': 'Mrs', 'Master': 'Master',
    'Dr': 'Rare', 'Rev': 'Rare', 'Col': 'Rare', 'Major': 'Rare',
    'Mlle': 'Miss', 'Countess': 'Rare', 'Ms': 'Miss', 'Lady': 'Rare',
    'Jonkheer': 'Rare', 'Don': 'Rare', 'Dona': 'Rare', 'Mme': 'Mrs',
    'Capt': 'Rare', 'Sir': 'Rare'
}
data['Title'] = data['Title'].map(title_mapping).fillna('Rare')
print(f"  åˆå¹¶å Title: {data['Title'].value_counts().to_dict()}")

# å¹´é¾„åˆ†ç»„
data['AgeBin'] = pd.cut(data['Age'], bins=[0, 12, 18, 35, 60, 100],
                        labels=[0, 1, 2, 3, 4])

# ç¥¨ä»·åˆ†ç»„
data['FareBin'] = pd.qcut(data['Fare'], q=4, labels=[0, 1, 2, 3])

# 3.3 ç‰¹å¾ç¼–ç 
print("\nç‰¹å¾ç¼–ç ...")

# Label Encoding
le = LabelEncoder()
data['Sex'] = le.fit_transform(data['Sex'])  # female=0, male=1
data['Embarked'] = le.fit_transform(data['Embarked'])
data['Title'] = le.fit_transform(data['Title'])
data['AgeBin'] = data['AgeBin'].astype(int)
data['FareBin'] = data['FareBin'].astype(int)

# 3.4 é€‰æ‹©ç‰¹å¾
features = ['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked',
            'FamilySize', 'IsAlone', 'Title', 'HasCabin', 'AgeBin', 'FareBin']

X = data[features]
y = data['Survived']

print(f"\næœ€ç»ˆç‰¹å¾: {features}")
print(f"ç‰¹å¾çŸ©é˜µå½¢çŠ¶: {X.shape}")

# ============================================================
print("\n" + "=" * 60)
print("4. æ•°æ®åˆ’åˆ†ä¸é¢„å¤„ç†")
print("=" * 60)

# åˆ’åˆ†æ•°æ®
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

print(f"è®­ç»ƒé›†: {X_train.shape[0]} ({y_train.mean():.2%} ç”Ÿå­˜)")
print(f"æµ‹è¯•é›†: {X_test.shape[0]} ({y_test.mean():.2%} ç”Ÿå­˜)")

# ç‰¹å¾ç¼©æ”¾
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# ============================================================
print("\n" + "=" * 60)
print("5. æ¨¡å‹è®­ç»ƒä¸è¯„ä¼°")
print("=" * 60)

def evaluate_model(model, X_train, X_test, y_train, y_test, name):
    """è®­ç»ƒå¹¶è¯„ä¼°åˆ†ç±»æ¨¡å‹"""
    # è®­ç»ƒ
    model.fit(X_train, y_train)

    # é¢„æµ‹
    y_pred = model.predict(X_test)
    y_prob = model.predict_proba(X_test)[:, 1]

    # è®¡ç®—æŒ‡æ ‡
    accuracy = accuracy_score(y_test, y_pred)
    auc = roc_auc_score(y_test, y_prob)

    # äº¤å‰éªŒè¯
    cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='accuracy')

    print(f"\n{name}:")
    print(f"  CV Accuracy: {cv_scores.mean():.4f} Â± {cv_scores.std():.4f}")
    print(f"  Test Accuracy: {accuracy:.4f}")
    print(f"  Test AUC: {auc:.4f}")

    return {
        'name': name,
        'model': model,
        'accuracy': accuracy,
        'auc': auc,
        'cv_mean': cv_scores.mean(),
        'cv_std': cv_scores.std(),
        'y_pred': y_pred,
        'y_prob': y_prob
    }

# è®­ç»ƒå¤šä¸ªæ¨¡å‹
models = {
    'Logistic Regression': LogisticRegression(max_iter=1000),
    'Decision Tree': DecisionTreeClassifier(max_depth=5, random_state=42),
    'Random Forest': RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42),
    'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, max_depth=5, random_state=42)
}

results = []
for name, model in models.items():
    result = evaluate_model(model, X_train_scaled, X_test_scaled, y_train, y_test, name)
    results.append(result)

# ============================================================
print("\n" + "=" * 60)
print("6. æ¨¡å‹å¯¹æ¯”å¯è§†åŒ–")
print("=" * 60)

# 6.1 æ€§èƒ½å¯¹æ¯”
fig, axes = plt.subplots(1, 2, figsize=(14, 5))

# Accuracy å¯¹æ¯”
names = [r['name'] for r in results]
accuracies = [r['accuracy'] for r in results]
aucs = [r['auc'] for r in results]

x = np.arange(len(names))
width = 0.35

axes[0].bar(x - width/2, accuracies, width, label='Accuracy', color='steelblue')
axes[0].bar(x + width/2, aucs, width, label='AUC', color='green')
axes[0].set_ylabel('Score')
axes[0].set_title('Model Comparison')
axes[0].set_xticks(x)
axes[0].set_xticklabels(names, rotation=45, ha='right')
axes[0].legend()
axes[0].set_ylim(0.7, 0.95)

# 6.2 ROC æ›²çº¿
for result in results:
    fpr, tpr, _ = roc_curve(y_test, result['y_prob'])
    axes[1].plot(fpr, tpr, label=f"{result['name']} (AUC={result['auc']:.3f})")

axes[1].plot([0, 1], [0, 1], 'k--')
axes[1].set_xlabel('False Positive Rate')
axes[1].set_ylabel('True Positive Rate')
axes[1].set_title('ROC Curves')
axes[1].legend(loc='lower right')

plt.tight_layout()
plt.savefig('model_comparison.png', dpi=150)
plt.show()

# æ‰“å°ç»“æœè¡¨æ ¼
print("\næ¨¡å‹å¯¹æ¯”ç»“æœ:")
results_df = pd.DataFrame([{
    'Model': r['name'],
    'CV Accuracy': f"{r['cv_mean']:.4f} Â± {r['cv_std']:.4f}",
    'Test Accuracy': r['accuracy'],
    'Test AUC': r['auc']
} for r in results])
print(results_df.to_string(index=False))

# ============================================================
print("\n" + "=" * 60)
print("7. æœ€ä½³æ¨¡å‹è¯¦ç»†åˆ†æ")
print("=" * 60)

# é€‰æ‹©æœ€ä½³æ¨¡å‹
best_result = max(results, key=lambda x: x['auc'])
print(f"æœ€ä½³æ¨¡å‹: {best_result['name']}")

# 7.1 åˆ†ç±»æŠ¥å‘Š
print(f"\nåˆ†ç±»æŠ¥å‘Š:")
print(classification_report(y_test, best_result['y_pred'], target_names=['Died', 'Survived']))

# 7.2 æ··æ·†çŸ©é˜µ
fig, axes = plt.subplots(1, 2, figsize=(14, 5))

cm = confusion_matrix(y_test, best_result['y_pred'])
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[0],
            xticklabels=['Died', 'Survived'], yticklabels=['Died', 'Survived'])
axes[0].set_xlabel('Predicted')
axes[0].set_ylabel('Actual')
axes[0].set_title(f'{best_result["name"]} - Confusion Matrix')

# 7.3 ç‰¹å¾é‡è¦æ€§
if hasattr(best_result['model'], 'feature_importances_'):
    importance = best_result['model'].feature_importances_
    indices = np.argsort(importance)[::-1]

    axes[1].barh(range(len(importance)), importance[indices])
    axes[1].set_yticks(range(len(importance)))
    axes[1].set_yticklabels([features[i] for i in indices])
    axes[1].set_xlabel('Importance')
    axes[1].set_title(f'{best_result["name"]} - Feature Importances')
    axes[1].invert_yaxis()

plt.tight_layout()
plt.savefig('best_model_analysis.png', dpi=150)
plt.show()

# æ‰“å°ç‰¹å¾é‡è¦æ€§
if hasattr(best_result['model'], 'feature_importances_'):
    print("\nç‰¹å¾é‡è¦æ€§æ’å:")
    for i, idx in enumerate(indices[:10]):
        print(f"  {i+1}. {features[idx]}: {importance[idx]:.4f}")

# ============================================================
print("\n" + "=" * 60)
print("8. è¶…å‚æ•°è°ƒä¼˜")
print("=" * 60)

print("å¯¹ Random Forest è¿›è¡Œç½‘æ ¼æœç´¢...")

param_grid = {
    'n_estimators': [100, 200, 300],
    'max_depth': [5, 10, 15],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

rf = RandomForestClassifier(random_state=42)
grid_search = GridSearchCV(rf, param_grid, cv=5, scoring='accuracy', n_jobs=-1, verbose=1)
grid_search.fit(X_train_scaled, y_train)

print(f"\næœ€ä½³å‚æ•°: {grid_search.best_params_}")
print(f"æœ€ä½³ CV Accuracy: {grid_search.best_score_:.4f}")

# æµ‹è¯•é›†è¯„ä¼°
best_rf = grid_search.best_estimator_
y_pred_tuned = best_rf.predict(X_test_scaled)
y_prob_tuned = best_rf.predict_proba(X_test_scaled)[:, 1]

print(f"\nè°ƒä¼˜åæµ‹è¯•é›†æ€§èƒ½:")
print(f"  Accuracy: {accuracy_score(y_test, y_pred_tuned):.4f}")
print(f"  AUC: {roc_auc_score(y_test, y_prob_tuned):.4f}")

# ============================================================
print("\n" + "=" * 60)
print("9. æ€»ç»“ä¸æ´å¯Ÿ")
print("=" * 60)

print("""
é¡¹ç›®æ€»ç»“ï¼š

1. æ•°æ®æ´å¯Ÿï¼š
   - å¥³æ€§ç”Ÿå­˜ç‡(74%)è¿œé«˜äºç”·æ€§(19%)
   - 1ç­‰èˆ±ç”Ÿå­˜ç‡(63%)æœ€é«˜ï¼Œ3ç­‰èˆ±æœ€ä½(24%)
   - å„¿ç«¥ç”Ÿå­˜ç‡è¾ƒé«˜

2. é‡è¦ç‰¹å¾ï¼š
   - æ€§åˆ«(Sex)æ˜¯æœ€é‡è¦çš„ç‰¹å¾
   - èˆ±ä½(Pclass)ã€ç¥¨ä»·(Fare)ä¹Ÿå¾ˆé‡è¦
   - å®¶åº­å¤§å°æœ‰ä¸€å®šå½±å“

3. æ¨¡å‹è¡¨ç°ï¼š
   - éšæœºæ£®æ—å’Œæ¢¯åº¦æå‡è¡¨ç°æœ€å¥½
   - æµ‹è¯•é›†å‡†ç¡®ç‡çº¦ 82-84%
   - AUC çº¦ 0.86-0.88

4. æ”¹è¿›æ–¹å‘ï¼š
   - æ›´ç²¾ç»†çš„ç‰¹å¾å·¥ç¨‹
   - å°è¯• XGBoost/LightGBM
   - ç‰¹å¾äº¤å‰ç»„åˆ
   - é›†æˆå¤šä¸ªæ¨¡å‹
""")

print("\né¡¹ç›®å®Œæˆï¼")
```

---

## æ‰©å±•ä»»åŠ¡

- [ ] ä½¿ç”¨ XGBoost/LightGBMï¼Œå¯¹æ¯”æ€§èƒ½
- [ ] åˆ›å»ºæ›´å¤šç‰¹å¾ï¼ˆå¦‚å§“åé•¿åº¦ã€èˆ±ä½å·å¼€å¤´å­—æ¯ç­‰ï¼‰
- [ ] å°è¯•ç‰¹å¾äº¤å‰ï¼ˆå¦‚ Sex * Pclassï¼‰
- [ ] ä½¿ç”¨ Stacking é›†æˆå¤šä¸ªæ¨¡å‹
- [ ] æäº¤åˆ° Kaggle ç«èµ›ï¼ŒæŸ¥çœ‹å®é™…æ’å

---

## â¡ï¸ ä¸‹ä¸€æ­¥

å®Œæˆæœ¬é¡¹ç›®åï¼Œç»§ç»­æŒ‘æˆ˜ [13-é¡¹ç›®-ç”¨æˆ·èšç±».md](./13-é¡¹ç›®-ç”¨æˆ·èšç±».md)
