# ğŸ“Š 07 - è¶…å‚æ•°è°ƒä¼˜

> æœ¬æ–‡è¯¦ç»†ä»‹ç»æ¨¡å‹è¶…å‚æ•°è°ƒä¼˜çš„æ–¹æ³•

---

## ç›®å½•

1. [è¶…å‚æ•° vs æ¨¡å‹å‚æ•°](#1-è¶…å‚æ•°-vs-æ¨¡å‹å‚æ•°)
2. [ç½‘æ ¼æœç´¢](#2-ç½‘æ ¼æœç´¢)
3. [éšæœºæœç´¢](#3-éšæœºæœç´¢)
4. [è´å¶æ–¯ä¼˜åŒ–](#4-è´å¶æ–¯ä¼˜åŒ–)
5. [å®ç”¨æŠ€å·§](#5-å®ç”¨æŠ€å·§)
6. [ç»ƒä¹ é¢˜](#6-ç»ƒä¹ é¢˜)

---

## 1. è¶…å‚æ•° vs æ¨¡å‹å‚æ•°

### 1.1 åŒºåˆ«

| | æ¨¡å‹å‚æ•° | è¶…å‚æ•° |
|---|---------|--------|
| å®šä¹‰ | æ¨¡å‹ä»æ•°æ®å­¦ä¹ å¾—åˆ° | è®­ç»ƒå‰éœ€è¦è®¾ç½® |
| ç¤ºä¾‹ | æƒé‡ã€åç½® | å­¦ä¹ ç‡ã€æ ‘æ·±åº¦ |
| è°ƒæ•´æ–¹å¼ | è®­ç»ƒè¿‡ç¨‹è‡ªåŠ¨ä¼˜åŒ– | äººå·¥æˆ–æœç´¢è®¾ç½® |

### 1.2 å¸¸è§è¶…å‚æ•°

```python
# å†³ç­–æ ‘
from sklearn.tree import DecisionTreeClassifier
DecisionTreeClassifier(
    max_depth=5,           # æœ€å¤§æ·±åº¦
    min_samples_split=10,  # åˆ†è£‚æœ€å°æ ·æœ¬æ•°
    min_samples_leaf=5,    # å¶å­æœ€å°æ ·æœ¬æ•°
    max_features='sqrt'    # æœ€å¤§ç‰¹å¾æ•°
)

# éšæœºæ£®æ—
from sklearn.ensemble import RandomForestClassifier
RandomForestClassifier(
    n_estimators=100,      # æ ‘çš„æ•°é‡
    max_depth=10,
    min_samples_split=5,
    max_features='sqrt',
    bootstrap=True
)

# XGBoost
from xgboost import XGBClassifier
XGBClassifier(
    n_estimators=100,
    max_depth=5,
    learning_rate=0.1,     # å­¦ä¹ ç‡
    subsample=0.8,         # æ ·æœ¬é‡‡æ ·æ¯”ä¾‹
    colsample_bytree=0.8,  # ç‰¹å¾é‡‡æ ·æ¯”ä¾‹
    reg_alpha=0.1,         # L1 æ­£åˆ™åŒ–
    reg_lambda=1           # L2 æ­£åˆ™åŒ–
)
```

---

## 2. ç½‘æ ¼æœç´¢

### 2.1 GridSearchCV

```python
from sklearn.model_selection import GridSearchCV
from sklearn.ensemble import RandomForestClassifier
from sklearn.datasets import make_classification
import time

# ç”Ÿæˆæ•°æ®
X, y = make_classification(n_samples=1000, n_features=20, random_state=42)

# å®šä¹‰å‚æ•°ç½‘æ ¼
param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [5, 10, 15, None],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

# è®¡ç®—æœç´¢ç©ºé—´å¤§å°
n_combinations = 1
for v in param_grid.values():
    n_combinations *= len(v)
print(f"æ€»ç»„åˆæ•°: {n_combinations}")

# ç½‘æ ¼æœç´¢
model = RandomForestClassifier(random_state=42)

start = time.time()
grid_search = GridSearchCV(
    model,
    param_grid,
    cv=5,               # 5 æŠ˜äº¤å‰éªŒè¯
    scoring='accuracy', # è¯„ä¼°æŒ‡æ ‡
    n_jobs=-1,          # å¹¶è¡Œ
    verbose=1,          # æ‰“å°è¿›åº¦
    return_train_score=True
)
grid_search.fit(X, y)
elapsed = time.time() - start

print(f"\næœç´¢è€—æ—¶: {elapsed:.2f}s")
print(f"æœ€ä½³å‚æ•°: {grid_search.best_params_}")
print(f"æœ€ä½³å¾—åˆ†: {grid_search.best_score_:.4f}")
```

### 2.2 æŸ¥çœ‹æœç´¢ç»“æœ

```python
import pandas as pd
import matplotlib.pyplot as plt

# è·å–æ‰€æœ‰ç»“æœ
results = pd.DataFrame(grid_search.cv_results_)

# æŸ¥çœ‹å‰å‡ è¡Œ
print(results[['params', 'mean_test_score', 'std_test_score', 'rank_test_score']].head(10))

# å¯è§†åŒ–ï¼šä¸åŒ n_estimators çš„æ•ˆæœ
param_name = 'param_n_estimators'
scores_by_param = results.groupby(param_name)['mean_test_score'].mean()

plt.figure(figsize=(8, 5))
plt.bar(scores_by_param.index.astype(str), scores_by_param.values)
plt.xlabel('n_estimators')
plt.ylabel('Mean CV Score')
plt.title('Effect of n_estimators')
plt.show()
```

### 2.3 ä½¿ç”¨æœ€ä½³æ¨¡å‹

```python
# è·å–æœ€ä½³æ¨¡å‹
best_model = grid_search.best_estimator_

# ç›´æ¥ä½¿ç”¨
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

best_model.fit(X_train, y_train)
print(f"æµ‹è¯•é›†å‡†ç¡®ç‡: {best_model.score(X_test, y_test):.4f}")
```

---

## 3. éšæœºæœç´¢

### 3.1 ä¸ºä»€ä¹ˆç”¨éšæœºæœç´¢ï¼Ÿ

- **ç½‘æ ¼æœç´¢**ï¼šéå†æ‰€æœ‰ç»„åˆï¼Œç»„åˆæ•°çˆ†ç‚¸
- **éšæœºæœç´¢**ï¼šéšæœºé‡‡æ ·ï¼Œæ›´é«˜æ•ˆæ‰¾åˆ°å¥½å‚æ•°

### 3.2 RandomizedSearchCV

```python
from sklearn.model_selection import RandomizedSearchCV
from scipy.stats import randint, uniform
import numpy as np

# å®šä¹‰å‚æ•°åˆ†å¸ƒï¼ˆå¯ä»¥æ˜¯è¿ç»­åˆ†å¸ƒï¼‰
param_dist = {
    'n_estimators': randint(50, 300),           # æ•´æ•°å‡åŒ€åˆ†å¸ƒ
    'max_depth': [5, 10, 15, 20, None],         # ç¦»æ•£å€¼
    'min_samples_split': randint(2, 20),
    'min_samples_leaf': randint(1, 10),
    'max_features': uniform(0.5, 0.5)           # è¿ç»­å‡åŒ€åˆ†å¸ƒ [0.5, 1.0]
}

model = RandomForestClassifier(random_state=42)

start = time.time()
random_search = RandomizedSearchCV(
    model,
    param_dist,
    n_iter=50,          # åªæœç´¢ 50 æ¬¡
    cv=5,
    scoring='accuracy',
    random_state=42,
    n_jobs=-1,
    verbose=1
)
random_search.fit(X, y)
elapsed = time.time() - start

print(f"\næœç´¢è€—æ—¶: {elapsed:.2f}s")
print(f"æœ€ä½³å‚æ•°: {random_search.best_params_}")
print(f"æœ€ä½³å¾—åˆ†: {random_search.best_score_:.4f}")
```

### 3.3 ç½‘æ ¼æœç´¢ vs éšæœºæœç´¢

```python
from sklearn.model_selection import GridSearchCV, RandomizedSearchCV
from sklearn.ensemble import GradientBoostingClassifier
import time

# å‚æ•°ç½‘æ ¼
param_grid = {
    'n_estimators': [50, 100, 150, 200],
    'max_depth': [3, 5, 7, 10],
    'learning_rate': [0.01, 0.05, 0.1, 0.2],
    'subsample': [0.6, 0.8, 1.0]
}

model = GradientBoostingClassifier(random_state=42)

# ç½‘æ ¼æœç´¢
print("ç½‘æ ¼æœç´¢...")
start = time.time()
grid = GridSearchCV(model, param_grid, cv=3, n_jobs=-1, verbose=0)
grid.fit(X, y)
grid_time = time.time() - start
print(f"  è€—æ—¶: {grid_time:.2f}s, æœ€ä½³å¾—åˆ†: {grid.best_score_:.4f}")

# éšæœºæœç´¢ï¼ˆç›¸åŒæ—¶é—´é¢„ç®—ï¼‰
print("\néšæœºæœç´¢ (n_iter=50)...")
start = time.time()
random = RandomizedSearchCV(model, param_grid, n_iter=50, cv=3, n_jobs=-1, verbose=0, random_state=42)
random.fit(X, y)
random_time = time.time() - start
print(f"  è€—æ—¶: {random_time:.2f}s, æœ€ä½³å¾—åˆ†: {random.best_score_:.4f}")

print(f"\néšæœºæœç´¢èŠ‚çœäº† {(grid_time - random_time) / grid_time * 100:.1f}% çš„æ—¶é—´")
```

---

## 4. è´å¶æ–¯ä¼˜åŒ–

### 4.1 åŸç†

è´å¶æ–¯ä¼˜åŒ–ä½¿ç”¨æ¦‚ç‡æ¨¡å‹æ¥é¢„æµ‹å‚æ•°çš„æ•ˆæœï¼Œæ™ºèƒ½åœ°é€‰æ‹©ä¸‹ä¸€ä¸ªè¦å°è¯•çš„å‚æ•°ç»„åˆã€‚

### 4.2 ä½¿ç”¨ Optuna

```python
# å®‰è£…ï¼špip install optuna

import optuna
from sklearn.model_selection import cross_val_score
from sklearn.ensemble import RandomForestClassifier

def objective(trial):
    """å®šä¹‰ä¼˜åŒ–ç›®æ ‡"""
    # å®šä¹‰è¶…å‚æ•°æœç´¢ç©ºé—´
    params = {
        'n_estimators': trial.suggest_int('n_estimators', 50, 300),
        'max_depth': trial.suggest_int('max_depth', 3, 20),
        'min_samples_split': trial.suggest_int('min_samples_split', 2, 20),
        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 10),
        'max_features': trial.suggest_float('max_features', 0.5, 1.0)
    }

    model = RandomForestClassifier(**params, random_state=42, n_jobs=-1)
    scores = cross_val_score(model, X, y, cv=5, scoring='accuracy')

    return scores.mean()

# åˆ›å»ºå¹¶è¿è¡Œä¼˜åŒ–
study = optuna.create_study(direction='maximize')  # æœ€å¤§åŒ–å‡†ç¡®ç‡
study.optimize(objective, n_trials=50, show_progress_bar=True)

print(f"\næœ€ä½³å‚æ•°: {study.best_params}")
print(f"æœ€ä½³å¾—åˆ†: {study.best_value:.4f}")
```

### 4.3 å¯è§†åŒ–ä¼˜åŒ–è¿‡ç¨‹

```python
import optuna.visualization as vis

# ä¼˜åŒ–å†å²
fig = vis.plot_optimization_history(study)
fig.show()

# å‚æ•°é‡è¦æ€§
fig = vis.plot_param_importances(study)
fig.show()

# å‚æ•°å…³ç³»
fig = vis.plot_slice(study)
fig.show()
```

### 4.4 XGBoost + Optuna å®Œæ•´ç¤ºä¾‹

```python
import optuna
from xgboost import XGBClassifier
from sklearn.model_selection import cross_val_score

def objective_xgb(trial):
    params = {
        'n_estimators': trial.suggest_int('n_estimators', 50, 300),
        'max_depth': trial.suggest_int('max_depth', 3, 10),
        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),
        'subsample': trial.suggest_float('subsample', 0.6, 1.0),
        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),
        'reg_alpha': trial.suggest_float('reg_alpha', 1e-8, 10.0, log=True),
        'reg_lambda': trial.suggest_float('reg_lambda', 1e-8, 10.0, log=True),
    }

    model = XGBClassifier(**params, random_state=42, n_jobs=-1, verbosity=0)
    scores = cross_val_score(model, X, y, cv=5, scoring='accuracy')

    return scores.mean()

study_xgb = optuna.create_study(direction='maximize')
study_xgb.optimize(objective_xgb, n_trials=100, show_progress_bar=True)

print(f"\næœ€ä½³å‚æ•°: {study_xgb.best_params}")
print(f"æœ€ä½³å¾—åˆ†: {study_xgb.best_value:.4f}")
```

---

## 5. å®ç”¨æŠ€å·§

### 5.1 è°ƒä¼˜ç­–ç•¥

```
æ¨èè°ƒä¼˜é¡ºåºï¼š
1. å…ˆç”¨éšæœºæœç´¢æ‰¾åˆ°å¤§è‡´èŒƒå›´
2. å†ç”¨ç½‘æ ¼æœç´¢åœ¨å°èŒƒå›´å†…ç²¾è°ƒ
3. æˆ–è€…ç›´æ¥ç”¨è´å¶æ–¯ä¼˜åŒ–
```

### 5.2 å¸¸è§æ¨¡å‹çš„é‡è¦è¶…å‚æ•°

```python
# éšæœºæ£®æ—ï¼šæœ€é‡è¦çš„è¶…å‚æ•°
rf_important = {
    'n_estimators': [100, 200, 300],    # é¦–å…ˆè°ƒ
    'max_depth': [5, 10, 15, None],     # å…¶æ¬¡è°ƒ
    'min_samples_split': [2, 5, 10],    # å†è°ƒ
}

# XGBoostï¼šæœ€é‡è¦çš„è¶…å‚æ•°
xgb_important = {
    'learning_rate': [0.01, 0.05, 0.1], # æœ€é‡è¦
    'n_estimators': [100, 200, 300],
    'max_depth': [3, 5, 7],
    'subsample': [0.8, 1.0],
}

# LightGBMï¼šæœ€é‡è¦çš„è¶…å‚æ•°
lgb_important = {
    'learning_rate': [0.01, 0.05, 0.1],
    'n_estimators': [100, 200, 300],
    'num_leaves': [31, 63, 127],        # LightGBM ç‰¹æœ‰
    'max_depth': [-1, 5, 10],
}
```

### 5.3 ä½¿ç”¨ Pipeline è°ƒä¼˜

```python
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.svm import SVC
from sklearn.model_selection import GridSearchCV

# åˆ›å»º Pipeline
pipeline = Pipeline([
    ('scaler', StandardScaler()),
    ('pca', PCA()),
    ('svc', SVC())
])

# å‚æ•°ç½‘æ ¼ï¼ˆæ³¨æ„å‘½åæ ¼å¼ï¼šstep_name__param_nameï¼‰
param_grid = {
    'pca__n_components': [5, 10, 15, 20],
    'svc__C': [0.1, 1, 10],
    'svc__kernel': ['rbf', 'linear'],
    'svc__gamma': ['scale', 'auto']
}

grid_search = GridSearchCV(pipeline, param_grid, cv=5, n_jobs=-1)
grid_search.fit(X, y)

print(f"æœ€ä½³å‚æ•°: {grid_search.best_params_}")
print(f"æœ€ä½³å¾—åˆ†: {grid_search.best_score_:.4f}")
```

### 5.4 è‡ªå®šä¹‰è¯„åˆ†å‡½æ•°

```python
from sklearn.metrics import make_scorer, f1_score
from sklearn.model_selection import GridSearchCV

# è‡ªå®šä¹‰è¯„åˆ†å‡½æ•°
def custom_scorer(y_true, y_pred):
    # å¯ä»¥è‡ªå®šä¹‰ä»»ä½•è¯„åˆ†é€»è¾‘
    return f1_score(y_true, y_pred, average='weighted')

scorer = make_scorer(custom_scorer)

grid_search = GridSearchCV(
    RandomForestClassifier(random_state=42),
    {'n_estimators': [50, 100], 'max_depth': [5, 10]},
    cv=5,
    scoring=scorer  # ä½¿ç”¨è‡ªå®šä¹‰è¯„åˆ†
)
grid_search.fit(X, y)
print(f"æœ€ä½³å¾—åˆ† (è‡ªå®šä¹‰): {grid_search.best_score_:.4f}")
```

### 5.5 åµŒå¥—äº¤å‰éªŒè¯

```python
from sklearn.model_selection import cross_val_score, GridSearchCV
from sklearn.ensemble import RandomForestClassifier

# å†…å±‚ï¼šè¶…å‚æ•°è°ƒä¼˜
param_grid = {
    'n_estimators': [50, 100],
    'max_depth': [5, 10]
}

inner_cv = GridSearchCV(
    RandomForestClassifier(random_state=42),
    param_grid,
    cv=3,
    n_jobs=-1
)

# å¤–å±‚ï¼šè¯„ä¼°æ³›åŒ–æ€§èƒ½
outer_scores = cross_val_score(inner_cv, X, y, cv=5, scoring='accuracy')

print(f"åµŒå¥— CV å¾—åˆ†: {outer_scores.mean():.4f} Â± {outer_scores.std():.4f}")
```

---

## 6. ç»ƒä¹ é¢˜

### åŸºç¡€ç»ƒä¹ 

1. ç”¨ GridSearchCV ä¸ºéšæœºæ£®æ—æ‰¾æœ€ä½³ `n_estimators` å’Œ `max_depth`
2. ç”¨ RandomizedSearchCV è°ƒä¼˜ XGBoost
3. æ¯”è¾ƒç½‘æ ¼æœç´¢å’Œéšæœºæœç´¢çš„æ•ˆç‡

### è¿›é˜¶ç»ƒä¹ 

4. ç”¨ Optuna è°ƒä¼˜ä¸€ä¸ª LightGBM æ¨¡å‹
5. å¯¹ä¸€ä¸ª Pipeline è¿›è¡Œè¶…å‚æ•°è°ƒä¼˜

### å‚è€ƒç­”æ¡ˆ

<details>
<summary>ç»ƒä¹  1 å‚è€ƒç­”æ¡ˆ</summary>

```python
from sklearn.model_selection import GridSearchCV
from sklearn.ensemble import RandomForestClassifier
from sklearn.datasets import make_classification

X, y = make_classification(n_samples=1000, n_features=20, random_state=42)

param_grid = {
    'n_estimators': [50, 100, 150, 200],
    'max_depth': [5, 10, 15, 20, None]
}

grid_search = GridSearchCV(
    RandomForestClassifier(random_state=42),
    param_grid,
    cv=5,
    scoring='accuracy',
    n_jobs=-1
)
grid_search.fit(X, y)

print(f"æœ€ä½³å‚æ•°: {grid_search.best_params_}")
print(f"æœ€ä½³å¾—åˆ†: {grid_search.best_score_:.4f}")
```

</details>

---

## â¡ï¸ ä¸‹ä¸€æ­¥

å­¦å®Œæœ¬èŠ‚åï¼Œç»§ç»­å­¦ä¹  [08-é¡¹ç›®-æˆ¿ä»·é¢„æµ‹.md](./08-é¡¹ç›®-æˆ¿ä»·é¢„æµ‹.md)

