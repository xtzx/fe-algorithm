# ğŸ“Š 09 - æ¨¡å‹å¯è§£é‡Šæ€§

> æœ¬æ–‡è¯¦ç»†ä»‹ç»å¦‚ä½•è§£é‡Šæœºå™¨å­¦ä¹ æ¨¡å‹çš„é¢„æµ‹

---

## ç›®å½•

1. [ä¸ºä»€ä¹ˆéœ€è¦å¯è§£é‡Šæ€§](#1-ä¸ºä»€ä¹ˆéœ€è¦å¯è§£é‡Šæ€§)
2. [ç‰¹å¾é‡è¦æ€§](#2-ç‰¹å¾é‡è¦æ€§)
3. [SHAP å€¼](#3-shap-å€¼)
4. [éƒ¨åˆ†ä¾èµ–å›¾](#4-éƒ¨åˆ†ä¾èµ–å›¾)
5. [LIME](#5-lime)
6. [å®æˆ˜æ¡ˆä¾‹](#6-å®æˆ˜æ¡ˆä¾‹)
7. [ç»ƒä¹ é¢˜](#7-ç»ƒä¹ é¢˜)

---

## 1. ä¸ºä»€ä¹ˆéœ€è¦å¯è§£é‡Šæ€§

### 1.1 å¯è§£é‡Šæ€§çš„é‡è¦æ€§

```
ä¸šåŠ¡éœ€æ±‚ï¼š
â”œâ”€ é‡‘èè´·æ¬¾ï¼šä¸ºä»€ä¹ˆæ‹’ç»è¿™ä¸ªäººçš„è´·æ¬¾ç”³è¯·ï¼Ÿ
â”œâ”€ åŒ»ç–—è¯Šæ–­ï¼šæ¨¡å‹ä¸ºä»€ä¹ˆè®¤ä¸ºè¿™æ˜¯æ¶æ€§è‚¿ç˜¤ï¼Ÿ
â”œâ”€ æ³•å¾‹åˆè§„ï¼šéœ€è¦è§£é‡Šè‡ªåŠ¨åŒ–å†³ç­–çš„åŸå› 
â””â”€ è°ƒè¯•æ¨¡å‹ï¼šå‘ç°æ¨¡å‹æ˜¯å¦å­¦åˆ°äº†é”™è¯¯çš„æ¨¡å¼

æŠ€æœ¯éœ€æ±‚ï¼š
â”œâ”€ å‘ç°æ•°æ®æ³„éœ²
â”œâ”€ æ£€æµ‹è¿‡æ‹Ÿåˆ
â”œâ”€ éªŒè¯æ¨¡å‹é€»è¾‘
â””â”€ æ”¹è¿›ç‰¹å¾å·¥ç¨‹
```

### 1.2 æ¨¡å‹å¯è§£é‡Šæ€§åˆ†ç±»

| ç±»å‹ | æè¿° | ç¤ºä¾‹ |
|------|------|------|
| **å†…åœ¨å¯è§£é‡Š** | æ¨¡å‹æœ¬èº«å°±èƒ½è§£é‡Š | çº¿æ€§å›å½’ã€å†³ç­–æ ‘ |
| **äº‹åè§£é‡Š** | ç”¨é¢å¤–æ–¹æ³•è§£é‡Šé»‘ç›’æ¨¡å‹ | SHAPã€LIME |
| **å…¨å±€è§£é‡Š** | è§£é‡Šæ•´ä½“æ¨¡å‹è¡Œä¸º | ç‰¹å¾é‡è¦æ€§ã€PDP |
| **å±€éƒ¨è§£é‡Š** | è§£é‡Šå•ä¸ªé¢„æµ‹ | SHAPã€LIME |

### 1.3 å‡†å¤‡å·¥ä½œ

```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.datasets import fetch_california_housing
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from xgboost import XGBRegressor

# åŠ è½½æ•°æ®
housing = fetch_california_housing()
X = pd.DataFrame(housing.data, columns=housing.feature_names)
y = housing.target

# åˆ’åˆ†æ•°æ®
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# è®­ç»ƒæ¨¡å‹
model = RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42)
model.fit(X_train, y_train)

print(f"æ¨¡å‹ RÂ² åˆ†æ•°: {model.score(X_test, y_test):.4f}")
print(f"ç‰¹å¾åˆ—: {list(X.columns)}")
```

---

## 2. ç‰¹å¾é‡è¦æ€§

### 2.1 åŸºäºæ ‘æ¨¡å‹çš„ç‰¹å¾é‡è¦æ€§

```python
import pandas as pd
import matplotlib.pyplot as plt

# è·å–ç‰¹å¾é‡è¦æ€§
importance = model.feature_importances_

# åˆ›å»º DataFrame å¹¶æ’åº
importance_df = pd.DataFrame({
    'Feature': X.columns,
    'Importance': importance
}).sort_values('Importance', ascending=True)

# å¯è§†åŒ–
plt.figure(figsize=(10, 6))
plt.barh(importance_df['Feature'], importance_df['Importance'])
plt.xlabel('Importance')
plt.title('Random Forest Feature Importances')
plt.tight_layout()
plt.show()

# æ‰“å°
print("\nç‰¹å¾é‡è¦æ€§æ’å:")
for i, row in importance_df[::-1].iterrows():
    print(f"  {row['Feature']}: {row['Importance']:.4f}")
```

### 2.2 ç‰¹å¾é‡è¦æ€§çš„ç±»å‹

```python
from sklearn.inspection import permutation_importance
import time

# ========== 1. åŸºäºä¸çº¯åº¦çš„é‡è¦æ€§ (MDI) ==========
# éšæœºæ£®æ—é»˜è®¤ä½¿ç”¨è¿™ç§æ–¹æ³•
mdi_importance = model.feature_importances_

# ========== 2. æ’åˆ—é‡è¦æ€§ (Permutation Importance) ==========
# æ‰“ä¹±æŸä¸ªç‰¹å¾ï¼Œè§‚å¯Ÿæ¨¡å‹æ€§èƒ½ä¸‹é™ç¨‹åº¦
start = time.time()
perm_importance = permutation_importance(
    model, X_test, y_test,
    n_repeats=10,
    random_state=42,
    n_jobs=-1
)
print(f"æ’åˆ—é‡è¦æ€§è®¡ç®—è€—æ—¶: {time.time() - start:.2f}s")

# å¯¹æ¯”ä¸¤ç§é‡è¦æ€§
importance_compare = pd.DataFrame({
    'Feature': X.columns,
    'MDI Importance': mdi_importance,
    'Permutation Importance': perm_importance.importances_mean
})

# å¯è§†åŒ–å¯¹æ¯”
fig, axes = plt.subplots(1, 2, figsize=(14, 5))

# MDI
importance_compare_sorted = importance_compare.sort_values('MDI Importance', ascending=True)
axes[0].barh(importance_compare_sorted['Feature'], importance_compare_sorted['MDI Importance'])
axes[0].set_xlabel('Importance')
axes[0].set_title('Mean Decrease Impurity (MDI)')

# Permutation
importance_compare_sorted = importance_compare.sort_values('Permutation Importance', ascending=True)
axes[1].barh(importance_compare_sorted['Feature'], importance_compare_sorted['Permutation Importance'])
axes[1].set_xlabel('Importance')
axes[1].set_title('Permutation Importance')

plt.tight_layout()
plt.show()
```

### 2.3 MDI vs æ’åˆ—é‡è¦æ€§

| æ–¹æ³• | ä¼˜ç‚¹ | ç¼ºç‚¹ |
|------|------|------|
| MDI | è®¡ç®—å¿« | åå‘é«˜åŸºæ•°ç‰¹å¾ï¼Œå¯èƒ½ä¸å¯é  |
| Permutation | æ›´å¯é ï¼Œé€‚ç”¨ä»»ä½•æ¨¡å‹ | è®¡ç®—æ…¢ï¼Œå—ç›¸å…³ç‰¹å¾å½±å“ |

---

## 3. SHAP å€¼

### 3.1 SHAP ç®€ä»‹

**SHAP = SHapley Additive exPlanations**

åŸºäºåšå¼ˆè®ºä¸­çš„ Shapley å€¼ï¼Œå…¬å¹³åœ°åˆ†é…æ¯ä¸ªç‰¹å¾å¯¹é¢„æµ‹çš„è´¡çŒ®ã€‚

```
é¢„æµ‹å€¼ = åŸºå‡†å€¼ + ç‰¹å¾1è´¡çŒ® + ç‰¹å¾2è´¡çŒ® + ... + ç‰¹å¾nè´¡çŒ®
```

### 3.2 å®‰è£…å’ŒåŸºæœ¬ä½¿ç”¨

```python
# å®‰è£…ï¼špip install shap

import shap

# åˆ›å»º SHAP è§£é‡Šå™¨
explainer = shap.TreeExplainer(model)

# è®¡ç®— SHAP å€¼
shap_values = explainer.shap_values(X_test)

print(f"SHAP å€¼å½¢çŠ¶: {shap_values.shape}")
print(f"ä¸ç‰¹å¾çŸ©é˜µå½¢çŠ¶ç›¸åŒ: {X_test.shape}")
```

### 3.3 SHAP æ‘˜è¦å›¾

```python
import shap

# ========== 1. æ¡å½¢å›¾ï¼šå¹³å‡ SHAP å€¼ ==========
plt.figure(figsize=(10, 6))
shap.summary_plot(shap_values, X_test, plot_type="bar", show=False)
plt.title('Feature Importance (mean |SHAP|)')
plt.tight_layout()
plt.show()

# ========== 2. èœ‚ç¾¤å›¾ï¼šSHAP å€¼åˆ†å¸ƒ ==========
plt.figure(figsize=(10, 6))
shap.summary_plot(shap_values, X_test, show=False)
plt.title('SHAP Summary Plot')
plt.tight_layout()
plt.show()
```

**èœ‚ç¾¤å›¾è§£è¯»**ï¼š
- æ¯ä¸ªç‚¹æ˜¯ä¸€ä¸ªæ ·æœ¬
- X è½´ï¼šSHAP å€¼ï¼ˆå¯¹é¢„æµ‹çš„å½±å“ï¼‰
- é¢œè‰²ï¼šç‰¹å¾å€¼çš„é«˜ä½ï¼ˆçº¢é«˜è“ä½ï¼‰
- çº¢ç‚¹åœ¨å³è¾¹ = é«˜å€¼ä½¿é¢„æµ‹å¢åŠ 

### 3.4 å•ä¸ªæ ·æœ¬è§£é‡Š

```python
import shap

# é€‰æ‹©ä¸€ä¸ªæ ·æœ¬
sample_idx = 0
sample = X_test.iloc[[sample_idx]]

# è®¡ç®—è¯¥æ ·æœ¬çš„ SHAP å€¼
sample_shap = explainer.shap_values(sample)

# ========== ç€‘å¸ƒå›¾ ==========
shap.plots.waterfall(shap.Explanation(
    values=sample_shap[0],
    base_values=explainer.expected_value,
    data=sample.values[0],
    feature_names=X.columns.tolist()
), show=False)
plt.title(f'Sample {sample_idx} Prediction Explanation')
plt.tight_layout()
plt.show()

# ========== åŠ›å›¾ï¼ˆForce Plotï¼‰ ==========
shap.initjs()
shap.force_plot(
    explainer.expected_value,
    sample_shap[0],
    sample.iloc[0],
    matplotlib=True
)
plt.show()

# æ‰“å°è§£é‡Š
print(f"\næ ·æœ¬ {sample_idx} çš„é¢„æµ‹è§£é‡Š:")
print(f"åŸºå‡†å€¼ï¼ˆå¹³å‡é¢„æµ‹ï¼‰: {explainer.expected_value:.4f}")
print(f"æœ€ç»ˆé¢„æµ‹: {model.predict(sample)[0]:.4f}")
print(f"\nå„ç‰¹å¾è´¡çŒ®:")
for feat, val, shap_val in zip(X.columns, sample.values[0], sample_shap[0]):
    direction = "â†‘" if shap_val > 0 else "â†“"
    print(f"  {feat} = {val:.2f}: {direction} {shap_val:+.4f}")
```

### 3.5 ç‰¹å¾äº¤äº’

```python
import shap

# ========== ä¾èµ–å›¾ï¼ˆå•ç‰¹å¾ï¼‰ ==========
shap.dependence_plot(
    "MedInc",  # æ”¶å…¥ä¸­ä½æ•°
    shap_values,
    X_test,
    interaction_index=None,  # è‡ªåŠ¨é€‰æ‹©äº¤äº’ç‰¹å¾
    show=False
)
plt.title('SHAP Dependence Plot: MedInc')
plt.tight_layout()
plt.show()

# ========== æŒ‡å®šäº¤äº’ç‰¹å¾ ==========
shap.dependence_plot(
    "MedInc",
    shap_values,
    X_test,
    interaction_index="AveRooms",  # å¹³å‡æˆ¿é—´æ•°
    show=False
)
plt.title('SHAP Dependence: MedInc vs AveRooms')
plt.tight_layout()
plt.show()
```

### 3.6 SHAP ç”¨äºåˆ†ç±»æ¨¡å‹

```python
from sklearn.ensemble import RandomForestClassifier
from sklearn.datasets import load_iris
import shap

# åŠ è½½åˆ†ç±»æ•°æ®
iris = load_iris()
X_iris = pd.DataFrame(iris.data, columns=iris.feature_names)
y_iris = iris.target

# è®­ç»ƒåˆ†ç±»å™¨
clf = RandomForestClassifier(n_estimators=100, random_state=42)
clf.fit(X_iris, y_iris)

# SHAP
explainer_clf = shap.TreeExplainer(clf)
shap_values_clf = explainer_clf.shap_values(X_iris)

# å¤šåˆ†ç±»çš„ SHAP å€¼æ˜¯ä¸€ä¸ªåˆ—è¡¨ï¼Œæ¯ä¸ªç±»åˆ«ä¸€ä¸ª
print(f"ç±»åˆ«æ•°: {len(shap_values_clf)}")
print(f"æ¯ä¸ªç±»åˆ«çš„ SHAP å€¼å½¢çŠ¶: {shap_values_clf[0].shape}")

# å¯è§†åŒ–æŸä¸ªç±»åˆ«çš„ SHAP å€¼
class_idx = 2  # virginica
plt.figure(figsize=(10, 6))
shap.summary_plot(shap_values_clf[class_idx], X_iris, show=False)
plt.title(f'SHAP Summary for Class: {iris.target_names[class_idx]}')
plt.tight_layout()
plt.show()
```

---

## 4. éƒ¨åˆ†ä¾èµ–å›¾

### 4.1 PDP (Partial Dependence Plot)

å±•ç¤ºæŸä¸ªç‰¹å¾ä¸é¢„æµ‹å€¼çš„è¾¹é™…å…³ç³»ã€‚

```python
from sklearn.inspection import PartialDependenceDisplay
import matplotlib.pyplot as plt

# ========== å•ç‰¹å¾ PDP ==========
fig, ax = plt.subplots(figsize=(10, 6))
PartialDependenceDisplay.from_estimator(
    model, X_train,
    features=['MedInc'],  # æ”¶å…¥ä¸­ä½æ•°
    ax=ax
)
plt.title('Partial Dependence: MedInc')
plt.tight_layout()
plt.show()

# ========== å¤šç‰¹å¾ PDP ==========
fig, ax = plt.subplots(figsize=(14, 4))
PartialDependenceDisplay.from_estimator(
    model, X_train,
    features=['MedInc', 'AveRooms', 'HouseAge', 'Population'],
    ax=ax,
    n_cols=4
)
plt.suptitle('Partial Dependence Plots')
plt.tight_layout()
plt.show()
```

### 4.2 äºŒç»´ PDP

```python
from sklearn.inspection import PartialDependenceDisplay

# ä¸¤ä¸ªç‰¹å¾çš„äº¤äº’æ•ˆæœ
fig, ax = plt.subplots(figsize=(10, 8))
PartialDependenceDisplay.from_estimator(
    model, X_train,
    features=[('MedInc', 'AveRooms')],  # äºŒç»´
    ax=ax,
    kind='both'  # åŒæ—¶æ˜¾ç¤ºç­‰é«˜çº¿å’Œ 3D
)
plt.title('2D Partial Dependence: MedInc vs AveRooms')
plt.tight_layout()
plt.show()
```

### 4.3 ICE å›¾ (Individual Conditional Expectation)

PDP æ˜¯ ICE çš„å¹³å‡ã€‚ICE å±•ç¤ºæ¯ä¸ªæ ·æœ¬çš„ä¾èµ–å…³ç³»ã€‚

```python
from sklearn.inspection import PartialDependenceDisplay

fig, axes = plt.subplots(1, 2, figsize=(14, 5))

# åªæ˜¾ç¤º PDP
PartialDependenceDisplay.from_estimator(
    model, X_train[:100],
    features=['MedInc'],
    kind='average',  # åªæ˜¾ç¤ºå¹³å‡
    ax=axes[0]
)
axes[0].set_title('PDP only')

# æ˜¾ç¤º ICE + PDP
PartialDependenceDisplay.from_estimator(
    model, X_train[:100],
    features=['MedInc'],
    kind='both',  # ICE + PDP
    ax=axes[1]
)
axes[1].set_title('ICE + PDP')

plt.tight_layout()
plt.show()
```

---

## 5. LIME

### 5.1 LIME ç®€ä»‹

**LIME = Local Interpretable Model-agnostic Explanations**

é€šè¿‡åœ¨æ ·æœ¬å‘¨å›´ç”Ÿæˆæ‰°åŠ¨æ•°æ®ï¼Œè®­ç»ƒä¸€ä¸ªç®€å•çš„å±€éƒ¨å¯è§£é‡Šæ¨¡å‹ã€‚

```python
# å®‰è£…ï¼špip install lime

import lime
import lime.lime_tabular
import numpy as np

# åˆ›å»º LIME è§£é‡Šå™¨
lime_explainer = lime.lime_tabular.LimeTabularExplainer(
    training_data=X_train.values,
    feature_names=X.columns.tolist(),
    mode='regression'
)

# è§£é‡Šå•ä¸ªæ ·æœ¬
sample_idx = 0
sample = X_test.iloc[sample_idx].values

# ç”Ÿæˆè§£é‡Š
lime_exp = lime_explainer.explain_instance(
    sample,
    model.predict,
    num_features=8
)

# å¯è§†åŒ–
lime_exp.as_pyplot_figure()
plt.title(f'LIME Explanation for Sample {sample_idx}')
plt.tight_layout()
plt.show()

# æ‰“å°è§£é‡Š
print(f"\næ ·æœ¬ {sample_idx} çš„ LIME è§£é‡Š:")
print(f"é¢„æµ‹å€¼: {model.predict([sample])[0]:.4f}")
print(f"\nè§„åˆ™è§£é‡Š:")
for feat, weight in lime_exp.as_list():
    direction = "â†‘" if weight > 0 else "â†“"
    print(f"  {feat}: {direction} {weight:+.4f}")
```

### 5.2 LIME ç”¨äºåˆ†ç±»

```python
import lime
import lime.lime_tabular
from sklearn.ensemble import RandomForestClassifier
from sklearn.datasets import load_iris

# è®­ç»ƒåˆ†ç±»å™¨
iris = load_iris()
clf = RandomForestClassifier(n_estimators=100, random_state=42)
clf.fit(iris.data, iris.target)

# åˆ›å»º LIME è§£é‡Šå™¨
lime_clf_explainer = lime.lime_tabular.LimeTabularExplainer(
    training_data=iris.data,
    feature_names=iris.feature_names,
    class_names=iris.target_names,
    mode='classification'
)

# è§£é‡Šä¸€ä¸ªæ ·æœ¬
sample = iris.data[100]  # ä¸€ä¸ª virginica æ ·æœ¬

lime_exp = lime_clf_explainer.explain_instance(
    sample,
    clf.predict_proba,
    num_features=4,
    top_labels=1
)

# å¯è§†åŒ–
lime_exp.show_in_notebook(show_table=True)

# æˆ–è€…ç”¨ matplotlib
fig = lime_exp.as_pyplot_figure()
plt.tight_layout()
plt.show()
```

### 5.3 SHAP vs LIME

| ç‰¹æ€§ | SHAP | LIME |
|------|------|------|
| ç†è®ºåŸºç¡€ | åšå¼ˆè®ºï¼ˆShapley å€¼ï¼‰ | å±€éƒ¨çº¿æ€§è¿‘ä¼¼ |
| ä¸€è‡´æ€§ | æœ‰ç†è®ºä¿è¯ | æ— ä¿è¯ |
| è®¡ç®—é€Ÿåº¦ | æ ‘æ¨¡å‹å¿«ï¼Œå…¶ä»–æ…¢ | è¾ƒå¿« |
| é€‚ç”¨æ¨¡å‹ | ä»»æ„ | ä»»æ„ |
| è§£é‡Šç±»å‹ | å…¨å±€ + å±€éƒ¨ | ä»…å±€éƒ¨ |
| ç¨³å®šæ€§ | ç¨³å®š | å¯èƒ½ä¸ç¨³å®š |

---

## 6. å®æˆ˜æ¡ˆä¾‹

### 6.1 ç»¼åˆå¯è§£é‡Šæ€§åˆ†æ

```python
import shap
import matplotlib.pyplot as plt
from sklearn.ensemble import GradientBoostingRegressor

# è®­ç»ƒä¸€ä¸ªæ›´å¤æ‚çš„æ¨¡å‹
gbm = GradientBoostingRegressor(n_estimators=100, max_depth=5, random_state=42)
gbm.fit(X_train, y_train)

print(f"GBM RÂ² åˆ†æ•°: {gbm.score(X_test, y_test):.4f}")

# ========== 1. ç‰¹å¾é‡è¦æ€§ ==========
fig, axes = plt.subplots(2, 2, figsize=(14, 10))

# 1.1 åŸºäºæ¨¡å‹çš„ç‰¹å¾é‡è¦æ€§
importance_df = pd.DataFrame({
    'Feature': X.columns,
    'Importance': gbm.feature_importances_
}).sort_values('Importance', ascending=True)
axes[0, 0].barh(importance_df['Feature'], importance_df['Importance'])
axes[0, 0].set_title('Feature Importance (MDI)')

# 1.2 SHAP é‡è¦æ€§
explainer = shap.TreeExplainer(gbm)
shap_values = explainer.shap_values(X_test)

shap_importance = np.abs(shap_values).mean(axis=0)
shap_df = pd.DataFrame({
    'Feature': X.columns,
    'SHAP': shap_importance
}).sort_values('SHAP', ascending=True)
axes[0, 1].barh(shap_df['Feature'], shap_df['SHAP'])
axes[0, 1].set_title('Feature Importance (mean |SHAP|)')

# ========== 2. å•æ ·æœ¬è§£é‡Š ==========
# é€‰æ‹©ä¸€ä¸ªé¢„æµ‹å€¼è¾ƒé«˜çš„æ ·æœ¬
high_pred_idx = np.argmax(gbm.predict(X_test))
sample = X_test.iloc[high_pred_idx]
sample_shap = shap_values[high_pred_idx]

# 2.1 SHAP ç€‘å¸ƒå›¾
ax = axes[1, 0]
sorted_idx = np.argsort(np.abs(sample_shap))[::-1]
colors = ['green' if v > 0 else 'red' for v in sample_shap[sorted_idx]]
ax.barh(X.columns[sorted_idx], sample_shap[sorted_idx], color=colors)
ax.set_xlabel('SHAP value')
ax.set_title(f'Sample {high_pred_idx} SHAP Values\n(Predicted: {gbm.predict([sample])[0]:.2f})')

# 2.2 ç‰¹å¾å€¼ä¸è´¡çŒ®
ax = axes[1, 1]
data_for_plot = pd.DataFrame({
    'Feature': X.columns,
    'Value': sample.values,
    'SHAP': sample_shap
}).sort_values('SHAP', key=abs, ascending=False).head(5)

texts = [f"{f}\n({v:.1f})" for f, v in zip(data_for_plot['Feature'], data_for_plot['Value'])]
colors = ['green' if s > 0 else 'red' for s in data_for_plot['SHAP']]
ax.barh(texts, data_for_plot['SHAP'], color=colors)
ax.set_xlabel('SHAP value')
ax.set_title('Top 5 Contributing Features')

plt.tight_layout()
plt.show()

# ========== 3. ä¾èµ–å…³ç³» ==========
fig, axes = plt.subplots(1, 2, figsize=(14, 5))

shap.dependence_plot('MedInc', shap_values, X_test, ax=axes[0], show=False)
axes[0].set_title('MedInc Dependence')

shap.dependence_plot('HouseAge', shap_values, X_test, ax=axes[1], show=False)
axes[1].set_title('HouseAge Dependence')

plt.tight_layout()
plt.show()
```

### 6.2 æ¨¡å‹è°ƒè¯•ç¤ºä¾‹

```python
# å‡è®¾æˆ‘ä»¬å‘ç°æ¨¡å‹å¯èƒ½æœ‰é—®é¢˜
# é€šè¿‡å¯è§£é‡Šæ€§å·¥å…·æ¥è°ƒè¯•

# 1. æ£€æŸ¥æ˜¯å¦æœ‰æ„å¤–çš„é«˜é‡è¦æ€§ç‰¹å¾
print("ç‰¹å¾é‡è¦æ€§æ£€æŸ¥:")
for feat, imp in zip(X.columns, gbm.feature_importances_):
    print(f"  {feat}: {imp:.4f}")

# 2. æ£€æŸ¥ SHAP ä¾èµ–æ˜¯å¦ç¬¦åˆç›´è§‰
# æ¯”å¦‚ï¼šæ”¶å…¥è¶Šé«˜ï¼Œæˆ¿ä»·åº”è¯¥è¶Šé«˜
print("\nç›´è§‰æ£€éªŒ:")
print("MedIncï¼ˆæ”¶å…¥ä¸­ä½æ•°ï¼‰ä¸ SHAP å€¼çš„ç›¸å…³æ€§:")
correlation = np.corrcoef(X_test['MedInc'], shap_values[:, X.columns.tolist().index('MedInc')])[0, 1]
print(f"  ç›¸å…³ç³»æ•°: {correlation:.4f}")
if correlation > 0:
    print("  âœ“ ç¬¦åˆç›´è§‰ï¼šæ”¶å…¥è¶Šé«˜ï¼Œé¢„æµ‹æˆ¿ä»·è¶Šé«˜")
else:
    print("  âœ— å¯èƒ½æœ‰é—®é¢˜ï¼šéœ€è¦è¿›ä¸€æ­¥æ£€æŸ¥")

# 3. æ‰¾å‡ºæ¨¡å‹å¯èƒ½å‡ºé”™çš„æ ·æœ¬
predictions = gbm.predict(X_test)
residuals = y_test - predictions
worst_idx = np.argmax(np.abs(residuals))

print(f"\nè¯¯å·®æœ€å¤§çš„æ ·æœ¬ (idx={worst_idx}):")
print(f"  çœŸå®å€¼: {y_test.iloc[worst_idx]:.4f}")
print(f"  é¢„æµ‹å€¼: {predictions[worst_idx]:.4f}")
print(f"  è¯¯å·®: {residuals.iloc[worst_idx]:.4f}")
print(f"  ç‰¹å¾å€¼:")
for feat, val in zip(X.columns, X_test.iloc[worst_idx]):
    print(f"    {feat}: {val:.4f}")
```

---

## 7. ç»ƒä¹ é¢˜

### åŸºç¡€ç»ƒä¹ 

1. è®­ç»ƒä¸€ä¸ªéšæœºæ£®æ—ï¼Œæå–å¹¶å¯è§†åŒ–ç‰¹å¾é‡è¦æ€§
2. ä½¿ç”¨ SHAP è§£é‡Šå•ä¸ªé¢„æµ‹
3. ç»˜åˆ¶ PDP å›¾ï¼Œåˆ†æç‰¹å¾ä¸é¢„æµ‹çš„å…³ç³»

### è¿›é˜¶ç»ƒä¹ 

4. å¯¹æ¯” MDI ç‰¹å¾é‡è¦æ€§å’Œæ’åˆ—é‡è¦æ€§çš„å·®å¼‚
5. ä½¿ç”¨ SHAP æ‰¾å‡ºæ¨¡å‹ä¸­æœ€é‡è¦çš„ç‰¹å¾äº¤äº’

### å‚è€ƒç­”æ¡ˆ

<details>
<summary>ç»ƒä¹  2 å‚è€ƒç­”æ¡ˆ</summary>

```python
import shap
from sklearn.ensemble import RandomForestRegressor
from sklearn.datasets import fetch_california_housing
import pandas as pd

# åŠ è½½æ•°æ®
housing = fetch_california_housing()
X = pd.DataFrame(housing.data, columns=housing.feature_names)
y = housing.target

# è®­ç»ƒæ¨¡å‹
model = RandomForestRegressor(n_estimators=100, random_state=42)
model.fit(X, y)

# åˆ›å»º SHAP è§£é‡Šå™¨
explainer = shap.TreeExplainer(model)

# é€‰æ‹©ä¸€ä¸ªæ ·æœ¬
sample = X.iloc[[0]]

# è®¡ç®— SHAP å€¼
shap_values = explainer.shap_values(sample)

# å¯è§†åŒ–
shap.waterfall_plot(shap.Explanation(
    values=shap_values[0],
    base_values=explainer.expected_value,
    data=sample.values[0],
    feature_names=X.columns.tolist()
))
```

</details>

---

## â¡ï¸ ä¸‹ä¸€æ­¥

å­¦å®Œæœ¬èŠ‚åï¼Œç»§ç»­å­¦ä¹  [10-é¡¹ç›®-é¸¢å°¾èŠ±å…¥é—¨.md](./10-é¡¹ç›®-é¸¢å°¾èŠ±å…¥é—¨.md)

