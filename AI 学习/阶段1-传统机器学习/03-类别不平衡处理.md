# ğŸ“Š 03 - ç±»åˆ«ä¸å¹³è¡¡å¤„ç†

> æœ¬æ–‡è¯¦ç»†ä»‹ç»å¦‚ä½•å¤„ç†ç±»åˆ«åˆ†å¸ƒä¸å‡åŒ€çš„åˆ†ç±»é—®é¢˜

---

## ç›®å½•

1. [ä»€ä¹ˆæ˜¯ç±»åˆ«ä¸å¹³è¡¡](#1-ä»€ä¹ˆæ˜¯ç±»åˆ«ä¸å¹³è¡¡)
2. [ä¸ºä»€ä¹ˆç±»åˆ«ä¸å¹³è¡¡æ˜¯é—®é¢˜](#2-ä¸ºä»€ä¹ˆç±»åˆ«ä¸å¹³è¡¡æ˜¯é—®é¢˜)
3. [é‡‡æ ·æ–¹æ³•](#3-é‡‡æ ·æ–¹æ³•)
4. [ç®—æ³•å±‚é¢å¤„ç†](#4-ç®—æ³•å±‚é¢å¤„ç†)
5. [è¯„ä¼°ç­–ç•¥](#5-è¯„ä¼°ç­–ç•¥)
6. [å®æˆ˜æ¡ˆä¾‹](#6-å®æˆ˜æ¡ˆä¾‹)
7. [ç»ƒä¹ é¢˜](#7-ç»ƒä¹ é¢˜)

---

## 1. ä»€ä¹ˆæ˜¯ç±»åˆ«ä¸å¹³è¡¡

### 1.1 å®šä¹‰

ç±»åˆ«ä¸å¹³è¡¡æŒ‡åˆ†ç±»ä»»åŠ¡ä¸­ï¼Œå„ç±»åˆ«æ ·æœ¬æ•°é‡å·®å¼‚æ‚¬æ®Šã€‚

```python
import numpy as np
import matplotlib.pyplot as plt

# ç¤ºä¾‹ï¼šä¿¡ç”¨å¡æ¬ºè¯ˆæ£€æµ‹
# æ­£å¸¸äº¤æ˜“ï¼š99.8%
# æ¬ºè¯ˆäº¤æ˜“ï¼š0.2%

labels = ['æ­£å¸¸äº¤æ˜“\n(99.8%)', 'æ¬ºè¯ˆäº¤æ˜“\n(0.2%)']
sizes = [99800, 200]

fig, axes = plt.subplots(1, 2, figsize=(12, 4))

# é¥¼å›¾
axes[0].pie(sizes, labels=labels, autopct='%1.1f%%', colors=['steelblue', 'coral'])
axes[0].set_title('ç±»åˆ«åˆ†å¸ƒï¼ˆé¥¼å›¾ï¼‰')

# æŸ±çŠ¶å›¾
axes[1].bar(labels, sizes, color=['steelblue', 'coral'])
axes[1].set_ylabel('æ ·æœ¬æ•°é‡')
axes[1].set_title('ç±»åˆ«åˆ†å¸ƒï¼ˆæŸ±çŠ¶å›¾ï¼‰')
axes[1].set_yscale('log')  # å¯¹æ•°åæ ‡ï¼Œå¦åˆ™çœ‹ä¸åˆ°å°ç±»

plt.tight_layout()
plt.show()
```

### 1.2 å¸¸è§çš„ä¸å¹³è¡¡åœºæ™¯

| åœºæ™¯ | æ­£ç±»ï¼ˆå°‘æ•°ç±»ï¼‰ | è´Ÿç±»ï¼ˆå¤šæ•°ç±»ï¼‰ | ä¸å¹³è¡¡æ¯”ä¾‹ |
|------|---------------|---------------|-----------|
| ä¿¡ç”¨å¡æ¬ºè¯ˆ | æ¬ºè¯ˆäº¤æ˜“ | æ­£å¸¸äº¤æ˜“ | 1:500 |
| ç–¾ç—…è¯Šæ–­ | æ‚£ç—… | å¥åº· | 1:100 |
| ç‚¹å‡»é¢„æµ‹ | ç‚¹å‡» | æœªç‚¹å‡» | 1:50 |
| å¼‚å¸¸æ£€æµ‹ | å¼‚å¸¸ | æ­£å¸¸ | 1:1000 |

### 1.3 ä¸å¹³è¡¡ç¨‹åº¦åˆ†ç±»

```
è½»åº¦ä¸å¹³è¡¡ï¼š1:2 ~ 1:10
ä¸­åº¦ä¸å¹³è¡¡ï¼š1:10 ~ 1:100
é‡åº¦ä¸å¹³è¡¡ï¼š1:100 ~ 1:1000
æåº¦ä¸å¹³è¡¡ï¼š> 1:1000
```

---

## 2. ä¸ºä»€ä¹ˆç±»åˆ«ä¸å¹³è¡¡æ˜¯é—®é¢˜

### 2.1 å‡†ç¡®ç‡é™·é˜±

```python
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report
import numpy as np

# åˆ›å»ºä¸¥é‡ä¸å¹³è¡¡æ•°æ®ï¼š1:99
np.random.seed(42)
n_samples = 10000
n_minority = 100

X = np.random.randn(n_samples, 5)
y = np.zeros(n_samples)
y[:n_minority] = 1  # åªæœ‰ 100 ä¸ªæ­£æ ·æœ¬

# æ‰“ä¹±
shuffle_idx = np.random.permutation(n_samples)
X, y = X[shuffle_idx], y[shuffle_idx]

print(f"æ­£æ ·æœ¬æ¯”ä¾‹: {y.mean():.2%}")

# åˆ’åˆ†æ•°æ®
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# è®­ç»ƒæ¨¡å‹
model = LogisticRegression()
model.fit(X_train, y_train)

# è¯„ä¼°
y_pred = model.predict(X_test)
print(f"\nå‡†ç¡®ç‡: {accuracy_score(y_test, y_pred):.2%}")
print(f"\nåˆ†ç±»æŠ¥å‘Š:\n{classification_report(y_test, y_pred)}")

# ä¸€ä¸ªæ°¸è¿œé¢„æµ‹å¤šæ•°ç±»çš„"å‚»ç“œæ¨¡å‹"
y_pred_dummy = np.zeros_like(y_test)
print(f"\nå‚»ç“œæ¨¡å‹å‡†ç¡®ç‡: {accuracy_score(y_test, y_pred_dummy):.2%}")
```

**ç»“è®º**ï¼šåœ¨æåº¦ä¸å¹³è¡¡æ•°æ®ä¸Šï¼Œä¸€ä¸ªæ°¸è¿œé¢„æµ‹å¤šæ•°ç±»çš„æ¨¡å‹ä¹Ÿèƒ½è¾¾åˆ° 99% å‡†ç¡®ç‡ï¼

### 2.2 é—®é¢˜æ ¹æº

```
æ¨¡å‹åå‘å¤šæ•°ç±»çš„åŸå› ï¼š
1. è®­ç»ƒæ—¶å¤šæ•°ç±»æ ·æœ¬æ›´å¤šï¼Œè´¡çŒ®æ›´å¤šæ¢¯åº¦
2. å†³ç­–è¾¹ç•Œè¢«å¤šæ•°ç±»"æ¨å‘"å°‘æ•°ç±»åŒºåŸŸ
3. å°‘æ•°ç±»çš„æ¨¡å¼è¢«æ·¹æ²¡åœ¨å¤šæ•°ç±»ä¸­
```

---

## 3. é‡‡æ ·æ–¹æ³•

### 3.1 æ¬ é‡‡æ ·ï¼ˆUndersamplingï¼‰

å‡å°‘å¤šæ•°ç±»æ ·æœ¬æ•°é‡ï¼Œä½¿ä¸¤ç±»å¹³è¡¡ã€‚

```python
from sklearn.datasets import make_classification
from collections import Counter

# åˆ›å»ºä¸å¹³è¡¡æ•°æ®
X, y = make_classification(
    n_samples=10000, n_features=10, n_classes=2,
    weights=[0.95, 0.05],  # 95% vs 5%
    random_state=42
)
print(f"åŸå§‹åˆ†å¸ƒ: {Counter(y)}")

# ========== éšæœºæ¬ é‡‡æ · ==========
from imblearn.under_sampling import RandomUnderSampler

rus = RandomUnderSampler(random_state=42)
X_rus, y_rus = rus.fit_resample(X, y)
print(f"éšæœºæ¬ é‡‡æ ·å: {Counter(y_rus)}")

# ========== Tomek Links ==========
from imblearn.under_sampling import TomekLinks

tl = TomekLinks()
X_tl, y_tl = tl.fit_resample(X, y)
print(f"Tomek Links å: {Counter(y_tl)}")

# ========== Edited Nearest Neighbors ==========
from imblearn.under_sampling import EditedNearestNeighbours

enn = EditedNearestNeighbours()
X_enn, y_enn = enn.fit_resample(X, y)
print(f"ENN å: {Counter(y_enn)}")
```

**æ¬ é‡‡æ ·ä¼˜ç¼ºç‚¹**ï¼š

| ä¼˜ç‚¹ | ç¼ºç‚¹ |
|------|------|
| å‡å°‘è®­ç»ƒæ—¶é—´ | ä¸¢å¤±å¤šæ•°ç±»ä¿¡æ¯ |
| ç®€å•æ˜“ç”¨ | å¯èƒ½ä¸¢å¤±é‡è¦æ ·æœ¬ |

### 3.2 è¿‡é‡‡æ ·ï¼ˆOversamplingï¼‰

å¢åŠ å°‘æ•°ç±»æ ·æœ¬æ•°é‡ã€‚

```python
# ========== éšæœºè¿‡é‡‡æ · ==========
from imblearn.over_sampling import RandomOverSampler

ros = RandomOverSampler(random_state=42)
X_ros, y_ros = ros.fit_resample(X, y)
print(f"éšæœºè¿‡é‡‡æ ·å: {Counter(y_ros)}")

# ========== SMOTEï¼ˆåˆæˆå°‘æ•°ç±»è¿‡é‡‡æ ·ï¼‰ ==========
from imblearn.over_sampling import SMOTE

smote = SMOTE(random_state=42)
X_smote, y_smote = smote.fit_resample(X, y)
print(f"SMOTE å: {Counter(y_smote)}")

# ========== ADASYNï¼ˆè‡ªé€‚åº”åˆæˆé‡‡æ ·ï¼‰ ==========
from imblearn.over_sampling import ADASYN

adasyn = ADASYN(random_state=42)
X_adasyn, y_adasyn = adasyn.fit_resample(X, y)
print(f"ADASYN å: {Counter(y_adasyn)}")
```

### 3.3 SMOTE åŸç†å¯è§†åŒ–

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import make_classification
from imblearn.over_sampling import SMOTE

# åˆ›å»ºç®€å•çš„ 2D ä¸å¹³è¡¡æ•°æ®
np.random.seed(42)
X_minority = np.random.randn(20, 2) + [2, 2]
X_majority = np.random.randn(200, 2) + [0, 0]
X = np.vstack([X_minority, X_majority])
y = np.array([1]*20 + [0]*200)

# SMOTE
smote = SMOTE(random_state=42, k_neighbors=5)
X_smote, y_smote = smote.fit_resample(X, y)

# å¯è§†åŒ–
fig, axes = plt.subplots(1, 2, figsize=(14, 5))

# åŸå§‹æ•°æ®
axes[0].scatter(X[y==0, 0], X[y==0, 1], c='blue', alpha=0.5, label='å¤šæ•°ç±»')
axes[0].scatter(X[y==1, 0], X[y==1, 1], c='red', s=100, label='å°‘æ•°ç±»')
axes[0].set_title(f'åŸå§‹æ•°æ®\nå¤šæ•°ç±»: {sum(y==0)}, å°‘æ•°ç±»: {sum(y==1)}')
axes[0].legend()

# SMOTE å
axes[1].scatter(X_smote[y_smote==0, 0], X_smote[y_smote==0, 1], c='blue', alpha=0.5, label='å¤šæ•°ç±»')
# åŸå§‹å°‘æ•°ç±»
original_minority_idx = np.where(y==1)[0]
axes[1].scatter(X_smote[original_minority_idx, 0], X_smote[original_minority_idx, 1],
                c='red', s=100, label='åŸå§‹å°‘æ•°ç±»')
# åˆæˆæ ·æœ¬
synthetic_idx = list(range(len(X), len(X_smote)))
axes[1].scatter(X_smote[synthetic_idx, 0], X_smote[synthetic_idx, 1],
                c='orange', s=50, marker='x', label='åˆæˆæ ·æœ¬')
axes[1].set_title(f'SMOTE å\nå¤šæ•°ç±»: {sum(y_smote==0)}, å°‘æ•°ç±»: {sum(y_smote==1)}')
axes[1].legend()

plt.tight_layout()
plt.show()
```

**SMOTE åŸç†**ï¼š
1. å¯¹äºæ¯ä¸ªå°‘æ•°ç±»æ ·æœ¬ï¼Œæ‰¾åˆ° K ä¸ªæœ€è¿‘é‚»ï¼ˆä¹Ÿæ˜¯å°‘æ•°ç±»ï¼‰
2. åœ¨è¯¥æ ·æœ¬å’Œé‚»å±…ä¹‹é—´çš„è¿çº¿ä¸Šéšæœºç”Ÿæˆæ–°æ ·æœ¬
3. é‡å¤ç›´åˆ°è¾¾åˆ°ç›®æ ‡æ•°é‡

### 3.4 ç»„åˆé‡‡æ ·

ç»“åˆæ¬ é‡‡æ ·å’Œè¿‡é‡‡æ ·çš„ä¼˜ç‚¹ã€‚

```python
from imblearn.combine import SMOTEENN, SMOTETomek

# SMOTE + ENN
smote_enn = SMOTEENN(random_state=42)
X_se, y_se = smote_enn.fit_resample(X, y)
print(f"SMOTE + ENN å: {Counter(y_se)}")

# SMOTE + Tomek Links
smote_tomek = SMOTETomek(random_state=42)
X_st, y_st = smote_tomek.fit_resample(X, y)
print(f"SMOTE + Tomek å: {Counter(y_st)}")
```

### 3.5 é‡‡æ ·æ–¹æ³•å¯¹æ¯”

```python
from sklearn.model_selection import cross_val_score
from sklearn.ensemble import RandomForestClassifier
from imblearn.pipeline import Pipeline as ImbPipeline
from imblearn.over_sampling import SMOTE, RandomOverSampler
from imblearn.under_sampling import RandomUnderSampler
import numpy as np

# åˆ›å»ºä¸å¹³è¡¡æ•°æ®
X, y = make_classification(
    n_samples=5000, n_features=20, n_classes=2,
    weights=[0.95, 0.05], random_state=42
)

# ä¸åŒé‡‡æ ·ç­–ç•¥
samplers = {
    'æ— é‡‡æ ·': None,
    'éšæœºæ¬ é‡‡æ ·': RandomUnderSampler(random_state=42),
    'éšæœºè¿‡é‡‡æ ·': RandomOverSampler(random_state=42),
    'SMOTE': SMOTE(random_state=42),
}

clf = RandomForestClassifier(n_estimators=100, random_state=42)

print(f"{'æ–¹æ³•':<15} {'F1-score (macro)':<18}")
print("-" * 35)

for name, sampler in samplers.items():
    if sampler is None:
        pipeline = clf
    else:
        pipeline = ImbPipeline([
            ('sampler', sampler),
            ('classifier', clf)
        ])

    scores = cross_val_score(pipeline, X, y, cv=5, scoring='f1_macro')
    print(f"{name:<15} {scores.mean():.4f} Â± {scores.std():.4f}")
```

---

## 4. ç®—æ³•å±‚é¢å¤„ç†

### 4.1 ç±»åˆ«æƒé‡ï¼ˆClass Weightï¼‰

å¤§å¤šæ•° sklearn åˆ†ç±»å™¨æ”¯æŒ `class_weight` å‚æ•°ã€‚

```python
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report
from sklearn.model_selection import train_test_split

# åˆ›å»ºä¸å¹³è¡¡æ•°æ®
X, y = make_classification(
    n_samples=5000, n_features=20, n_classes=2,
    weights=[0.95, 0.05], random_state=42
)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# ä¸ä½¿ç”¨ç±»åˆ«æƒé‡
model_no_weight = LogisticRegression(max_iter=1000)
model_no_weight.fit(X_train, y_train)
y_pred_no = model_no_weight.predict(X_test)
print("æ— ç±»åˆ«æƒé‡:")
print(classification_report(y_test, y_pred_no))

# ä½¿ç”¨ç±»åˆ«æƒé‡
model_balanced = LogisticRegression(class_weight='balanced', max_iter=1000)
model_balanced.fit(X_train, y_train)
y_pred_bal = model_balanced.predict(X_test)
print("\nä½¿ç”¨ class_weight='balanced':")
print(classification_report(y_test, y_pred_bal))
```

**class_weight='balanced' åŸç†**ï¼š
$$w_j = \frac{n}{k \times n_j}$$
- n: æ€»æ ·æœ¬æ•°
- k: ç±»åˆ«æ•°
- n_j: ç¬¬ j ç±»çš„æ ·æœ¬æ•°

### 4.2 è‡ªå®šä¹‰ç±»åˆ«æƒé‡

```python
from sklearn.utils.class_weight import compute_class_weight
import numpy as np

# è®¡ç®—ç±»åˆ«æƒé‡
classes = np.unique(y_train)
weights = compute_class_weight('balanced', classes=classes, y=y_train)
class_weight_dict = dict(zip(classes, weights))

print(f"è®¡ç®—å¾—åˆ°çš„ç±»åˆ«æƒé‡: {class_weight_dict}")

# ä¹Ÿå¯ä»¥æ‰‹åŠ¨è®¾ç½®
custom_weight = {0: 1, 1: 10}  # ç»™å°‘æ•°ç±» 10 å€æƒé‡
model_custom = LogisticRegression(class_weight=custom_weight, max_iter=1000)
model_custom.fit(X_train, y_train)
```

### 4.3 è°ƒæ•´å†³ç­–é˜ˆå€¼

é»˜è®¤é˜ˆå€¼æ˜¯ 0.5ï¼Œå¯ä»¥æ ¹æ®éœ€æ±‚è°ƒæ•´ã€‚

```python
from sklearn.metrics import precision_recall_curve, f1_score
import matplotlib.pyplot as plt
import numpy as np

# è·å–é¢„æµ‹æ¦‚ç‡
y_proba = model_balanced.predict_proba(X_test)[:, 1]

# è®¡ç®—ä¸åŒé˜ˆå€¼ä¸‹çš„ Precision å’Œ Recall
precisions, recalls, thresholds = precision_recall_curve(y_test, y_proba)

# è®¡ç®—ä¸åŒé˜ˆå€¼ä¸‹çš„ F1
f1_scores = []
for thresh in thresholds:
    y_pred_thresh = (y_proba >= thresh).astype(int)
    f1_scores.append(f1_score(y_test, y_pred_thresh))

# æ‰¾åˆ°æœ€ä½³é˜ˆå€¼
best_idx = np.argmax(f1_scores)
best_threshold = thresholds[best_idx]
print(f"æœ€ä½³é˜ˆå€¼: {best_threshold:.4f}")
print(f"æœ€ä½³ F1: {f1_scores[best_idx]:.4f}")

# å¯è§†åŒ–
fig, axes = plt.subplots(1, 2, figsize=(14, 5))

# Precision-Recall vs Threshold
axes[0].plot(thresholds, precisions[:-1], 'b-', label='Precision')
axes[0].plot(thresholds, recalls[:-1], 'r-', label='Recall')
axes[0].axvline(best_threshold, color='green', linestyle='--', label=f'Best: {best_threshold:.2f}')
axes[0].set_xlabel('Threshold')
axes[0].set_ylabel('Score')
axes[0].set_title('Precision & Recall vs Threshold')
axes[0].legend()
axes[0].grid(True)

# F1 vs Threshold
axes[1].plot(thresholds, f1_scores, 'g-')
axes[1].axvline(best_threshold, color='red', linestyle='--')
axes[1].scatter([best_threshold], [f1_scores[best_idx]], color='red', s=100, zorder=5)
axes[1].set_xlabel('Threshold')
axes[1].set_ylabel('F1 Score')
axes[1].set_title('F1 Score vs Threshold')
axes[1].grid(True)

plt.tight_layout()
plt.show()

# ä½¿ç”¨æœ€ä½³é˜ˆå€¼é¢„æµ‹
y_pred_best = (y_proba >= best_threshold).astype(int)
print(f"\nä½¿ç”¨æœ€ä½³é˜ˆå€¼çš„åˆ†ç±»æŠ¥å‘Š:")
print(classification_report(y_test, y_pred_best))
```

### 4.4 ä»£ä»·æ•æ„Ÿå­¦ä¹ 

ä¸åŒç±»å‹é”™è¯¯æœ‰ä¸åŒä»£ä»·ã€‚

```python
# ä»£ä»·çŸ©é˜µç¤ºä¾‹ï¼ˆç–¾ç—…è¯Šæ–­ï¼‰
#                 é¢„æµ‹å¥åº·    é¢„æµ‹æ‚£ç—…
# çœŸå®å¥åº·          0          1      (è™šæƒŠä¸€åœº)
# çœŸå®æ‚£ç—…         10          0      (æ¼è¯Šï¼Œä»£ä»·å¾ˆé«˜)

cost_matrix = np.array([[0, 1], [10, 0]])

# ä½¿ç”¨ä»£ä»·çŸ©é˜µä½œä¸ºæ ·æœ¬æƒé‡
sample_weight = np.ones(len(y_train))
# å°‘æ•°ç±»é”™åˆ†ä»£ä»·æ›´é«˜
sample_weight[y_train == 1] = 10

model_cost = LogisticRegression(max_iter=1000)
model_cost.fit(X_train, y_train, sample_weight=sample_weight)
```

---

## 5. è¯„ä¼°ç­–ç•¥

### 5.1 é€‰æ‹©æ­£ç¡®çš„è¯„ä¼°æŒ‡æ ‡

| åœºæ™¯ | æ¨èæŒ‡æ ‡ | åŸå›  |
|------|---------|------|
| é€šç”¨åœºæ™¯ | F1-score (macro) | å¹³è¡¡è€ƒè™‘å„ç±»åˆ« |
| å…³æ³¨å°‘æ•°ç±»å¬å› | Recall | å¦‚ç–¾ç—…æ£€æµ‹ |
| å…³æ³¨å°‘æ•°ç±»å‡†ç¡® | Precision | å¦‚åƒåœ¾é‚®ä»¶æ£€æµ‹ |
| éœ€è¦æ’åºèƒ½åŠ› | AUC-ROC, AUC-PR | é˜ˆå€¼æ— å…³ |
| æåº¦ä¸å¹³è¡¡ | AUC-PR | æ¯” AUC-ROC æ›´æ•æ„Ÿ |

### 5.2 åˆ†å±‚äº¤å‰éªŒè¯

```python
from sklearn.model_selection import StratifiedKFold, cross_val_score

# åˆ†å±‚ K-Fold ä¿æŒæ¯æŠ˜çš„ç±»åˆ«æ¯”ä¾‹ä¸€è‡´
skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

model = RandomForestClassifier(n_estimators=100, random_state=42)
scores = cross_val_score(model, X, y, cv=skf, scoring='f1_macro')

print(f"åˆ†å±‚ 5-Fold F1 (macro): {scores.mean():.4f} Â± {scores.std():.4f}")

# éªŒè¯æ¯æŠ˜çš„ç±»åˆ«æ¯”ä¾‹
print("\næ¯æŠ˜çš„ç±»åˆ«æ¯”ä¾‹:")
for i, (train_idx, val_idx) in enumerate(skf.split(X, y)):
    train_ratio = y[train_idx].mean()
    val_ratio = y[val_idx].mean()
    print(f"Fold {i+1}: è®­ç»ƒé›†æ­£æ ·æœ¬æ¯”ä¾‹={train_ratio:.4f}, éªŒè¯é›†æ­£æ ·æœ¬æ¯”ä¾‹={val_ratio:.4f}")
```

### 5.3 æ··æ·†çŸ©é˜µå½’ä¸€åŒ–

```python
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
import matplotlib.pyplot as plt

y_pred = model_balanced.predict(X_test)
cm = confusion_matrix(y_test, y_pred)

fig, axes = plt.subplots(1, 2, figsize=(12, 5))

# åŸå§‹æ··æ·†çŸ©é˜µ
ConfusionMatrixDisplay(cm, display_labels=['è´Ÿç±»', 'æ­£ç±»']).plot(ax=axes[0], cmap='Blues')
axes[0].set_title('æ··æ·†çŸ©é˜µï¼ˆåŸå§‹ï¼‰')

# å½’ä¸€åŒ–æ··æ·†çŸ©é˜µï¼ˆæŒ‰çœŸå®æ ‡ç­¾å½’ä¸€åŒ–ï¼‰
cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
ConfusionMatrixDisplay(cm_normalized, display_labels=['è´Ÿç±»', 'æ­£ç±»']).plot(ax=axes[1], cmap='Blues', values_format='.2%')
axes[1].set_title('æ··æ·†çŸ©é˜µï¼ˆå½’ä¸€åŒ–ï¼‰')

plt.tight_layout()
plt.show()
```

---

## 6. å®æˆ˜æ¡ˆä¾‹

### 6.1 ä¿¡ç”¨å¡æ¬ºè¯ˆæ£€æµ‹

```python
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, roc_auc_score
from imblearn.over_sampling import SMOTE
from imblearn.pipeline import Pipeline as ImbPipeline
import numpy as np

# æ¨¡æ‹Ÿä¿¡ç”¨å¡æ¬ºè¯ˆæ•°æ®ï¼ˆ1:99 ä¸å¹³è¡¡ï¼‰
X, y = make_classification(
    n_samples=10000, n_features=30, n_classes=2,
    weights=[0.99, 0.01],  # 1% æ¬ºè¯ˆ
    n_informative=15, n_redundant=5,
    random_state=42
)

print(f"åŸå§‹æ•°æ®åˆ†å¸ƒ: æ­£å¸¸={sum(y==0)}, æ¬ºè¯ˆ={sum(y==1)}")

# åˆ’åˆ†æ•°æ®
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# ========== æ–¹æ¡ˆ 1ï¼šåŸºçº¿æ¨¡å‹ ==========
print("\n=== æ–¹æ¡ˆ 1ï¼šåŸºçº¿æ¨¡å‹ ===")
baseline = RandomForestClassifier(n_estimators=100, random_state=42)
baseline.fit(X_train, y_train)
y_pred_baseline = baseline.predict(X_test)
print(classification_report(y_test, y_pred_baseline))
print(f"AUC: {roc_auc_score(y_test, baseline.predict_proba(X_test)[:, 1]):.4f}")

# ========== æ–¹æ¡ˆ 2ï¼šç±»åˆ«æƒé‡ ==========
print("\n=== æ–¹æ¡ˆ 2ï¼šç±»åˆ«æƒé‡ ===")
weighted = RandomForestClassifier(n_estimators=100, class_weight='balanced', random_state=42)
weighted.fit(X_train, y_train)
y_pred_weighted = weighted.predict(X_test)
print(classification_report(y_test, y_pred_weighted))
print(f"AUC: {roc_auc_score(y_test, weighted.predict_proba(X_test)[:, 1]):.4f}")

# ========== æ–¹æ¡ˆ 3ï¼šSMOTE ==========
print("\n=== æ–¹æ¡ˆ 3ï¼šSMOTE ===")
smote_pipeline = ImbPipeline([
    ('smote', SMOTE(random_state=42)),
    ('classifier', RandomForestClassifier(n_estimators=100, random_state=42))
])
smote_pipeline.fit(X_train, y_train)
y_pred_smote = smote_pipeline.predict(X_test)
print(classification_report(y_test, y_pred_smote))
print(f"AUC: {roc_auc_score(y_test, smote_pipeline.predict_proba(X_test)[:, 1]):.4f}")

# ========== æ–¹æ¡ˆ 4ï¼šSMOTE + ç±»åˆ«æƒé‡ ==========
print("\n=== æ–¹æ¡ˆ 4ï¼šSMOTE + ç±»åˆ«æƒé‡ ===")
combined_pipeline = ImbPipeline([
    ('smote', SMOTE(random_state=42)),
    ('classifier', RandomForestClassifier(n_estimators=100, class_weight='balanced', random_state=42))
])
combined_pipeline.fit(X_train, y_train)
y_pred_combined = combined_pipeline.predict(X_test)
print(classification_report(y_test, y_pred_combined))
print(f"AUC: {roc_auc_score(y_test, combined_pipeline.predict_proba(X_test)[:, 1]):.4f}")
```

### 6.2 å¤„ç†ç­–ç•¥é€‰æ‹©æŒ‡å—

```
â”Œâ”€ æ•°æ®é‡å……è¶³ï¼Ÿ
â”‚  â”œâ”€ æ˜¯ â†’ å¯ä»¥å°è¯•æ¬ é‡‡æ ·
â”‚  â””â”€ å¦ â†’ ä¼˜å…ˆè€ƒè™‘è¿‡é‡‡æ ·
â”‚
â”œâ”€ ä¸å¹³è¡¡ç¨‹åº¦ï¼Ÿ
â”‚  â”œâ”€ è½»åº¦ (1:10 ä»¥å†…) â†’ class_weight é€šå¸¸è¶³å¤Ÿ
â”‚  â”œâ”€ ä¸­åº¦ (1:10 ~ 1:100) â†’ SMOTE + class_weight
â”‚  â””â”€ é‡åº¦ (> 1:100) â†’ ç»„åˆç­–ç•¥ + è°ƒæ•´é˜ˆå€¼
â”‚
â””â”€ ä¸šåŠ¡éœ€æ±‚ï¼Ÿ
   â”œâ”€ é«˜å¬å› â†’ é™ä½é˜ˆå€¼
   â””â”€ é«˜ç²¾ç¡® â†’ æé«˜é˜ˆå€¼
```

---

## 7. ç»ƒä¹ é¢˜

### åŸºç¡€ç»ƒä¹ 

1. åˆ›å»ºä¸€ä¸ª 1:20 ä¸å¹³è¡¡æ•°æ®é›†ï¼Œæ¯”è¾ƒæœ‰æ—  `class_weight='balanced'` çš„æ•ˆæœ
2. ä½¿ç”¨ SMOTE å¤„ç†ä¸å¹³è¡¡æ•°æ®ï¼Œè§‚å¯Ÿé‡‡æ ·å‰åçš„åˆ†å¸ƒå˜åŒ–
3. æ‰¾åˆ°æœ€ä½³å†³ç­–é˜ˆå€¼æ¥æœ€å¤§åŒ– F1 åˆ†æ•°

### è¿›é˜¶ç»ƒä¹ 

4. æ¯”è¾ƒ SMOTEã€ADASYNã€SMOTE-Tomek ä¸‰ç§æ–¹æ³•çš„æ•ˆæœ
5. å¯¹ä¸€ä¸ªä¸å¹³è¡¡æ•°æ®é›†ï¼Œç»¼åˆä½¿ç”¨é‡‡æ · + ç±»åˆ«æƒé‡ + é˜ˆå€¼è°ƒæ•´

### å‚è€ƒç­”æ¡ˆ

<details>
<summary>ç»ƒä¹  1 å‚è€ƒç­”æ¡ˆ</summary>

```python
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report

# åˆ›å»º 1:20 ä¸å¹³è¡¡æ•°æ®
X, y = make_classification(
    n_samples=2100, n_features=10, n_classes=2,
    weights=[0.95, 0.05], random_state=42
)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# æ— ç±»åˆ«æƒé‡
model1 = RandomForestClassifier(random_state=42)
model1.fit(X_train, y_train)
print("æ— ç±»åˆ«æƒé‡:")
print(classification_report(y_test, model1.predict(X_test)))

# æœ‰ç±»åˆ«æƒé‡
model2 = RandomForestClassifier(class_weight='balanced', random_state=42)
model2.fit(X_train, y_train)
print("\næœ‰ç±»åˆ«æƒé‡:")
print(classification_report(y_test, model2.predict(X_test)))
```

</details>

---

## â¡ï¸ ä¸‹ä¸€æ­¥

å­¦å®Œæœ¬èŠ‚åï¼Œç»§ç»­å­¦ä¹  [04-è¿‡æ‹Ÿåˆä¸æ­£åˆ™åŒ–.md](./04-è¿‡æ‹Ÿåˆä¸æ­£åˆ™åŒ–.md)

