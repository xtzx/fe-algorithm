# ğŸ“Š 06 - æ ‘æ¨¡å‹ä¸é›†æˆå­¦ä¹ 

> æœ¬æ–‡è¯¦ç»†ä»‹ç»å†³ç­–æ ‘ã€éšæœºæ£®æ—ã€XGBoost/LightGBM ç­‰æ¨¡å‹

---

## ç›®å½•

1. [å†³ç­–æ ‘](#1-å†³ç­–æ ‘)
2. [é›†æˆå­¦ä¹ æ¦‚è¿°](#2-é›†æˆå­¦ä¹ æ¦‚è¿°)
3. [Baggingï¼šéšæœºæ£®æ—](#3-baggingéšæœºæ£®æ—)
4. [Boostingï¼šXGBoost/LightGBM](#4-boostingxgboostlightgbm)
5. [æ¨¡å‹å¯¹æ¯”ä¸é€‰æ‹©](#5-æ¨¡å‹å¯¹æ¯”ä¸é€‰æ‹©)
6. [ç»ƒä¹ é¢˜](#6-ç»ƒä¹ é¢˜)

---

## 1. å†³ç­–æ ‘

### 1.1 åŸºæœ¬åŸç†

**æ ¸å¿ƒæ€æƒ³**ï¼šé€šè¿‡ä¸€ç³»åˆ— if-else è§„åˆ™è¿›è¡Œåˆ†ç±»/å›å½’

```
          [ç‰¹å¾1 > 5?]
          /          \
        æ˜¯            å¦
        /              \
   [ç‰¹å¾2 > 3?]      ç±»åˆ« A
    /        \
  æ˜¯          å¦
  /            \
ç±»åˆ« B       ç±»åˆ« C
```

### 1.2 åˆ†ç±»å†³ç­–æ ‘

```python
from sklearn.tree import DecisionTreeClassifier, plot_tree
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report
import matplotlib.pyplot as plt

# åŠ è½½æ•°æ®
iris = load_iris()
X, y = iris.data, iris.target
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# è®­ç»ƒå†³ç­–æ ‘
model = DecisionTreeClassifier(
    max_depth=3,           # æœ€å¤§æ·±åº¦
    min_samples_split=5,   # åˆ†è£‚æ‰€éœ€æœ€å°æ ·æœ¬æ•°
    min_samples_leaf=2,    # å¶èŠ‚ç‚¹æœ€å°æ ·æœ¬æ•°
    random_state=42
)
model.fit(X_train, y_train)

# è¯„ä¼°
y_pred = model.predict(X_test)
print(f"å‡†ç¡®ç‡: {accuracy_score(y_test, y_pred):.4f}")
print(f"\nåˆ†ç±»æŠ¥å‘Š:\n{classification_report(y_test, y_pred, target_names=iris.target_names)}")

# å¯è§†åŒ–å†³ç­–æ ‘
plt.figure(figsize=(20, 10))
plot_tree(model,
          feature_names=iris.feature_names,
          class_names=iris.target_names,
          filled=True,
          rounded=True,
          fontsize=12)
plt.title('Decision Tree Visualization')
plt.tight_layout()
plt.show()
```

### 1.3 å›å½’å†³ç­–æ ‘

```python
from sklearn.tree import DecisionTreeRegressor
from sklearn.datasets import fetch_california_housing
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score
import numpy as np
import matplotlib.pyplot as plt

# åŠ è½½æ•°æ®
housing = fetch_california_housing()
X, y = housing.data, housing.target
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# è®­ç»ƒå›å½’æ ‘
model = DecisionTreeRegressor(max_depth=5, random_state=42)
model.fit(X_train, y_train)

# è¯„ä¼°
y_pred = model.predict(X_test)
print(f"RÂ² Score: {r2_score(y_test, y_pred):.4f}")
print(f"RMSE: {np.sqrt(mean_squared_error(y_test, y_pred)):.4f}")

# å¯è§†åŒ–é¢„æµ‹æ•ˆæœ
plt.figure(figsize=(8, 6))
plt.scatter(y_test, y_pred, alpha=0.5)
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)
plt.xlabel('True Values')
plt.ylabel('Predicted Values')
plt.title('Decision Tree Regressor')
plt.show()
```

### 1.4 ç‰¹å¾é‡è¦æ€§

```python
import pandas as pd
import matplotlib.pyplot as plt

# è·å–ç‰¹å¾é‡è¦æ€§
feature_importance = model.feature_importances_
feature_names = housing.feature_names

# æ’åº
importance_df = pd.DataFrame({
    'Feature': feature_names,
    'Importance': feature_importance
}).sort_values('Importance', ascending=True)

# å¯è§†åŒ–
plt.figure(figsize=(10, 6))
plt.barh(importance_df['Feature'], importance_df['Importance'])
plt.xlabel('Importance')
plt.title('Feature Importances in Decision Tree')
plt.tight_layout()
plt.show()
```

### 1.5 å†³ç­–æ ‘çš„è¶…å‚æ•°

```python
from sklearn.model_selection import GridSearchCV

# è¶…å‚æ•°ç½‘æ ¼
param_grid = {
    'max_depth': [3, 5, 7, 10, None],
    'min_samples_split': [2, 5, 10, 20],
    'min_samples_leaf': [1, 2, 5, 10],
    'max_features': ['sqrt', 'log2', None]
}

# ç½‘æ ¼æœç´¢
grid_search = GridSearchCV(
    DecisionTreeClassifier(random_state=42),
    param_grid,
    cv=5,
    scoring='accuracy',
    n_jobs=-1
)
grid_search.fit(X_train, y_train)

print(f"æœ€ä½³å‚æ•°: {grid_search.best_params_}")
print(f"æœ€ä½³ CV å¾—åˆ†: {grid_search.best_score_:.4f}")
```

### 1.6 å†³ç­–æ ‘çš„ä¼˜ç¼ºç‚¹

| ä¼˜ç‚¹ | ç¼ºç‚¹ |
|------|------|
| æ˜“äºç†è§£å’Œå¯è§†åŒ– | å®¹æ˜“è¿‡æ‹Ÿåˆ |
| ä¸éœ€è¦ç‰¹å¾ç¼©æ”¾ | å¯¹æ•°æ®å˜åŒ–æ•æ„Ÿ |
| å¯å¤„ç†æ•°å€¼å’Œç±»åˆ«ç‰¹å¾ | å†³ç­–è¾¹ç•Œæ˜¯è½´å¹³è¡Œçš„ |
| å¯æå–ç‰¹å¾é‡è¦æ€§ | ä¸ç¨³å®šï¼ˆæ•°æ®å°å˜åŒ–å¯èƒ½å¯¼è‡´å¤§å˜åŒ–ï¼‰ |

---

## 2. é›†æˆå­¦ä¹ æ¦‚è¿°

### 2.1 æ ¸å¿ƒæ€æƒ³

**"ä¸‰ä¸ªè‡­çš®åŒ ï¼Œé¡¶ä¸ªè¯¸è‘›äº®"**

```
å•ä¸ªå¼±å­¦ä¹ å™¨ï¼šå‡†ç¡®ç‡ 60%
å¤šä¸ªå¼±å­¦ä¹ å™¨é›†æˆï¼šå‡†ç¡®ç‡ 90%+
```

### 2.2 ä¸¤å¤§æµæ´¾

```
é›†æˆå­¦ä¹ 
â”œâ”€â”€ Baggingï¼ˆå¹¶è¡Œï¼‰
â”‚   â”œâ”€â”€ å¤šä¸ªæ¨¡å‹ç‹¬ç«‹è®­ç»ƒ
â”‚   â”œâ”€â”€ æŠ•ç¥¨/å¹³å‡é¢„æµ‹
â”‚   â””â”€â”€ ä»£è¡¨ï¼šRandom Forest
â”‚
â””â”€â”€ Boostingï¼ˆä¸²è¡Œï¼‰
    â”œâ”€â”€ æ¨¡å‹é¡ºåºè®­ç»ƒ
    â”œâ”€â”€ åé¢çš„æ¨¡å‹ä¿®æ­£å‰é¢çš„é”™è¯¯
    â””â”€â”€ ä»£è¡¨ï¼šAdaBoost, XGBoost, LightGBM
```

### 2.3 å›¾ç¤ºå¯¹æ¯”

```python
import matplotlib.pyplot as plt
import matplotlib.patches as mpatches

fig, axes = plt.subplots(1, 2, figsize=(14, 5))

# Bagging å›¾ç¤º
ax1 = axes[0]
ax1.set_xlim(0, 10)
ax1.set_ylim(0, 8)
ax1.set_title('Bagging (Parallel)', fontsize=14)

# æ•°æ®
ax1.add_patch(mpatches.Rectangle((4, 6.5), 2, 1, fill=True, color='lightblue'))
ax1.text(5, 7, 'Data', ha='center', va='center')

# é‡‡æ ·æ•°æ®
for i, x in enumerate([1, 3.5, 6.5]):
    ax1.add_patch(mpatches.Rectangle((x, 4.5), 1.5, 1, fill=True, color='lightgreen'))
    ax1.text(x+0.75, 5, f'Sample {i+1}', ha='center', va='center', fontsize=9)
    ax1.annotate('', xy=(x+0.75, 5.5), xytext=(5, 6.5),
                arrowprops=dict(arrowstyle='->', color='gray'))

# æ¨¡å‹
for i, x in enumerate([1, 3.5, 6.5]):
    ax1.add_patch(mpatches.Rectangle((x, 2.5), 1.5, 1, fill=True, color='lightyellow'))
    ax1.text(x+0.75, 3, f'Model {i+1}', ha='center', va='center', fontsize=9)
    ax1.annotate('', xy=(x+0.75, 3.5), xytext=(x+0.75, 4.5),
                arrowprops=dict(arrowstyle='->', color='gray'))

# ç»“æœ
ax1.add_patch(mpatches.Rectangle((4, 0.5), 2, 1, fill=True, color='lightcoral'))
ax1.text(5, 1, 'Vote/Average', ha='center', va='center')
for x in [1.75, 4.25, 7.25]:
    ax1.annotate('', xy=(5, 1.5), xytext=(x, 2.5),
                arrowprops=dict(arrowstyle='->', color='gray'))

ax1.axis('off')

# Boosting å›¾ç¤º
ax2 = axes[1]
ax2.set_xlim(0, 10)
ax2.set_ylim(0, 8)
ax2.set_title('Boosting (Sequential)', fontsize=14)

# æ•°æ®
ax2.add_patch(mpatches.Rectangle((0.5, 6.5), 2, 1, fill=True, color='lightblue'))
ax2.text(1.5, 7, 'Data', ha='center', va='center')

# é¡ºåºæ¨¡å‹
positions = [(1, 4.5), (4, 4.5), (7, 4.5)]
for i, (x, y) in enumerate(positions):
    ax2.add_patch(mpatches.Rectangle((x, y), 2, 1, fill=True, color='lightyellow'))
    ax2.text(x+1, y+0.5, f'Model {i+1}', ha='center', va='center')

    if i > 0:
        ax2.annotate('', xy=(x, y+0.5), xytext=(x-1, y+0.5),
                    arrowprops=dict(arrowstyle='->', color='red', lw=2))
        ax2.text(x-0.5, y+1.2, 'Learn\nErrors', ha='center', va='center', fontsize=8, color='red')

ax2.annotate('', xy=(1.5, 5.5), xytext=(1.5, 6.5),
            arrowprops=dict(arrowstyle='->', color='gray'))

# ç»“æœ
ax2.add_patch(mpatches.Rectangle((4, 1.5), 2, 1, fill=True, color='lightcoral'))
ax2.text(5, 2, 'Sum', ha='center', va='center')

for x in [2, 5, 8]:
    ax2.annotate('', xy=(5, 2.5), xytext=(x, 4.5),
                arrowprops=dict(arrowstyle='->', color='gray'))

ax2.axis('off')

plt.tight_layout()
plt.show()
```

---

## 3. Baggingï¼šéšæœºæ£®æ—

### 3.1 åŸºæœ¬åŸç†

**Random Forest = å¤šæ£µå†³ç­–æ ‘ + Bootstrap é‡‡æ · + ç‰¹å¾éšæœº**

```
éšæœºæ€§æ¥æºï¼š
1. æ ·æœ¬éšæœºï¼ˆBootstrapï¼‰ï¼šæ¯æ£µæ ‘ä½¿ç”¨æœ‰æ”¾å›æŠ½æ ·çš„æ•°æ®
2. ç‰¹å¾éšæœºï¼šæ¯æ¬¡åˆ†è£‚åªè€ƒè™‘éƒ¨åˆ†ç‰¹å¾
```

### 3.2 åˆ†ç±»ä»»åŠ¡

```python
from sklearn.ensemble import RandomForestClassifier
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, accuracy_score
import numpy as np

# ç”Ÿæˆæ•°æ®
X, y = make_classification(n_samples=1000, n_features=20, n_informative=10, random_state=42)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# è®­ç»ƒéšæœºæ£®æ—
model = RandomForestClassifier(
    n_estimators=100,      # æ ‘çš„æ•°é‡
    max_depth=10,          # æ¯æ£µæ ‘çš„æœ€å¤§æ·±åº¦
    min_samples_split=5,   # åˆ†è£‚æ‰€éœ€æœ€å°æ ·æœ¬
    min_samples_leaf=2,    # å¶èŠ‚ç‚¹æœ€å°æ ·æœ¬
    max_features='sqrt',   # æ¯æ¬¡åˆ†è£‚è€ƒè™‘çš„ç‰¹å¾æ•°
    bootstrap=True,        # æ˜¯å¦ bootstrap é‡‡æ ·
    oob_score=True,        # æ˜¯å¦è®¡ç®—è¢‹å¤–å¾—åˆ†
    n_jobs=-1,             # å¹¶è¡Œ
    random_state=42
)
model.fit(X_train, y_train)

# è¯„ä¼°
y_pred = model.predict(X_test)
print(f"æµ‹è¯•å‡†ç¡®ç‡: {accuracy_score(y_test, y_pred):.4f}")
print(f"OOB å¾—åˆ†: {model.oob_score_:.4f}")
print(f"\nåˆ†ç±»æŠ¥å‘Š:\n{classification_report(y_test, y_pred)}")
```

### 3.3 å›å½’ä»»åŠ¡

```python
from sklearn.ensemble import RandomForestRegressor
from sklearn.datasets import fetch_california_housing
from sklearn.metrics import mean_squared_error, r2_score
import numpy as np

# æ•°æ®
housing = fetch_california_housing()
X, y = housing.data, housing.target
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# è®­ç»ƒ
model = RandomForestRegressor(
    n_estimators=100,
    max_depth=15,
    min_samples_split=5,
    n_jobs=-1,
    random_state=42
)
model.fit(X_train, y_train)

# è¯„ä¼°
y_pred = model.predict(X_test)
print(f"RÂ² Score: {r2_score(y_test, y_pred):.4f}")
print(f"RMSE: {np.sqrt(mean_squared_error(y_test, y_pred)):.4f}")
```

### 3.4 ç‰¹å¾é‡è¦æ€§

```python
import pandas as pd
import matplotlib.pyplot as plt

# è·å–ç‰¹å¾é‡è¦æ€§
importances = model.feature_importances_
std = np.std([tree.feature_importances_ for tree in model.estimators_], axis=0)

# æ’åº
indices = np.argsort(importances)[::-1]

# å¯è§†åŒ–
plt.figure(figsize=(12, 6))
plt.bar(range(len(importances)), importances[indices], yerr=std[indices], align='center')
plt.xticks(range(len(importances)), [housing.feature_names[i] for i in indices], rotation=45)
plt.xlabel('Features')
plt.ylabel('Importance')
plt.title('Random Forest Feature Importances (with std)')
plt.tight_layout()
plt.show()

# æ‰“å°
print("\nç‰¹å¾é‡è¦æ€§æ’å:")
for i, idx in enumerate(indices):
    print(f"{i+1}. {housing.feature_names[idx]}: {importances[idx]:.4f}")
```

### 3.5 æ ‘çš„æ•°é‡å¯¹æ€§èƒ½çš„å½±å“

```python
import matplotlib.pyplot as plt
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import cross_val_score

n_estimators_range = [10, 50, 100, 200, 300, 500]
scores = []
times = []

import time

for n in n_estimators_range:
    start = time.time()
    model = RandomForestClassifier(n_estimators=n, max_depth=10, random_state=42, n_jobs=-1)
    cv_scores = cross_val_score(model, X, y, cv=5)
    scores.append(cv_scores.mean())
    times.append(time.time() - start)
    print(f"n_estimators={n}: CV Score = {cv_scores.mean():.4f}, Time = {times[-1]:.2f}s")

# å¯è§†åŒ–
fig, axes = plt.subplots(1, 2, figsize=(14, 5))

axes[0].plot(n_estimators_range, scores, 'bo-')
axes[0].set_xlabel('Number of Trees')
axes[0].set_ylabel('CV Accuracy')
axes[0].set_title('Accuracy vs Number of Trees')
axes[0].grid(True)

axes[1].plot(n_estimators_range, times, 'ro-')
axes[1].set_xlabel('Number of Trees')
axes[1].set_ylabel('Training Time (s)')
axes[1].set_title('Training Time vs Number of Trees')
axes[1].grid(True)

plt.tight_layout()
plt.show()
```

---

## 4. Boostingï¼šXGBoost/LightGBM

### 4.1 Boosting åŸç†

```
ç¬¬ 1 æ£µæ ‘ï¼šæ‹ŸåˆåŸå§‹æ•°æ®
ç¬¬ 2 æ£µæ ‘ï¼šæ‹Ÿåˆç¬¬ 1 æ£µæ ‘çš„æ®‹å·®ï¼ˆé”™è¯¯ï¼‰
ç¬¬ 3 æ£µæ ‘ï¼šæ‹Ÿåˆç¬¬ 1+2 æ£µæ ‘çš„æ®‹å·®
...
æœ€ç»ˆé¢„æµ‹ = æ‰€æœ‰æ ‘çš„é¢„æµ‹ä¹‹å’Œ
```

### 4.2 XGBoost

```python
# å®‰è£…ï¼špip install xgboost

from xgboost import XGBClassifier, XGBRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, mean_squared_error
import numpy as np

# ========== åˆ†ç±» ==========
from sklearn.datasets import make_classification
X, y = make_classification(n_samples=1000, n_features=20, random_state=42)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

xgb_clf = XGBClassifier(
    n_estimators=100,      # æ ‘çš„æ•°é‡
    max_depth=5,           # æœ€å¤§æ·±åº¦
    learning_rate=0.1,     # å­¦ä¹ ç‡
    subsample=0.8,         # æ ·æœ¬é‡‡æ ·æ¯”ä¾‹
    colsample_bytree=0.8,  # ç‰¹å¾é‡‡æ ·æ¯”ä¾‹
    reg_alpha=0.1,         # L1 æ­£åˆ™åŒ–
    reg_lambda=1,          # L2 æ­£åˆ™åŒ–
    random_state=42,
    n_jobs=-1,
    eval_metric='logloss'
)
xgb_clf.fit(X_train, y_train)

y_pred = xgb_clf.predict(X_test)
print(f"XGBoost åˆ†ç±»å‡†ç¡®ç‡: {accuracy_score(y_test, y_pred):.4f}")

# ========== å›å½’ ==========
from sklearn.datasets import fetch_california_housing
housing = fetch_california_housing()
X, y = housing.data, housing.target
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

xgb_reg = XGBRegressor(
    n_estimators=100,
    max_depth=5,
    learning_rate=0.1,
    random_state=42,
    n_jobs=-1
)
xgb_reg.fit(X_train, y_train)

y_pred = xgb_reg.predict(X_test)
print(f"XGBoost å›å½’ RMSE: {np.sqrt(mean_squared_error(y_test, y_pred)):.4f}")
```

### 4.3 LightGBM

```python
# å®‰è£…ï¼špip install lightgbm

from lightgbm import LGBMClassifier, LGBMRegressor

# ========== åˆ†ç±» ==========
lgb_clf = LGBMClassifier(
    n_estimators=100,
    max_depth=5,
    learning_rate=0.1,
    num_leaves=31,         # å¶å­èŠ‚ç‚¹æ•°ï¼ˆLightGBM ç‰¹æœ‰ï¼‰
    subsample=0.8,
    colsample_bytree=0.8,
    random_state=42,
    n_jobs=-1,
    verbose=-1
)

# ä½¿ç”¨åˆ†ç±»æ•°æ®
from sklearn.datasets import make_classification
X, y = make_classification(n_samples=1000, n_features=20, random_state=42)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

lgb_clf.fit(X_train, y_train)
y_pred = lgb_clf.predict(X_test)
print(f"LightGBM åˆ†ç±»å‡†ç¡®ç‡: {accuracy_score(y_test, y_pred):.4f}")

# ========== å›å½’ ==========
lgb_reg = LGBMRegressor(
    n_estimators=100,
    max_depth=5,
    learning_rate=0.1,
    num_leaves=31,
    random_state=42,
    verbose=-1
)

housing = fetch_california_housing()
X, y = housing.data, housing.target
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

lgb_reg.fit(X_train, y_train)
y_pred = lgb_reg.predict(X_test)
print(f"LightGBM å›å½’ RMSE: {np.sqrt(mean_squared_error(y_test, y_pred)):.4f}")
```

### 4.4 CatBoost

```python
# å®‰è£…ï¼špip install catboost

from catboost import CatBoostClassifier, CatBoostRegressor

# CatBoost çš„ç‰¹ç‚¹ï¼šè‡ªåŠ¨å¤„ç†ç±»åˆ«ç‰¹å¾

cat_clf = CatBoostClassifier(
    iterations=100,
    depth=5,
    learning_rate=0.1,
    random_seed=42,
    verbose=0
)

X, y = make_classification(n_samples=1000, n_features=20, random_state=42)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

cat_clf.fit(X_train, y_train)
y_pred = cat_clf.predict(X_test)
print(f"CatBoost åˆ†ç±»å‡†ç¡®ç‡: {accuracy_score(y_test, y_pred):.4f}")
```

### 4.5 æ—©åœï¼ˆEarly Stoppingï¼‰

```python
from xgboost import XGBClassifier
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split

X, y = make_classification(n_samples=5000, n_features=20, random_state=42)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# è¿›ä¸€æ­¥åˆ’åˆ†éªŒè¯é›†
X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)

model = XGBClassifier(
    n_estimators=1000,  # è®¾ç½®è¾ƒå¤§çš„å€¼
    max_depth=5,
    learning_rate=0.1,
    random_state=42,
    early_stopping_rounds=20  # 20 è½®æ— æ”¹å–„åˆ™åœæ­¢
)

# è®­ç»ƒæ—¶æŒ‡å®šéªŒè¯é›†
model.fit(
    X_train, y_train,
    eval_set=[(X_val, y_val)],
    verbose=10
)

print(f"\næœ€ç»ˆä½¿ç”¨çš„æ ‘æ•°é‡: {model.best_iteration}")
print(f"æµ‹è¯•å‡†ç¡®ç‡: {model.score(X_test, y_test):.4f}")
```

### 4.6 ç‰¹å¾é‡è¦æ€§å¯¹æ¯”

```python
import matplotlib.pyplot as plt
import numpy as np

# ä½¿ç”¨åŠ å·æˆ¿ä»·æ•°æ®
housing = fetch_california_housing()
X, y = housing.data, housing.target
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# è®­ç»ƒä¸‰ä¸ªæ¨¡å‹
from sklearn.ensemble import RandomForestRegressor
from xgboost import XGBRegressor
from lightgbm import LGBMRegressor

models = {
    'Random Forest': RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42),
    'XGBoost': XGBRegressor(n_estimators=100, max_depth=5, random_state=42),
    'LightGBM': LGBMRegressor(n_estimators=100, max_depth=5, random_state=42, verbose=-1)
}

fig, axes = plt.subplots(1, 3, figsize=(18, 5))

for ax, (name, model) in zip(axes, models.items()):
    model.fit(X_train, y_train)
    importances = model.feature_importances_
    indices = np.argsort(importances)[::-1]

    ax.bar(range(len(importances)), importances[indices])
    ax.set_xticks(range(len(importances)))
    ax.set_xticklabels([housing.feature_names[i] for i in indices], rotation=45)
    ax.set_title(f'{name} Feature Importances')
    ax.set_xlabel('Features')
    ax.set_ylabel('Importance')

plt.tight_layout()
plt.show()
```

---

## 5. æ¨¡å‹å¯¹æ¯”ä¸é€‰æ‹©

### 5.1 æ€§èƒ½å¯¹æ¯”

```python
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from xgboost import XGBClassifier
from lightgbm import LGBMClassifier
from sklearn.model_selection import cross_val_score
from sklearn.datasets import make_classification
import time

# ç”Ÿæˆæ•°æ®
X, y = make_classification(n_samples=5000, n_features=50, n_informative=20, random_state=42)

models = {
    'Random Forest': RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42, n_jobs=-1),
    'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, max_depth=5, random_state=42),
    'XGBoost': XGBClassifier(n_estimators=100, max_depth=5, random_state=42, n_jobs=-1),
    'LightGBM': LGBMClassifier(n_estimators=100, max_depth=5, random_state=42, n_jobs=-1, verbose=-1)
}

print(f"{'Model':<20} {'CV Score':<12} {'Time (s)':<10}")
print("-" * 45)

results = {}
for name, model in models.items():
    start = time.time()
    scores = cross_val_score(model, X, y, cv=5, scoring='accuracy')
    elapsed = time.time() - start
    results[name] = {'score': scores.mean(), 'std': scores.std(), 'time': elapsed}
    print(f"{name:<20} {scores.mean():.4f}Â±{scores.std():.4f} {elapsed:.2f}")
```

### 5.2 é€‰æ‹©æŒ‡å—

| åœºæ™¯ | æ¨èæ¨¡å‹ | åŸå›  |
|------|---------|------|
| å¿«é€ŸåŸå‹ | Random Forest | é»˜è®¤å‚æ•°å°±ä¸é”™ |
| ç«èµ›/è¿½æ±‚æœ€é«˜ç²¾åº¦ | XGBoost/LightGBM | ç²¾åº¦é«˜ï¼Œå¯è°ƒå‚æ•°å¤š |
| å¤§æ•°æ®é›† | LightGBM | é€Ÿåº¦å¿«ï¼Œå†…å­˜æ•ˆç‡é«˜ |
| æœ‰ç±»åˆ«ç‰¹å¾ | CatBoost | è‡ªåŠ¨å¤„ç†ç±»åˆ« |
| éœ€è¦å¯è§£é‡Šæ€§ | Decision Tree | å•æ£µæ ‘æ˜“è§£é‡Š |
| å¹¶è¡Œè®­ç»ƒ | Random Forest | å¤©ç„¶å¹¶è¡Œ |

### 5.3 è¶…å‚æ•°è°ƒä¼˜æ¨¡æ¿

```python
from sklearn.model_selection import RandomizedSearchCV
from xgboost import XGBClassifier
import numpy as np

# å‚æ•°ç©ºé—´
param_dist = {
    'n_estimators': [100, 200, 300],
    'max_depth': [3, 5, 7, 10],
    'learning_rate': [0.01, 0.05, 0.1, 0.2],
    'subsample': [0.6, 0.8, 1.0],
    'colsample_bytree': [0.6, 0.8, 1.0],
    'reg_alpha': [0, 0.1, 1],
    'reg_lambda': [0, 0.1, 1]
}

model = XGBClassifier(random_state=42, n_jobs=-1)

random_search = RandomizedSearchCV(
    model, param_dist,
    n_iter=50,
    cv=3,
    scoring='accuracy',
    random_state=42,
    n_jobs=-1,
    verbose=1
)

# random_search.fit(X_train, y_train)
# print(f"æœ€ä½³å‚æ•°: {random_search.best_params_}")
# print(f"æœ€ä½³å¾—åˆ†: {random_search.best_score_:.4f}")
```

---

## 6. ç»ƒä¹ é¢˜

### åŸºç¡€ç»ƒä¹ 

1. è®­ç»ƒä¸€ä¸ªå†³ç­–æ ‘åˆ†ç±»å™¨ï¼Œå¯è§†åŒ–æ ‘ç»“æ„
2. è®­ç»ƒéšæœºæ£®æ—ï¼Œæ¯”è¾ƒä¸åŒ n_estimators çš„æ•ˆæœ
3. ä½¿ç”¨ XGBoost åšå›å½’ï¼Œå¹¶æå–ç‰¹å¾é‡è¦æ€§

### è¿›é˜¶ç»ƒä¹ 

4. æ¯”è¾ƒ Random Forestã€XGBoostã€LightGBM åœ¨åŒä¸€æ•°æ®é›†ä¸Šçš„è¡¨ç°
5. ä½¿ç”¨æ—©åœè®­ç»ƒ XGBoostï¼Œæ‰¾åˆ°æœ€ä¼˜çš„æ ‘æ•°é‡

### å‚è€ƒç­”æ¡ˆ

<details>
<summary>ç»ƒä¹  4 å‚è€ƒç­”æ¡ˆ</summary>

```python
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier
from lightgbm import LGBMClassifier
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split, cross_val_score
import time

X, y = make_classification(n_samples=5000, n_features=30, random_state=42)

models = {
    'RandomForest': RandomForestClassifier(n_estimators=100, random_state=42),
    'XGBoost': XGBClassifier(n_estimators=100, random_state=42, n_jobs=-1),
    'LightGBM': LGBMClassifier(n_estimators=100, random_state=42, verbose=-1)
}

for name, model in models.items():
    start = time.time()
    scores = cross_val_score(model, X, y, cv=5)
    elapsed = time.time() - start
    print(f"{name}: {scores.mean():.4f} Â± {scores.std():.4f} ({elapsed:.2f}s)")
```

</details>

---

## â¡ï¸ ä¸‹ä¸€æ­¥

å­¦å®Œæœ¬èŠ‚åï¼Œç»§ç»­å­¦ä¹  [07-æ— ç›‘ç£å­¦ä¹ .md](./07-æ— ç›‘ç£å­¦ä¹ .md)

