# ğŸ  é¡¹ç›®ï¼šæˆ¿ä»·é¢„æµ‹ï¼ˆå›å½’ï¼‰

> å®Œæ•´çš„å›å½’é¡¹ç›®å®æˆ˜ï¼Œä»æ•°æ®æ¢ç´¢åˆ°æ¨¡å‹éƒ¨ç½²

---

## é¡¹ç›®æ¦‚è¿°

| é¡¹ç›® | è¯´æ˜ |
|------|------|
| **ç›®æ ‡** | é¢„æµ‹æˆ¿ä»·ï¼ˆè¿ç»­å€¼ï¼‰ |
| **ç±»å‹** | ç›‘ç£å­¦ä¹  - å›å½’ |
| **æ•°æ®** | California Housing |
| **è¯„ä¼°** | RMSEã€MAEã€RÂ² |

---

## å®Œæ•´ä»£ç 

```python
# ============================================================
# æˆ¿ä»·é¢„æµ‹é¡¹ç›® - å®Œæ•´ä»£ç 
# ============================================================

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.datasets import fetch_california_housing
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import warnings
warnings.filterwarnings('ignore')

# è®¾ç½®æ˜¾ç¤º
pd.set_option('display.max_columns', None)
plt.rcParams['figure.figsize'] = (10, 6)

print("=" * 60)
print("1. æ•°æ®åŠ è½½ä¸æ¢ç´¢")
print("=" * 60)

# åŠ è½½æ•°æ®
housing = fetch_california_housing()
df = pd.DataFrame(housing.data, columns=housing.feature_names)
df['MedHouseVal'] = housing.target

print(f"æ•°æ®å½¢çŠ¶: {df.shape}")
print(f"\nç‰¹å¾è¯´æ˜:")
print(housing.DESCR[:1000])

print(f"\nå‰ 5 è¡Œæ•°æ®:")
print(df.head())

print(f"\næ•°æ®ç»Ÿè®¡:")
print(df.describe())

print(f"\næ•°æ®ç±»å‹:")
print(df.dtypes)

print(f"\nç¼ºå¤±å€¼:")
print(df.isnull().sum())

# ============================================================
print("\n" + "=" * 60)
print("2. æ¢ç´¢æ€§æ•°æ®åˆ†æ (EDA)")
print("=" * 60)

# 2.1 ç›®æ ‡å˜é‡åˆ†å¸ƒ
fig, axes = plt.subplots(1, 2, figsize=(14, 5))

axes[0].hist(df['MedHouseVal'], bins=50, edgecolor='black', alpha=0.7)
axes[0].set_xlabel('Median House Value ($100k)')
axes[0].set_ylabel('Frequency')
axes[0].set_title('Target Distribution')
axes[0].axvline(df['MedHouseVal'].mean(), color='red', linestyle='--', label=f"Mean: {df['MedHouseVal'].mean():.2f}")
axes[0].legend()

# 2.2 ç›¸å…³æ€§çƒ­åŠ›å›¾
corr = df.corr()
mask = np.triu(np.ones_like(corr, dtype=bool))
sns.heatmap(corr, mask=mask, annot=True, fmt='.2f', cmap='coolwarm',
            center=0, ax=axes[1], square=True)
axes[1].set_title('Correlation Matrix')

plt.tight_layout()
plt.savefig('eda_overview.png', dpi=150)
plt.show()

# 2.3 ç‰¹å¾ä¸ç›®æ ‡çš„å…³ç³»
fig, axes = plt.subplots(2, 4, figsize=(16, 8))
axes = axes.flatten()

for i, col in enumerate(housing.feature_names):
    axes[i].scatter(df[col], df['MedHouseVal'], alpha=0.3, s=5)
    axes[i].set_xlabel(col)
    axes[i].set_ylabel('MedHouseVal')
    axes[i].set_title(f'{col} vs Target (r={corr.loc[col, "MedHouseVal"]:.2f})')

plt.tight_layout()
plt.savefig('feature_target_relations.png', dpi=150)
plt.show()

# æ‰“å°ç›¸å…³æ€§æ’å
print("\nç‰¹å¾ä¸æˆ¿ä»·çš„ç›¸å…³æ€§æ’å:")
target_corr = corr['MedHouseVal'].drop('MedHouseVal').sort_values(key=abs, ascending=False)
for feat, corr_val in target_corr.items():
    print(f"  {feat}: {corr_val:.4f}")

# ============================================================
print("\n" + "=" * 60)
print("3. æ•°æ®é¢„å¤„ç†")
print("=" * 60)

# åˆ†ç¦»ç‰¹å¾å’Œæ ‡ç­¾
X = df.drop('MedHouseVal', axis=1)
y = df['MedHouseVal']

# åˆ’åˆ†æ•°æ®
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

print(f"è®­ç»ƒé›†å¤§å°: {X_train.shape[0]}")
print(f"æµ‹è¯•é›†å¤§å°: {X_test.shape[0]}")

# ç‰¹å¾ç¼©æ”¾
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

print(f"\nç‰¹å¾ç¼©æ”¾å:")
print(f"  è®­ç»ƒé›†å‡å€¼: {X_train_scaled.mean(axis=0).round(2)}")
print(f"  è®­ç»ƒé›†æ ‡å‡†å·®: {X_train_scaled.std(axis=0).round(2)}")

# ============================================================
print("\n" + "=" * 60)
print("4. æ¨¡å‹è®­ç»ƒä¸è¯„ä¼°")
print("=" * 60)

def evaluate_model(model, X_train, X_test, y_train, y_test, name):
    """è®­ç»ƒå¹¶è¯„ä¼°æ¨¡å‹"""
    # è®­ç»ƒ
    model.fit(X_train, y_train)

    # é¢„æµ‹
    y_train_pred = model.predict(X_train)
    y_test_pred = model.predict(X_test)

    # è®¡ç®—æŒ‡æ ‡
    train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))
    test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))
    test_mae = mean_absolute_error(y_test, y_test_pred)
    test_r2 = r2_score(y_test, y_test_pred)

    print(f"\n{name}:")
    print(f"  Train RMSE: {train_rmse:.4f}")
    print(f"  Test RMSE:  {test_rmse:.4f}")
    print(f"  Test MAE:   {test_mae:.4f}")
    print(f"  Test RÂ²:    {test_r2:.4f}")

    return {
        'name': name,
        'model': model,
        'train_rmse': train_rmse,
        'test_rmse': test_rmse,
        'test_mae': test_mae,
        'test_r2': test_r2,
        'y_test_pred': y_test_pred
    }

# è®­ç»ƒå¤šä¸ªæ¨¡å‹
models = {
    'Linear Regression': LinearRegression(),
    'Ridge (Î±=1)': Ridge(alpha=1),
    'Lasso (Î±=0.01)': Lasso(alpha=0.01),
    'Random Forest': RandomForestRegressor(n_estimators=100, max_depth=15, random_state=42, n_jobs=-1),
    'Gradient Boosting': GradientBoostingRegressor(n_estimators=100, max_depth=5, random_state=42)
}

results = []
for name, model in models.items():
    result = evaluate_model(model, X_train_scaled, X_test_scaled, y_train, y_test, name)
    results.append(result)

# ============================================================
print("\n" + "=" * 60)
print("5. æ¨¡å‹å¯¹æ¯”å¯è§†åŒ–")
print("=" * 60)

# 5.1 æ€§èƒ½å¯¹æ¯”
results_df = pd.DataFrame([{
    'Model': r['name'],
    'Test RMSE': r['test_rmse'],
    'Test MAE': r['test_mae'],
    'Test RÂ²': r['test_r2']
} for r in results])

fig, axes = plt.subplots(1, 3, figsize=(15, 5))

# RMSE
axes[0].barh(results_df['Model'], results_df['Test RMSE'], color='steelblue')
axes[0].set_xlabel('Test RMSE')
axes[0].set_title('Model Comparison - RMSE')
axes[0].invert_yaxis()

# MAE
axes[1].barh(results_df['Model'], results_df['Test MAE'], color='green')
axes[1].set_xlabel('Test MAE')
axes[1].set_title('Model Comparison - MAE')
axes[1].invert_yaxis()

# RÂ²
axes[2].barh(results_df['Model'], results_df['Test RÂ²'], color='orange')
axes[2].set_xlabel('Test RÂ²')
axes[2].set_title('Model Comparison - RÂ²')
axes[2].invert_yaxis()

plt.tight_layout()
plt.savefig('model_comparison.png', dpi=150)
plt.show()

print("\næ¨¡å‹å¯¹æ¯”ç»“æœ:")
print(results_df.to_string(index=False))

# ============================================================
print("\n" + "=" * 60)
print("6. æœ€ä½³æ¨¡å‹åˆ†æ")
print("=" * 60)

# æ‰¾æœ€ä½³æ¨¡å‹
best_result = min(results, key=lambda x: x['test_rmse'])
print(f"æœ€ä½³æ¨¡å‹: {best_result['name']}")
print(f"  Test RMSE: {best_result['test_rmse']:.4f}")
print(f"  Test RÂ²: {best_result['test_r2']:.4f}")

# 6.1 é¢„æµ‹ vs çœŸå®
fig, axes = plt.subplots(1, 2, figsize=(14, 5))

# æ•£ç‚¹å›¾
axes[0].scatter(y_test, best_result['y_test_pred'], alpha=0.5, s=10)
axes[0].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)
axes[0].set_xlabel('True Values')
axes[0].set_ylabel('Predicted Values')
axes[0].set_title(f'{best_result["name"]}: Predicted vs True')

# æ®‹å·®åˆ†å¸ƒ
residuals = y_test - best_result['y_test_pred']
axes[1].hist(residuals, bins=50, edgecolor='black', alpha=0.7)
axes[1].axvline(0, color='red', linestyle='--')
axes[1].set_xlabel('Residuals')
axes[1].set_ylabel('Frequency')
axes[1].set_title(f'Residual Distribution (Mean: {residuals.mean():.4f})')

plt.tight_layout()
plt.savefig('best_model_analysis.png', dpi=150)
plt.show()

# 6.2 ç‰¹å¾é‡è¦æ€§ï¼ˆå¦‚æœæ˜¯æ ‘æ¨¡å‹ï¼‰
if hasattr(best_result['model'], 'feature_importances_'):
    importance = best_result['model'].feature_importances_
    indices = np.argsort(importance)[::-1]

    plt.figure(figsize=(10, 6))
    plt.bar(range(len(importance)), importance[indices])
    plt.xticks(range(len(importance)), [housing.feature_names[i] for i in indices], rotation=45)
    plt.xlabel('Features')
    plt.ylabel('Importance')
    plt.title(f'{best_result["name"]} - Feature Importances')
    plt.tight_layout()
    plt.savefig('feature_importance.png', dpi=150)
    plt.show()

    print("\nç‰¹å¾é‡è¦æ€§æ’å:")
    for i, idx in enumerate(indices):
        print(f"  {i+1}. {housing.feature_names[idx]}: {importance[idx]:.4f}")

# ============================================================
print("\n" + "=" * 60)
print("7. è¶…å‚æ•°è°ƒä¼˜")
print("=" * 60)

# å¯¹ Gradient Boosting è¿›è¡Œè°ƒä¼˜
print("å¯¹ Gradient Boosting è¿›è¡Œç½‘æ ¼æœç´¢è°ƒä¼˜...")

param_grid = {
    'n_estimators': [100, 200],
    'max_depth': [3, 5, 7],
    'learning_rate': [0.05, 0.1, 0.2],
    'subsample': [0.8, 1.0]
}

gb_model = GradientBoostingRegressor(random_state=42)
grid_search = GridSearchCV(
    gb_model, param_grid,
    cv=3,
    scoring='neg_root_mean_squared_error',
    n_jobs=-1,
    verbose=1
)
grid_search.fit(X_train_scaled, y_train)

print(f"\næœ€ä½³å‚æ•°: {grid_search.best_params_}")
print(f"æœ€ä½³ CV RMSE: {-grid_search.best_score_:.4f}")

# ç”¨æœ€ä½³æ¨¡å‹åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°
best_gb = grid_search.best_estimator_
y_pred_tuned = best_gb.predict(X_test_scaled)
tuned_rmse = np.sqrt(mean_squared_error(y_test, y_pred_tuned))
tuned_r2 = r2_score(y_test, y_pred_tuned)

print(f"\nè°ƒä¼˜åæµ‹è¯•é›†æ€§èƒ½:")
print(f"  Test RMSE: {tuned_rmse:.4f}")
print(f"  Test RÂ²: {tuned_r2:.4f}")

# ============================================================
print("\n" + "=" * 60)
print("8. æ€»ç»“")
print("=" * 60)

print("""
é¡¹ç›®æ€»ç»“ï¼š
1. æ•°æ®åŒ…å« 8 ä¸ªç‰¹å¾ï¼Œ20640 ä¸ªæ ·æœ¬
2. ç›®æ ‡å˜é‡ï¼ˆæˆ¿ä»·ä¸­ä½æ•°ï¼‰å‘ˆå³ååˆ†å¸ƒï¼Œæœ€é«˜å€¼è¢«æˆªæ–­åœ¨ 5.0
3. æœ€é‡è¦çš„ç‰¹å¾æ˜¯ MedIncï¼ˆæ”¶å…¥ä¸­ä½æ•°ï¼‰å’Œåœ°ç†ä½ç½®
4. Gradient Boosting å’Œ Random Forest è¡¨ç°æœ€å¥½
5. é€šè¿‡è¶…å‚æ•°è°ƒä¼˜å¯ä»¥è¿›ä¸€æ­¥æå‡æ€§èƒ½

æ”¹è¿›æ–¹å‘ï¼š
- ç‰¹å¾å·¥ç¨‹ï¼šæ„é€ äº¤äº’ç‰¹å¾ã€åˆ†ç®±ç­‰
- å°è¯• XGBoost/LightGBM
- å¼‚å¸¸å€¼å¤„ç†
- ç›®æ ‡å˜é‡å˜æ¢ï¼ˆå¦‚å¯¹æ•°å˜æ¢ï¼‰
""")

print("\né¡¹ç›®å®Œæˆï¼")
```

---

## æ‰©å±•ä»»åŠ¡

- [ ] ä½¿ç”¨ XGBoost å’Œ LightGBMï¼Œå¯¹æ¯”æ€§èƒ½
- [ ] æ·»åŠ ç‰¹å¾äº¤äº’ï¼ˆå¦‚ `Rooms * Bedrooms`ï¼‰
- [ ] å¯¹ç›®æ ‡å˜é‡åšå¯¹æ•°å˜æ¢ï¼Œè§‚å¯Ÿæ•ˆæœ
- [ ] ä½¿ç”¨ Optuna è¿›è¡Œæ›´æ™ºèƒ½çš„è¶…å‚æ•°è°ƒä¼˜
- [ ] åˆ†æé¢„æµ‹é”™è¯¯è¾ƒå¤§çš„æ ·æœ¬ï¼Œæ‰¾å‡ºåŸå› 

