# ğŸ“Š 01 - æœºå™¨å­¦ä¹ æµæ°´çº¿

> æœ¬æ–‡è¯¦ç»†ä»‹ç»æœºå™¨å­¦ä¹ çš„å®Œæ•´æµç¨‹ï¼šä»åŸå§‹æ•°æ®åˆ°å¯ç”¨æ¨¡å‹

---

## ç›®å½•

1. [æ•´ä½“æµç¨‹æ¦‚è§ˆ](#1-æ•´ä½“æµç¨‹æ¦‚è§ˆ)
2. [æ•°æ®åŠ è½½ä¸æ¢ç´¢](#2-æ•°æ®åŠ è½½ä¸æ¢ç´¢)
3. [æ•°æ®æ¸…æ´—](#3-æ•°æ®æ¸…æ´—)
4. [ç‰¹å¾å·¥ç¨‹](#4-ç‰¹å¾å·¥ç¨‹)
5. [æ•°æ®åˆ’åˆ†](#5-æ•°æ®åˆ’åˆ†)
6. [å®Œæ•´ç¤ºä¾‹](#6-å®Œæ•´ç¤ºä¾‹)
7. [ç»ƒä¹ é¢˜](#7-ç»ƒä¹ é¢˜)

---

## 1. æ•´ä½“æµç¨‹æ¦‚è§ˆ

### æœºå™¨å­¦ä¹ æµæ°´çº¿

```
åŸå§‹æ•°æ®
    â†“
â‘  æ•°æ®åŠ è½½ä¸æ¢ç´¢ï¼ˆEDAï¼‰
    â†“
â‘¡ æ•°æ®æ¸…æ´—
   - å¤„ç†ç¼ºå¤±å€¼
   - å¤„ç†å¼‚å¸¸å€¼
   - å¤„ç†é‡å¤å€¼
    â†“
â‘¢ ç‰¹å¾å·¥ç¨‹
   - ç‰¹å¾ç¼–ç ï¼ˆç±»åˆ«â†’æ•°å€¼ï¼‰
   - ç‰¹å¾ç¼©æ”¾ï¼ˆæ ‡å‡†åŒ–/å½’ä¸€åŒ–ï¼‰
   - ç‰¹å¾é€‰æ‹©/æ„é€ 
    â†“
â‘£ æ•°æ®åˆ’åˆ†ï¼ˆTrain/Val/Testï¼‰
    â†“
â‘¤ æ¨¡å‹è®­ç»ƒ
    â†“
â‘¥ æ¨¡å‹è¯„ä¼°
    â†“
â‘¦ è°ƒå‚ä¼˜åŒ–
    â†“
æœ€ç»ˆæ¨¡å‹
```

### ä¸ºä»€ä¹ˆé¡ºåºå¾ˆé‡è¦ï¼Ÿ

```
âŒ é”™è¯¯åšæ³•ï¼šå…ˆå¯¹å…¨éƒ¨æ•°æ®åšæ ‡å‡†åŒ–ï¼Œå†åˆ’åˆ†
   é—®é¢˜ï¼šæµ‹è¯•é›†ä¿¡æ¯æ³„éœ²åˆ°è®­ç»ƒè¿‡ç¨‹

âœ… æ­£ç¡®åšæ³•ï¼šå…ˆåˆ’åˆ†ï¼Œå†ç”¨è®­ç»ƒé›† fit æ ‡å‡†åŒ–å™¨ï¼Œå¯¹æµ‹è¯•é›†åª transform
   è¿™æ ·æ‰èƒ½æ¨¡æ‹ŸçœŸå®åœºæ™¯ï¼šæ¨¡å‹åœ¨æœªè§è¿‡çš„æ•°æ®ä¸Šè¡¨ç°
```

---

## 2. æ•°æ®åŠ è½½ä¸æ¢ç´¢

### 2.1 åŠ è½½æ•°æ®

```python
import pandas as pd
import numpy as np

# ä» CSV åŠ è½½
df = pd.read_csv('data.csv')

# ä» Excel åŠ è½½
df = pd.read_excel('data.xlsx', sheet_name='Sheet1')

# ä» sklearn åŠ è½½å†…ç½®æ•°æ®é›†
from sklearn.datasets import load_iris, fetch_california_housing

iris = load_iris()
X, y = iris.data, iris.target

# è½¬ä¸º DataFrame æ›´æ–¹ä¾¿æ“ä½œ
df = pd.DataFrame(iris.data, columns=iris.feature_names)
df['target'] = iris.target
```

### 2.2 æ¢ç´¢æ€§æ•°æ®åˆ†æï¼ˆEDAï¼‰

#### åŸºæœ¬ä¿¡æ¯

```python
import pandas as pd
import numpy as np

# åˆ›å»ºç¤ºä¾‹æ•°æ®
np.random.seed(42)
df = pd.DataFrame({
    'age': np.random.randint(18, 70, 100),
    'income': np.random.randint(30000, 150000, 100),
    'city': np.random.choice(['Beijing', 'Shanghai', 'Shenzhen'], 100),
    'purchased': np.random.choice([0, 1], 100)
})
df.loc[5, 'age'] = np.nan  # æ·»åŠ ä¸€äº›ç¼ºå¤±å€¼
df.loc[10, 'income'] = np.nan

# ========== åŸºæœ¬ä¿¡æ¯ ==========
print("=" * 50)
print("æ•°æ®å½¢çŠ¶:", df.shape)
print("=" * 50)

print("\næ•°æ®ç±»å‹:")
print(df.dtypes)

print("\nå‰ 5 è¡Œ:")
print(df.head())

print("\nåŸºæœ¬ç»Ÿè®¡:")
print(df.describe())

print("\nç¼ºå¤±å€¼ç»Ÿè®¡:")
print(df.isnull().sum())

print("\nå”¯ä¸€å€¼æ•°é‡:")
print(df.nunique())
```

#### æ•°æ®åˆ†å¸ƒå¯è§†åŒ–

```python
import matplotlib.pyplot as plt
import seaborn as sns

fig, axes = plt.subplots(2, 2, figsize=(12, 10))

# 1. æ•°å€¼ç‰¹å¾åˆ†å¸ƒ
axes[0, 0].hist(df['age'].dropna(), bins=20, edgecolor='black')
axes[0, 0].set_title('Age Distribution')
axes[0, 0].set_xlabel('Age')

# 2. æ”¶å…¥åˆ†å¸ƒ
axes[0, 1].hist(df['income'].dropna(), bins=20, edgecolor='black', color='green')
axes[0, 1].set_title('Income Distribution')
axes[0, 1].set_xlabel('Income')

# 3. ç±»åˆ«ç‰¹å¾åˆ†å¸ƒ
df['city'].value_counts().plot(kind='bar', ax=axes[1, 0], color='steelblue')
axes[1, 0].set_title('City Distribution')
axes[1, 0].set_xticklabels(axes[1, 0].get_xticklabels(), rotation=0)

# 4. ç›®æ ‡å˜é‡åˆ†å¸ƒ
df['purchased'].value_counts().plot(kind='bar', ax=axes[1, 1], color=['salmon', 'lightgreen'])
axes[1, 1].set_title('Target Distribution')
axes[1, 1].set_xticklabels(['Not Purchased', 'Purchased'], rotation=0)

plt.tight_layout()
plt.show()
```

#### ç›¸å…³æ€§åˆ†æ

```python
import seaborn as sns
import matplotlib.pyplot as plt

# åªé€‰æ‹©æ•°å€¼åˆ—
numeric_cols = df.select_dtypes(include=[np.number]).columns
corr_matrix = df[numeric_cols].corr()

plt.figure(figsize=(8, 6))
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0,
            fmt='.2f', square=True, linewidths=0.5)
plt.title('Correlation Matrix')
plt.tight_layout()
plt.show()
```

---

## 3. æ•°æ®æ¸…æ´—

### 3.1 å¤„ç†ç¼ºå¤±å€¼

#### æ£€æµ‹ç¼ºå¤±å€¼

```python
import pandas as pd
import numpy as np

# åˆ›å»ºå¸¦ç¼ºå¤±å€¼çš„æ•°æ®
df = pd.DataFrame({
    'A': [1, 2, np.nan, 4, 5],
    'B': [np.nan, 2, 3, np.nan, 5],
    'C': ['x', 'y', np.nan, 'w', 'v'],
    'D': [1.0, 2.0, 3.0, 4.0, 5.0]
})

print("åŸå§‹æ•°æ®:")
print(df)

# æ£€æµ‹ç¼ºå¤±å€¼
print("\næ¯åˆ—ç¼ºå¤±å€¼æ•°é‡:")
print(df.isnull().sum())

print("\nç¼ºå¤±å€¼æ¯”ä¾‹:")
print((df.isnull().sum() / len(df) * 100).round(2))

# å¯è§†åŒ–ç¼ºå¤±å€¼
import seaborn as sns
import matplotlib.pyplot as plt

plt.figure(figsize=(8, 4))
sns.heatmap(df.isnull(), cbar=True, yticklabels=False, cmap='viridis')
plt.title('Missing Values Heatmap')
plt.show()
```

#### å¤„ç†ç­–ç•¥

```python
import pandas as pd
import numpy as np

df = pd.DataFrame({
    'age': [25, 30, np.nan, 35, 28, np.nan, 40],
    'salary': [50000, 60000, 55000, np.nan, 52000, 58000, np.nan],
    'city': ['Beijing', 'Shanghai', np.nan, 'Beijing', None, 'Shenzhen', 'Beijing']
})

print("åŸå§‹æ•°æ®:")
print(df)

# ========== æ–¹æ³• 1ï¼šåˆ é™¤ ==========
# åˆ é™¤åŒ…å«ç¼ºå¤±å€¼çš„è¡Œ
df_drop_rows = df.dropna()
print(f"\nåˆ é™¤ç¼ºå¤±è¡Œå: {len(df)} â†’ {len(df_drop_rows)}")

# åˆ é™¤ç¼ºå¤±å€¼è¶…è¿‡é˜ˆå€¼çš„åˆ—
threshold = 0.5  # ç¼ºå¤±è¶…è¿‡ 50% çš„åˆ—
df_drop_cols = df.dropna(axis=1, thresh=int(len(df) * (1 - threshold)))

# ========== æ–¹æ³• 2ï¼šå¡«å…… ==========
df_filled = df.copy()

# æ•°å€¼åˆ—ï¼šç”¨å‡å€¼/ä¸­ä½æ•°/ä¼—æ•°å¡«å……
df_filled['age'] = df_filled['age'].fillna(df_filled['age'].median())
df_filled['salary'] = df_filled['salary'].fillna(df_filled['salary'].mean())

# ç±»åˆ«åˆ—ï¼šç”¨ä¼—æ•°æˆ–å›ºå®šå€¼å¡«å……
df_filled['city'] = df_filled['city'].fillna(df_filled['city'].mode()[0])
# æˆ–è€…ç”¨ 'Unknown'
# df_filled['city'] = df_filled['city'].fillna('Unknown')

print("\nå¡«å……åæ•°æ®:")
print(df_filled)

# ========== æ–¹æ³• 3ï¼šæ’å€¼ ==========
df_interpolate = df.copy()
df_interpolate['age'] = df_interpolate['age'].interpolate(method='linear')
df_interpolate['salary'] = df_interpolate['salary'].interpolate(method='linear')

print("\næ’å€¼åæ•°æ®:")
print(df_interpolate)
```

#### ä½¿ç”¨ sklearn å¤„ç†ç¼ºå¤±å€¼

```python
from sklearn.impute import SimpleImputer
import numpy as np

# æ•°å€¼æ•°æ®
X = np.array([[1, 2], [np.nan, 3], [7, 6], [4, np.nan]])

# å‡å€¼å¡«å……
imputer_mean = SimpleImputer(strategy='mean')
X_mean = imputer_mean.fit_transform(X)
print("å‡å€¼å¡«å……:")
print(X_mean)

# ä¸­ä½æ•°å¡«å……
imputer_median = SimpleImputer(strategy='median')
X_median = imputer_median.fit_transform(X)
print("\nä¸­ä½æ•°å¡«å……:")
print(X_median)

# ä¼—æ•°å¡«å……ï¼ˆé€‚ç”¨äºç±»åˆ«æ•°æ®ï¼‰
imputer_mode = SimpleImputer(strategy='most_frequent')
X_mode = imputer_mode.fit_transform(X)
print("\nä¼—æ•°å¡«å……:")
print(X_mode)

# å¸¸æ•°å¡«å……
imputer_const = SimpleImputer(strategy='constant', fill_value=-1)
X_const = imputer_const.fit_transform(X)
print("\nå¸¸æ•°å¡«å……:")
print(X_const)
```

### 3.2 å¤„ç†å¼‚å¸¸å€¼

#### æ£€æµ‹å¼‚å¸¸å€¼

```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# åˆ›å»ºå¸¦å¼‚å¸¸å€¼çš„æ•°æ®
np.random.seed(42)
data = np.random.normal(50, 10, 100)
data = np.append(data, [150, 200, -50])  # æ·»åŠ å¼‚å¸¸å€¼

df = pd.DataFrame({'value': data})

# ========== æ–¹æ³• 1ï¼šç»Ÿè®¡æ–¹æ³• ==========
mean = df['value'].mean()
std = df['value'].std()

# 3-sigma åŸåˆ™ï¼šè¶…è¿‡ 3 ä¸ªæ ‡å‡†å·®è§†ä¸ºå¼‚å¸¸
lower = mean - 3 * std
upper = mean + 3 * std
outliers_3sigma = df[(df['value'] < lower) | (df['value'] > upper)]
print(f"3-sigma å¼‚å¸¸å€¼æ•°é‡: {len(outliers_3sigma)}")

# ========== æ–¹æ³• 2ï¼šIQR æ–¹æ³• ==========
Q1 = df['value'].quantile(0.25)
Q3 = df['value'].quantile(0.75)
IQR = Q3 - Q1

lower_iqr = Q1 - 1.5 * IQR
upper_iqr = Q3 + 1.5 * IQR
outliers_iqr = df[(df['value'] < lower_iqr) | (df['value'] > upper_iqr)]
print(f"IQR å¼‚å¸¸å€¼æ•°é‡: {len(outliers_iqr)}")

# ========== å¯è§†åŒ– ==========
fig, axes = plt.subplots(1, 2, figsize=(12, 5))

# ç®±çº¿å›¾
axes[0].boxplot(df['value'])
axes[0].set_title('Box Plot')
axes[0].set_ylabel('Value')

# ç›´æ–¹å›¾ + è¾¹ç•Œ
axes[1].hist(df['value'], bins=30, edgecolor='black', alpha=0.7)
axes[1].axvline(lower_iqr, color='red', linestyle='--', label=f'Lower: {lower_iqr:.1f}')
axes[1].axvline(upper_iqr, color='red', linestyle='--', label=f'Upper: {upper_iqr:.1f}')
axes[1].set_title('Histogram with IQR Bounds')
axes[1].legend()

plt.tight_layout()
plt.show()
```

#### å¤„ç†å¼‚å¸¸å€¼

```python
import numpy as np
import pandas as pd

np.random.seed(42)
data = np.random.normal(50, 10, 100)
data = np.append(data, [150, 200, -50])
df = pd.DataFrame({'value': data})

Q1 = df['value'].quantile(0.25)
Q3 = df['value'].quantile(0.75)
IQR = Q3 - Q1
lower = Q1 - 1.5 * IQR
upper = Q3 + 1.5 * IQR

# ========== æ–¹æ³• 1ï¼šåˆ é™¤ ==========
df_dropped = df[(df['value'] >= lower) & (df['value'] <= upper)]
print(f"åˆ é™¤å: {len(df)} â†’ {len(df_dropped)}")

# ========== æ–¹æ³• 2ï¼šæˆªæ–­ï¼ˆClippingï¼‰ ==========
df_clipped = df.copy()
df_clipped['value'] = df_clipped['value'].clip(lower, upper)
print(f"\næˆªæ–­å:")
print(f"  åŸå§‹èŒƒå›´: [{df['value'].min():.1f}, {df['value'].max():.1f}]")
print(f"  æˆªæ–­èŒƒå›´: [{df_clipped['value'].min():.1f}, {df_clipped['value'].max():.1f}]")

# ========== æ–¹æ³• 3ï¼šæ›¿æ¢ä¸ºè¾¹ç•Œå€¼ ==========
df_replaced = df.copy()
df_replaced.loc[df_replaced['value'] < lower, 'value'] = lower
df_replaced.loc[df_replaced['value'] > upper, 'value'] = upper

# ========== æ–¹æ³• 4ï¼šæ›¿æ¢ä¸ºç¼ºå¤±å€¼ï¼Œå†å¡«å…… ==========
df_na = df.copy()
df_na.loc[(df_na['value'] < lower) | (df_na['value'] > upper), 'value'] = np.nan
df_na['value'] = df_na['value'].fillna(df_na['value'].median())
```

### 3.3 å¤„ç†é‡å¤å€¼

```python
import pandas as pd

df = pd.DataFrame({
    'name': ['Alice', 'Bob', 'Alice', 'Charlie', 'Bob'],
    'age': [25, 30, 25, 35, 30],
    'city': ['Beijing', 'Shanghai', 'Beijing', 'Shenzhen', 'Shanghai']
})

print("åŸå§‹æ•°æ®:")
print(df)

# æ£€æµ‹é‡å¤å€¼
print(f"\né‡å¤è¡Œæ•°é‡: {df.duplicated().sum()}")
print(f"é‡å¤çš„è¡Œ:\n{df[df.duplicated()]}")

# åˆ é™¤é‡å¤å€¼
df_unique = df.drop_duplicates()
print(f"\nåˆ é™¤é‡å¤å: {len(df)} â†’ {len(df_unique)}")

# åªä¿ç•™ç¬¬ä¸€æ¬¡å‡ºç°
df_first = df.drop_duplicates(keep='first')

# åªä¿ç•™æœ€åä¸€æ¬¡å‡ºç°
df_last = df.drop_duplicates(keep='last')

# æ ¹æ®ç‰¹å®šåˆ—å»é‡
df_by_name = df.drop_duplicates(subset=['name'], keep='first')
print(f"\næŒ‰ name å»é‡:")
print(df_by_name)
```

---

## 4. ç‰¹å¾å·¥ç¨‹

### 4.1 ç‰¹å¾ç±»å‹

```
ç‰¹å¾ç±»å‹
â”œâ”€â”€ æ•°å€¼ç‰¹å¾ï¼ˆNumericalï¼‰
â”‚   â”œâ”€â”€ è¿ç»­å‹ï¼šå¹´é¾„ã€æ”¶å…¥ã€æ¸©åº¦
â”‚   â””â”€â”€ ç¦»æ•£å‹ï¼šæ•°é‡ã€æ¬¡æ•°
â”‚
â””â”€â”€ ç±»åˆ«ç‰¹å¾ï¼ˆCategoricalï¼‰
    â”œâ”€â”€ åä¹‰å‹ï¼ˆæ— åºï¼‰ï¼šé¢œè‰²ã€åŸå¸‚ã€æ€§åˆ«
    â””â”€â”€ æœ‰åºå‹ï¼ˆæœ‰åºï¼‰ï¼šå­¦å†ã€ç­‰çº§ã€æ»¡æ„åº¦
```

### 4.2 ç±»åˆ«ç¼–ç 

#### Label Encodingï¼ˆæ ‡ç­¾ç¼–ç ï¼‰

```python
from sklearn.preprocessing import LabelEncoder
import pandas as pd

# é€‚ç”¨äºï¼šæœ‰åºç±»åˆ«ç‰¹å¾ï¼Œæˆ–è€…æ ‘æ¨¡å‹

df = pd.DataFrame({
    'size': ['S', 'M', 'L', 'XL', 'M', 'S'],
    'color': ['red', 'blue', 'green', 'red', 'blue', 'green']
})

# æ–¹æ³• 1ï¼šsklearn LabelEncoder
le = LabelEncoder()
df['size_encoded'] = le.fit_transform(df['size'])
print("Label Encoding:")
print(df)
print(f"\næ˜ å°„: {dict(zip(le.classes_, range(len(le.classes_))))}")

# æ–¹æ³• 2ï¼šæ‰‹åŠ¨æ˜ å°„ï¼ˆæ›´å¯æ§ï¼‰
size_mapping = {'S': 0, 'M': 1, 'L': 2, 'XL': 3}
df['size_manual'] = df['size'].map(size_mapping)
```

#### One-Hot Encodingï¼ˆç‹¬çƒ­ç¼–ç ï¼‰

```python
import pandas as pd
from sklearn.preprocessing import OneHotEncoder

# é€‚ç”¨äºï¼šæ— åºç±»åˆ«ç‰¹å¾ï¼Œçº¿æ€§æ¨¡å‹

df = pd.DataFrame({
    'color': ['red', 'blue', 'green', 'red', 'blue'],
    'value': [1, 2, 3, 4, 5]
})

# æ–¹æ³• 1ï¼špandas get_dummies
df_onehot = pd.get_dummies(df, columns=['color'], prefix='color')
print("One-Hot Encoding (pandas):")
print(df_onehot)

# æ–¹æ³• 2ï¼šsklearn OneHotEncoder
ohe = OneHotEncoder(sparse_output=False, handle_unknown='ignore')
color_encoded = ohe.fit_transform(df[['color']])
print(f"\nOne-Hot Encoding (sklearn):")
print(color_encoded)
print(f"ç‰¹å¾å: {ohe.get_feature_names_out(['color'])}")

# å¤„ç†æ–°ç±»åˆ«
new_data = pd.DataFrame({'color': ['red', 'yellow']})  # yellow æ˜¯æ–°ç±»åˆ«
new_encoded = ohe.transform(new_data)
print(f"\nå¤„ç†æ–°ç±»åˆ«:")
print(new_encoded)  # yellow ä¼šè¢«ç¼–ç ä¸ºå…¨ 0
```

#### Target Encodingï¼ˆç›®æ ‡ç¼–ç ï¼‰

```python
import pandas as pd
import numpy as np

# é€‚ç”¨äºï¼šé«˜åŸºæ•°ç±»åˆ«ç‰¹å¾ï¼ˆå¦‚åŸå¸‚ã€ç”¨æˆ·IDï¼‰

np.random.seed(42)
df = pd.DataFrame({
    'city': np.random.choice(['A', 'B', 'C', 'D', 'E'], 100),
    'target': np.random.randint(0, 2, 100)
})

# è®¡ç®—æ¯ä¸ªåŸå¸‚çš„ç›®æ ‡å‡å€¼
city_target_mean = df.groupby('city')['target'].mean()
df['city_target_encoded'] = df['city'].map(city_target_mean)

print("Target Encoding:")
print(df.head(10))
print(f"\nåŸå¸‚-ç›®æ ‡å‡å€¼æ˜ å°„:\n{city_target_mean}")

# âš ï¸ æ³¨æ„ï¼šTarget Encoding å®¹æ˜“è¿‡æ‹Ÿåˆï¼Œå®é™…ä½¿ç”¨éœ€è¦ï¼š
# 1. åªåœ¨è®­ç»ƒé›†ä¸Šè®¡ç®—å‡å€¼
# 2. ä½¿ç”¨äº¤å‰éªŒè¯é˜²æ­¢ä¿¡æ¯æ³„éœ²
# 3. æ·»åŠ å¹³æ»‘é¡¹å¤„ç†ä½é¢‘ç±»åˆ«
```

### 4.3 ç‰¹å¾ç¼©æ”¾

#### ä¸ºä»€ä¹ˆéœ€è¦ç¼©æ”¾ï¼Ÿ

```python
import numpy as np
from sklearn.preprocessing import StandardScaler, MinMaxScaler

# ä¸åŒç‰¹å¾é‡çº§å·®å¼‚å¾ˆå¤§
X = np.array([
    [25, 50000],   # å¹´é¾„ï¼š20-70ï¼Œæ”¶å…¥ï¼š30000-200000
    [30, 80000],
    [45, 120000],
    [28, 45000]
])

print("åŸå§‹æ•°æ®:")
print(f"  å¹´é¾„èŒƒå›´: {X[:, 0].min()}-{X[:, 0].max()}")
print(f"  æ”¶å…¥èŒƒå›´: {X[:, 1].min()}-{X[:, 1].max()}")

# é—®é¢˜ï¼šåŸºäºè·ç¦»çš„ç®—æ³•ï¼ˆKNNã€SVMï¼‰ä¼šè¢«å¤§æ•°å€¼ç‰¹å¾ä¸»å¯¼
# è§£å†³ï¼šç‰¹å¾ç¼©æ”¾
```

#### StandardScalerï¼ˆZ-score æ ‡å‡†åŒ–ï¼‰

```python
from sklearn.preprocessing import StandardScaler
import numpy as np

X = np.array([[25, 50000], [30, 80000], [45, 120000], [28, 45000]])

# æ ‡å‡†åŒ–ï¼šå‡å€¼ä¸º 0ï¼Œæ ‡å‡†å·®ä¸º 1
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

print("StandardScaler (Z-score):")
print(f"ç¼©æ”¾åå‡å€¼: {X_scaled.mean(axis=0)}")
print(f"ç¼©æ”¾åæ ‡å‡†å·®: {X_scaled.std(axis=0)}")
print(f"\nç¼©æ”¾åæ•°æ®:\n{X_scaled}")

# é‡è¦ï¼šfit å’Œ transform è¦åˆ†å¼€
# è®­ç»ƒé›†ï¼šfit_transform
# æµ‹è¯•é›†ï¼šåª transform
X_train = X[:3]
X_test = X[3:]

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)  # fit + transform
X_test_scaled = scaler.transform(X_test)        # åª transform

# é€†å˜æ¢
X_original = scaler.inverse_transform(X_train_scaled)
```

#### MinMaxScalerï¼ˆMin-Max å½’ä¸€åŒ–ï¼‰

```python
from sklearn.preprocessing import MinMaxScaler
import numpy as np

X = np.array([[25, 50000], [30, 80000], [45, 120000], [28, 45000]])

# å½’ä¸€åŒ–ï¼šç¼©æ”¾åˆ° [0, 1]
scaler = MinMaxScaler()
X_scaled = scaler.fit_transform(X)

print("MinMaxScaler:")
print(f"ç¼©æ”¾åèŒƒå›´: [{X_scaled.min()}, {X_scaled.max()}]")
print(f"\nç¼©æ”¾åæ•°æ®:\n{X_scaled}")

# è‡ªå®šä¹‰èŒƒå›´
scaler_custom = MinMaxScaler(feature_range=(-1, 1))
X_custom = scaler_custom.fit_transform(X)
print(f"\nç¼©æ”¾åˆ° [-1, 1]:\n{X_custom}")
```

#### RobustScalerï¼ˆé²æ£’ç¼©æ”¾ï¼‰

```python
from sklearn.preprocessing import RobustScaler
import numpy as np

# å¯¹å¼‚å¸¸å€¼æ›´é²æ£’ï¼Œä½¿ç”¨ä¸­ä½æ•°å’Œ IQR
X = np.array([[25, 50000], [30, 80000], [45, 120000], [28, 45000], [100, 500000]])  # æœ€åä¸€è¡Œæ˜¯å¼‚å¸¸å€¼

scaler_standard = StandardScaler()
scaler_robust = RobustScaler()

X_standard = scaler_standard.fit_transform(X)
X_robust = scaler_robust.fit_transform(X)

print("StandardScaler vs RobustScalerï¼ˆæœ‰å¼‚å¸¸å€¼æ—¶ï¼‰:")
print(f"StandardScaler å‡å€¼: {X_standard.mean(axis=0)}")
print(f"RobustScaler ä¸­ä½æ•°: {np.median(X_robust, axis=0)}")
```

#### ä½•æ—¶ä½¿ç”¨å“ªç§ç¼©æ”¾ï¼Ÿ

| æ–¹æ³• | ç‰¹ç‚¹ | é€‚ç”¨åœºæ™¯ |
|------|------|---------|
| StandardScaler | å‡å€¼ 0ï¼Œæ ‡å‡†å·® 1 | å¤§å¤šæ•°æƒ…å†µï¼Œç‰¹åˆ«æ˜¯çº¿æ€§æ¨¡å‹ |
| MinMaxScaler | ç¼©æ”¾åˆ°å›ºå®šèŒƒå›´ | ç¥ç»ç½‘ç»œã€å›¾åƒæ•°æ® |
| RobustScaler | ä½¿ç”¨ä¸­ä½æ•°å’Œ IQR | æ•°æ®æœ‰å¼‚å¸¸å€¼ |

### 4.4 ç‰¹å¾é€‰æ‹©

```python
from sklearn.feature_selection import SelectKBest, f_classif, mutual_info_classif
from sklearn.feature_selection import RFE
from sklearn.ensemble import RandomForestClassifier
from sklearn.datasets import make_classification
import numpy as np

# ç”Ÿæˆæ•°æ®
X, y = make_classification(n_samples=100, n_features=20, n_informative=5, random_state=42)

# ========== æ–¹æ³• 1ï¼šåŸºäºç»Ÿè®¡æ£€éªŒ ==========
# é€‰æ‹© K ä¸ªæœ€ä½³ç‰¹å¾
selector = SelectKBest(score_func=f_classif, k=5)
X_selected = selector.fit_transform(X, y)

print(f"åŸå§‹ç‰¹å¾æ•°: {X.shape[1]}")
print(f"é€‰æ‹©åç‰¹å¾æ•°: {X_selected.shape[1]}")
print(f"è¢«é€‰ä¸­çš„ç‰¹å¾ç´¢å¼•: {selector.get_support(indices=True)}")

# ========== æ–¹æ³• 2ï¼šåŸºäºæ¨¡å‹çš„ç‰¹å¾é‡è¦æ€§ ==========
rf = RandomForestClassifier(n_estimators=100, random_state=42)
rf.fit(X, y)

# è·å–ç‰¹å¾é‡è¦æ€§
importances = rf.feature_importances_
indices = np.argsort(importances)[::-1]

print(f"\nç‰¹å¾é‡è¦æ€§æ’åï¼ˆTop 5ï¼‰:")
for i in range(5):
    print(f"  Feature {indices[i]}: {importances[indices[i]]:.4f}")

# ========== æ–¹æ³• 3ï¼šé€’å½’ç‰¹å¾æ¶ˆé™¤ (RFE) ==========
rfe = RFE(estimator=rf, n_features_to_select=5)
X_rfe = rfe.fit_transform(X, y)

print(f"\nRFE é€‰æ‹©çš„ç‰¹å¾ç´¢å¼•: {np.where(rfe.support_)[0]}")
```

### 4.5 ç‰¹å¾æ„é€ 

```python
import pandas as pd
import numpy as np

# ç¤ºä¾‹æ•°æ®
df = pd.DataFrame({
    'date': pd.date_range('2024-01-01', periods=5),
    'price': [100, 120, 110, 130, 125],
    'quantity': [10, 15, 12, 18, 14],
    'category': ['A', 'B', 'A', 'C', 'B']
})

print("åŸå§‹æ•°æ®:")
print(df)

# ========== æ•°å€¼ç‰¹å¾æ„é€  ==========
# äº¤äº’ç‰¹å¾
df['total_value'] = df['price'] * df['quantity']

# æ¯”ç‡ç‰¹å¾
df['price_per_unit'] = df['price'] / df['quantity']

# å¤šé¡¹å¼ç‰¹å¾
df['price_squared'] = df['price'] ** 2

# åˆ†ç®±
df['price_bin'] = pd.cut(df['price'], bins=[0, 110, 120, 200], labels=['low', 'medium', 'high'])

# ========== æ—¶é—´ç‰¹å¾æ„é€  ==========
df['year'] = df['date'].dt.year
df['month'] = df['date'].dt.month
df['day'] = df['date'].dt.day
df['dayofweek'] = df['date'].dt.dayofweek  # 0=å‘¨ä¸€
df['is_weekend'] = df['dayofweek'].isin([5, 6]).astype(int)

print("\næ„é€ åæ•°æ®:")
print(df)
```

---

## 5. æ•°æ®åˆ’åˆ†

### 5.1 åŸºæœ¬åˆ’åˆ†

```python
from sklearn.model_selection import train_test_split
import numpy as np

# ç”Ÿæˆæ•°æ®
X = np.random.randn(1000, 10)
y = np.random.randint(0, 2, 1000)

# ========== ç®€å•åˆ’åˆ†ï¼šè®­ç»ƒé›† + æµ‹è¯•é›† ==========
X_train, X_test, y_train, y_test = train_test_split(
    X, y,
    test_size=0.2,      # 20% ä½œä¸ºæµ‹è¯•é›†
    random_state=42,    # éšæœºç§å­ï¼Œä¿è¯å¯å¤ç°
    shuffle=True        # æ‰“ä¹±æ•°æ®
)

print("ç®€å•åˆ’åˆ†:")
print(f"  è®­ç»ƒé›†: {X_train.shape[0]}")
print(f"  æµ‹è¯•é›†: {X_test.shape[0]}")

# ========== ä¸‰åˆ†åˆ’åˆ†ï¼šè®­ç»ƒé›† + éªŒè¯é›† + æµ‹è¯•é›† ==========
# æ–¹æ³•ï¼šå…ˆåˆ†å‡ºæµ‹è¯•é›†ï¼Œå†ä»è®­ç»ƒé›†åˆ†å‡ºéªŒè¯é›†
X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.25, random_state=42)
# æœ€ç»ˆæ¯”ä¾‹ï¼š60% è®­ç»ƒï¼Œ20% éªŒè¯ï¼Œ20% æµ‹è¯•

print("\nä¸‰åˆ†åˆ’åˆ†:")
print(f"  è®­ç»ƒé›†: {X_train.shape[0]} ({X_train.shape[0]/len(X)*100:.0f}%)")
print(f"  éªŒè¯é›†: {X_val.shape[0]} ({X_val.shape[0]/len(X)*100:.0f}%)")
print(f"  æµ‹è¯•é›†: {X_test.shape[0]} ({X_test.shape[0]/len(X)*100:.0f}%)")
```

### 5.2 åˆ†å±‚æŠ½æ ·

```python
from sklearn.model_selection import train_test_split
import numpy as np

# ä¸å¹³è¡¡æ•°æ®é›†
y = np.array([0]*900 + [1]*100)  # 90% è´Ÿæ ·æœ¬ï¼Œ10% æ­£æ ·æœ¬
X = np.random.randn(1000, 5)

# ========== ä¸åˆ†å±‚ ==========
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
print("ä¸åˆ†å±‚æŠ½æ ·:")
print(f"  åŸå§‹æ­£æ ·æœ¬æ¯”ä¾‹: {y.mean():.2%}")
print(f"  è®­ç»ƒé›†æ­£æ ·æœ¬æ¯”ä¾‹: {y_train.mean():.2%}")
print(f"  æµ‹è¯•é›†æ­£æ ·æœ¬æ¯”ä¾‹: {y_test.mean():.2%}")

# ========== åˆ†å±‚æŠ½æ · ==========
X_train, X_test, y_train, y_test = train_test_split(
    X, y,
    test_size=0.2,
    random_state=42,
    stratify=y  # å…³é”®å‚æ•°ï¼šæŒ‰ y çš„æ¯”ä¾‹åˆ†å±‚
)
print("\nåˆ†å±‚æŠ½æ ·:")
print(f"  åŸå§‹æ­£æ ·æœ¬æ¯”ä¾‹: {y.mean():.2%}")
print(f"  è®­ç»ƒé›†æ­£æ ·æœ¬æ¯”ä¾‹: {y_train.mean():.2%}")
print(f"  æµ‹è¯•é›†æ­£æ ·æœ¬æ¯”ä¾‹: {y_test.mean():.2%}")
```

### 5.3 æ—¶é—´åºåˆ—åˆ’åˆ†

```python
import numpy as np
import pandas as pd

# æ—¶é—´åºåˆ—æ•°æ®ä¸èƒ½éšæœºæ‰“ä¹±
dates = pd.date_range('2020-01-01', '2023-12-31', freq='D')
n = len(dates)
data = pd.DataFrame({
    'date': dates,
    'value': np.cumsum(np.random.randn(n)) + 100
})

print(f"æ•°æ®èŒƒå›´: {data['date'].min()} åˆ° {data['date'].max()}")
print(f"æ€»æ ·æœ¬æ•°: {len(data)}")

# ========== æŒ‰æ—¶é—´é¡ºåºåˆ’åˆ† ==========
train_end = '2022-12-31'
val_end = '2023-06-30'

train_data = data[data['date'] <= train_end]
val_data = data[(data['date'] > train_end) & (data['date'] <= val_end)]
test_data = data[data['date'] > val_end]

print(f"\nè®­ç»ƒé›†: {train_data['date'].min()} ~ {train_data['date'].max()} ({len(train_data)} æ¡)")
print(f"éªŒè¯é›†: {val_data['date'].min()} ~ {val_data['date'].max()} ({len(val_data)} æ¡)")
print(f"æµ‹è¯•é›†: {test_data['date'].min()} ~ {test_data['date'].max()} ({len(test_data)} æ¡)")
```

---

## 6. å®Œæ•´ç¤ºä¾‹

å°†ä¸Šè¿°æ‰€æœ‰æ­¥éª¤æ•´åˆï¼š

```python
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.impute import SimpleImputer
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report

# ========== 1. åŠ è½½æ•°æ® ==========
# åˆ›å»ºæ¨¡æ‹Ÿæ•°æ®
np.random.seed(42)
n = 500
df = pd.DataFrame({
    'age': np.random.randint(18, 70, n).astype(float),
    'income': np.random.randint(30000, 150000, n).astype(float),
    'education': np.random.choice(['é«˜ä¸­', 'æœ¬ç§‘', 'ç¡•å£«', 'åšå£«'], n),
    'city': np.random.choice(['åŒ—äº¬', 'ä¸Šæµ·', 'å¹¿å·', 'æ·±åœ³'], n),
    'purchased': np.random.choice([0, 1], n)
})

# æ·»åŠ ç¼ºå¤±å€¼
df.loc[np.random.choice(n, 20), 'age'] = np.nan
df.loc[np.random.choice(n, 30), 'income'] = np.nan

print("========== åŸå§‹æ•°æ® ==========")
print(df.head())
print(f"\nç¼ºå¤±å€¼:\n{df.isnull().sum()}")

# ========== 2. æ•°æ®æ¸…æ´— ==========
# åˆ†ç¦»ç‰¹å¾å’Œæ ‡ç­¾
X = df.drop('purchased', axis=1)
y = df['purchased']

# æ•°å€¼åˆ—å’Œç±»åˆ«åˆ—
num_cols = ['age', 'income']
cat_cols = ['education', 'city']

# ========== 3. æ•°æ®åˆ’åˆ†ï¼ˆå…ˆåˆ’åˆ†å†å¤„ç†ï¼Œé˜²æ­¢ä¿¡æ¯æ³„éœ²ï¼‰ ==========
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

print(f"\n========== æ•°æ®åˆ’åˆ† ==========")
print(f"è®­ç»ƒé›†: {len(X_train)}, æµ‹è¯•é›†: {len(X_test)}")

# ========== 4. ç‰¹å¾å·¥ç¨‹ ==========
# å¤åˆ¶ï¼Œé¿å… SettingWithCopyWarning
X_train = X_train.copy()
X_test = X_test.copy()

# 4.1 å¤„ç†ç¼ºå¤±å€¼ï¼ˆåœ¨è®­ç»ƒé›†ä¸Š fitï¼‰
imputer = SimpleImputer(strategy='median')
X_train[num_cols] = imputer.fit_transform(X_train[num_cols])
X_test[num_cols] = imputer.transform(X_test[num_cols])

# 4.2 ç±»åˆ«ç¼–ç 
# æœ‰åºç±»åˆ«ï¼šLabel Encoding
edu_order = {'é«˜ä¸­': 0, 'æœ¬ç§‘': 1, 'ç¡•å£«': 2, 'åšå£«': 3}
X_train['education'] = X_train['education'].map(edu_order)
X_test['education'] = X_test['education'].map(edu_order)

# æ— åºç±»åˆ«ï¼šOne-Hot Encoding
X_train = pd.get_dummies(X_train, columns=['city'], prefix='city')
X_test = pd.get_dummies(X_test, columns=['city'], prefix='city')

# ç¡®ä¿è®­ç»ƒé›†å’Œæµ‹è¯•é›†æœ‰ç›¸åŒçš„åˆ—
X_train, X_test = X_train.align(X_test, join='left', axis=1, fill_value=0)

# 4.3 ç‰¹å¾ç¼©æ”¾
scaler = StandardScaler()
X_train[num_cols] = scaler.fit_transform(X_train[num_cols])
X_test[num_cols] = scaler.transform(X_test[num_cols])

print(f"\n========== å¤„ç†åç‰¹å¾ ==========")
print(f"ç‰¹å¾åˆ—: {list(X_train.columns)}")
print(X_train.head())

# ========== 5. æ¨¡å‹è®­ç»ƒ ==========
model = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=42)
model.fit(X_train, y_train)

# ========== 6. æ¨¡å‹è¯„ä¼° ==========
y_pred = model.predict(X_test)

print(f"\n========== æ¨¡å‹è¯„ä¼° ==========")
print(f"å‡†ç¡®ç‡: {accuracy_score(y_test, y_pred):.4f}")
print(f"\nåˆ†ç±»æŠ¥å‘Š:\n{classification_report(y_test, y_pred)}")
```

---

## 7. ç»ƒä¹ é¢˜

### åŸºç¡€ç»ƒä¹ 

1. **ç¼ºå¤±å€¼å¤„ç†**ï¼šç»™å®šä¸€ä¸ª DataFrameï¼Œæ£€æµ‹ç¼ºå¤±å€¼å¹¶ç”¨åˆé€‚çš„ç­–ç•¥å¡«å……
2. **å¼‚å¸¸å€¼æ£€æµ‹**ï¼šä½¿ç”¨ IQR æ–¹æ³•æ£€æµ‹å¹¶å¤„ç†æ•°æ®ä¸­çš„å¼‚å¸¸å€¼
3. **ç‰¹å¾ç¼–ç **ï¼šå¯¹ä¸€ä¸ªåŒ…å«æœ‰åºå’Œæ— åºç±»åˆ«çš„æ•°æ®é›†è¿›è¡Œæ­£ç¡®ç¼–ç 

### è¿›é˜¶ç»ƒä¹ 

4. **å®Œæ•´æµæ°´çº¿**ï¼šä¸‹è½½ä¸€ä¸ª Kaggle æ•°æ®é›†ï¼Œå®Œæˆä»æ•°æ®æ¸…æ´—åˆ°ç‰¹å¾å·¥ç¨‹çš„å®Œæ•´æµç¨‹
5. **Pipeline**ï¼šä½¿ç”¨ sklearn çš„ Pipeline å°†é¢„å¤„ç†æ­¥éª¤ä¸²è”èµ·æ¥

### å‚è€ƒç­”æ¡ˆ

<details>
<summary>ç»ƒä¹  5 å‚è€ƒç­”æ¡ˆ</summary>

```python
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.impute import SimpleImputer
from sklearn.ensemble import RandomForestClassifier

# å®šä¹‰æ•°å€¼å’Œç±»åˆ«åˆ—
num_cols = ['age', 'income']
cat_cols = ['city']

# æ•°å€¼ç‰¹å¾å¤„ç†æµæ°´çº¿
num_pipeline = Pipeline([
    ('imputer', SimpleImputer(strategy='median')),
    ('scaler', StandardScaler())
])

# ç±»åˆ«ç‰¹å¾å¤„ç†æµæ°´çº¿
cat_pipeline = Pipeline([
    ('imputer', SimpleImputer(strategy='most_frequent')),
    ('encoder', OneHotEncoder(handle_unknown='ignore'))
])

# ç»„åˆ
preprocessor = ColumnTransformer([
    ('num', num_pipeline, num_cols),
    ('cat', cat_pipeline, cat_cols)
])

# å®Œæ•´æµæ°´çº¿
full_pipeline = Pipeline([
    ('preprocessor', preprocessor),
    ('classifier', RandomForestClassifier(n_estimators=100, random_state=42))
])

# ä½¿ç”¨
# full_pipeline.fit(X_train, y_train)
# y_pred = full_pipeline.predict(X_test)
```

</details>

---

## â¡ï¸ ä¸‹ä¸€æ­¥

å­¦å®Œæœ¬èŠ‚åï¼Œç»§ç»­å­¦ä¹  [02-æ¨¡å‹è¯„ä¼°æŒ‡æ ‡.md](./02-æ¨¡å‹è¯„ä¼°æŒ‡æ ‡.md)

