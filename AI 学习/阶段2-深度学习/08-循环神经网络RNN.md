# ğŸ“ 08 - å¾ªç¯ç¥ç»ç½‘ç»œ RNN

> RNN å¤„ç†åºåˆ—æ•°æ®ï¼ŒLSTM/GRU è§£å†³é•¿æœŸä¾èµ–é—®é¢˜

---

## ç›®å½•

1. [RNN åŸºç¡€](#1-rnn-åŸºç¡€)
2. [LSTM](#2-lstm)
3. [GRU](#3-gru)
4. [åŒå‘ RNN](#4-åŒå‘-rnn)
5. [å®æˆ˜ï¼šæƒ…æ„Ÿåˆ†ç±»](#5-å®æˆ˜æƒ…æ„Ÿåˆ†ç±»)
6. [ç»ƒä¹ é¢˜](#6-ç»ƒä¹ é¢˜)

---

## 1. RNN åŸºç¡€

### 1.1 åºåˆ—æ•°æ®ä¸ RNN

```
åºåˆ—æ•°æ®ç‰¹ç‚¹ï¼š
- æœ‰å…ˆåé¡ºåºï¼ˆæ–‡æœ¬ã€æ—¶é—´åºåˆ—ã€éŸ³é¢‘ç­‰ï¼‰
- ä¸åŒä½ç½®ä¹‹é—´æœ‰ä¾èµ–å…³ç³»

RNN æ ¸å¿ƒæ€æƒ³ï¼š
- å¤„ç†æ¯ä¸ªæ—¶é—´æ­¥æ—¶ï¼Œè€ƒè™‘"è®°å¿†"ï¼ˆéšè—çŠ¶æ€ï¼‰
- éšè—çŠ¶æ€ä¼ é€’å†å²ä¿¡æ¯

h_t = tanh(W_hh * h_{t-1} + W_xh * x_t + b)
y_t = W_hy * h_t + b_y
```

### 1.2 PyTorch RNN

```python
import torch
import torch.nn as nn

# RNN å±‚
rnn = nn.RNN(
    input_size=10,    # è¾“å…¥ç‰¹å¾ç»´åº¦
    hidden_size=20,   # éšè—çŠ¶æ€ç»´åº¦
    num_layers=2,     # RNN å±‚æ•°
    batch_first=True, # è¾“å…¥æ ¼å¼ [batch, seq, feature]
    dropout=0.1,      # å±‚é—´ dropoutï¼ˆnum_layers > 1 æ—¶ç”Ÿæ•ˆï¼‰
    bidirectional=False
)

# è¾“å…¥ï¼š[batch, seq_len, input_size]
x = torch.randn(32, 15, 10)  # 32 ä¸ªæ ·æœ¬ï¼Œåºåˆ—é•¿åº¦ 15ï¼Œç‰¹å¾ç»´åº¦ 10

# å¯é€‰ï¼šåˆå§‹éšè—çŠ¶æ€ [num_layers, batch, hidden_size]
h0 = torch.zeros(2, 32, 20)

# å‰å‘ä¼ æ’­
output, h_n = rnn(x, h0)
# output: æ¯ä¸ªæ—¶é—´æ­¥çš„è¾“å‡º [batch, seq_len, hidden_size]
# h_n: æœ€åæ—¶é—´æ­¥çš„éšè—çŠ¶æ€ [num_layers, batch, hidden_size]

print(f"Output å½¢çŠ¶: {output.shape}")  # [32, 15, 20]
print(f"Hidden å½¢çŠ¶: {h_n.shape}")     # [2, 32, 20]
```

### 1.3 æ‰‹å†™ RNN Cell

```python
class SimpleRNNCell(nn.Module):
    """å•ä¸ª RNN å•å…ƒ"""
    def __init__(self, input_size, hidden_size):
        super().__init__()
        self.hidden_size = hidden_size

        # è¾“å…¥åˆ°éšè—
        self.i2h = nn.Linear(input_size, hidden_size)
        # éšè—åˆ°éšè—
        self.h2h = nn.Linear(hidden_size, hidden_size)

    def forward(self, x, h_prev):
        # x: [batch, input_size]
        # h_prev: [batch, hidden_size]
        h_new = torch.tanh(self.i2h(x) + self.h2h(h_prev))
        return h_new

class SimpleRNN(nn.Module):
    """æ‰‹å†™ RNN"""
    def __init__(self, input_size, hidden_size):
        super().__init__()
        self.hidden_size = hidden_size
        self.cell = SimpleRNNCell(input_size, hidden_size)

    def forward(self, x, h0=None):
        # x: [batch, seq_len, input_size]
        batch_size, seq_len, _ = x.shape

        if h0 is None:
            h0 = torch.zeros(batch_size, self.hidden_size, device=x.device)

        outputs = []
        h = h0

        for t in range(seq_len):
            h = self.cell(x[:, t, :], h)
            outputs.append(h)

        # [batch, seq_len, hidden_size]
        outputs = torch.stack(outputs, dim=1)

        return outputs, h

# æµ‹è¯•
rnn = SimpleRNN(10, 20)
x = torch.randn(32, 15, 10)
output, h_n = rnn(x)
print(f"æ‰‹å†™ RNN Output: {output.shape}")  # [32, 15, 20]
```

---

## 2. LSTM

### 2.1 LSTM ç»“æ„

```
LSTM è§£å†³ RNN çš„æ¢¯åº¦æ¶ˆå¤±é—®é¢˜ï¼Œé€šè¿‡"é—¨"æœºåˆ¶æ§åˆ¶ä¿¡æ¯æµ

ä¸‰ä¸ªé—¨ + ç»†èƒçŠ¶æ€ï¼š
- é—å¿˜é—¨ (f)ï¼šå†³å®šä¸¢å¼ƒå¤šå°‘æ—§ä¿¡æ¯
- è¾“å…¥é—¨ (i)ï¼šå†³å®šæ·»åŠ å¤šå°‘æ–°ä¿¡æ¯
- è¾“å‡ºé—¨ (o)ï¼šå†³å®šè¾“å‡ºå¤šå°‘ä¿¡æ¯
- ç»†èƒçŠ¶æ€ (c)ï¼šé•¿æœŸè®°å¿†

f_t = Ïƒ(W_f Â· [h_{t-1}, x_t] + b_f)
i_t = Ïƒ(W_i Â· [h_{t-1}, x_t] + b_i)
cÌƒ_t = tanh(W_c Â· [h_{t-1}, x_t] + b_c)
c_t = f_t * c_{t-1} + i_t * cÌƒ_t
o_t = Ïƒ(W_o Â· [h_{t-1}, x_t] + b_o)
h_t = o_t * tanh(c_t)
```

### 2.2 PyTorch LSTM

```python
# LSTM å±‚
lstm = nn.LSTM(
    input_size=10,
    hidden_size=20,
    num_layers=2,
    batch_first=True,
    dropout=0.1,
    bidirectional=False
)

x = torch.randn(32, 15, 10)

# åˆå§‹çŠ¶æ€ï¼š(h0, c0)
h0 = torch.zeros(2, 32, 20)  # hidden state
c0 = torch.zeros(2, 32, 20)  # cell state

output, (h_n, c_n) = lstm(x, (h0, c0))

print(f"Output: {output.shape}")  # [32, 15, 20]
print(f"h_n: {h_n.shape}")        # [2, 32, 20]
print(f"c_n: {c_n.shape}")        # [2, 32, 20]
```

### 2.3 æ‰‹å†™ LSTM Cell

```python
class LSTMCell(nn.Module):
    """æ‰‹å†™ LSTM å•å…ƒ"""
    def __init__(self, input_size, hidden_size):
        super().__init__()
        self.hidden_size = hidden_size

        # åˆå¹¶è®¡ç®—æ‰€æœ‰é—¨ï¼ˆæ•ˆç‡æ›´é«˜ï¼‰
        self.gates = nn.Linear(input_size + hidden_size, 4 * hidden_size)

    def forward(self, x, state):
        h_prev, c_prev = state

        # æ‹¼æ¥è¾“å…¥å’Œéšè—çŠ¶æ€
        combined = torch.cat([x, h_prev], dim=1)

        # è®¡ç®—æ‰€æœ‰é—¨
        gates = self.gates(combined)

        # åˆ†å‰²æˆå››ä¸ªé—¨
        i, f, g, o = gates.chunk(4, dim=1)

        # æ¿€æ´»
        i = torch.sigmoid(i)  # è¾“å…¥é—¨
        f = torch.sigmoid(f)  # é—å¿˜é—¨
        g = torch.tanh(g)     # å€™é€‰ç»†èƒçŠ¶æ€
        o = torch.sigmoid(o)  # è¾“å‡ºé—¨

        # æ›´æ–°ç»†èƒçŠ¶æ€
        c = f * c_prev + i * g

        # æ›´æ–°éšè—çŠ¶æ€
        h = o * torch.tanh(c)

        return h, c

# æµ‹è¯•
cell = LSTMCell(10, 20)
x = torch.randn(32, 10)
h = torch.zeros(32, 20)
c = torch.zeros(32, 20)
h_new, c_new = cell(x, (h, c))
print(f"LSTM Cell: h={h_new.shape}, c={c_new.shape}")
```

---

## 3. GRU

### 3.1 GRU ç»“æ„

```
GRU ç®€åŒ–äº† LSTMï¼Œåªæœ‰ä¸¤ä¸ªé—¨ï¼Œæ²¡æœ‰ç»†èƒçŠ¶æ€

- é‡ç½®é—¨ (r)ï¼šæ§åˆ¶å¿½ç•¥å¤šå°‘å†å²ä¿¡æ¯
- æ›´æ–°é—¨ (z)ï¼šæ§åˆ¶ä¿ç•™å¤šå°‘å†å²ä¿¡æ¯

z_t = Ïƒ(W_z Â· [h_{t-1}, x_t])
r_t = Ïƒ(W_r Â· [h_{t-1}, x_t])
hÌƒ_t = tanh(W Â· [r_t * h_{t-1}, x_t])
h_t = (1 - z_t) * h_{t-1} + z_t * hÌƒ_t
```

### 3.2 PyTorch GRU

```python
# GRU å±‚
gru = nn.GRU(
    input_size=10,
    hidden_size=20,
    num_layers=2,
    batch_first=True,
    dropout=0.1,
    bidirectional=False
)

x = torch.randn(32, 15, 10)
h0 = torch.zeros(2, 32, 20)

output, h_n = gru(x, h0)

print(f"Output: {output.shape}")  # [32, 15, 20]
print(f"h_n: {h_n.shape}")        # [2, 32, 20]
```

### 3.3 LSTM vs GRU

```
LSTM:
- 3 ä¸ªé—¨ + ç»†èƒçŠ¶æ€
- å‚æ•°æ›´å¤š
- åœ¨é•¿åºåˆ—ä¸Šå¯èƒ½è¡¨ç°æ›´å¥½

GRU:
- 2 ä¸ªé—¨ï¼Œæ²¡æœ‰ç»†èƒçŠ¶æ€
- å‚æ•°æ›´å°‘ï¼Œè®­ç»ƒæ›´å¿«
- åœ¨è¾ƒçŸ­åºåˆ—ä¸Šæ•ˆæœç›¸å½“

å®è·µå»ºè®®ï¼š
- å…ˆå°è¯• GRUï¼ˆæ›´å¿«ï¼‰
- å¦‚æœæ•ˆæœä¸å¥½ï¼Œå†å°è¯• LSTM
- ç°ä»£ NLP å¤šç”¨ Transformer
```

---

## 4. åŒå‘ RNN

### 4.1 åŒå‘ RNN åŸç†

```
å•å‘ï¼šåªçœ‹å‰æ–‡
åŒå‘ï¼šåŒæ—¶çœ‹å‰æ–‡å’Œåæ–‡

"I love [MASK] learning"
å‰å‘ï¼šæ ¹æ® "I love" é¢„æµ‹
åå‘ï¼šæ ¹æ® "learning" é¢„æµ‹
åŒå‘ï¼šç»“åˆä¸¤ä¸ªæ–¹å‘çš„ä¿¡æ¯
```

### 4.2 PyTorch åŒå‘ RNN

```python
# åŒå‘ LSTM
bilstm = nn.LSTM(
    input_size=10,
    hidden_size=20,
    num_layers=2,
    batch_first=True,
    bidirectional=True  # å…³é”®å‚æ•°
)

x = torch.randn(32, 15, 10)

# åŒå‘æ—¶ï¼Œnum_directions = 2
# h0: [num_layers * num_directions, batch, hidden_size]
h0 = torch.zeros(4, 32, 20)  # 2 * 2 = 4
c0 = torch.zeros(4, 32, 20)

output, (h_n, c_n) = bilstm(x, (h0, c0))

# output: [batch, seq_len, hidden_size * num_directions]
print(f"Output: {output.shape}")  # [32, 15, 40]
# h_n: [num_layers * num_directions, batch, hidden_size]
print(f"h_n: {h_n.shape}")        # [4, 32, 20]

# åˆ†ç¦»å‰å‘å’Œåå‘
# output åœ¨æœ€åä¸€ç»´ä¸Šæ‹¼æ¥ï¼š[forward, backward]
forward_output = output[:, :, :20]
backward_output = output[:, :, 20:]

# h_n äº¤æ›¿æ’åˆ—ï¼š[layer0_forward, layer0_backward, layer1_forward, layer1_backward]
```

### 4.3 è·å–å¥å­è¡¨ç¤º

```python
class BiLSTMClassifier(nn.Module):
    def __init__(self, vocab_size, embed_dim, hidden_dim, num_classes, num_layers=1):
        super().__init__()

        self.embedding = nn.Embedding(vocab_size, embed_dim)
        self.lstm = nn.LSTM(
            embed_dim, hidden_dim, num_layers,
            batch_first=True, bidirectional=True
        )
        self.fc = nn.Linear(hidden_dim * 2, num_classes)  # *2 å› ä¸ºåŒå‘

    def forward(self, x):
        # x: [batch, seq_len]
        embedded = self.embedding(x)  # [batch, seq_len, embed_dim]

        output, (h_n, c_n) = self.lstm(embedded)
        # output: [batch, seq_len, hidden*2]
        # h_n: [num_layers*2, batch, hidden]

        # æ–¹æ³• 1ï¼šå–æœ€åæ—¶é—´æ­¥çš„è¾“å‡º
        # last_output = output[:, -1, :]  # [batch, hidden*2]

        # æ–¹æ³• 2ï¼šæ‹¼æ¥å‰å‘å’Œåå‘çš„æœ€ç»ˆéšè—çŠ¶æ€ï¼ˆæ›´å¸¸ç”¨ï¼‰
        # h_n[-2]: æœ€åä¸€å±‚å‰å‘çš„æœ€ç»ˆçŠ¶æ€
        # h_n[-1]: æœ€åä¸€å±‚åå‘çš„æœ€ç»ˆçŠ¶æ€
        h_forward = h_n[-2]
        h_backward = h_n[-1]
        h_concat = torch.cat([h_forward, h_backward], dim=1)  # [batch, hidden*2]

        # æ–¹æ³• 3ï¼šå¯¹æ‰€æœ‰æ—¶é—´æ­¥æ±‚å¹³å‡ï¼ˆMean Poolingï¼‰
        # mean_output = output.mean(dim=1)  # [batch, hidden*2]

        out = self.fc(h_concat)  # [batch, num_classes]
        return out

# æµ‹è¯•
model = BiLSTMClassifier(
    vocab_size=10000,
    embed_dim=128,
    hidden_dim=64,
    num_classes=2
)

x = torch.randint(0, 10000, (32, 50))  # [batch, seq_len]
y = model(x)
print(f"åˆ†ç±»è¾“å‡º: {y.shape}")  # [32, 2]
```

---

## 5. å®æˆ˜ï¼šæƒ…æ„Ÿåˆ†ç±»

### 5.1 æ•°æ®å‡†å¤‡

```python
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from collections import Counter

# ç¤ºä¾‹æ•°æ®
texts = [
    "I love this movie it is great",
    "This film is terrible and boring",
    "Amazing performance by the actors",
    "Worst movie ever do not watch",
    "Highly recommended excellent film",
    "Disappointing and waste of time",
]
labels = [1, 0, 1, 0, 1, 0]  # 1=positive, 0=negative

# æ„å»ºè¯è¡¨
def build_vocab(texts, min_freq=1):
    word_freq = Counter()
    for text in texts:
        word_freq.update(text.lower().split())

    vocab = {'<PAD>': 0, '<UNK>': 1}
    for word, freq in word_freq.items():
        if freq >= min_freq:
            vocab[word] = len(vocab)
    return vocab

vocab = build_vocab(texts)
print(f"è¯è¡¨å¤§å°: {len(vocab)}")

# æ•°æ®é›†ç±»
class TextDataset(Dataset):
    def __init__(self, texts, labels, vocab, max_len=32):
        self.texts = texts
        self.labels = labels
        self.vocab = vocab
        self.max_len = max_len

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        text = self.texts[idx].lower().split()

        # è½¬æ¢ä¸º ID
        ids = [self.vocab.get(w, self.vocab['<UNK>']) for w in text]

        # æˆªæ–­æˆ–å¡«å……
        if len(ids) > self.max_len:
            ids = ids[:self.max_len]
        else:
            ids = ids + [self.vocab['<PAD>']] * (self.max_len - len(ids))

        return torch.tensor(ids), torch.tensor(self.labels[idx])

dataset = TextDataset(texts, labels, vocab)
loader = DataLoader(dataset, batch_size=2, shuffle=True)
```

### 5.2 æ¨¡å‹å®šä¹‰

```python
class SentimentLSTM(nn.Module):
    def __init__(self, vocab_size, embed_dim, hidden_dim, num_classes,
                 num_layers=1, dropout=0.5, pad_idx=0):
        super().__init__()

        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=pad_idx)

        self.lstm = nn.LSTM(
            embed_dim, hidden_dim, num_layers,
            batch_first=True, bidirectional=True, dropout=dropout if num_layers > 1 else 0
        )

        self.dropout = nn.Dropout(dropout)
        self.fc = nn.Linear(hidden_dim * 2, num_classes)

    def forward(self, x):
        # x: [batch, seq_len]
        embedded = self.embedding(x)  # [batch, seq_len, embed_dim]
        embedded = self.dropout(embedded)

        output, (h_n, c_n) = self.lstm(embedded)

        # æ‹¼æ¥åŒå‘æœ€ç»ˆéšè—çŠ¶æ€
        h_concat = torch.cat([h_n[-2], h_n[-1]], dim=1)
        h_concat = self.dropout(h_concat)

        out = self.fc(h_concat)
        return out

# åˆ›å»ºæ¨¡å‹
model = SentimentLSTM(
    vocab_size=len(vocab),
    embed_dim=64,
    hidden_dim=32,
    num_classes=2,
    num_layers=1,
    dropout=0.3
)

print(model)
```

### 5.3 è®­ç»ƒä¸è¯„ä¼°

```python
import torch.optim as optim

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model = model.to(device)

criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

# è®­ç»ƒ
def train_epoch(model, loader, criterion, optimizer, device):
    model.train()
    total_loss = 0
    correct = 0
    total = 0

    for texts, labels in loader:
        texts, labels = texts.to(device), labels.to(device)

        optimizer.zero_grad()
        outputs = model(texts)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        total_loss += loss.item()
        _, predicted = outputs.max(1)
        total += labels.size(0)
        correct += predicted.eq(labels).sum().item()

    return total_loss / len(loader), correct / total

# è®­ç»ƒå¾ªç¯
num_epochs = 50
for epoch in range(num_epochs):
    loss, acc = train_epoch(model, loader, criterion, optimizer, device)
    if (epoch + 1) % 10 == 0:
        print(f'Epoch {epoch+1}: Loss={loss:.4f}, Acc={acc:.4f}')

# æ¨ç†
model.eval()
test_text = "this movie is great"
test_ids = [vocab.get(w, vocab['<UNK>']) for w in test_text.split()]
test_ids = test_ids + [vocab['<PAD>']] * (32 - len(test_ids))
test_tensor = torch.tensor([test_ids]).to(device)

with torch.no_grad():
    output = model(test_tensor)
    pred = output.argmax(1).item()
    print(f"'{test_text}' -> {'Positive' if pred == 1 else 'Negative'}")
```

### 5.4 å¤„ç†å˜é•¿åºåˆ—

```python
from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence

def collate_fn(batch):
    """å¤„ç†å˜é•¿åºåˆ—çš„ collate å‡½æ•°"""
    texts, labels = zip(*batch)

    # è·å–å®é™…é•¿åº¦
    lengths = torch.tensor([len(t) for t in texts])

    # å¡«å……
    texts_padded = pad_sequence(texts, batch_first=True, padding_value=0)
    labels = torch.tensor(labels)

    return texts_padded, labels, lengths

class LSTMWithPacking(nn.Module):
    """ä½¿ç”¨ packing å¤„ç†å˜é•¿åºåˆ—"""
    def __init__(self, vocab_size, embed_dim, hidden_dim, num_classes):
        super().__init__()
        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)
        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True, bidirectional=True)
        self.fc = nn.Linear(hidden_dim * 2, num_classes)

    def forward(self, x, lengths):
        embedded = self.embedding(x)

        # Packï¼ˆå»æ‰å¡«å……ï¼Œæé«˜æ•ˆç‡ï¼‰
        packed = pack_padded_sequence(embedded, lengths.cpu(),
                                       batch_first=True, enforce_sorted=False)

        output, (h_n, c_n) = self.lstm(packed)

        # Unpackï¼ˆå¯é€‰ï¼Œå¦‚æœéœ€è¦æ‰€æœ‰æ—¶é—´æ­¥çš„è¾“å‡ºï¼‰
        # output, _ = pad_packed_sequence(output, batch_first=True)

        h_concat = torch.cat([h_n[-2], h_n[-1]], dim=1)
        return self.fc(h_concat)
```

---

## 6. ç»ƒä¹ é¢˜

### åŸºç¡€ç»ƒä¹ 

1. æ‰‹å†™ GRU Cell
2. ç”¨ LSTM åšæ—¶é—´åºåˆ—é¢„æµ‹ï¼ˆå¦‚æ­£å¼¦æ³¢ï¼‰
3. æ¯”è¾ƒ RNNã€LSTMã€GRU åœ¨åŒä¸€ä»»åŠ¡ä¸Šçš„è¡¨ç°

### å‚è€ƒç­”æ¡ˆ

<details>
<summary>ç‚¹å‡»æŸ¥çœ‹ç­”æ¡ˆ</summary>

```python
# 1. æ‰‹å†™ GRU Cell
class GRUCell(nn.Module):
    def __init__(self, input_size, hidden_size):
        super().__init__()
        self.hidden_size = hidden_size

        # é‡ç½®é—¨å’Œæ›´æ–°é—¨
        self.gate = nn.Linear(input_size + hidden_size, 2 * hidden_size)
        # å€™é€‰éšè—çŠ¶æ€
        self.candidate = nn.Linear(input_size + hidden_size, hidden_size)

    def forward(self, x, h_prev):
        combined = torch.cat([x, h_prev], dim=1)

        # è®¡ç®—é—¨
        gates = torch.sigmoid(self.gate(combined))
        r, z = gates.chunk(2, dim=1)  # é‡ç½®é—¨ã€æ›´æ–°é—¨

        # è®¡ç®—å€™é€‰éšè—çŠ¶æ€
        combined_reset = torch.cat([x, r * h_prev], dim=1)
        h_candidate = torch.tanh(self.candidate(combined_reset))

        # æ›´æ–°éšè—çŠ¶æ€
        h_new = (1 - z) * h_prev + z * h_candidate

        return h_new

# æµ‹è¯•
cell = GRUCell(10, 20)
x = torch.randn(32, 10)
h = torch.zeros(32, 20)
h_new = cell(x, h)
print(f"GRU Cell: {h_new.shape}")


# 2. æ—¶é—´åºåˆ—é¢„æµ‹
import numpy as np

# ç”Ÿæˆæ­£å¼¦æ³¢æ•°æ®
t = np.linspace(0, 100, 1000)
data = np.sin(t)

# å‡†å¤‡æ•°æ®
def create_sequences(data, seq_len):
    X, y = [], []
    for i in range(len(data) - seq_len):
        X.append(data[i:i+seq_len])
        y.append(data[i+seq_len])
    return torch.tensor(X, dtype=torch.float32).unsqueeze(-1), \
           torch.tensor(y, dtype=torch.float32)

seq_len = 50
X, y = create_sequences(data, seq_len)
print(f"X: {X.shape}, y: {y.shape}")

# ç®€å•çš„ LSTM é¢„æµ‹å™¨
class TimeSeriesLSTM(nn.Module):
    def __init__(self):
        super().__init__()
        self.lstm = nn.LSTM(1, 32, batch_first=True)
        self.fc = nn.Linear(32, 1)

    def forward(self, x):
        out, _ = self.lstm(x)
        return self.fc(out[:, -1, :])

model = TimeSeriesLSTM()
criterion = nn.MSELoss()
optimizer = optim.Adam(model.parameters(), lr=0.01)

# è®­ç»ƒ
for epoch in range(100):
    pred = model(X[:100])
    loss = criterion(pred.squeeze(), y[:100])

    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

    if (epoch + 1) % 20 == 0:
        print(f'Epoch {epoch+1}: Loss={loss.item():.6f}')
```

</details>

---

## â¡ï¸ ä¸‹ä¸€æ­¥

å­¦å®Œæœ¬èŠ‚åï¼Œç»§ç»­å­¦ä¹  [09-è®­ç»ƒæŠ€å·§ä¸å¯è§†åŒ–.md](./09-è®­ç»ƒæŠ€å·§ä¸å¯è§†åŒ–.md)

