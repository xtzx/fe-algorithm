# ğŸ–¼ï¸ é¡¹ç›®ï¼šCIFAR-10 å›¾åƒåˆ†ç±»

> å®Œæ•´çš„å›¾åƒåˆ†ç±»é¡¹ç›®ï¼šä»æ•°æ®åŠ è½½åˆ°æ¨¡å‹è®­ç»ƒã€è¯„ä¼°å’Œå¯è§†åŒ–

---

## ç›®å½•

1. [é¡¹ç›®æ¦‚è¿°](#1-é¡¹ç›®æ¦‚è¿°)
2. [æ•°æ®å‡†å¤‡](#2-æ•°æ®å‡†å¤‡)
3. [è‡ªå®šä¹‰ CNN](#3-è‡ªå®šä¹‰-cnn)
4. [è¿ç§»å­¦ä¹ ](#4-è¿ç§»å­¦ä¹ )
5. [è®­ç»ƒä¸è¯„ä¼°](#5-è®­ç»ƒä¸è¯„ä¼°)
6. [ç»“æœå¯è§†åŒ–](#6-ç»“æœå¯è§†åŒ–)
7. [ä¼˜åŒ–æ–¹å‘](#7-ä¼˜åŒ–æ–¹å‘)

---

## 1. é¡¹ç›®æ¦‚è¿°

### 1.1 ä»»åŠ¡è¯´æ˜

```
æ•°æ®é›†ï¼šCIFAR-10
- 10 ä¸ªç±»åˆ«ï¼šé£æœºã€æ±½è½¦ã€é¸Ÿã€çŒ«ã€é¹¿ã€ç‹—ã€é’è›™ã€é©¬ã€èˆ¹ã€å¡è½¦
- è®­ç»ƒé›†ï¼š50,000 å¼  32x32 å½©è‰²å›¾åƒ
- æµ‹è¯•é›†ï¼š10,000 å¼ 

ç›®æ ‡ï¼šè®­ç»ƒæ¨¡å‹å‡†ç¡®åˆ†ç±»è¿™äº›å›¾åƒ

æ–¹æ¡ˆï¼š
1. è‡ªå®šä¹‰å°å‹ CNN
2. é¢„è®­ç»ƒ ResNet è¿ç§»å­¦ä¹ 
```

### 1.2 é¡¹ç›®ç»“æ„

```
cifar10_project/
â”œâ”€â”€ data/              # æ•°æ®ç›®å½•
â”œâ”€â”€ models/            # æ¨¡å‹å®šä¹‰
â”œâ”€â”€ utils/             # å·¥å…·å‡½æ•°
â”œâ”€â”€ train.py           # è®­ç»ƒè„šæœ¬
â”œâ”€â”€ evaluate.py        # è¯„ä¼°è„šæœ¬
â””â”€â”€ checkpoints/       # ä¿å­˜çš„æ¨¡å‹
```

---

## 2. æ•°æ®å‡†å¤‡

```python
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, random_split
from torchvision import datasets, transforms
import matplotlib.pyplot as plt
import numpy as np

# ========== æ•°æ®å¢å¼º ==========
train_transform = transforms.Compose([
    transforms.RandomCrop(32, padding=4),
    transforms.RandomHorizontalFlip(),
    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),
    transforms.ToTensor(),
    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616))
])

test_transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616))
])

# ========== åŠ è½½æ•°æ® ==========
train_dataset = datasets.CIFAR10(
    root='./data', train=True, download=True, transform=train_transform
)
test_dataset = datasets.CIFAR10(
    root='./data', train=False, download=True, transform=test_transform
)

# åˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›†
train_size = int(0.9 * len(train_dataset))
val_size = len(train_dataset) - train_size
train_dataset, val_dataset = random_split(
    train_dataset, [train_size, val_size],
    generator=torch.Generator().manual_seed(42)
)

# æ³¨æ„ï¼šéªŒè¯é›†åº”è¯¥ç”¨ test_transform
# è¿™é‡Œç®€åŒ–å¤„ç†ï¼Œå®é™…é¡¹ç›®ä¸­å»ºè®®ç”¨ Subset é‡æ–°å°è£…

# ========== DataLoader ==========
train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=2)
val_loader = DataLoader(val_dataset, batch_size=256, shuffle=False, num_workers=2)
test_loader = DataLoader(test_dataset, batch_size=256, shuffle=False, num_workers=2)

# ========== å¯è§†åŒ–æ ·æœ¬ ==========
classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')

def imshow(img, title=None):
    """æ˜¾ç¤ºå›¾åƒï¼ˆåå½’ä¸€åŒ–ï¼‰"""
    img = img.numpy().transpose((1, 2, 0))
    mean = np.array([0.4914, 0.4822, 0.4465])
    std = np.array([0.2470, 0.2435, 0.2616])
    img = std * img + mean
    img = np.clip(img, 0, 1)
    plt.imshow(img)
    if title:
        plt.title(title)

# æ˜¾ç¤ºä¸€æ‰¹æ ·æœ¬
images, labels = next(iter(train_loader))
fig, axes = plt.subplots(2, 8, figsize=(16, 4))
for i, ax in enumerate(axes.flat):
    imshow(images[i], title=classes[labels[i]])
    ax.axis('off')
plt.tight_layout()
plt.show()

print(f"è®­ç»ƒé›†: {len(train_dataset)}")
print(f"éªŒè¯é›†: {len(val_dataset)}")
print(f"æµ‹è¯•é›†: {len(test_dataset)}")
```

---

## 3. è‡ªå®šä¹‰ CNN

```python
class CIFAR10Net(nn.Module):
    """è‡ªå®šä¹‰ CNN for CIFAR-10"""
    def __init__(self, num_classes=10):
        super().__init__()

        # å·ç§¯å— 1
        self.conv1 = nn.Sequential(
            nn.Conv2d(3, 64, 3, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.Conv2d(64, 64, 3, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2, 2),  # 32 -> 16
            nn.Dropout(0.25)
        )

        # å·ç§¯å— 2
        self.conv2 = nn.Sequential(
            nn.Conv2d(64, 128, 3, padding=1),
            nn.BatchNorm2d(128),
            nn.ReLU(inplace=True),
            nn.Conv2d(128, 128, 3, padding=1),
            nn.BatchNorm2d(128),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2, 2),  # 16 -> 8
            nn.Dropout(0.25)
        )

        # å·ç§¯å— 3
        self.conv3 = nn.Sequential(
            nn.Conv2d(128, 256, 3, padding=1),
            nn.BatchNorm2d(256),
            nn.ReLU(inplace=True),
            nn.Conv2d(256, 256, 3, padding=1),
            nn.BatchNorm2d(256),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2, 2),  # 8 -> 4
            nn.Dropout(0.25)
        )

        # åˆ†ç±»å™¨
        self.classifier = nn.Sequential(
            nn.Flatten(),
            nn.Linear(256 * 4 * 4, 512),
            nn.ReLU(inplace=True),
            nn.Dropout(0.5),
            nn.Linear(512, num_classes)
        )

    def forward(self, x):
        x = self.conv1(x)
        x = self.conv2(x)
        x = self.conv3(x)
        x = self.classifier(x)
        return x

# åˆ›å»ºæ¨¡å‹
model = CIFAR10Net()

# æŸ¥çœ‹æ¨¡å‹ç»“æ„
print(model)

# å‚æ•°ç»Ÿè®¡
total_params = sum(p.numel() for p in model.parameters())
print(f"æ€»å‚æ•°é‡: {total_params:,}")

# æµ‹è¯•å‰å‘ä¼ æ’­
x = torch.randn(2, 3, 32, 32)
y = model(x)
print(f"è¾“å‡ºå½¢çŠ¶: {y.shape}")  # [2, 10]
```

---

## 4. è¿ç§»å­¦ä¹ 

```python
import torchvision.models as models

def create_resnet_model(num_classes=10, pretrained=True):
    """åˆ›å»ºé¢„è®­ç»ƒ ResNet æ¨¡å‹"""

    # åŠ è½½é¢„è®­ç»ƒ ResNet-18
    if pretrained:
        weights = models.ResNet18_Weights.IMAGENET1K_V1
    else:
        weights = None

    model = models.resnet18(weights=weights)

    # ä¿®æ”¹ç¬¬ä¸€ä¸ªå·ç§¯å±‚ï¼ˆé€‚åº” 32x32 è¾“å…¥ï¼‰
    model.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)
    model.maxpool = nn.Identity()  # ç§»é™¤ maxpool

    # ä¿®æ”¹æœ€åä¸€å±‚
    model.fc = nn.Linear(model.fc.in_features, num_classes)

    return model

def create_vit_model(num_classes=10):
    """åˆ›å»º Vision Transformerï¼ˆéœ€è¦è°ƒæ•´è¾“å…¥å¤§å°ï¼‰"""
    # ViT é€šå¸¸éœ€è¦æ›´å¤§çš„è¾“å…¥ï¼ˆ224x224ï¼‰
    # è¿™é‡Œéœ€è¦åœ¨æ•°æ®é¢„å¤„ç†ä¸­ resize

    model = models.vit_b_16(weights=models.ViT_B_16_Weights.IMAGENET1K_V1)
    model.heads.head = nn.Linear(model.heads.head.in_features, num_classes)

    return model

# åˆ›å»º ResNet æ¨¡å‹
resnet_model = create_resnet_model(num_classes=10, pretrained=True)

# å†»ç»“æ—©æœŸå±‚ï¼ˆå¯é€‰ï¼‰
def freeze_early_layers(model, num_layers_to_freeze=6):
    """å†»ç»“å‰å‡ å±‚"""
    layers = list(model.children())
    for layer in layers[:num_layers_to_freeze]:
        for param in layer.parameters():
            param.requires_grad = False

    # æ‰“å°å¯è®­ç»ƒå‚æ•°
    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)
    total = sum(p.numel() for p in model.parameters())
    print(f"å¯è®­ç»ƒå‚æ•°: {trainable:,} / {total:,} ({100*trainable/total:.1f}%)")

# freeze_early_layers(resnet_model, 6)
```

---

## 5. è®­ç»ƒä¸è¯„ä¼°

```python
import time
from tqdm import tqdm

# ========== è®¾ç½® ==========
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"ä½¿ç”¨è®¾å¤‡: {device}")

# é€‰æ‹©æ¨¡å‹
model = CIFAR10Net()  # æˆ– create_resnet_model()
model = model.to(device)

# æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
criterion = nn.CrossEntropyLoss()
optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.01)

# å­¦ä¹ ç‡è°ƒåº¦
scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=100)

# ========== è®­ç»ƒå‡½æ•° ==========
def train_one_epoch(model, loader, criterion, optimizer, device):
    model.train()
    running_loss = 0.0
    correct = 0
    total = 0

    pbar = tqdm(loader, desc='Training')
    for inputs, labels in pbar:
        inputs, labels = inputs.to(device), labels.to(device)

        optimizer.zero_grad()
        outputs = model(inputs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        running_loss += loss.item()
        _, predicted = outputs.max(1)
        total += labels.size(0)
        correct += predicted.eq(labels).sum().item()

        pbar.set_postfix({
            'loss': f'{running_loss/total*labels.size(0):.4f}',
            'acc': f'{100.*correct/total:.2f}%'
        })

    return running_loss / len(loader), correct / total

def evaluate(model, loader, criterion, device):
    model.eval()
    running_loss = 0.0
    correct = 0
    total = 0

    with torch.no_grad():
        for inputs, labels in loader:
            inputs, labels = inputs.to(device), labels.to(device)
            outputs = model(inputs)
            loss = criterion(outputs, labels)

            running_loss += loss.item()
            _, predicted = outputs.max(1)
            total += labels.size(0)
            correct += predicted.eq(labels).sum().item()

    return running_loss / len(loader), correct / total

# ========== è®­ç»ƒå¾ªç¯ ==========
num_epochs = 50
best_val_acc = 0
train_losses, val_losses = [], []
train_accs, val_accs = [], []

print(f"\nå¼€å§‹è®­ç»ƒï¼Œå…± {num_epochs} ä¸ª epoch")
print("=" * 60)

for epoch in range(num_epochs):
    start_time = time.time()

    # è®­ç»ƒ
    train_loss, train_acc = train_one_epoch(model, train_loader, criterion, optimizer, device)

    # éªŒè¯
    val_loss, val_acc = evaluate(model, val_loader, criterion, device)

    # æ›´æ–°å­¦ä¹ ç‡
    scheduler.step()

    # è®°å½•
    train_losses.append(train_loss)
    val_losses.append(val_loss)
    train_accs.append(train_acc)
    val_accs.append(val_acc)

    # ä¿å­˜æœ€ä½³æ¨¡å‹
    if val_acc > best_val_acc:
        best_val_acc = val_acc
        torch.save(model.state_dict(), 'best_model.pth')
        print(f"  âœ“ ä¿å­˜æœ€ä½³æ¨¡å‹ï¼ŒéªŒè¯å‡†ç¡®ç‡: {val_acc:.4f}")

    elapsed = time.time() - start_time

    print(f"Epoch {epoch+1:3d}/{num_epochs} | "
          f"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f} | "
          f"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f} | "
          f"LR: {scheduler.get_last_lr()[0]:.6f} | Time: {elapsed:.1f}s")

print("=" * 60)
print(f"è®­ç»ƒå®Œæˆï¼æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {best_val_acc:.4f}")

# ========== æµ‹è¯• ==========
model.load_state_dict(torch.load('best_model.pth'))
test_loss, test_acc = evaluate(model, test_loader, criterion, device)
print(f"æµ‹è¯•é›†å‡†ç¡®ç‡: {test_acc:.4f}")
```

---

## 6. ç»“æœå¯è§†åŒ–

```python
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix, classification_report

# ========== è®­ç»ƒæ›²çº¿ ==========
fig, axes = plt.subplots(1, 2, figsize=(14, 5))

# Loss æ›²çº¿
axes[0].plot(train_losses, label='Train', linewidth=2)
axes[0].plot(val_losses, label='Validation', linewidth=2)
axes[0].set_xlabel('Epoch')
axes[0].set_ylabel('Loss')
axes[0].set_title('Loss Curves')
axes[0].legend()
axes[0].grid(True, alpha=0.3)

# Accuracy æ›²çº¿
axes[1].plot(train_accs, label='Train', linewidth=2)
axes[1].plot(val_accs, label='Validation', linewidth=2)
axes[1].set_xlabel('Epoch')
axes[1].set_ylabel('Accuracy')
axes[1].set_title('Accuracy Curves')
axes[1].legend()
axes[1].grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig('training_curves.png', dpi=150)
plt.show()

# ========== æ··æ·†çŸ©é˜µ ==========
def get_predictions(model, loader, device):
    model.eval()
    all_preds = []
    all_labels = []

    with torch.no_grad():
        for inputs, labels in loader:
            inputs = inputs.to(device)
            outputs = model(inputs)
            _, preds = outputs.max(1)
            all_preds.extend(preds.cpu().numpy())
            all_labels.extend(labels.numpy())

    return np.array(all_preds), np.array(all_labels)

predictions, labels = get_predictions(model, test_loader, device)

# æ··æ·†çŸ©é˜µ
cm = confusion_matrix(labels, predictions)

plt.figure(figsize=(10, 8))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=classes, yticklabels=classes)
plt.xlabel('Predicted')
plt.ylabel('True')
plt.title('Confusion Matrix')
plt.tight_layout()
plt.savefig('confusion_matrix.png', dpi=150)
plt.show()

# åˆ†ç±»æŠ¥å‘Š
print("\nåˆ†ç±»æŠ¥å‘Š:")
print(classification_report(labels, predictions, target_names=classes))

# ========== å¯è§†åŒ–é¢„æµ‹ç»“æœ ==========
def visualize_predictions(model, loader, device, num_images=16):
    model.eval()
    images, labels = next(iter(loader))
    images, labels = images[:num_images], labels[:num_images]

    with torch.no_grad():
        outputs = model(images.to(device))
        probs = torch.softmax(outputs, dim=1)
        _, preds = outputs.max(1)

    fig, axes = plt.subplots(4, 4, figsize=(12, 12))
    for i, ax in enumerate(axes.flat):
        # åå½’ä¸€åŒ–æ˜¾ç¤ºå›¾åƒ
        img = images[i].numpy().transpose((1, 2, 0))
        mean = np.array([0.4914, 0.4822, 0.4465])
        std = np.array([0.2470, 0.2435, 0.2616])
        img = std * img + mean
        img = np.clip(img, 0, 1)

        ax.imshow(img)

        pred_label = classes[preds[i]]
        true_label = classes[labels[i]]
        confidence = probs[i][preds[i]].item()

        color = 'green' if preds[i] == labels[i] else 'red'
        ax.set_title(f'Pred: {pred_label}\nTrue: {true_label}\nConf: {confidence:.2f}',
                     color=color, fontsize=10)
        ax.axis('off')

    plt.tight_layout()
    plt.savefig('predictions.png', dpi=150)
    plt.show()

visualize_predictions(model, test_loader, device)

# ========== é”™è¯¯åˆ†æ ==========
def analyze_errors(model, loader, device, num_errors=16):
    """åˆ†æåˆ†ç±»é”™è¯¯çš„æ ·æœ¬"""
    model.eval()
    errors = []

    with torch.no_grad():
        for images, labels in loader:
            outputs = model(images.to(device))
            probs = torch.softmax(outputs, dim=1)
            _, preds = outputs.max(1)

            for i in range(len(labels)):
                if preds[i] != labels[i]:
                    errors.append({
                        'image': images[i],
                        'true': labels[i].item(),
                        'pred': preds[i].item(),
                        'confidence': probs[i][preds[i]].item()
                    })

                    if len(errors) >= num_errors:
                        break

            if len(errors) >= num_errors:
                break

    # å¯è§†åŒ–é”™è¯¯æ ·æœ¬
    fig, axes = plt.subplots(4, 4, figsize=(12, 12))
    for i, ax in enumerate(axes.flat):
        if i < len(errors):
            error = errors[i]
            img = error['image'].numpy().transpose((1, 2, 0))
            mean = np.array([0.4914, 0.4822, 0.4465])
            std = np.array([0.2470, 0.2435, 0.2616])
            img = std * img + mean
            img = np.clip(img, 0, 1)

            ax.imshow(img)
            ax.set_title(f"True: {classes[error['true']]}\n"
                        f"Pred: {classes[error['pred']]}\n"
                        f"Conf: {error['confidence']:.2f}",
                        color='red', fontsize=10)
        ax.axis('off')

    plt.suptitle('Classification Errors', fontsize=14)
    plt.tight_layout()
    plt.savefig('errors.png', dpi=150)
    plt.show()

analyze_errors(model, test_loader, device)
```

---

## 7. ä¼˜åŒ–æ–¹å‘

### 7.1 æå‡å‡†ç¡®ç‡

```python
# 1. æ›´å¼ºçš„æ•°æ®å¢å¼º
advanced_transform = transforms.Compose([
    transforms.RandomCrop(32, padding=4),
    transforms.RandomHorizontalFlip(),
    transforms.AutoAugment(transforms.AutoAugmentPolicy.CIFAR10),
    transforms.ToTensor(),
    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616)),
    transforms.RandomErasing(p=0.5)
])

# 2. Label Smoothing
criterion = nn.CrossEntropyLoss(label_smoothing=0.1)

# 3. MixUp
def mixup_data(x, y, alpha=0.2):
    lam = np.random.beta(alpha, alpha)
    batch_size = x.size(0)
    index = torch.randperm(batch_size).to(x.device)
    mixed_x = lam * x + (1 - lam) * x[index, :]
    y_a, y_b = y, y[index]
    return mixed_x, y_a, y_b, lam

# 4. æ›´æ·±çš„æ¨¡å‹
model = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)

# 5. é›†æˆå­¦ä¹ 
def ensemble_predict(models, x):
    outputs = [model(x) for model in models]
    return torch.stack(outputs).mean(dim=0)
```

### 7.2 æœŸæœ›æ•ˆæœ

```
æ¨¡å‹                    CIFAR-10 æµ‹è¯•å‡†ç¡®ç‡
--------------------------------------------
è‡ªå®šä¹‰å° CNN            ~85%
ResNet-18 (é¢„è®­ç»ƒ)      ~92%
ResNet-50 (é¢„è®­ç»ƒ)      ~94%
+ é«˜çº§æ•°æ®å¢å¼º          ~95%
+ MixUp/CutMix         ~96%
```

### 7.3 å®Œæ•´è®­ç»ƒè„šæœ¬

```python
#!/usr/bin/env python3
"""CIFAR-10 è®­ç»ƒè„šæœ¬"""

import argparse
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from torchvision import datasets, transforms
import torchvision.models as models

def main(args):
    # è®¾å¤‡
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

    # æ•°æ®
    transform_train = transforms.Compose([
        transforms.RandomCrop(32, padding=4),
        transforms.RandomHorizontalFlip(),
        transforms.ToTensor(),
        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616))
    ])

    transform_test = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616))
    ])

    train_dataset = datasets.CIFAR10('./data', train=True, download=True, transform=transform_train)
    test_dataset = datasets.CIFAR10('./data', train=False, transform=transform_test)

    train_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True, num_workers=4)
    test_loader = DataLoader(test_dataset, batch_size=args.batch_size * 2, shuffle=False, num_workers=4)

    # æ¨¡å‹
    if args.model == 'custom':
        model = CIFAR10Net()
    else:
        model = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)
        model.conv1 = nn.Conv2d(3, 64, 3, 1, 1, bias=False)
        model.maxpool = nn.Identity()
        model.fc = nn.Linear(512, 10)

    model = model.to(device)

    # è®­ç»ƒ
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.AdamW(model.parameters(), lr=args.lr, weight_decay=0.01)
    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=args.epochs)

    best_acc = 0
    for epoch in range(args.epochs):
        model.train()
        for inputs, labels in train_loader:
            inputs, labels = inputs.to(device), labels.to(device)
            optimizer.zero_grad()
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()

        scheduler.step()

        # è¯„ä¼°
        model.eval()
        correct = 0
        total = 0
        with torch.no_grad():
            for inputs, labels in test_loader:
                inputs, labels = inputs.to(device), labels.to(device)
                outputs = model(inputs)
                _, predicted = outputs.max(1)
                total += labels.size(0)
                correct += predicted.eq(labels).sum().item()

        acc = correct / total
        print(f'Epoch {epoch+1}/{args.epochs}: Test Acc = {acc:.4f}')

        if acc > best_acc:
            best_acc = acc
            torch.save(model.state_dict(), 'best_model.pth')

    print(f'Best accuracy: {best_acc:.4f}')

if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument('--model', type=str, default='resnet', choices=['custom', 'resnet'])
    parser.add_argument('--epochs', type=int, default=100)
    parser.add_argument('--batch_size', type=int, default=128)
    parser.add_argument('--lr', type=float, default=0.001)
    args = parser.parse_args()
    main(args)
```

---

## â¡ï¸ ä¸‹ä¸€æ­¥

å®Œæˆå›¾åƒåˆ†ç±»é¡¹ç›®åï¼Œç»§ç»­å­¦ä¹  [11-é¡¹ç›®-æ–‡æœ¬æƒ…æ„Ÿåˆ†æ.md](./11-é¡¹ç›®-æ–‡æœ¬æƒ…æ„Ÿåˆ†æ.md)

