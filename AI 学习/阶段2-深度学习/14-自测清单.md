# âœ… 14 - é˜¶æ®µ 2 è‡ªæµ‹æ¸…å•

> å®Œæˆæœ¬é˜¶æ®µå­¦ä¹ åï¼Œä½ åº”è¯¥èƒ½å¤Ÿå›ç­”ä»¥ä¸‹é—®é¢˜å¹¶å®Œæˆç¼–ç¨‹ä»»åŠ¡

---

## æ¦‚å¿µç†è§£ï¼ˆ10 é¢˜ï¼‰

### 1. æ¿€æ´»å‡½æ•°

**é—®é¢˜**ï¼šä¸ºä»€ä¹ˆç¥ç»ç½‘ç»œéœ€è¦æ¿€æ´»å‡½æ•°ï¼Ÿå¦‚æœæ²¡æœ‰æ¿€æ´»å‡½æ•°ä¼šæ€æ ·ï¼Ÿ

<details>
<summary>å‚è€ƒç­”æ¡ˆ</summary>

æ²¡æœ‰æ¿€æ´»å‡½æ•°ï¼Œå¤šå±‚ç¥ç»ç½‘ç»œå°±ç­‰ä»·äºå•å±‚çº¿æ€§å˜æ¢ï¼š
- `h1 = W1 @ x + b1`
- `h2 = W2 @ h1 + b2 = (W2 @ W1) @ x + (W2 @ b1 + b2)`

è¿™å°±æ˜¯ä¸€ä¸ªçº¿æ€§å‡½æ•°ï¼Œæ— æ³•å­¦ä¹ éçº¿æ€§æ¨¡å¼ã€‚æ¿€æ´»å‡½æ•°å¼•å…¥éçº¿æ€§ï¼Œè®©ç½‘ç»œèƒ½å¤Ÿæ‹Ÿåˆå¤æ‚å‡½æ•°ã€‚

å¸¸ç”¨æ¿€æ´»å‡½æ•°ï¼š
- ReLUï¼šç®€å•é«˜æ•ˆï¼Œé»˜è®¤é€‰æ‹©
- GELU/SiLUï¼šTransformer ä¸­å¸¸ç”¨ï¼Œå¹³æ»‘ç‰ˆ ReLU
- Sigmoidï¼šç”¨äºäºŒåˆ†ç±»è¾“å‡ºå±‚
- Softmaxï¼šç”¨äºå¤šåˆ†ç±»è¾“å‡ºå±‚

</details>

---

### 2. BatchNorm vs LayerNorm

**é—®é¢˜**ï¼šBatchNorm å’Œ LayerNorm çš„åŒºåˆ«æ˜¯ä»€ä¹ˆï¼Ÿå„è‡ªé€‚ç”¨äºä»€ä¹ˆåœºæ™¯ï¼Ÿ

<details>
<summary>å‚è€ƒç­”æ¡ˆ</summary>

| ç‰¹æ€§ | BatchNorm | LayerNorm |
|------|-----------|-----------|
| å½’ä¸€åŒ–ç»´åº¦ | åœ¨ batch ç»´åº¦ä¸Š | åœ¨ feature ç»´åº¦ä¸Š |
| è®­ç»ƒ/æ¨ç† | è¡Œä¸ºä¸åŒï¼ˆæ¨ç†ç”¨ç§»åŠ¨å¹³å‡ï¼‰ | è¡Œä¸ºä¸€è‡´ |
| batch size ä¾èµ– | æ˜¯ï¼ˆå° batch ä¸ç¨³å®šï¼‰ | å¦ |
| å¸¸ç”¨åœºæ™¯ | CNN | Transformer, RNN |

```python
# BatchNorm: å¯¹æ¯ä¸ª featureï¼Œåœ¨ batch ä¸Šè®¡ç®—å‡å€¼æ–¹å·®
x = torch.randn(32, 64)  # [batch, features]
bn = nn.BatchNorm1d(64)
# å¯¹ 32 ä¸ªæ ·æœ¬è®¡ç®—æ¯ä¸ª feature çš„å‡å€¼æ–¹å·®

# LayerNorm: å¯¹æ¯ä¸ªæ ·æœ¬ï¼Œåœ¨ feature ä¸Šè®¡ç®—å‡å€¼æ–¹å·®
ln = nn.LayerNorm(64)
# å¯¹ 64 ä¸ª feature è®¡ç®—æ¯ä¸ªæ ·æœ¬çš„å‡å€¼æ–¹å·®
```

</details>

---

### 3. æ¢¯åº¦æ¶ˆå¤±å’Œçˆ†ç‚¸

**é—®é¢˜**ï¼šä»€ä¹ˆæ˜¯æ¢¯åº¦æ¶ˆå¤±å’Œæ¢¯åº¦çˆ†ç‚¸ï¼Ÿå¦‚ä½•è§£å†³ï¼Ÿ

<details>
<summary>å‚è€ƒç­”æ¡ˆ</summary>

**æ¢¯åº¦æ¶ˆå¤±**ï¼šæ·±å±‚ç½‘ç»œä¸­ï¼Œæ¢¯åº¦åœ¨åå‘ä¼ æ’­è¿‡ç¨‹ä¸­é€å±‚å˜å°ï¼Œå¯¼è‡´å‰é¢çš„å±‚å‡ ä¹æ— æ³•æ›´æ–°ã€‚
- åŸå› ï¼šæ¿€æ´»å‡½æ•°ï¼ˆå¦‚ sigmoidï¼‰å¯¼æ•° < 1ï¼Œè¿ä¹˜åè¶‹è¿‘äº 0

**æ¢¯åº¦çˆ†ç‚¸**ï¼šæ¢¯åº¦åœ¨åå‘ä¼ æ’­è¿‡ç¨‹ä¸­é€å±‚å˜å¤§ï¼Œå¯¼è‡´å‚æ•°æ›´æ–°è¿‡çŒ›ï¼Œè®­ç»ƒä¸ç¨³å®šã€‚
- åŸå› ï¼šæƒé‡åˆå§‹åŒ–è¿‡å¤§æˆ–æ¿€æ´»å‡½æ•°é—®é¢˜

**è§£å†³æ–¹æ¡ˆ**ï¼š
1. ä½¿ç”¨ ReLU æ¿€æ´»å‡½æ•°ï¼ˆæ­£åŒºé—´æ¢¯åº¦æ’ä¸º 1ï¼‰
2. æ®‹å·®è¿æ¥ï¼ˆResNetï¼‰
3. æ¢¯åº¦è£å‰ªï¼ˆ`clip_grad_norm_`ï¼‰
4. åˆé€‚çš„æƒé‡åˆå§‹åŒ–ï¼ˆKaiming, Xavierï¼‰
5. ä½¿ç”¨ BatchNorm/LayerNorm

</details>

---

### 4. æ®‹å·®è¿æ¥

**é—®é¢˜**ï¼šè§£é‡Š ResNet ä¸­æ®‹å·®è¿æ¥çš„åŸç†å’Œä½œç”¨ã€‚

<details>
<summary>å‚è€ƒç­”æ¡ˆ</summary>

**é—®é¢˜**ï¼šç½‘ç»œè¶Šæ·±ï¼Œè®­ç»ƒè¶Šéš¾ï¼Œç”šè‡³å‡†ç¡®ç‡åè€Œä¸‹é™ï¼ˆé€€åŒ–é—®é¢˜ï¼‰ã€‚

**æ®‹å·®è¿æ¥çš„æ€æƒ³**ï¼š
- ä¸ç›´æ¥å­¦ä¹ ç›®æ ‡å‡½æ•° H(x)ï¼Œè€Œæ˜¯å­¦ä¹ æ®‹å·® F(x) = H(x) - x
- è¾“å‡ºå˜æˆ H(x) = F(x) + x

**ä¸ºä»€ä¹ˆæœ‰æ•ˆ**ï¼š
- å¦‚æœæ–°å¢çš„å±‚æ˜¯"å¤šä½™çš„"ï¼Œç½‘ç»œåªéœ€è¦è®© F(x) = 0 å³å¯ä¿æŒæ’ç­‰æ˜ å°„
- å­¦ä¹ "ä»€ä¹ˆéƒ½ä¸åš"æ¯”å­¦ä¹ å¤æ‚çš„æ’ç­‰æ˜ å°„å®¹æ˜“å¾—å¤š
- æ¢¯åº¦å¯ä»¥ç›´æ¥é€šè¿‡ shortcut ä¼ æ’­ï¼Œç¼“è§£æ¢¯åº¦æ¶ˆå¤±

```python
class ResidualBlock(nn.Module):
    def forward(self, x):
        residual = x
        out = self.conv1(x)
        out = self.conv2(out)
        out = out + residual  # æ®‹å·®è¿æ¥
        return F.relu(out)
```

</details>

---

### 5. Vision Transformer

**é—®é¢˜**ï¼šViT å¦‚ä½•å°†å›¾åƒè½¬æ¢ä¸º Transformer å¯ä»¥å¤„ç†çš„åºåˆ—ï¼Ÿ

<details>
<summary>å‚è€ƒç­”æ¡ˆ</summary>

**æ­¥éª¤**ï¼š
1. **Patch Embedding**ï¼šå°†å›¾åƒåˆ†æˆå›ºå®šå¤§å°çš„ patchï¼ˆå¦‚ 16x16ï¼‰
   - 224x224 å›¾åƒ â†’ 14x14 = 196 ä¸ª patch

2. **çº¿æ€§æŠ•å½±**ï¼šæ¯ä¸ª patch å±•å¹³åé€šè¿‡çº¿æ€§å±‚æ˜ å°„åˆ° embedding ç»´åº¦
   - 16x16x3 = 768 ç»´ â†’ D ç»´

3. **æ·»åŠ ä½ç½®ç¼–ç **ï¼šå› ä¸º Transformer ä¸æ„ŸçŸ¥ä½ç½®ï¼Œéœ€è¦åŠ ä½ç½®ä¿¡æ¯

4. **æ·»åŠ  [CLS] token**ï¼šç”¨äºåˆ†ç±»ä»»åŠ¡çš„ç‰¹æ®Š token

5. **é€å…¥ Transformer Encoder**

6. **åˆ†ç±»å¤´**ï¼šå– [CLS] token çš„è¾“å‡ºåšåˆ†ç±»

```python
# æ ¸å¿ƒä»£ç 
patches = self.patch_embed(image)  # [B, 196, D]
cls_token = self.cls_token.expand(B, -1, -1)  # [B, 1, D]
x = torch.cat([cls_token, patches], dim=1)  # [B, 197, D]
x = x + self.pos_embed  # åŠ ä½ç½®ç¼–ç 
x = self.transformer(x)
out = self.head(x[:, 0])  # å– [CLS] token
```

</details>

---

### 6. LSTM é—¨æœºåˆ¶

**é—®é¢˜**ï¼šLSTM æœ‰å“ªäº›é—¨ï¼Ÿå„è‡ªçš„ä½œç”¨æ˜¯ä»€ä¹ˆï¼Ÿ

<details>
<summary>å‚è€ƒç­”æ¡ˆ</summary>

LSTM æœ‰ä¸‰ä¸ªé—¨ + ä¸€ä¸ªç»†èƒçŠ¶æ€ï¼š

1. **é—å¿˜é—¨ (Forget Gate)**ï¼šå†³å®šä¸¢å¼ƒå¤šå°‘æ—§ä¿¡æ¯
   - `f = sigmoid(W_f @ [h_{t-1}, x_t])`
   - è¾“å‡º 0-1 ä¹‹é—´ï¼Œ0 è¡¨ç¤ºå®Œå…¨ä¸¢å¼ƒï¼Œ1 è¡¨ç¤ºå®Œå…¨ä¿ç•™

2. **è¾“å…¥é—¨ (Input Gate)**ï¼šå†³å®šæ·»åŠ å¤šå°‘æ–°ä¿¡æ¯
   - `i = sigmoid(W_i @ [h_{t-1}, x_t])`
   - å€™é€‰å€¼ï¼š`g = tanh(W_g @ [h_{t-1}, x_t])`

3. **è¾“å‡ºé—¨ (Output Gate)**ï¼šå†³å®šè¾“å‡ºå¤šå°‘ä¿¡æ¯
   - `o = sigmoid(W_o @ [h_{t-1}, x_t])`

4. **ç»†èƒçŠ¶æ€ (Cell State)**ï¼šé•¿æœŸè®°å¿†
   - `c_t = f * c_{t-1} + i * g`
   - `h_t = o * tanh(c_t)`

**ä½œç”¨**ï¼šé€šè¿‡é—¨æ§æœºåˆ¶ï¼ŒLSTM å¯ä»¥é€‰æ‹©æ€§åœ°è®°å¿†æˆ–é—å¿˜ä¿¡æ¯ï¼Œè§£å†³é•¿æœŸä¾èµ–é—®é¢˜ã€‚

</details>

---

### 7. æ··åˆç²¾åº¦è®­ç»ƒ

**é—®é¢˜**ï¼šä»€ä¹ˆæ˜¯æ··åˆç²¾åº¦è®­ç»ƒï¼Ÿä¸ºä»€ä¹ˆèƒ½åŠ é€Ÿè®­ç»ƒï¼Ÿ

<details>
<summary>å‚è€ƒç­”æ¡ˆ</summary>

**æ··åˆç²¾åº¦è®­ç»ƒ**ï¼šåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æ··åˆä½¿ç”¨ FP32 å’Œ FP16ï¼š
- å‰å‘ä¼ æ’­å’Œåå‘ä¼ æ’­ä½¿ç”¨ FP16
- å‚æ•°å­˜å‚¨å’Œæ›´æ–°ä½¿ç”¨ FP32

**åŠ é€ŸåŸå› **ï¼š
1. **æ˜¾å­˜å‡åŠ**ï¼šFP16 å ç”¨ç©ºé—´æ˜¯ FP32 çš„ä¸€åŠï¼Œå¯ä»¥ç”¨æ›´å¤§çš„ batch
2. **è®¡ç®—æ›´å¿«**ï¼šç°ä»£ GPUï¼ˆTensor Coreï¼‰å¯¹ FP16 æœ‰ç¡¬ä»¶åŠ é€Ÿ
3. **å¸¦å®½å‡å°‘**ï¼šæ•°æ®ä¼ è¾“æ›´å¿«

**GradScaler çš„ä½œç”¨**ï¼š
- FP16 æ•°å€¼èŒƒå›´å°ï¼Œæ¢¯åº¦å¯èƒ½ä¸‹æº¢ï¼ˆå˜æˆ 0ï¼‰
- GradScaler æ”¾å¤§ lossï¼Œä½¿æ¢¯åº¦åœ¨ FP16 èŒƒå›´å†…
- æ›´æ–°å‚æ•°å‰å†ç¼©æ”¾å›æ¥

```python
scaler = GradScaler()
with autocast():
    output = model(data)
    loss = criterion(output, target)
scaler.scale(loss).backward()
scaler.step(optimizer)
scaler.update()
```

</details>

---

### 8. å­¦ä¹ ç‡è°ƒåº¦

**é—®é¢˜**ï¼šä¸ºä»€ä¹ˆéœ€è¦å­¦ä¹ ç‡è°ƒåº¦ï¼ŸWarmup çš„ä½œç”¨æ˜¯ä»€ä¹ˆï¼Ÿ

<details>
<summary>å‚è€ƒç­”æ¡ˆ</summary>

**ä¸ºä»€ä¹ˆéœ€è¦å­¦ä¹ ç‡è°ƒåº¦**ï¼š
- è®­ç»ƒåˆæœŸï¼šå¤§å­¦ä¹ ç‡ â†’ å¿«é€Ÿæ”¶æ•›
- è®­ç»ƒåæœŸï¼šå°å­¦ä¹ ç‡ â†’ ç²¾ç»†è°ƒæ•´ï¼Œé¿å…éœ‡è¡

**Warmup çš„ä½œç”¨**ï¼š
- è®­ç»ƒåˆšå¼€å§‹æ—¶ï¼Œæ¨¡å‹å‚æ•°æ˜¯éšæœºçš„ï¼Œæ¢¯åº¦æ–¹å‘ä¸å‡†ç¡®
- å¤§å­¦ä¹ ç‡å¯èƒ½å¯¼è‡´å‚æ•°æ›´æ–°è¿‡çŒ›ï¼Œè®­ç»ƒä¸ç¨³å®š
- Warmup é€æ¸å¢å¤§å­¦ä¹ ç‡ï¼Œè®©æ¨¡å‹"çƒ­èº«"

**å¸¸è§ç­–ç•¥ç»„åˆ**ï¼šWarmup + Cosine Decay
```
å­¦ä¹ ç‡å˜åŒ–ï¼š
0 â†’ warmup_steps: çº¿æ€§å¢é•¿ 0 â†’ lr
warmup_steps â†’ end: ä½™å¼¦è¡°å‡ lr â†’ 0
```

</details>

---

### 9. è¿‡æ‹Ÿåˆè¯Šæ–­

**é—®é¢˜**ï¼šå¦‚ä½•ä» loss æ›²çº¿åˆ¤æ–­è¿‡æ‹Ÿåˆï¼Ÿæœ‰å“ªäº›è§£å†³æ–¹æ³•ï¼Ÿ

<details>
<summary>å‚è€ƒç­”æ¡ˆ</summary>

**è¿‡æ‹Ÿåˆç‰¹å¾**ï¼š
- Train loss æŒç»­ä¸‹é™
- Val loss å…ˆä¸‹é™åä¸Šå‡
- Train-Val gap è¶Šæ¥è¶Šå¤§

**è§£å†³æ–¹æ³•**ï¼š
1. **æ•°æ®å±‚é¢**ï¼š
   - å¢åŠ è®­ç»ƒæ•°æ®
   - æ•°æ®å¢å¼ºï¼ˆç¿»è½¬ã€è£å‰ªã€é¢œè‰²æŠ–åŠ¨ç­‰ï¼‰
   - MixUp/CutMix

2. **æ¨¡å‹å±‚é¢**ï¼š
   - å‡å°‘æ¨¡å‹å¤æ‚åº¦ï¼ˆå±‚æ•°ã€å‚æ•°ï¼‰
   - Dropout
   - BatchNorm

3. **è®­ç»ƒå±‚é¢**ï¼š
   - L1/L2 æ­£åˆ™åŒ–ï¼ˆweight_decayï¼‰
   - Early Stopping
   - Label Smoothing

4. **é›†æˆæ–¹æ³•**ï¼š
   - å¤šæ¨¡å‹é›†æˆ
   - Dropout æ¨ç†ï¼ˆMC Dropoutï¼‰

</details>

---

### 10. PyTorch è®­ç»ƒæ¨¡å¼

**é—®é¢˜**ï¼š`model.train()` å’Œ `model.eval()` çš„åŒºåˆ«æ˜¯ä»€ä¹ˆï¼Ÿä»€ä¹ˆæ—¶å€™ç”¨ï¼Ÿ

<details>
<summary>å‚è€ƒç­”æ¡ˆ</summary>

**`model.train()`**ï¼š
- è®¾ç½®æ¨¡å‹ä¸ºè®­ç»ƒæ¨¡å¼
- Dropout ç”Ÿæ•ˆï¼ˆéšæœºä¸¢å¼ƒç¥ç»å…ƒï¼‰
- BatchNorm ä½¿ç”¨å½“å‰ batch çš„ç»Ÿè®¡å€¼ï¼Œå¹¶æ›´æ–°ç§»åŠ¨å¹³å‡

**`model.eval()`**ï¼š
- è®¾ç½®æ¨¡å‹ä¸ºè¯„ä¼°æ¨¡å¼
- Dropout å…³é—­ï¼ˆæ‰€æœ‰ç¥ç»å…ƒéƒ½å‚ä¸ï¼‰
- BatchNorm ä½¿ç”¨è®­ç»ƒæ—¶ç´¯ç§¯çš„ç§»åŠ¨å¹³å‡å€¼

**ä½¿ç”¨åœºæ™¯**ï¼š
```python
# è®­ç»ƒ
model.train()
for batch in train_loader:
    ...

# éªŒè¯/æµ‹è¯•
model.eval()
with torch.no_grad():  # åŒæ—¶å…³é—­æ¢¯åº¦è®¡ç®—
    for batch in val_loader:
        ...
```

**é‡è¦**ï¼šéªŒè¯/æµ‹è¯•æ—¶ä¸€å®šè¦è°ƒç”¨ `model.eval()`ï¼Œå¦åˆ™ Dropout å’Œ BatchNorm ä¼šå¯¼è‡´ç»“æœä¸ä¸€è‡´ã€‚

</details>

---

## ç¼–ç¨‹ä»»åŠ¡ï¼ˆ3 é¢˜ï¼‰

### ä»»åŠ¡ 1ï¼šå®ç°ä¸€ä¸ªç®€å•çš„ CNN

**è¦æ±‚**ï¼šå®ç°ä¸€ä¸ª CNN åˆ†ç±» MNISTï¼ŒåŒ…å«ï¼š
- è‡³å°‘ 2 ä¸ªå·ç§¯å±‚
- ä½¿ç”¨ BatchNorm å’Œ Dropout
- å‡†ç¡®ç‡ > 99%

<details>
<summary>å‚è€ƒä»£ç </summary>

```python
import torch
import torch.nn as nn
import torch.optim as optim
from torchvision import datasets, transforms
from torch.utils.data import DataLoader

class MNISTCNN(nn.Module):
    def __init__(self):
        super().__init__()
        self.features = nn.Sequential(
            nn.Conv2d(1, 32, 3, padding=1),
            nn.BatchNorm2d(32),
            nn.ReLU(),
            nn.Conv2d(32, 64, 3, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(),
            nn.MaxPool2d(2),  # 28 -> 14
            nn.Dropout(0.25),

            nn.Conv2d(64, 128, 3, padding=1),
            nn.BatchNorm2d(128),
            nn.ReLU(),
            nn.MaxPool2d(2),  # 14 -> 7
            nn.Dropout(0.25),
        )
        self.classifier = nn.Sequential(
            nn.Flatten(),
            nn.Linear(128 * 7 * 7, 256),
            nn.ReLU(),
            nn.Dropout(0.5),
            nn.Linear(256, 10)
        )

    def forward(self, x):
        x = self.features(x)
        return self.classifier(x)

# è®­ç»ƒ
transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.1307,), (0.3081,))
])

train_loader = DataLoader(
    datasets.MNIST('./data', train=True, download=True, transform=transform),
    batch_size=64, shuffle=True
)
test_loader = DataLoader(
    datasets.MNIST('./data', train=False, transform=transform),
    batch_size=1000
)

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model = MNISTCNN().to(device)
optimizer = optim.Adam(model.parameters(), lr=0.001)
criterion = nn.CrossEntropyLoss()

for epoch in range(10):
    model.train()
    for data, target in train_loader:
        data, target = data.to(device), target.to(device)
        optimizer.zero_grad()
        output = model(data)
        loss = criterion(output, target)
        loss.backward()
        optimizer.step()

    model.eval()
    correct = 0
    with torch.no_grad():
        for data, target in test_loader:
            data, target = data.to(device), target.to(device)
            pred = model(data).argmax(1)
            correct += pred.eq(target).sum().item()

    print(f'Epoch {epoch+1}: Accuracy = {100.*correct/10000:.2f}%')
```

</details>

---

### ä»»åŠ¡ 2ï¼šå®ç°æ®‹å·®å—

**è¦æ±‚**ï¼šå®ç°ä¸€ä¸ªå¯å¤ç”¨çš„æ®‹å·®å—ï¼Œæ”¯æŒï¼š
- ç»´åº¦ä¸å˜çš„æƒ…å†µ
- ä¸‹é‡‡æ ·çš„æƒ…å†µï¼ˆstride=2ï¼‰

<details>
<summary>å‚è€ƒä»£ç </summary>

```python
class ResidualBlock(nn.Module):
    def __init__(self, in_channels, out_channels, stride=1):
        super().__init__()

        # ä¸»è·¯å¾„
        self.conv1 = nn.Conv2d(in_channels, out_channels, 3, stride, padding=1, bias=False)
        self.bn1 = nn.BatchNorm2d(out_channels)
        self.conv2 = nn.Conv2d(out_channels, out_channels, 3, 1, padding=1, bias=False)
        self.bn2 = nn.BatchNorm2d(out_channels)

        # Shortcut
        self.shortcut = nn.Sequential()
        if stride != 1 or in_channels != out_channels:
            self.shortcut = nn.Sequential(
                nn.Conv2d(in_channels, out_channels, 1, stride, bias=False),
                nn.BatchNorm2d(out_channels)
            )

    def forward(self, x):
        identity = self.shortcut(x)

        out = torch.relu(self.bn1(self.conv1(x)))
        out = self.bn2(self.conv2(out))

        out += identity
        out = torch.relu(out)

        return out

# æµ‹è¯•
block_same = ResidualBlock(64, 64)
block_down = ResidualBlock(64, 128, stride=2)

x = torch.randn(2, 64, 32, 32)
print(f"ç»´åº¦ä¸å˜: {x.shape} -> {block_same(x).shape}")  # [2, 64, 32, 32]
print(f"ä¸‹é‡‡æ ·: {x.shape} -> {block_down(x).shape}")    # [2, 128, 16, 16]
```

</details>

---

### ä»»åŠ¡ 3ï¼šå®ç°å®Œæ•´çš„è®­ç»ƒå¾ªç¯

**è¦æ±‚**ï¼šå®ç°ä¸€ä¸ªè®­ç»ƒå‡½æ•°ï¼ŒåŒ…å«ï¼š
- æ··åˆç²¾åº¦è®­ç»ƒ
- æ¢¯åº¦è£å‰ª
- å­¦ä¹ ç‡è°ƒåº¦ï¼ˆWarmup + Cosineï¼‰
- æ¨¡å‹ä¿å­˜ï¼ˆæœ€ä½³æ¨¡å‹ + checkpointï¼‰
- TensorBoard æ—¥å¿—

<details>
<summary>å‚è€ƒä»£ç </summary>

```python
import torch
import torch.nn as nn
import torch.optim as optim
from torch.cuda.amp import autocast, GradScaler
from torch.utils.tensorboard import SummaryWriter
import math

def train(model, train_loader, val_loader, num_epochs, device, save_dir='./runs'):
    model = model.to(device)

    # ä¼˜åŒ–å™¨
    optimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=0.01)
    criterion = nn.CrossEntropyLoss()
    scaler = GradScaler()

    # å­¦ä¹ ç‡è°ƒåº¦
    total_steps = num_epochs * len(train_loader)
    warmup_steps = int(0.1 * total_steps)

    def lr_lambda(step):
        if step < warmup_steps:
            return step / warmup_steps
        progress = (step - warmup_steps) / (total_steps - warmup_steps)
        return 0.5 * (1 + math.cos(math.pi * progress))

    scheduler = optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)

    # TensorBoard
    writer = SummaryWriter(save_dir)

    # è®­ç»ƒ
    best_val_acc = 0
    global_step = 0

    for epoch in range(num_epochs):
        model.train()
        train_loss = 0
        correct = 0
        total = 0

        for batch_idx, (data, target) in enumerate(train_loader):
            data, target = data.to(device), target.to(device)

            optimizer.zero_grad()

            # æ··åˆç²¾åº¦
            with autocast():
                output = model(data)
                loss = criterion(output, target)

            scaler.scale(loss).backward()

            # æ¢¯åº¦è£å‰ª
            scaler.unscale_(optimizer)
            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)

            scaler.step(optimizer)
            scaler.update()
            scheduler.step()

            # è®°å½•
            train_loss += loss.item()
            _, pred = output.max(1)
            total += target.size(0)
            correct += pred.eq(target).sum().item()

            # TensorBoard
            writer.add_scalar('Train/Loss', loss.item(), global_step)
            writer.add_scalar('Train/GradNorm', grad_norm, global_step)
            writer.add_scalar('Train/LR', scheduler.get_last_lr()[0], global_step)
            global_step += 1

        train_acc = correct / total

        # éªŒè¯
        model.eval()
        val_loss = 0
        correct = 0
        total = 0

        with torch.no_grad():
            for data, target in val_loader:
                data, target = data.to(device), target.to(device)
                output = model(data)
                val_loss += criterion(output, target).item()
                _, pred = output.max(1)
                total += target.size(0)
                correct += pred.eq(target).sum().item()

        val_acc = correct / total
        val_loss /= len(val_loader)

        # TensorBoard
        writer.add_scalar('Val/Loss', val_loss, epoch)
        writer.add_scalar('Val/Accuracy', val_acc, epoch)

        print(f'Epoch {epoch+1}/{num_epochs}: '
              f'Train Loss={train_loss/len(train_loader):.4f}, '
              f'Train Acc={train_acc:.4f}, '
              f'Val Loss={val_loss:.4f}, '
              f'Val Acc={val_acc:.4f}')

        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if val_acc > best_val_acc:
            best_val_acc = val_acc
            torch.save(model.state_dict(), f'{save_dir}/best_model.pth')

        # ä¿å­˜ checkpoint
        torch.save({
            'epoch': epoch,
            'model_state_dict': model.state_dict(),
            'optimizer_state_dict': optimizer.state_dict(),
            'scheduler_state_dict': scheduler.state_dict(),
            'best_val_acc': best_val_acc,
        }, f'{save_dir}/checkpoint.pth')

    writer.close()
    return best_val_acc
```

</details>

---

## è‡ªè¯„æ ‡å‡†

| æŒæ¡ç¨‹åº¦ | æ¦‚å¿µé¢˜ | ç¼–ç¨‹ä»»åŠ¡ |
|---------|--------|---------|
| ä¼˜ç§€ | èƒ½å›ç­” 8+ é¢˜ | èƒ½ç‹¬ç«‹å®Œæˆæ‰€æœ‰ä»»åŠ¡ |
| è‰¯å¥½ | èƒ½å›ç­” 6-7 é¢˜ | èƒ½å®Œæˆ 2 ä¸ªä»»åŠ¡ |
| åŠæ ¼ | èƒ½å›ç­” 5 é¢˜ | èƒ½å®Œæˆ 1 ä¸ªä»»åŠ¡ |
| éœ€åŠ å¼º | å°‘äº 5 é¢˜ | éœ€è¦å¤§é‡å‚è€ƒç­”æ¡ˆ |

---

## ğŸ‰ æ­å–œå®Œæˆé˜¶æ®µ 2ï¼

ä½ å·²ç»æŒæ¡äº†ï¼š
- PyTorch åŸºç¡€ï¼šTensorã€autogradã€nn.Module
- æ·±åº¦å­¦ä¹ æ ¸å¿ƒï¼šCNNã€RNNã€ResNetã€ViT
- è°ƒè¯•æŠ€å·§ï¼šç‰¹å¾å›¾å¯è§†åŒ–ã€æ¢¯åº¦åˆ†æã€è®­ç»ƒè¯Šæ–­
- è®­ç»ƒæŠ€å·§ï¼šå­¦ä¹ ç‡è°ƒåº¦ã€æ¢¯åº¦è£å‰ªã€æ··åˆç²¾åº¦
- å®æˆ˜ç»éªŒï¼šMNISTã€CIFAR-10 å›¾åƒåˆ†ç±»ã€æ–‡æœ¬æƒ…æ„Ÿåˆ†æ

---

## â¡ï¸ ä¸‹ä¸€æ­¥

è¿›å…¥ [é˜¶æ®µ 3ï¼šå¤§æ¨¡å‹æ ¸å¿ƒ](../é˜¶æ®µ3-å¤§æ¨¡å‹æ ¸å¿ƒ/00-æ¦‚è¿°.md)ï¼Œå­¦ä¹  Transformer å’Œå¤§è¯­è¨€æ¨¡å‹ï¼

