# ğŸ”¥ 03 - PyTorch åŸºç¡€ï¼šautograd è‡ªåŠ¨æ±‚å¯¼

> autograd æ˜¯ PyTorch çš„æ ¸å¿ƒï¼Œè‡ªåŠ¨è®¡ç®—æ¢¯åº¦ï¼Œè®©åå‘ä¼ æ’­å˜å¾—ç®€å•

---

## ç›®å½•

1. [è‡ªåŠ¨æ±‚å¯¼åŸºç¡€](#1-è‡ªåŠ¨æ±‚å¯¼åŸºç¡€)
2. [è®¡ç®—å›¾](#2-è®¡ç®—å›¾)
3. [æ¢¯åº¦è®¡ç®—ç»†èŠ‚](#3-æ¢¯åº¦è®¡ç®—ç»†èŠ‚)
4. [å¸¸è§æ“ä½œ](#4-å¸¸è§æ“ä½œ)
5. [åå‘ä¼ æ’­å®æˆ˜](#5-åå‘ä¼ æ’­å®æˆ˜)
6. [ç»ƒä¹ é¢˜](#6-ç»ƒä¹ é¢˜)

---

## 1. è‡ªåŠ¨æ±‚å¯¼åŸºç¡€

### 1.1 requires_grad

```python
import torch

# éœ€è¦è®¡ç®—æ¢¯åº¦çš„ Tensor
x = torch.tensor([1., 2., 3.], requires_grad=True)
print(f"requires_grad: {x.requires_grad}")

# é»˜è®¤ä¸éœ€è¦æ¢¯åº¦
y = torch.tensor([4., 5., 6.])
print(f"é»˜è®¤ requires_grad: {y.requires_grad}")

# ä¿®æ”¹ requires_grad
y.requires_grad_(True)  # åŸåœ°ä¿®æ”¹
print(f"ä¿®æ”¹å: {y.requires_grad}")

# åˆ›å»ºæ—¶æŒ‡å®š
z = torch.randn(3, requires_grad=True)
```

### 1.2 ç®€å•æ±‚å¯¼ç¤ºä¾‹

```python
# è®¡ç®— y = x^2 çš„å¯¼æ•° dy/dx = 2x
x = torch.tensor([1., 2., 3.], requires_grad=True)
y = x ** 2

print(f"x = {x}")
print(f"y = x^2 = {y}")

# å¯¹æ ‡é‡åå‘ä¼ æ’­
y_sum = y.sum()  # éœ€è¦æ˜¯æ ‡é‡æ‰èƒ½ç›´æ¥ backward
y_sum.backward()

print(f"dy/dx = 2x = {x.grad}")  # [2., 4., 6.]

# éªŒè¯ï¼šåœ¨ x=2 å¤„ï¼Œdy/dx = 2*2 = 4 âœ“
```

### 1.3 é“¾å¼æ³•åˆ™

```python
# y = x^2, z = y^3
# dz/dx = dz/dy * dy/dx = 3y^2 * 2x = 3(x^2)^2 * 2x = 6x^5

x = torch.tensor(2.0, requires_grad=True)
y = x ** 2
z = y ** 3

z.backward()

print(f"x = {x}")
print(f"y = x^2 = {y}")
print(f"z = y^3 = {z}")
print(f"dz/dx = {x.grad}")  # 6 * 2^5 = 192

# æ‰‹åŠ¨éªŒè¯
print(f"æ‰‹åŠ¨è®¡ç®—: 6 * x^5 = {6 * (2.0 ** 5)}")  # 192.0 âœ“
```

---

## 2. è®¡ç®—å›¾

### 2.1 åŠ¨æ€è®¡ç®—å›¾

```
PyTorch ä½¿ç”¨åŠ¨æ€è®¡ç®—å›¾ï¼ˆDefine-by-Runï¼‰ï¼š
- æ¯æ¬¡å‰å‘ä¼ æ’­éƒ½ä¼šæ„å»ºæ–°çš„è®¡ç®—å›¾
- åå‘ä¼ æ’­åå›¾ä¼šè¢«é‡Šæ”¾ï¼ˆé™¤é retain_graph=Trueï¼‰
- å…è®¸ä½¿ç”¨ Python æ§åˆ¶æµï¼ˆif/forï¼‰
```

```python
import torch

x = torch.tensor(2.0, requires_grad=True)

# åŠ¨æ€è®¡ç®—å›¾ï¼šå¯ä»¥ä½¿ç”¨ Python æ§åˆ¶æµ
if x > 0:
    y = x ** 2
else:
    y = -x ** 2

y.backward()
print(f"æ¢¯åº¦: {x.grad}")  # 4.0

# æ¡ä»¶ä¸åŒï¼Œè®¡ç®—å›¾ä¹Ÿä¸åŒ
x = torch.tensor(-2.0, requires_grad=True)
if x > 0:
    y = x ** 2
else:
    y = -x ** 2

y.backward()
print(f"æ¢¯åº¦: {x.grad}")  # 4.0 (å› ä¸º y = -x^2, dy/dx = -2x = -2*(-2) = 4)
```

### 2.2 å¶å­èŠ‚ç‚¹

```python
# å¶å­èŠ‚ç‚¹ï¼šç”±ç”¨æˆ·åˆ›å»ºä¸” requires_grad=True çš„ Tensor
x = torch.tensor([1., 2., 3.], requires_grad=True)
y = x * 2
z = y + 1

print(f"x is_leaf: {x.is_leaf}")  # True - å¶å­èŠ‚ç‚¹
print(f"y is_leaf: {y.is_leaf}")  # False - ç”±è¿ç®—äº§ç”Ÿ
print(f"z is_leaf: {z.is_leaf}")  # False

# åªæœ‰å¶å­èŠ‚ç‚¹çš„æ¢¯åº¦ä¼šè¢«ä¿ç•™
z.sum().backward()
print(f"x.grad: {x.grad}")  # ä¿ç•™
print(f"y.grad: {y.grad}")  # Noneï¼Œä¸­é—´èŠ‚ç‚¹æ¢¯åº¦ä¸ä¿ç•™

# å¦‚æœéœ€è¦ä¸­é—´èŠ‚ç‚¹çš„æ¢¯åº¦ï¼Œä½¿ç”¨ retain_grad()
x = torch.tensor([1., 2., 3.], requires_grad=True)
y = x * 2
y.retain_grad()  # ä¿ç•™ y çš„æ¢¯åº¦
z = y + 1
z.sum().backward()
print(f"y.grad (retain): {y.grad}")  # ç°åœ¨æœ‰å€¼äº†
```

### 2.3 grad_fn

```python
# æ¯ä¸ª Tensor è®°å½•äº†äº§ç”Ÿå®ƒçš„æ“ä½œ
x = torch.tensor([1., 2., 3.], requires_grad=True)
y = x ** 2
z = y.sum()

print(f"x.grad_fn: {x.grad_fn}")  # None - å¶å­èŠ‚ç‚¹
print(f"y.grad_fn: {y.grad_fn}")  # <PowBackward0>
print(f"z.grad_fn: {z.grad_fn}")  # <SumBackward0>

# grad_fn æ„æˆäº†è®¡ç®—å›¾çš„åå‘é“¾æ¥
```

---

## 3. æ¢¯åº¦è®¡ç®—ç»†èŠ‚

### 3.1 æ¢¯åº¦ç´¯ç§¯

```python
# æ¢¯åº¦æ˜¯ç´¯ç§¯çš„ï¼Œä¸ä¼šè‡ªåŠ¨æ¸…é›¶
x = torch.tensor([1., 2., 3.], requires_grad=True)

# ç¬¬ä¸€æ¬¡
y = (x ** 2).sum()
y.backward()
print(f"ç¬¬ä¸€æ¬¡: {x.grad}")  # [2, 4, 6]

# ç¬¬äºŒæ¬¡ï¼ˆæ¢¯åº¦ç´¯ç§¯ï¼‰
y = (x ** 2).sum()
y.backward()
print(f"ç¬¬äºŒæ¬¡ï¼ˆç´¯ç§¯ï¼‰: {x.grad}")  # [4, 8, 12]

# æ¸…é›¶æ¢¯åº¦
x.grad.zero_()  # æˆ– x.grad = None
y = (x ** 2).sum()
y.backward()
print(f"æ¸…é›¶å: {x.grad}")  # [2, 4, 6]

# åœ¨è®­ç»ƒå¾ªç¯ä¸­ï¼ŒåŠ¡å¿…æ¸…é›¶æ¢¯åº¦ï¼
# optimizer.zero_grad()
```

### 3.2 éæ ‡é‡åå‘ä¼ æ’­

```python
# backward() é»˜è®¤åªèƒ½å¯¹æ ‡é‡è°ƒç”¨
x = torch.tensor([1., 2., 3.], requires_grad=True)
y = x ** 2  # y æ˜¯å‘é‡

# ç›´æ¥ backward ä¼šæŠ¥é”™
# y.backward()  # RuntimeError

# æ–¹æ³• 1ï¼šå…ˆæ±‚å’Œå˜æˆæ ‡é‡
y.sum().backward()
print(f"y.sum().backward(): {x.grad}")

# æ–¹æ³• 2ï¼šä¼ å…¥ gradient å‚æ•°
x = torch.tensor([1., 2., 3.], requires_grad=True)
y = x ** 2
gradient = torch.tensor([1., 1., 1.])  # å¤–éƒ¨æ¢¯åº¦
y.backward(gradient)
print(f"y.backward(gradient): {x.grad}")

# gradient çš„ä½œç”¨æ˜¯æŒ‡å®š "å¤–éƒ¨å¯¹ y çš„æ¢¯åº¦"
# å®é™…è®¡ç®—çš„æ˜¯ (dy/dx) * gradient
```

### 3.3 åœæ­¢æ¢¯åº¦

```python
x = torch.tensor([1., 2., 3.], requires_grad=True)

# æ–¹æ³• 1ï¼šwith torch.no_grad()
with torch.no_grad():
    y = x ** 2
print(f"no_grad å†…: y.requires_grad = {y.requires_grad}")  # False

# æ–¹æ³• 2ï¼šdetach()
y = x ** 2
y_detached = y.detach()  # è¿”å›ä¸€ä¸ªä¸éœ€è¦æ¢¯åº¦çš„å‰¯æœ¬
print(f"detach: y_detached.requires_grad = {y_detached.requires_grad}")  # False

# å¸¸è§ç”¨é€”ï¼š
# 1. æ¨ç†æ—¶ä¸éœ€è¦æ¢¯åº¦
# 2. å†»ç»“éƒ¨åˆ†ç½‘ç»œ
# 3. è®¡ç®—æŒ‡æ ‡æ—¶
```

### 3.4 ä¿ç•™è®¡ç®—å›¾

```python
x = torch.tensor([1., 2., 3.], requires_grad=True)
y = x ** 2
z = y.sum()

# ç¬¬ä¸€æ¬¡ backward åå›¾ä¼šè¢«é‡Šæ”¾
z.backward(retain_graph=True)  # ä¿ç•™å›¾
print(f"ç¬¬ä¸€æ¬¡: {x.grad}")

x.grad.zero_()
z.backward()  # å¯ä»¥å†æ¬¡ backward
print(f"ç¬¬äºŒæ¬¡: {x.grad}")

# ä¸ä¿ç•™çš„è¯ç¬¬äºŒæ¬¡ä¼šæŠ¥é”™
# RuntimeError: Trying to backward through the graph a second time
```

---

## 4. å¸¸è§æ“ä½œ

### 4.1 å†»ç»“å‚æ•°

```python
import torch.nn as nn

model = nn.Sequential(
    nn.Linear(10, 64),
    nn.ReLU(),
    nn.Linear(64, 2)
)

# å†»ç»“å‰ä¸¤å±‚
for param in model[0].parameters():
    param.requires_grad = False

# æ£€æŸ¥
for name, param in model.named_parameters():
    print(f"{name}: requires_grad = {param.requires_grad}")

# åªä¼˜åŒ–æœªå†»ç»“çš„å‚æ•°
optimizer = torch.optim.Adam(
    filter(lambda p: p.requires_grad, model.parameters()),
    lr=0.001
)
```

### 4.2 æ£€æŸ¥æ¢¯åº¦

```python
x = torch.randn(3, requires_grad=True)
y = x.sum()
y.backward()

# æ£€æŸ¥æ¢¯åº¦
print(f"æ¢¯åº¦: {x.grad}")
print(f"æ¢¯åº¦æ˜¯å¦å­˜åœ¨: {x.grad is not None}")
print(f"æ¢¯åº¦å½¢çŠ¶: {x.grad.shape}")

# æ£€æŸ¥æ¢¯åº¦æ˜¯å¦æœ‰é—®é¢˜
def check_gradients(model):
    for name, param in model.named_parameters():
        if param.grad is not None:
            grad_norm = param.grad.norm()
            print(f"{name}: grad_norm = {grad_norm:.6f}")
            if torch.isnan(grad_norm):
                print(f"  è­¦å‘Šï¼šå‘ç° NaN æ¢¯åº¦ï¼")
            if grad_norm > 100:
                print(f"  è­¦å‘Šï¼šæ¢¯åº¦å¯èƒ½çˆ†ç‚¸ï¼")
```

### 4.3 æ¢¯åº¦è£å‰ª

```python
import torch.nn.utils as utils

model = nn.Linear(10, 2)
optimizer = torch.optim.Adam(model.parameters())

# è®­ç»ƒæ­¥éª¤
x = torch.randn(32, 10)
y = torch.randint(0, 2, (32,))

output = model(x)
loss = nn.CrossEntropyLoss()(output, y)

optimizer.zero_grad()
loss.backward()

# æ¢¯åº¦è£å‰ªï¼ˆé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸ï¼‰
utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
# æˆ–æŒ‰å€¼è£å‰ª
utils.clip_grad_value_(model.parameters(), clip_value=0.5)

optimizer.step()
```

---

## 5. åå‘ä¼ æ’­å®æˆ˜

### 5.1 æ‰‹å†™çº¿æ€§å›å½’

```python
import torch
import matplotlib.pyplot as plt

# ç”Ÿæˆæ•°æ®
torch.manual_seed(42)
X = torch.randn(100, 1)
y = 3 * X + 2 + torch.randn(100, 1) * 0.3  # y = 3x + 2 + noise

# åˆå§‹åŒ–å‚æ•°
w = torch.randn(1, requires_grad=True)
b = torch.zeros(1, requires_grad=True)

# è¶…å‚æ•°
lr = 0.1
epochs = 100
losses = []

# è®­ç»ƒ
for epoch in range(epochs):
    # å‰å‘ä¼ æ’­
    y_pred = X * w + b

    # è®¡ç®—æŸå¤±
    loss = ((y_pred - y) ** 2).mean()
    losses.append(loss.item())

    # åå‘ä¼ æ’­
    loss.backward()

    # æ›´æ–°å‚æ•°ï¼ˆæ‰‹åŠ¨æ¢¯åº¦ä¸‹é™ï¼‰
    with torch.no_grad():
        w -= lr * w.grad
        b -= lr * b.grad

    # æ¸…é›¶æ¢¯åº¦
    w.grad.zero_()
    b.grad.zero_()

    if (epoch + 1) % 20 == 0:
        print(f"Epoch {epoch+1}: loss = {loss.item():.4f}, w = {w.item():.4f}, b = {b.item():.4f}")

print(f"\næœ€ç»ˆ: w = {w.item():.4f} (çœŸå®: 3), b = {b.item():.4f} (çœŸå®: 2)")

# å¯è§†åŒ–
fig, axes = plt.subplots(1, 2, figsize=(12, 4))

axes[0].scatter(X.numpy(), y.numpy(), alpha=0.5, label='Data')
axes[0].plot(X.numpy(), (X * w + b).detach().numpy(), 'r-', label='Fitted')
axes[0].legend()
axes[0].set_title('Linear Regression')

axes[1].plot(losses)
axes[1].set_xlabel('Epoch')
axes[1].set_ylabel('Loss')
axes[1].set_title('Training Loss')

plt.tight_layout()
plt.show()
```

### 5.2 ç”¨ nn.Module é‡å†™

```python
import torch
import torch.nn as nn
import torch.optim as optim

# æ•°æ®
X = torch.randn(100, 1)
y = 3 * X + 2 + torch.randn(100, 1) * 0.3

# æ¨¡å‹
model = nn.Linear(1, 1)

# æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
criterion = nn.MSELoss()
optimizer = optim.SGD(model.parameters(), lr=0.1)

# è®­ç»ƒ
for epoch in range(100):
    # å‰å‘ä¼ æ’­
    y_pred = model(X)
    loss = criterion(y_pred, y)

    # åå‘ä¼ æ’­
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

    if (epoch + 1) % 20 == 0:
        print(f"Epoch {epoch+1}: loss = {loss.item():.4f}")

# æŸ¥çœ‹å­¦åˆ°çš„å‚æ•°
print(f"\nå­¦åˆ°çš„å‚æ•°:")
print(f"  w = {model.weight.item():.4f} (çœŸå®: 3)")
print(f"  b = {model.bias.item():.4f} (çœŸå®: 2)")
```

### 5.3 å®Œæ•´è®­ç»ƒæ¨¡æ¿

```python
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset

# ========== å‡†å¤‡æ•°æ® ==========
X = torch.randn(1000, 10)
y = torch.randint(0, 2, (1000,))

dataset = TensorDataset(X, y)
train_loader = DataLoader(dataset, batch_size=32, shuffle=True)

# ========== å®šä¹‰æ¨¡å‹ ==========
class MLP(nn.Module):
    def __init__(self):
        super().__init__()
        self.fc1 = nn.Linear(10, 64)
        self.fc2 = nn.Linear(64, 2)
        self.relu = nn.ReLU()

    def forward(self, x):
        x = self.relu(self.fc1(x))
        return self.fc2(x)

model = MLP()
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model = model.to(device)

# ========== æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨ ==========
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

# ========== è®­ç»ƒå¾ªç¯ ==========
num_epochs = 10

for epoch in range(num_epochs):
    model.train()  # è®­ç»ƒæ¨¡å¼
    total_loss = 0
    correct = 0
    total = 0

    for batch_x, batch_y in train_loader:
        # æ•°æ®è½¬ç§»åˆ°è®¾å¤‡
        batch_x = batch_x.to(device)
        batch_y = batch_y.to(device)

        # å‰å‘ä¼ æ’­
        outputs = model(batch_x)
        loss = criterion(outputs, batch_y)

        # åå‘ä¼ æ’­
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        # ç»Ÿè®¡
        total_loss += loss.item()
        _, predicted = outputs.max(1)
        total += batch_y.size(0)
        correct += predicted.eq(batch_y).sum().item()

    avg_loss = total_loss / len(train_loader)
    accuracy = 100. * correct / total
    print(f"Epoch {epoch+1}/{num_epochs}: Loss = {avg_loss:.4f}, Accuracy = {accuracy:.2f}%")

print("\nè®­ç»ƒå®Œæˆï¼")
```

---

## 6. ç»ƒä¹ é¢˜

### åŸºç¡€ç»ƒä¹ 

1. è®¡ç®— f(x) = xÂ³ + 2xÂ² - 5x + 3 åœ¨ x=2 å¤„çš„å¯¼æ•°
2. ç”¨ autograd éªŒè¯é“¾å¼æ³•åˆ™ï¼šf(g(x)) çš„å¯¼æ•°
3. å®ç°ä¸€ä¸ªç®€å•çš„ç¥ç»ç½‘ç»œè®­ç»ƒå¾ªç¯

### å‚è€ƒç­”æ¡ˆ

<details>
<summary>ç‚¹å‡»æŸ¥çœ‹ç­”æ¡ˆ</summary>

```python
import torch

# 1. è®¡ç®—å¯¼æ•°
x = torch.tensor(2.0, requires_grad=True)
f = x**3 + 2*x**2 - 5*x + 3
f.backward()

print(f"f(x) = xÂ³ + 2xÂ² - 5x + 3")
print(f"f(2) = {f.item()}")
print(f"f'(x) = 3xÂ² + 4x - 5")
print(f"f'(2) = {x.grad.item()}")  # 3*4 + 4*2 - 5 = 15

# éªŒè¯
manual = 3 * (2**2) + 4 * 2 - 5
print(f"æ‰‹åŠ¨è®¡ç®—: {manual}")

# 2. é“¾å¼æ³•åˆ™éªŒè¯
# f(x) = sin(x), g(x) = x^2
# h(x) = f(g(x)) = sin(x^2)
# h'(x) = f'(g(x)) * g'(x) = cos(x^2) * 2x

x = torch.tensor(1.0, requires_grad=True)
h = torch.sin(x ** 2)
h.backward()

print(f"\nh(x) = sin(xÂ²)")
print(f"h'(x) = cos(xÂ²) * 2x")
print(f"h'(1) = {x.grad.item():.6f}")

# éªŒè¯
import math
manual = math.cos(1**2) * 2 * 1
print(f"æ‰‹åŠ¨è®¡ç®—: {manual:.6f}")

# 3. ç®€å•ç¥ç»ç½‘ç»œè®­ç»ƒ
import torch.nn as nn

# æ•°æ®
X = torch.randn(100, 5)
y = (X.sum(dim=1) > 0).long()  # ç®€å•äºŒåˆ†ç±»

# æ¨¡å‹
model = nn.Sequential(
    nn.Linear(5, 10),
    nn.ReLU(),
    nn.Linear(10, 2)
)

criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.01)

# è®­ç»ƒ
for epoch in range(50):
    output = model(X)
    loss = criterion(output, y)

    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

    if (epoch + 1) % 10 == 0:
        acc = (output.argmax(1) == y).float().mean()
        print(f"Epoch {epoch+1}: Loss = {loss.item():.4f}, Acc = {acc.item():.2%}")
```

</details>

---

## â¡ï¸ ä¸‹ä¸€æ­¥

å­¦å®Œæœ¬èŠ‚åï¼Œç»§ç»­å­¦ä¹  [04-nn.Moduleä¸æ¨¡å‹æ„å»º.md](./04-nn.Moduleä¸æ¨¡å‹æ„å»º.md)

