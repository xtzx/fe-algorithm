# ğŸ”¥ 04 - nn.Module ä¸æ¨¡å‹æ„å»º

> nn.Module æ˜¯ PyTorch æ„å»ºç¥ç»ç½‘ç»œçš„åŸºç±»ï¼ŒæŒæ¡å®ƒæ˜¯æ·±åº¦å­¦ä¹ çš„æ ¸å¿ƒæŠ€èƒ½

---

## ç›®å½•

1. [nn.Module åŸºç¡€](#1-nnmodule-åŸºç¡€)
2. [å¸¸ç”¨å±‚](#2-å¸¸ç”¨å±‚)
3. [æ¨¡å‹æ„å»ºæ–¹å¼](#3-æ¨¡å‹æ„å»ºæ–¹å¼)
4. [å‚æ•°ç®¡ç†](#4-å‚æ•°ç®¡ç†)
5. [è‡ªå®šä¹‰å±‚](#5-è‡ªå®šä¹‰å±‚)
6. [æ¨¡å‹ç»„åˆ](#6-æ¨¡å‹ç»„åˆ)
7. [ç»ƒä¹ é¢˜](#7-ç»ƒä¹ é¢˜)

---

## 1. nn.Module åŸºç¡€

### 1.1 åŸºæœ¬ç»“æ„

```python
import torch
import torch.nn as nn

class MyModel(nn.Module):
    def __init__(self):
        super().__init__()  # å¿…é¡»è°ƒç”¨çˆ¶ç±»åˆå§‹åŒ–
        # åœ¨è¿™é‡Œå®šä¹‰å±‚
        self.layer1 = nn.Linear(10, 64)
        self.layer2 = nn.Linear(64, 2)

    def forward(self, x):
        # å®šä¹‰å‰å‘ä¼ æ’­
        x = torch.relu(self.layer1(x))
        x = self.layer2(x)
        return x

# ä½¿ç”¨
model = MyModel()
x = torch.randn(32, 10)
output = model(x)  # è‡ªåŠ¨è°ƒç”¨ forward
print(f"è¾“å‡ºå½¢çŠ¶: {output.shape}")  # [32, 2]
```

### 1.2 train/eval æ¨¡å¼

```python
model = MyModel()

# è®­ç»ƒæ¨¡å¼ï¼ˆDropoutã€BatchNorm ç”Ÿæ•ˆï¼‰
model.train()
print(f"è®­ç»ƒæ¨¡å¼: {model.training}")  # True

# è¯„ä¼°æ¨¡å¼ï¼ˆDropout å…³é—­ï¼ŒBatchNorm ä½¿ç”¨ç»Ÿè®¡å€¼ï¼‰
model.eval()
print(f"è¯„ä¼°æ¨¡å¼: {model.training}")  # False

# æ¨ç†æ—¶ä½¿ç”¨
model.eval()
with torch.no_grad():  # ä¸è®¡ç®—æ¢¯åº¦
    output = model(x)
```

### 1.3 æ¨¡å‹ä¿¡æ¯

```python
model = MyModel()

# æ‰“å°æ¨¡å‹ç»“æ„
print(model)

# å‚æ•°æ•°é‡
total_params = sum(p.numel() for p in model.parameters())
trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
print(f"æ€»å‚æ•°: {total_params:,}")
print(f"å¯è®­ç»ƒå‚æ•°: {trainable_params:,}")

# éå†å­æ¨¡å—
for name, module in model.named_children():
    print(f"{name}: {module}")

# éå†æ‰€æœ‰æ¨¡å—ï¼ˆåŒ…æ‹¬åµŒå¥—ï¼‰
for name, module in model.named_modules():
    print(f"{name}: {type(module).__name__}")
```

---

## 2. å¸¸ç”¨å±‚

### 2.1 çº¿æ€§å±‚

```python
import torch.nn as nn

# å…¨è¿æ¥å±‚
linear = nn.Linear(in_features=10, out_features=64)
# ç­‰ä»·äºï¼šy = x @ W.T + b
# W å½¢çŠ¶: [out_features, in_features] = [64, 10]
# b å½¢çŠ¶: [out_features] = [64]

x = torch.randn(32, 10)
y = linear(x)
print(f"è¾“å…¥: {x.shape} â†’ è¾“å‡º: {y.shape}")  # [32, 10] â†’ [32, 64]

# ä¸å¸¦åç½®
linear_no_bias = nn.Linear(10, 64, bias=False)
```

### 2.2 æ¿€æ´»å‡½æ•°

```python
# ä½œä¸ºå±‚ä½¿ç”¨
relu = nn.ReLU()
gelu = nn.GELU()
silu = nn.SiLU()
leaky = nn.LeakyReLU(negative_slope=0.1)
sigmoid = nn.Sigmoid()
tanh = nn.Tanh()
softmax = nn.Softmax(dim=1)

# ä½œä¸ºå‡½æ•°ä½¿ç”¨
import torch.nn.functional as F
y = F.relu(x)
y = F.gelu(x)
y = F.softmax(x, dim=1)
```

### 2.3 æ­£åˆ™åŒ–å±‚

```python
# Dropout
dropout = nn.Dropout(p=0.5)  # 50% æ¦‚ç‡ä¸¢å¼ƒ
dropout2d = nn.Dropout2d(p=0.5)  # ç”¨äº CNN

# BatchNorm
bn1d = nn.BatchNorm1d(num_features=64)  # ç”¨äºå…¨è¿æ¥å±‚
bn2d = nn.BatchNorm2d(num_features=64)  # ç”¨äº CNN

# LayerNormï¼ˆTransformer å¸¸ç”¨ï¼‰
ln = nn.LayerNorm(normalized_shape=64)  # å¯ä»¥æ˜¯å•ä¸ªæ•°æˆ–å…ƒç»„
ln_seq = nn.LayerNorm([seq_len, hidden_dim])  # å¯¹å¤šä¸ªç»´åº¦å½’ä¸€åŒ–

# ç¤ºä¾‹
x = torch.randn(32, 64)  # [batch, features]
y_bn = bn1d(x)  # åœ¨ batch ç»´åº¦å½’ä¸€åŒ–
y_ln = ln(x)    # åœ¨ feature ç»´åº¦å½’ä¸€åŒ–

print(f"BatchNorm åå‡å€¼: {y_bn.mean(dim=0).mean():.6f}")  # çº¦ 0
print(f"LayerNorm åå‡å€¼: {y_ln.mean(dim=1).mean():.6f}")  # çº¦ 0
```

### 2.4 Embedding

```python
# è¯åµŒå…¥å±‚ï¼ˆç”¨äº NLPï¼‰
vocab_size = 10000
embed_dim = 256

embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embed_dim)

# è¾“å…¥æ˜¯æ•´æ•°ç´¢å¼•
token_ids = torch.tensor([1, 5, 100, 50])  # 4 ä¸ª token
embedded = embedding(token_ids)
print(f"åµŒå…¥å½¢çŠ¶: {embedded.shape}")  # [4, 256]

# æ‰¹é‡è¾“å…¥
batch_tokens = torch.randint(0, vocab_size, (32, 128))  # [batch, seq_len]
batch_embedded = embedding(batch_tokens)
print(f"æ‰¹é‡åµŒå…¥å½¢çŠ¶: {batch_embedded.shape}")  # [32, 128, 256]

# padding_idxï¼šæŒ‡å®šå¡«å……ç´¢å¼•ï¼Œå…¶åµŒå…¥å›ºå®šä¸º 0
embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)
```

---

## 3. æ¨¡å‹æ„å»ºæ–¹å¼

### 3.1 ç»§æ‰¿ nn.Module

```python
class MLP(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim, dropout=0.1):
        super().__init__()
        self.fc1 = nn.Linear(input_dim, hidden_dim)
        self.fc2 = nn.Linear(hidden_dim, hidden_dim)
        self.fc3 = nn.Linear(hidden_dim, output_dim)
        self.dropout = nn.Dropout(dropout)
        self.relu = nn.ReLU()

    def forward(self, x):
        x = self.relu(self.fc1(x))
        x = self.dropout(x)
        x = self.relu(self.fc2(x))
        x = self.dropout(x)
        x = self.fc3(x)
        return x

model = MLP(input_dim=10, hidden_dim=64, output_dim=2)
```

### 3.2 nn.Sequential

```python
# ç®€å•çš„é¡ºåºæ¨¡å‹
model = nn.Sequential(
    nn.Linear(10, 64),
    nn.ReLU(),
    nn.Dropout(0.1),
    nn.Linear(64, 64),
    nn.ReLU(),
    nn.Dropout(0.1),
    nn.Linear(64, 2)
)

x = torch.randn(32, 10)
output = model(x)
print(f"è¾“å‡º: {output.shape}")

# å¸¦å‘½åçš„ Sequential
from collections import OrderedDict

model = nn.Sequential(OrderedDict([
    ('fc1', nn.Linear(10, 64)),
    ('relu1', nn.ReLU()),
    ('dropout1', nn.Dropout(0.1)),
    ('fc2', nn.Linear(64, 2))
]))

# å¯ä»¥é€šè¿‡åå­—è®¿é—®
print(model.fc1)
```

### 3.3 nn.ModuleList

```python
# å½“éœ€è¦åŠ¨æ€æ•°é‡çš„å±‚æ—¶ä½¿ç”¨
class DynamicMLP(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim, num_layers):
        super().__init__()

        # ä½¿ç”¨ ModuleList
        self.layers = nn.ModuleList()
        self.layers.append(nn.Linear(input_dim, hidden_dim))
        for _ in range(num_layers - 2):
            self.layers.append(nn.Linear(hidden_dim, hidden_dim))
        self.layers.append(nn.Linear(hidden_dim, output_dim))

        self.relu = nn.ReLU()

    def forward(self, x):
        for i, layer in enumerate(self.layers[:-1]):
            x = self.relu(layer(x))
        return self.layers[-1](x)

model = DynamicMLP(10, 64, 2, num_layers=5)
print(f"å±‚æ•°: {len(model.layers)}")  # 5
```

### 3.4 nn.ModuleDict

```python
# å½“éœ€è¦æŒ‰åå­—è®¿é—®å±‚æ—¶ä½¿ç”¨
class MultiHeadModel(nn.Module):
    def __init__(self, input_dim, hidden_dim):
        super().__init__()

        self.backbone = nn.Linear(input_dim, hidden_dim)

        # å¤šä¸ªè¾“å‡ºå¤´
        self.heads = nn.ModuleDict({
            'classification': nn.Linear(hidden_dim, 10),
            'regression': nn.Linear(hidden_dim, 1),
            'embedding': nn.Linear(hidden_dim, 128)
        })

    def forward(self, x, task='classification'):
        features = torch.relu(self.backbone(x))
        return self.heads[task](features)

model = MultiHeadModel(10, 64)
x = torch.randn(32, 10)

cls_output = model(x, 'classification')  # [32, 10]
reg_output = model(x, 'regression')      # [32, 1]
emb_output = model(x, 'embedding')       # [32, 128]
```

---

## 4. å‚æ•°ç®¡ç†

### 4.1 è®¿é—®å‚æ•°

```python
model = nn.Sequential(
    nn.Linear(10, 64),
    nn.Linear(64, 2)
)

# éå†å‚æ•°
for name, param in model.named_parameters():
    print(f"{name}: {param.shape}")

# ç›´æ¥è·å–å‚æ•°
print(model[0].weight.shape)  # [64, 10]
print(model[0].bias.shape)    # [64]

# å‚æ•°ç»Ÿè®¡
params = list(model.parameters())
print(f"å‚æ•°ç»„æ•°é‡: {len(params)}")
print(f"æ€»å‚æ•°é‡: {sum(p.numel() for p in params)}")
```

### 4.2 å‚æ•°åˆå§‹åŒ–

```python
import torch.nn.init as init

class MyModel(nn.Module):
    def __init__(self):
        super().__init__()
        self.fc1 = nn.Linear(10, 64)
        self.fc2 = nn.Linear(64, 2)

        # è‡ªå®šä¹‰åˆå§‹åŒ–
        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Linear):
                # Xavier åˆå§‹åŒ–
                init.xavier_uniform_(m.weight)
                # æˆ– Kaiming åˆå§‹åŒ–ï¼ˆé…åˆ ReLUï¼‰
                # init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
                if m.bias is not None:
                    init.zeros_(m.bias)

    def forward(self, x):
        x = torch.relu(self.fc1(x))
        return self.fc2(x)

# å¸¸ç”¨åˆå§‹åŒ–æ–¹æ³•
# init.zeros_(tensor)           # å…¨é›¶
# init.ones_(tensor)            # å…¨ä¸€
# init.constant_(tensor, val)   # å¸¸æ•°
# init.uniform_(tensor, a, b)   # å‡åŒ€åˆ†å¸ƒ
# init.normal_(tensor, mean, std)  # æ­£æ€åˆ†å¸ƒ
# init.xavier_uniform_(tensor)  # Xavier å‡åŒ€
# init.xavier_normal_(tensor)   # Xavier æ­£æ€
# init.kaiming_uniform_(tensor) # Kaiming å‡åŒ€
# init.kaiming_normal_(tensor)  # Kaiming æ­£æ€
```

### 4.3 å†»ç»“å‚æ•°

```python
model = nn.Sequential(
    nn.Linear(10, 64),
    nn.ReLU(),
    nn.Linear(64, 2)
)

# å†»ç»“ç‰¹å®šå±‚
for param in model[0].parameters():
    param.requires_grad = False

# å†»ç»“å‰ N å±‚
def freeze_layers(model, n):
    for i, (name, param) in enumerate(model.named_parameters()):
        if i < n:
            param.requires_grad = False
            print(f"å†»ç»“: {name}")

# è§£å†»
def unfreeze_all(model):
    for param in model.parameters():
        param.requires_grad = True

# æ£€æŸ¥å“ªäº›å‚æ•°ä¼šè¢«è®­ç»ƒ
for name, param in model.named_parameters():
    print(f"{name}: requires_grad = {param.requires_grad}")
```

---

## 5. è‡ªå®šä¹‰å±‚

### 5.1 ç®€å•è‡ªå®šä¹‰å±‚

```python
class ScaledDotProductAttention(nn.Module):
    """ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›"""
    def __init__(self, d_k):
        super().__init__()
        self.d_k = d_k

    def forward(self, Q, K, V, mask=None):
        # Q, K, V: [batch, seq_len, d_k]
        scores = torch.matmul(Q, K.transpose(-2, -1)) / (self.d_k ** 0.5)

        if mask is not None:
            scores = scores.masked_fill(mask == 0, -1e9)

        attention = torch.softmax(scores, dim=-1)
        output = torch.matmul(attention, V)
        return output, attention

# ä½¿ç”¨
attn = ScaledDotProductAttention(d_k=64)
Q = K = V = torch.randn(32, 10, 64)
output, attention = attn(Q, K, V)
print(f"è¾“å‡º: {output.shape}")  # [32, 10, 64]
```

### 5.2 å¸¦å¯å­¦ä¹ å‚æ•°çš„å±‚

```python
class LayerScale(nn.Module):
    """å±‚ç¼©æ”¾ï¼ˆç”¨äº Vision Transformerï¼‰"""
    def __init__(self, dim, init_value=1e-5):
        super().__init__()
        # å¯å­¦ä¹ å‚æ•°
        self.gamma = nn.Parameter(torch.ones(dim) * init_value)

    def forward(self, x):
        return x * self.gamma

# ä½¿ç”¨
layer_scale = LayerScale(dim=64)
x = torch.randn(32, 10, 64)
y = layer_scale(x)

# æ£€æŸ¥å‚æ•°
for name, param in layer_scale.named_parameters():
    print(f"{name}: {param.shape}")  # gamma: [64]
```

### 5.3 ç»„åˆç°æœ‰å±‚

```python
class ResidualBlock(nn.Module):
    """æ®‹å·®å—"""
    def __init__(self, dim, dropout=0.1):
        super().__init__()
        self.norm = nn.LayerNorm(dim)
        self.fc1 = nn.Linear(dim, dim * 4)
        self.fc2 = nn.Linear(dim * 4, dim)
        self.dropout = nn.Dropout(dropout)
        self.gelu = nn.GELU()

    def forward(self, x):
        residual = x
        x = self.norm(x)
        x = self.fc1(x)
        x = self.gelu(x)
        x = self.dropout(x)
        x = self.fc2(x)
        x = self.dropout(x)
        return x + residual  # æ®‹å·®è¿æ¥

# ä½¿ç”¨
block = ResidualBlock(dim=64)
x = torch.randn(32, 10, 64)
y = block(x)
print(f"è¾“å…¥è¾“å‡ºå½¢çŠ¶ç›¸åŒ: {x.shape} â†’ {y.shape}")
```

---

## 6. æ¨¡å‹ç»„åˆ

### 6.1 ç¼–ç å™¨-è§£ç å™¨ç»“æ„

```python
class Encoder(nn.Module):
    def __init__(self, input_dim, hidden_dim, latent_dim):
        super().__init__()
        self.fc1 = nn.Linear(input_dim, hidden_dim)
        self.fc2 = nn.Linear(hidden_dim, latent_dim)

    def forward(self, x):
        x = torch.relu(self.fc1(x))
        return self.fc2(x)

class Decoder(nn.Module):
    def __init__(self, latent_dim, hidden_dim, output_dim):
        super().__init__()
        self.fc1 = nn.Linear(latent_dim, hidden_dim)
        self.fc2 = nn.Linear(hidden_dim, output_dim)

    def forward(self, x):
        x = torch.relu(self.fc1(x))
        return self.fc2(x)

class AutoEncoder(nn.Module):
    def __init__(self, input_dim, hidden_dim, latent_dim):
        super().__init__()
        self.encoder = Encoder(input_dim, hidden_dim, latent_dim)
        self.decoder = Decoder(latent_dim, hidden_dim, input_dim)

    def forward(self, x):
        latent = self.encoder(x)
        reconstructed = self.decoder(latent)
        return reconstructed

    def encode(self, x):
        return self.encoder(x)

# ä½¿ç”¨
ae = AutoEncoder(input_dim=784, hidden_dim=256, latent_dim=32)
x = torch.randn(32, 784)
reconstructed = ae(x)
latent = ae.encode(x)
print(f"é‡å»º: {reconstructed.shape}, æ½œåœ¨è¡¨ç¤º: {latent.shape}")
```

### 6.2 å…±äº«æƒé‡

```python
class SiameseNetwork(nn.Module):
    """å­ªç”Ÿç½‘ç»œï¼šä¸¤ä¸ªåˆ†æ”¯å…±äº«æƒé‡"""
    def __init__(self, input_dim, hidden_dim, output_dim):
        super().__init__()
        # å…±äº«çš„ç¼–ç å™¨
        self.encoder = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, output_dim)
        )

    def forward_one(self, x):
        return self.encoder(x)

    def forward(self, x1, x2):
        # ä¸¤ä¸ªè¾“å…¥ç”¨åŒä¸€ä¸ªç¼–ç å™¨
        emb1 = self.forward_one(x1)
        emb2 = self.forward_one(x2)
        return emb1, emb2

# ä½¿ç”¨
siamese = SiameseNetwork(784, 256, 64)
x1 = torch.randn(32, 784)
x2 = torch.randn(32, 784)
emb1, emb2 = siamese(x1, x2)

# è®¡ç®—ç›¸ä¼¼åº¦
similarity = torch.cosine_similarity(emb1, emb2, dim=1)
print(f"ç›¸ä¼¼åº¦: {similarity.shape}")  # [32]
```

---

## 7. ç»ƒä¹ é¢˜

### åŸºç¡€ç»ƒä¹ 

1. å®ç°ä¸€ä¸ªå¯é…ç½®æ·±åº¦çš„ MLP
2. å®ç°ä¸€ä¸ªå¸¦æ®‹å·®è¿æ¥çš„ 2 å±‚ç½‘ç»œ
3. ç”¨ ModuleList å®ç°ä¸€ä¸ª"é˜¶æ¢¯"ç½‘ç»œï¼ˆæ¯å±‚ç»´åº¦é€’å‡ï¼‰

### å‚è€ƒç­”æ¡ˆ

<details>
<summary>ç‚¹å‡»æŸ¥çœ‹ç­”æ¡ˆ</summary>

```python
import torch
import torch.nn as nn

# 1. å¯é…ç½®æ·±åº¦çš„ MLP
class ConfigurableMLP(nn.Module):
    def __init__(self, dims, activation='relu', dropout=0.0):
        """
        dims: [input_dim, hidden1, hidden2, ..., output_dim]
        """
        super().__init__()

        layers = []
        for i in range(len(dims) - 1):
            layers.append(nn.Linear(dims[i], dims[i+1]))
            if i < len(dims) - 2:  # æœ€åä¸€å±‚ä¸åŠ æ¿€æ´»
                if activation == 'relu':
                    layers.append(nn.ReLU())
                elif activation == 'gelu':
                    layers.append(nn.GELU())
                if dropout > 0:
                    layers.append(nn.Dropout(dropout))

        self.network = nn.Sequential(*layers)

    def forward(self, x):
        return self.network(x)

# æµ‹è¯•
model = ConfigurableMLP([10, 64, 32, 16, 2], dropout=0.1)
x = torch.randn(32, 10)
print(f"è¾“å‡º: {model(x).shape}")


# 2. å¸¦æ®‹å·®è¿æ¥çš„ç½‘ç»œ
class ResidualNet(nn.Module):
    def __init__(self, dim):
        super().__init__()
        self.fc1 = nn.Linear(dim, dim)
        self.fc2 = nn.Linear(dim, dim)
        self.norm1 = nn.LayerNorm(dim)
        self.norm2 = nn.LayerNorm(dim)

    def forward(self, x):
        # ç¬¬ä¸€ä¸ªæ®‹å·®å—
        residual = x
        x = self.norm1(x)
        x = torch.relu(self.fc1(x))
        x = x + residual

        # ç¬¬äºŒä¸ªæ®‹å·®å—
        residual = x
        x = self.norm2(x)
        x = torch.relu(self.fc2(x))
        x = x + residual

        return x

model = ResidualNet(dim=64)
x = torch.randn(32, 64)
print(f"æ®‹å·®ç½‘ç»œè¾“å‡º: {model(x).shape}")


# 3. é˜¶æ¢¯ç½‘ç»œ
class StaircaseMLP(nn.Module):
    def __init__(self, input_dim, output_dim, num_layers=4, reduction=2):
        super().__init__()

        dims = [input_dim]
        current_dim = input_dim
        for _ in range(num_layers - 1):
            current_dim = max(current_dim // reduction, output_dim)
            dims.append(current_dim)
        dims.append(output_dim)

        self.layers = nn.ModuleList()
        for i in range(len(dims) - 1):
            self.layers.append(nn.Linear(dims[i], dims[i+1]))

        print(f"ç»´åº¦å˜åŒ–: {dims}")

    def forward(self, x):
        for i, layer in enumerate(self.layers[:-1]):
            x = torch.relu(layer(x))
        return self.layers[-1](x)

model = StaircaseMLP(input_dim=256, output_dim=10, num_layers=5)
x = torch.randn(32, 256)
print(f"é˜¶æ¢¯ç½‘ç»œè¾“å‡º: {model(x).shape}")
```

</details>

---

## â¡ï¸ ä¸‹ä¸€æ­¥

å­¦å®Œæœ¬èŠ‚åï¼Œç»§ç»­å­¦ä¹  [05-æ•°æ®åŠ è½½ä¸æ¨¡å‹ä¿å­˜.md](./05-æ•°æ®åŠ è½½ä¸æ¨¡å‹ä¿å­˜.md)

