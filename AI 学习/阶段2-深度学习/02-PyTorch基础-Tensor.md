# ğŸ”¥ 02 - PyTorch åŸºç¡€ï¼šTensor

> Tensor æ˜¯ PyTorch çš„æ ¸å¿ƒæ•°æ®ç»“æ„ï¼Œç±»ä¼¼ NumPy çš„ ndarrayï¼Œä½†æ”¯æŒ GPU åŠ é€Ÿå’Œè‡ªåŠ¨æ±‚å¯¼

---

## ç›®å½•

1. [Tensor åˆ›å»º](#1-tensor-åˆ›å»º)
2. [Tensor å±æ€§](#2-tensor-å±æ€§)
3. [ç´¢å¼•ä¸åˆ‡ç‰‡](#3-ç´¢å¼•ä¸åˆ‡ç‰‡)
4. [å½¢çŠ¶æ“ä½œ](#4-å½¢çŠ¶æ“ä½œ)
5. [æ•°å­¦è¿ç®—](#5-æ•°å­¦è¿ç®—)
6. [è®¾å¤‡ç®¡ç†](#6-è®¾å¤‡ç®¡ç†)
7. [ä¸ NumPy äº’è½¬](#7-ä¸-numpy-äº’è½¬)
8. [ç»ƒä¹ é¢˜](#8-ç»ƒä¹ é¢˜)

---

## 1. Tensor åˆ›å»º

### 1.1 ä»æ•°æ®åˆ›å»º

```python
import torch
import numpy as np

# ä» Python åˆ—è¡¨
t1 = torch.tensor([1, 2, 3, 4, 5])
print(f"ä»åˆ—è¡¨: {t1}")

# ä»åµŒå¥—åˆ—è¡¨ï¼ˆçŸ©é˜µï¼‰
t2 = torch.tensor([[1, 2, 3], [4, 5, 6]])
print(f"ä»åµŒå¥—åˆ—è¡¨:\n{t2}")

# ä» NumPy æ•°ç»„
arr = np.array([1.0, 2.0, 3.0])
t3 = torch.from_numpy(arr)  # å…±äº«å†…å­˜
t4 = torch.tensor(arr)      # å¤åˆ¶æ•°æ®
print(f"ä» NumPy: {t3}")

# æŒ‡å®šæ•°æ®ç±»å‹
t5 = torch.tensor([1, 2, 3], dtype=torch.float32)
t6 = torch.tensor([1, 2, 3], dtype=torch.int64)
print(f"float32: {t5.dtype}")
print(f"int64: {t6.dtype}")
```

### 1.2 ç‰¹æ®Š Tensor

```python
# å…¨é›¶
zeros = torch.zeros(3, 4)
print(f"å…¨é›¶:\n{zeros}")

# å…¨ä¸€
ones = torch.ones(3, 4)
print(f"å…¨ä¸€:\n{ones}")

# å•ä½çŸ©é˜µ
eye = torch.eye(3)
print(f"å•ä½çŸ©é˜µ:\n{eye}")

# å¡«å……ç‰¹å®šå€¼
full = torch.full((2, 3), fill_value=7)
print(f"å¡«å……7:\n{full}")

# æœªåˆå§‹åŒ–ï¼ˆéšæœºå€¼ï¼Œé€Ÿåº¦å¿«ï¼‰
empty = torch.empty(2, 3)
print(f"æœªåˆå§‹åŒ–:\n{empty}")

# ä¸å¦ä¸€ä¸ª Tensor ç›¸åŒå½¢çŠ¶
x = torch.randn(2, 3)
zeros_like = torch.zeros_like(x)
ones_like = torch.ones_like(x)
```

### 1.3 åºåˆ—å’Œéšæœº

```python
# arange
t = torch.arange(0, 10, 2)  # [0, 2, 4, 6, 8]
print(f"arange: {t}")

# linspace
t = torch.linspace(0, 1, 5)  # 5 ä¸ªå‡åŒ€åˆ†å¸ƒçš„ç‚¹
print(f"linspace: {t}")

# éšæœºæ•°
torch.manual_seed(42)  # è®¾ç½®éšæœºç§å­

# å‡åŒ€åˆ†å¸ƒ [0, 1)
uniform = torch.rand(3, 4)

# æ ‡å‡†æ­£æ€åˆ†å¸ƒ
normal = torch.randn(3, 4)

# æŒ‡å®šèŒƒå›´çš„éšæœºæ•´æ•°
integers = torch.randint(0, 10, (3, 4))

# æŒ‡å®šèŒƒå›´çš„å‡åŒ€åˆ†å¸ƒ
uniform_range = torch.empty(3, 4).uniform_(-1, 1)

# æŒ‡å®šå‚æ•°çš„æ­£æ€åˆ†å¸ƒ
normal_params = torch.normal(mean=0, std=1, size=(3, 4))

print(f"å‡åŒ€åˆ†å¸ƒ:\n{uniform}")
print(f"æ­£æ€åˆ†å¸ƒ:\n{normal}")
```

---

## 2. Tensor å±æ€§

```python
t = torch.randn(2, 3, 4)

# åŸºæœ¬å±æ€§
print(f"å½¢çŠ¶: {t.shape}")           # torch.Size([2, 3, 4])
print(f"å½¢çŠ¶: {t.size()}")          # åŒä¸Š
print(f"ç»´åº¦æ•°: {t.dim()}")         # 3
print(f"å…ƒç´ æ€»æ•°: {t.numel()}")     # 24
print(f"æ•°æ®ç±»å‹: {t.dtype}")       # torch.float32
print(f"è®¾å¤‡: {t.device}")          # cpu æˆ– cuda:0

# æ£€æŸ¥å±æ€§
print(f"æ˜¯å¦éœ€è¦æ¢¯åº¦: {t.requires_grad}")
print(f"æ˜¯å¦æ˜¯å¶å­èŠ‚ç‚¹: {t.is_leaf}")
print(f"æ˜¯å¦æ˜¯ CUDA: {t.is_cuda}")

# å¸¸ç”¨æ•°æ®ç±»å‹
dtypes = [
    torch.float32,   # torch.float, é»˜è®¤æµ®ç‚¹ç±»å‹
    torch.float64,   # torch.double
    torch.float16,   # torch.half, æ··åˆç²¾åº¦è®­ç»ƒ
    torch.bfloat16,  # Brain Float 16
    torch.int32,     # torch.int
    torch.int64,     # torch.long, é»˜è®¤æ•´æ•°ç±»å‹
    torch.bool       # å¸ƒå°”ç±»å‹
]
```

---

## 3. ç´¢å¼•ä¸åˆ‡ç‰‡

### 3.1 åŸºæœ¬ç´¢å¼•

```python
t = torch.arange(12).reshape(3, 4)
print(f"åŸå§‹ Tensor:\n{t}")
# tensor([[ 0,  1,  2,  3],
#         [ 4,  5,  6,  7],
#         [ 8,  9, 10, 11]])

# å•ä¸ªå…ƒç´ 
print(f"t[1, 2] = {t[1, 2]}")  # 6

# è¡Œ
print(f"t[0] = {t[0]}")  # [0, 1, 2, 3]

# åˆ—
print(f"t[:, 1] = {t[:, 1]}")  # [1, 5, 9]

# åˆ‡ç‰‡
print(f"t[0:2, 1:3] =\n{t[0:2, 1:3]}")
# [[1, 2],
#  [5, 6]]

# æ­¥é•¿
print(f"t[::2, ::2] =\n{t[::2, ::2]}")
# [[ 0,  2],
#  [ 8, 10]]
```

### 3.2 é«˜çº§ç´¢å¼•

```python
# å¸ƒå°”ç´¢å¼•
t = torch.randn(4, 4)
mask = t > 0
print(f"æ­£æ•°: {t[mask]}")

# è®¾ç½®å€¼
t[t < 0] = 0  # å°†æ‰€æœ‰è´Ÿæ•°è®¾ä¸º 0
print(t)

# èŠ±å¼ç´¢å¼•
t = torch.arange(12).reshape(3, 4)
indices = torch.tensor([0, 2])
print(f"é€‰æ‹©ç¬¬ 0 å’Œ 2 è¡Œ:\n{t[indices]}")

# gather: æŒ‰ç´¢å¼•æ”¶é›†
t = torch.tensor([[1, 2], [3, 4]])
idx = torch.tensor([[0, 0], [1, 0]])
result = torch.gather(t, dim=1, index=idx)
print(f"gather ç»“æœ:\n{result}")
# [[1, 1],
#  [4, 3]]
```

### 3.3 å¸¸ç”¨ç´¢å¼•æ“ä½œ

```python
# where: æ¡ä»¶é€‰æ‹©
x = torch.randn(3, 3)
y = torch.ones(3, 3)
result = torch.where(x > 0, x, y)  # x>0 å– xï¼Œå¦åˆ™å– y
print(f"where ç»“æœ:\n{result}")

# masked_select: æŒ‰æ©ç é€‰æ‹©
mask = x > 0
selected = torch.masked_select(x, mask)
print(f"æ­£æ•°å…ƒç´ : {selected}")

# index_select: æŒ‰ç´¢å¼•é€‰æ‹©
t = torch.arange(12).reshape(3, 4)
result = torch.index_select(t, dim=0, index=torch.tensor([0, 2]))
print(f"é€‰æ‹©ç¬¬ 0 å’Œ 2 è¡Œ:\n{result}")
```

---

## 4. å½¢çŠ¶æ“ä½œ

### 4.1 reshape å’Œ view

```python
t = torch.arange(12)

# reshape: æ”¹å˜å½¢çŠ¶
t1 = t.reshape(3, 4)
t2 = t.reshape(2, 2, 3)
t3 = t.reshape(3, -1)  # -1 è‡ªåŠ¨è®¡ç®—

print(f"reshape(3, 4):\n{t1}")
print(f"reshape(2, 2, 3):\n{t2}")

# view: å’Œ reshape ç±»ä¼¼ï¼Œä½†è¦æ±‚å†…å­˜è¿ç»­
t4 = t.view(3, 4)

# ä»€ä¹ˆæ—¶å€™ç”¨ reshape vs viewï¼Ÿ
# view æ›´å¿«ï¼ˆä¸å¤åˆ¶æ•°æ®ï¼‰ï¼Œä½†è¦æ±‚å†…å­˜è¿ç»­
# reshape æ›´é€šç”¨ï¼Œå¿…è¦æ—¶ä¼šå¤åˆ¶æ•°æ®
```

### 4.2 squeeze å’Œ unsqueeze

```python
# squeeze: å»é™¤å¤§å°ä¸º 1 çš„ç»´åº¦
t = torch.randn(1, 3, 1, 4)
print(f"åŸå§‹å½¢çŠ¶: {t.shape}")  # [1, 3, 1, 4]

t1 = t.squeeze()      # å»é™¤æ‰€æœ‰å¤§å°ä¸º 1 çš„ç»´åº¦
print(f"squeeze(): {t1.shape}")  # [3, 4]

t2 = t.squeeze(0)     # åªå»é™¤ç¬¬ 0 ç»´
print(f"squeeze(0): {t2.shape}")  # [3, 1, 4]

# unsqueeze: å¢åŠ å¤§å°ä¸º 1 çš„ç»´åº¦
t = torch.randn(3, 4)
print(f"åŸå§‹å½¢çŠ¶: {t.shape}")  # [3, 4]

t1 = t.unsqueeze(0)   # åœ¨ç¬¬ 0 ç»´å¢åŠ 
print(f"unsqueeze(0): {t1.shape}")  # [1, 3, 4]

t2 = t.unsqueeze(-1)  # åœ¨æœ€åå¢åŠ 
print(f"unsqueeze(-1): {t2.shape}")  # [3, 4, 1]

# å¸¸è§ç”¨æ³•ï¼šç»™ batch ç»´åº¦
single_image = torch.randn(3, 224, 224)  # [C, H, W]
batch_image = single_image.unsqueeze(0)  # [1, C, H, W]
```

### 4.3 è½¬ç½®å’Œç»´åº¦äº¤æ¢

```python
t = torch.randn(2, 3, 4)

# transpose: äº¤æ¢ä¸¤ä¸ªç»´åº¦
t1 = t.transpose(0, 1)
print(f"transpose(0, 1): {t1.shape}")  # [3, 2, 4]

# permute: ä»»æ„é‡æ’ç»´åº¦
t2 = t.permute(2, 0, 1)
print(f"permute(2, 0, 1): {t2.shape}")  # [4, 2, 3]

# 2D çŸ©é˜µè½¬ç½®
m = torch.randn(3, 4)
mt = m.T  # æˆ– m.t()
print(f"çŸ©é˜µè½¬ç½®: {m.shape} -> {mt.shape}")  # [3, 4] -> [4, 3]

# å›¾åƒæ ¼å¼è½¬æ¢
# PyTorch: [N, C, H, W]
# TensorFlow/PIL: [N, H, W, C]
img_pytorch = torch.randn(32, 3, 224, 224)
img_tf = img_pytorch.permute(0, 2, 3, 1)
print(f"PyTorch -> TF: {img_pytorch.shape} -> {img_tf.shape}")
```

### 4.4 æ‹¼æ¥å’Œåˆ†å‰²

```python
# cat: æ²¿ç°æœ‰ç»´åº¦æ‹¼æ¥
a = torch.randn(2, 3)
b = torch.randn(2, 3)

c = torch.cat([a, b], dim=0)  # æ²¿ç¬¬ 0 ç»´
print(f"cat dim=0: {c.shape}")  # [4, 3]

d = torch.cat([a, b], dim=1)  # æ²¿ç¬¬ 1 ç»´
print(f"cat dim=1: {d.shape}")  # [2, 6]

# stack: æ²¿æ–°ç»´åº¦å †å 
e = torch.stack([a, b], dim=0)
print(f"stack dim=0: {e.shape}")  # [2, 2, 3]

# split: å‡åŒ€åˆ†å‰²
chunks = torch.split(c, split_size_or_sections=2, dim=0)
print(f"split: {[chunk.shape for chunk in chunks]}")  # [[2, 3], [2, 3]]

# chunk: åˆ†æˆ N ä»½
chunks = torch.chunk(c, chunks=2, dim=0)
print(f"chunk: {[chunk.shape for chunk in chunks]}")  # [[2, 3], [2, 3]]
```

---

## 5. æ•°å­¦è¿ç®—

### 5.1 åŸºæœ¬è¿ç®—

```python
a = torch.tensor([1., 2., 3.])
b = torch.tensor([4., 5., 6.])

# åŠ å‡ä¹˜é™¤
print(f"a + b = {a + b}")
print(f"a - b = {a - b}")
print(f"a * b = {a * b}")  # é€å…ƒç´ ä¹˜æ³•
print(f"a / b = {a / b}")
print(f"a ** 2 = {a ** 2}")  # å¹‚è¿ç®—

# å‡½æ•°å½¢å¼
print(f"torch.add(a, b) = {torch.add(a, b)}")
print(f"torch.mul(a, b) = {torch.mul(a, b)}")

# åŸåœ°æ“ä½œï¼ˆèŠ‚çœå†…å­˜ï¼Œä½†ä¼šå½±å“æ¢¯åº¦è®¡ç®—ï¼‰
a.add_(1)  # a = a + 1
print(f"åŸåœ°åŠ æ³•: {a}")
```

### 5.2 çŸ©é˜µè¿ç®—

```python
A = torch.randn(2, 3)
B = torch.randn(3, 4)

# çŸ©é˜µä¹˜æ³•
C = torch.mm(A, B)       # 2D çŸ©é˜µä¹˜æ³•
C = torch.matmul(A, B)   # é€šç”¨ï¼Œæ”¯æŒå¹¿æ’­
C = A @ B                # è¿ç®—ç¬¦å½¢å¼
print(f"çŸ©é˜µä¹˜æ³•: {A.shape} @ {B.shape} = {C.shape}")  # [2, 4]

# æ‰¹é‡çŸ©é˜µä¹˜æ³•
batch_A = torch.randn(32, 2, 3)
batch_B = torch.randn(32, 3, 4)
batch_C = torch.bmm(batch_A, batch_B)  # [32, 2, 4]
# æˆ–
batch_C = batch_A @ batch_B

# å‘é‡ç‚¹ç§¯
v1 = torch.tensor([1., 2., 3.])
v2 = torch.tensor([4., 5., 6.])
dot = torch.dot(v1, v2)
print(f"ç‚¹ç§¯: {dot}")  # 32

# å¤–ç§¯
outer = torch.outer(v1, v2)
print(f"å¤–ç§¯å½¢çŠ¶: {outer.shape}")  # [3, 3]
```

### 5.3 è§„çº¦è¿ç®—

```python
t = torch.tensor([[1., 2., 3.], [4., 5., 6.]])

# æ±‚å’Œ
print(f"æ€»å’Œ: {t.sum()}")
print(f"æŒ‰è¡Œæ±‚å’Œ: {t.sum(dim=1)}")  # [6, 15]
print(f"æŒ‰åˆ—æ±‚å’Œ: {t.sum(dim=0)}")  # [5, 7, 9]

# å‡å€¼
print(f"å‡å€¼: {t.mean()}")
print(f"æŒ‰è¡Œå‡å€¼: {t.mean(dim=1)}")

# æœ€å€¼
print(f"æœ€å¤§å€¼: {t.max()}")
print(f"æœ€å°å€¼: {t.min()}")

# æœ€å€¼åŠå…¶ç´¢å¼•
max_val, max_idx = t.max(dim=1)
print(f"æ¯è¡Œæœ€å¤§å€¼: {max_val}, ç´¢å¼•: {max_idx}")

# argmax/argmin
print(f"æœ€å¤§å€¼ç´¢å¼•: {t.argmax()}")
print(f"æ¯è¡Œæœ€å¤§å€¼ç´¢å¼•: {t.argmax(dim=1)}")

# å…¶ä»–
print(f"æ ‡å‡†å·®: {t.std()}")
print(f"æ–¹å·®: {t.var()}")
print(f"ç´¯ç§¯å’Œ: {t.cumsum(dim=1)}")
```

### 5.4 å¹¿æ’­æœºåˆ¶

```python
# PyTorch çš„å¹¿æ’­è§„åˆ™ä¸ NumPy ç›¸åŒ
a = torch.randn(3, 4)
b = torch.randn(4)      # è‡ªåŠ¨æ‰©å±•ä¸º [3, 4]
c = torch.randn(3, 1)   # è‡ªåŠ¨æ‰©å±•ä¸º [3, 4]

print(f"a + b: {(a + b).shape}")  # [3, 4]
print(f"a + c: {(a + c).shape}")  # [3, 4]
print(f"b + c: {(b + c).shape}")  # [3, 4]

# å¸¸è§ç”¨æ³•ï¼šå¯¹æ¯è¡Œå‡å»å‡å€¼
x = torch.randn(32, 100)
mean = x.mean(dim=1, keepdim=True)  # [32, 1]
x_centered = x - mean  # å¹¿æ’­ç›¸å‡
```

---

## 6. è®¾å¤‡ç®¡ç†

### 6.1 GPU åŸºç¡€

```python
# æ£€æŸ¥ CUDA
print(f"CUDA å¯ç”¨: {torch.cuda.is_available()}")
print(f"GPU æ•°é‡: {torch.cuda.device_count()}")
if torch.cuda.is_available():
    print(f"å½“å‰ GPU: {torch.cuda.current_device()}")
    print(f"GPU åç§°: {torch.cuda.get_device_name(0)}")
```

### 6.2 è®¾å¤‡è½¬ç§»

```python
# å®šä¹‰è®¾å¤‡
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"ä½¿ç”¨è®¾å¤‡: {device}")

# åˆ›å»ºæ—¶æŒ‡å®šè®¾å¤‡
t = torch.randn(3, 4, device=device)
print(f"Tensor è®¾å¤‡: {t.device}")

# è½¬ç§»è®¾å¤‡
t_cpu = torch.randn(3, 4)
t_gpu = t_cpu.to(device)  # æ¨èæ–¹å¼
# æˆ–
t_gpu = t_cpu.cuda()      # ç›´æ¥è½¬åˆ° GPU
t_cpu = t_gpu.cpu()       # è½¬å› CPU

# æ¨¡å‹è½¬ç§»
model = MyModel()
model = model.to(device)

# æ•°æ®è½¬ç§»
for x, y in dataloader:
    x = x.to(device)
    y = y.to(device)
    output = model(x)
```

### 6.3 å†…å­˜ç®¡ç†

```python
if torch.cuda.is_available():
    # æŸ¥çœ‹ GPU å†…å­˜
    print(f"å·²åˆ†é…: {torch.cuda.memory_allocated() / 1024**2:.2f} MB")
    print(f"å·²ç¼“å­˜: {torch.cuda.memory_reserved() / 1024**2:.2f} MB")

    # æ¸…ç©ºç¼“å­˜
    torch.cuda.empty_cache()

    # é‡ç½®ç»Ÿè®¡ä¿¡æ¯
    torch.cuda.reset_peak_memory_stats()
```

---

## 7. ä¸ NumPy äº’è½¬

```python
import numpy as np

# Tensor â†’ NumPy
t = torch.randn(3, 4)
arr = t.numpy()  # å…±äº«å†…å­˜ï¼ˆCPU ä¸Šï¼‰
arr = t.detach().cpu().numpy()  # å®‰å…¨æ–¹å¼ï¼ˆGPU æˆ–æœ‰æ¢¯åº¦æ—¶ï¼‰

# NumPy â†’ Tensor
arr = np.array([1., 2., 3.])
t = torch.from_numpy(arr)  # å…±äº«å†…å­˜
t = torch.tensor(arr)      # å¤åˆ¶æ•°æ®

# æ³¨æ„ï¼šå…±äº«å†…å­˜æ„å‘³ç€ä¿®æ”¹ä¸€ä¸ªä¼šå½±å“å¦ä¸€ä¸ª
arr = np.array([1., 2., 3.])
t = torch.from_numpy(arr)
arr[0] = 100
print(t)  # tensor([100.,   2.,   3.])
```

---

## 8. ç»ƒä¹ é¢˜

### åŸºç¡€ç»ƒä¹ 

1. åˆ›å»ºä¸€ä¸ªå½¢çŠ¶ä¸º [3, 4, 5] çš„éšæœº Tensorï¼Œç„¶å reshape æˆ [12, 5]
2. ç»™å®šä¸€ä¸ªå½¢çŠ¶ä¸º [32, 10] çš„ Tensorï¼ˆè¡¨ç¤º 32 ä¸ªæ ·æœ¬çš„ 10 åˆ†ç±» logitsï¼‰ï¼Œæ‰¾å‡ºæ¯ä¸ªæ ·æœ¬çš„é¢„æµ‹ç±»åˆ«
3. å®ç°æ‰¹é‡å½’ä¸€åŒ–ï¼šå¯¹å½¢çŠ¶ [batch, features] çš„ Tensorï¼Œæ¯ä¸ªç‰¹å¾å‡å‡å€¼é™¤æ ‡å‡†å·®

### å‚è€ƒç­”æ¡ˆ

<details>
<summary>ç‚¹å‡»æŸ¥çœ‹ç­”æ¡ˆ</summary>

```python
import torch

# 1. reshape
t = torch.randn(3, 4, 5)
t_reshaped = t.reshape(12, 5)
print(f"åŸå§‹: {t.shape}, reshapeå: {t_reshaped.shape}")

# 2. æ‰¾é¢„æµ‹ç±»åˆ«
logits = torch.randn(32, 10)
predictions = logits.argmax(dim=1)
print(f"é¢„æµ‹ç±»åˆ«å½¢çŠ¶: {predictions.shape}")  # [32]
print(f"å‰5ä¸ªé¢„æµ‹: {predictions[:5]}")

# 3. æ‰¹é‡å½’ä¸€åŒ–
x = torch.randn(32, 100)
mean = x.mean(dim=0, keepdim=True)  # [1, 100]
std = x.std(dim=0, keepdim=True)    # [1, 100]
x_normalized = (x - mean) / (std + 1e-8)

print(f"å½’ä¸€åŒ–åå‡å€¼: {x_normalized.mean(dim=0).mean():.6f}")  # çº¦ 0
print(f"å½’ä¸€åŒ–åæ ‡å‡†å·®: {x_normalized.std(dim=0).mean():.6f}")  # çº¦ 1
```

</details>

---

## â¡ï¸ ä¸‹ä¸€æ­¥

å­¦å®Œæœ¬èŠ‚åï¼Œç»§ç»­å­¦ä¹  [03-PyTorchåŸºç¡€-autograd.md](./03-PyTorchåŸºç¡€-autograd.md)

