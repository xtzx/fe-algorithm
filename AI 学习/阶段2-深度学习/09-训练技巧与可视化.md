# ⚡ 09 - 训练技巧与可视化

> 掌握训练技巧让模型训练更稳定、更高效

---

## 目录

1. [学习率调度](#1-学习率调度)
2. [梯度裁剪](#2-梯度裁剪)
3. [混合精度训练](#3-混合精度训练)
4. [梯度累积](#4-梯度累积)
5. [TensorBoard](#5-tensorboard)
6. [Weights & Biases](#6-weights--biases)
7. [训练诊断](#7-训练诊断)
8. [练习题](#8-练习题)

---

## 1. 学习率调度

### 1.1 为什么需要学习率调度？

```
训练初期：学习率大 → 快速收敛
训练后期：学习率小 → 精细调整

常见策略：
1. Step Decay：每 N 个 epoch 衰减
2. Cosine Annealing：余弦曲线衰减
3. Warmup：开始时逐渐增大学习率
4. ReduceLROnPlateau：验证指标不下降时衰减
```

### 1.2 常用调度器

```python
import torch
import torch.optim as optim
from torch.optim import lr_scheduler
import matplotlib.pyplot as plt

model = torch.nn.Linear(10, 2)
optimizer = optim.Adam(model.parameters(), lr=0.001)

# 1. StepLR：每 step_size 个 epoch 衰减 gamma 倍
scheduler = lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)
# lr: 0.001 → 0.0001 → 0.00001 ...

# 2. MultiStepLR：在指定 epoch 衰减
scheduler = lr_scheduler.MultiStepLR(optimizer, milestones=[30, 80], gamma=0.1)

# 3. ExponentialLR：指数衰减
scheduler = lr_scheduler.ExponentialLR(optimizer, gamma=0.95)

# 4. CosineAnnealingLR：余弦退火
scheduler = lr_scheduler.CosineAnnealingLR(optimizer, T_max=100, eta_min=0)

# 5. CosineAnnealingWarmRestarts：带重启的余弦退火
scheduler = lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=2)

# 6. ReduceLROnPlateau：验证指标不下降时衰减
scheduler = lr_scheduler.ReduceLROnPlateau(
    optimizer, mode='min', factor=0.1, patience=10, verbose=True
)
```

### 1.3 Warmup + Cosine

```python
import math

def get_cosine_schedule_with_warmup(optimizer, num_warmup_steps, num_training_steps):
    """Warmup + Cosine Decay"""

    def lr_lambda(current_step):
        # Warmup 阶段
        if current_step < num_warmup_steps:
            return float(current_step) / float(max(1, num_warmup_steps))
        # Cosine Decay 阶段
        progress = float(current_step - num_warmup_steps) / \
                   float(max(1, num_training_steps - num_warmup_steps))
        return max(0.0, 0.5 * (1.0 + math.cos(math.pi * progress)))

    return lr_scheduler.LambdaLR(optimizer, lr_lambda)

# 使用
total_steps = 1000
warmup_steps = 100

scheduler = get_cosine_schedule_with_warmup(optimizer, warmup_steps, total_steps)

# 可视化
lrs = []
for step in range(total_steps):
    lrs.append(optimizer.param_groups[0]['lr'])
    optimizer.step()
    scheduler.step()

plt.plot(lrs)
plt.xlabel('Step')
plt.ylabel('Learning Rate')
plt.title('Warmup + Cosine Schedule')
plt.show()
```

### 1.4 训练循环中使用

```python
# 大多数调度器：每个 epoch 调用一次
for epoch in range(num_epochs):
    train_one_epoch()
    scheduler.step()  # 在 epoch 结束时调用

# ReduceLROnPlateau：需要传入验证指标
for epoch in range(num_epochs):
    train_one_epoch()
    val_loss = validate()
    scheduler.step(val_loss)  # 传入验证损失

# Warmup：每个 step 调用一次
for epoch in range(num_epochs):
    for batch in train_loader:
        train_step()
        scheduler.step()  # 在每个 batch 后调用
```

---

## 2. 梯度裁剪

### 2.1 为什么需要梯度裁剪？

```
梯度爆炸：梯度过大 → 参数更新过猛 → 训练不稳定/发散
常见于：RNN/LSTM、深层网络、大学习率

梯度裁剪：限制梯度的大小
```

### 2.2 裁剪方法

```python
import torch.nn.utils as utils

# 方法 1：按范数裁剪（推荐）
# 如果梯度的总范数超过 max_norm，则等比例缩小所有梯度
loss.backward()
grad_norm = utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
optimizer.step()

# 返回裁剪前的梯度范数，可用于监控
print(f"Gradient norm: {grad_norm:.4f}")

# 方法 2：按值裁剪
# 将每个梯度元素限制在 [-clip_value, clip_value]
loss.backward()
utils.clip_grad_value_(model.parameters(), clip_value=0.5)
optimizer.step()
```

### 2.3 监控梯度

```python
def monitor_gradients(model):
    """监控梯度健康状况"""
    total_norm = 0
    for name, param in model.named_parameters():
        if param.grad is not None:
            param_norm = param.grad.data.norm(2)
            total_norm += param_norm.item() ** 2

            # 检测异常
            if torch.isnan(param.grad).any():
                print(f"Warning: NaN gradient in {name}")
            if torch.isinf(param.grad).any():
                print(f"Warning: Inf gradient in {name}")

    total_norm = total_norm ** 0.5
    return total_norm

# 在训练循环中使用
for batch in train_loader:
    loss = train_step(batch)
    loss.backward()

    grad_norm = monitor_gradients(model)
    if grad_norm > 10:
        print(f"Warning: Large gradient norm: {grad_norm:.2f}")

    utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
    optimizer.step()
```

---

## 3. 混合精度训练

### 3.1 为什么用混合精度？

```
FP32（单精度）：默认，32 位浮点数
FP16（半精度）：16 位浮点数

混合精度优势：
1. 内存减半 → 可以用更大 batch size
2. 计算更快 → 现代 GPU 有 FP16 加速（Tensor Core）
3. 几乎不损失精度

混合精度策略：
- 前向传播和反向传播用 FP16
- 参数更新用 FP32（避免数值问题）
```

### 3.2 使用 torch.cuda.amp

```python
from torch.cuda.amp import autocast, GradScaler

# 创建 GradScaler
scaler = GradScaler()

model = model.cuda()
optimizer = optim.Adam(model.parameters())

for epoch in range(num_epochs):
    for data, target in train_loader:
        data, target = data.cuda(), target.cuda()

        optimizer.zero_grad()

        # 自动混合精度
        with autocast():
            output = model(data)
            loss = criterion(output, target)

        # 缩放损失并反向传播
        scaler.scale(loss).backward()

        # 更新参数
        scaler.step(optimizer)

        # 更新缩放因子
        scaler.update()

# 注意：
# 1. autocast 只在 CUDA 上生效
# 2. 某些操作（如 loss 计算）会自动转回 FP32
# 3. GradScaler 用于处理 FP16 的数值范围问题
```

### 3.3 完整示例

```python
import torch
import torch.nn as nn
import torch.optim as optim
from torch.cuda.amp import autocast, GradScaler
from torch.utils.data import DataLoader, TensorDataset

# 模拟数据
X = torch.randn(10000, 100)
y = torch.randint(0, 10, (10000,))
dataset = TensorDataset(X, y)
loader = DataLoader(dataset, batch_size=256, shuffle=True)

# 模型
model = nn.Sequential(
    nn.Linear(100, 512),
    nn.ReLU(),
    nn.Linear(512, 512),
    nn.ReLU(),
    nn.Linear(512, 10)
).cuda()

criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)
scaler = GradScaler()

# 训练
for epoch in range(10):
    total_loss = 0

    for data, target in loader:
        data, target = data.cuda(), target.cuda()

        optimizer.zero_grad()

        with autocast():
            output = model(data)
            loss = criterion(output, target)

        scaler.scale(loss).backward()

        # 可选：梯度裁剪（需要先 unscale）
        scaler.unscale_(optimizer)
        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)

        scaler.step(optimizer)
        scaler.update()

        total_loss += loss.item()

    print(f"Epoch {epoch+1}: Loss = {total_loss/len(loader):.4f}")
```

---

## 4. 梯度累积

### 4.1 为什么需要梯度累积？

```
问题：显存不够，batch size 太小
解决：累积多个小 batch 的梯度，模拟大 batch

效果：
accumulation_steps=4, batch_size=16 ≈ batch_size=64
```

### 4.2 实现

```python
accumulation_steps = 4
optimizer.zero_grad()

for i, (data, target) in enumerate(train_loader):
    data, target = data.cuda(), target.cuda()

    # 前向和反向
    output = model(data)
    loss = criterion(output, target)
    loss = loss / accumulation_steps  # 损失除以累积步数
    loss.backward()

    # 每 accumulation_steps 步更新一次
    if (i + 1) % accumulation_steps == 0:
        optimizer.step()
        optimizer.zero_grad()

# 处理最后不完整的累积
if (i + 1) % accumulation_steps != 0:
    optimizer.step()
    optimizer.zero_grad()
```

### 4.3 结合混合精度

```python
accumulation_steps = 4
scaler = GradScaler()

for epoch in range(num_epochs):
    optimizer.zero_grad()

    for i, (data, target) in enumerate(train_loader):
        data, target = data.cuda(), target.cuda()

        with autocast():
            output = model(data)
            loss = criterion(output, target)
            loss = loss / accumulation_steps

        scaler.scale(loss).backward()

        if (i + 1) % accumulation_steps == 0:
            scaler.step(optimizer)
            scaler.update()
            optimizer.zero_grad()
```

---

## 5. TensorBoard

### 5.1 安装和启动

```bash
pip install tensorboard

# 启动 TensorBoard
tensorboard --logdir=runs
# 然后在浏览器打开 http://localhost:6006
```

### 5.2 基本使用

```python
from torch.utils.tensorboard import SummaryWriter

# 创建 writer
writer = SummaryWriter('runs/experiment_1')

# 记录标量
for epoch in range(100):
    train_loss = ...
    val_loss = ...

    writer.add_scalar('Loss/train', train_loss, epoch)
    writer.add_scalar('Loss/val', val_loss, epoch)
    writer.add_scalar('Accuracy/train', train_acc, epoch)
    writer.add_scalar('Accuracy/val', val_acc, epoch)

# 记录多个标量到同一图
writer.add_scalars('Loss', {
    'train': train_loss,
    'val': val_loss
}, epoch)

# 关闭 writer
writer.close()
```

### 5.3 记录更多信息

```python
# 记录图像
images = ...  # [N, C, H, W]
writer.add_images('Sample Images', images, epoch)

# 记录单张图像
writer.add_image('Sample', image, epoch)

# 记录模型图
model = ...
dummy_input = torch.randn(1, 3, 224, 224)
writer.add_graph(model, dummy_input)

# 记录直方图（查看权重/梯度分布）
for name, param in model.named_parameters():
    writer.add_histogram(f'Parameters/{name}', param, epoch)
    if param.grad is not None:
        writer.add_histogram(f'Gradients/{name}', param.grad, epoch)

# 记录 Embedding
features = model.get_features(data)  # [N, D]
metadata = [str(label) for label in labels]
writer.add_embedding(features, metadata=metadata, global_step=epoch)

# 记录文本
writer.add_text('Description', 'Experiment 1: baseline model', 0)

# 记录超参数
hparams = {'lr': 0.001, 'batch_size': 32, 'hidden_dim': 128}
metrics = {'best_accuracy': 0.95, 'best_loss': 0.1}
writer.add_hparams(hparams, metrics)
```

### 5.4 完整训练示例

```python
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.tensorboard import SummaryWriter
from datetime import datetime

# 创建带时间戳的日志目录
timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
writer = SummaryWriter(f'runs/experiment_{timestamp}')

# 记录超参数
hparams = {'lr': 0.001, 'batch_size': 32, 'epochs': 100}
writer.add_text('Hyperparameters', str(hparams), 0)

model = ...
optimizer = optim.Adam(model.parameters(), lr=hparams['lr'])

# 记录模型结构
dummy_input = torch.randn(1, 3, 224, 224)
writer.add_graph(model, dummy_input)

for epoch in range(hparams['epochs']):
    # 训练
    model.train()
    train_loss, train_acc = train_one_epoch(model, train_loader)

    # 验证
    model.eval()
    val_loss, val_acc = evaluate(model, val_loader)

    # 记录指标
    writer.add_scalar('Loss/train', train_loss, epoch)
    writer.add_scalar('Loss/val', val_loss, epoch)
    writer.add_scalar('Accuracy/train', train_acc, epoch)
    writer.add_scalar('Accuracy/val', val_acc, epoch)
    writer.add_scalar('Learning_rate', optimizer.param_groups[0]['lr'], epoch)

    # 每 10 个 epoch 记录权重分布
    if epoch % 10 == 0:
        for name, param in model.named_parameters():
            writer.add_histogram(f'Weights/{name}', param, epoch)

writer.close()
```

---

## 6. Weights & Biases

### 6.1 安装和配置

```bash
pip install wandb
wandb login  # 需要注册账号
```

### 6.2 基本使用

```python
import wandb

# 初始化
wandb.init(
    project='my-project',
    name='experiment-1',
    config={
        'learning_rate': 0.001,
        'batch_size': 32,
        'epochs': 100,
        'architecture': 'ResNet-18'
    }
)

# 访问配置
config = wandb.config

# 训练循环中记录
for epoch in range(config.epochs):
    train_loss, train_acc = train()
    val_loss, val_acc = validate()

    wandb.log({
        'epoch': epoch,
        'train_loss': train_loss,
        'train_acc': train_acc,
        'val_loss': val_loss,
        'val_acc': val_acc,
    })

# 记录图像
wandb.log({'images': [wandb.Image(img) for img in images]})

# 记录模型
torch.save(model.state_dict(), 'model.pth')
wandb.save('model.pth')

# 结束
wandb.finish()
```

### 6.3 高级功能

```python
# 记录表格
table = wandb.Table(columns=['Image', 'Prediction', 'Label'])
for img, pred, label in predictions:
    table.add_data(wandb.Image(img), pred, label)
wandb.log({'predictions': table})

# 超参数搜索
sweep_config = {
    'method': 'bayes',
    'metric': {'name': 'val_acc', 'goal': 'maximize'},
    'parameters': {
        'learning_rate': {'min': 0.0001, 'max': 0.1},
        'batch_size': {'values': [16, 32, 64]},
        'epochs': {'value': 50}
    }
}

sweep_id = wandb.sweep(sweep_config, project='my-project')
wandb.agent(sweep_id, function=train_function, count=10)

# 模型监控
wandb.watch(model, log='all')  # 记录梯度和参数
```

---

## 7. 训练诊断

### 7.1 Loss 曲线分析

```
正常训练：
- Train loss 平滑下降
- Val loss 跟随下降，然后趋于平稳

过拟合：
- Train loss 持续下降
- Val loss 先降后升

欠拟合：
- Train loss 和 Val loss 都很高
- 两者差距不大

学习率过大：
- Loss 剧烈震荡或发散（NaN）

学习率过小：
- Loss 下降极慢
```

### 7.2 诊断工具

```python
import matplotlib.pyplot as plt

def plot_training_curves(train_losses, val_losses, train_accs, val_accs):
    fig, axes = plt.subplots(1, 2, figsize=(12, 4))

    # Loss 曲线
    axes[0].plot(train_losses, label='Train')
    axes[0].plot(val_losses, label='Validation')
    axes[0].set_xlabel('Epoch')
    axes[0].set_ylabel('Loss')
    axes[0].set_title('Loss Curves')
    axes[0].legend()

    # Accuracy 曲线
    axes[1].plot(train_accs, label='Train')
    axes[1].plot(val_accs, label='Validation')
    axes[1].set_xlabel('Epoch')
    axes[1].set_ylabel('Accuracy')
    axes[1].set_title('Accuracy Curves')
    axes[1].legend()

    plt.tight_layout()
    plt.show()

def diagnose_training(train_losses, val_losses):
    """简单的训练诊断"""
    # 检查 Loss 趋势
    if len(train_losses) < 10:
        print("训练轮数太少，无法诊断")
        return

    recent_train = train_losses[-10:]
    recent_val = val_losses[-10:]

    # 过拟合检测
    if recent_train[-1] < recent_train[0] and recent_val[-1] > recent_val[0]:
        print("⚠️ 可能过拟合：Train loss 下降但 Val loss 上升")
        print("建议：增加 Dropout、使用数据增强、减少模型复杂度")

    # 欠拟合检测
    if recent_train[-1] > 0.5 and abs(recent_train[-1] - recent_val[-1]) < 0.1:
        print("⚠️ 可能欠拟合：Train 和 Val loss 都较高且接近")
        print("建议：增加模型复杂度、增加训练轮数、检查数据")

    # 收敛检测
    if abs(recent_train[-1] - recent_train[0]) < 0.01:
        print("ℹ️ 模型可能已收敛")
        print("建议：尝试更小的学习率微调")
```

### 7.3 常见问题排查

```python
def debug_nan_loss(model, data, target, criterion):
    """排查 NaN Loss"""

    # 检查输入数据
    if torch.isnan(data).any():
        print("问题：输入数据包含 NaN")
        return

    # 前向传播
    output = model(data)

    # 检查输出
    if torch.isnan(output).any():
        print("问题：模型输出包含 NaN")
        # 检查各层
        for name, module in model.named_modules():
            # 注册 hook 检查中间输出
            pass
        return

    # 检查损失
    loss = criterion(output, target)
    if torch.isnan(loss):
        print("问题：损失计算产生 NaN")
        return

    # 反向传播
    loss.backward()

    # 检查梯度
    for name, param in model.named_parameters():
        if param.grad is not None:
            if torch.isnan(param.grad).any():
                print(f"问题：{name} 的梯度包含 NaN")
                return
            if torch.isinf(param.grad).any():
                print(f"问题：{name} 的梯度包含 Inf")
                return

    print("未发现 NaN 问题")
```

---

## 8. 练习题

### 基础练习

1. 实现 Warmup + Cosine 学习率调度，并可视化
2. 用混合精度训练 CIFAR-10，对比训练速度和显存使用
3. 设置 TensorBoard 记录一次完整训练

### 参考答案

<details>
<summary>点击查看答案</summary>

```python
# 1. Warmup + Cosine 学习率调度
import math
import matplotlib.pyplot as plt

def warmup_cosine_schedule(optimizer, warmup_epochs, total_epochs):
    def lr_lambda(epoch):
        if epoch < warmup_epochs:
            return epoch / warmup_epochs
        progress = (epoch - warmup_epochs) / (total_epochs - warmup_epochs)
        return 0.5 * (1 + math.cos(math.pi * progress))
    return torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)

# 可视化
model = torch.nn.Linear(10, 2)
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
scheduler = warmup_cosine_schedule(optimizer, warmup_epochs=10, total_epochs=100)

lrs = []
for epoch in range(100):
    lrs.append(optimizer.param_groups[0]['lr'])
    scheduler.step()

plt.plot(lrs)
plt.xlabel('Epoch')
plt.ylabel('Learning Rate')
plt.title('Warmup + Cosine Schedule')
plt.axvline(x=10, color='r', linestyle='--', label='Warmup End')
plt.legend()
plt.show()


# 2. 混合精度训练
from torch.cuda.amp import autocast, GradScaler
import time

def train_with_amp(use_amp=True):
    model = ...  # 你的模型
    model = model.cuda()
    optimizer = torch.optim.Adam(model.parameters())
    criterion = torch.nn.CrossEntropyLoss()
    scaler = GradScaler() if use_amp else None

    start_time = time.time()

    for epoch in range(10):
        for data, target in train_loader:
            data, target = data.cuda(), target.cuda()
            optimizer.zero_grad()

            if use_amp:
                with autocast():
                    output = model(data)
                    loss = criterion(output, target)
                scaler.scale(loss).backward()
                scaler.step(optimizer)
                scaler.update()
            else:
                output = model(data)
                loss = criterion(output, target)
                loss.backward()
                optimizer.step()

    elapsed = time.time() - start_time
    return elapsed

# 对比
time_amp = train_with_amp(use_amp=True)
time_fp32 = train_with_amp(use_amp=False)
print(f"AMP 时间: {time_amp:.2f}s")
print(f"FP32 时间: {time_fp32:.2f}s")
print(f"加速比: {time_fp32/time_amp:.2f}x")
```

</details>

---

## ➡️ 下一步

学完本节后，继续学习 [10-模型调试与可视化.md](./10-模型调试与可视化.md)

