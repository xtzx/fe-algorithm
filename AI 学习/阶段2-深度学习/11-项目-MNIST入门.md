# ğŸ”¢ 11 - é¡¹ç›®ï¼šMNIST æ‰‹å†™æ•°å­—è¯†åˆ«

> æ·±åº¦å­¦ä¹ çš„ "Hello World"ï¼Œå…¥é—¨é¦–é€‰é¡¹ç›®

---

## ç›®å½•

1. [é¡¹ç›®æ¦‚è¿°](#1-é¡¹ç›®æ¦‚è¿°)
2. [æ•°æ®å‡†å¤‡](#2-æ•°æ®å‡†å¤‡)
3. [MLP æ¨¡å‹](#3-mlp-æ¨¡å‹)
4. [CNN æ¨¡å‹](#4-cnn-æ¨¡å‹)
5. [è®­ç»ƒä¸è¯„ä¼°](#5-è®­ç»ƒä¸è¯„ä¼°)
6. [ç»“æœåˆ†æ](#6-ç»“æœåˆ†æ)
7. [æ‰©å±•ä»»åŠ¡](#7-æ‰©å±•ä»»åŠ¡)

---

## 1. é¡¹ç›®æ¦‚è¿°

### 1.1 ä»»åŠ¡è¯´æ˜

```
æ•°æ®é›†ï¼šMNIST æ‰‹å†™æ•°å­—
â”œâ”€â”€ è®­ç»ƒé›†ï¼š60,000 å¼  28x28 ç°åº¦å›¾åƒ
â”œâ”€â”€ æµ‹è¯•é›†ï¼š10,000 å¼ 
â”œâ”€â”€ ç±»åˆ«ï¼š0-9 å…± 10 ä¸ªæ•°å­—
â””â”€â”€ éš¾åº¦ï¼šâ­ï¼ˆå…¥é—¨çº§ï¼‰

ç›®æ ‡ï¼šè¯†åˆ«æ‰‹å†™æ•°å­—å›¾åƒå±äºå“ªä¸ªç±»åˆ«

æ–¹æ¡ˆï¼š
1. MLPï¼ˆå…¨è¿æ¥ç½‘ç»œï¼‰
2. ç®€å• CNN
```

### 1.2 ä¸ºä»€ä¹ˆé€‰æ‹© MNIST

| ç‰¹ç‚¹ | è¯´æ˜ |
|------|------|
| ç®€å• | å›¾åƒå°ï¼ˆ28x28ï¼‰ï¼Œç±»åˆ«å°‘ï¼ˆ10 ç±»ï¼‰ |
| å¿«é€Ÿ | CPU ä¹Ÿèƒ½å¿«é€Ÿè®­ç»ƒ |
| ç»å…¸ | æ·±åº¦å­¦ä¹ å…¥é—¨å¿…åšé¡¹ç›® |
| æ˜“è°ƒè¯• | å®¹æ˜“è¾¾åˆ° 99% å‡†ç¡®ç‡ |

---

## 2. æ•°æ®å‡†å¤‡

### 2.1 å®Œæ•´ä»£ç 

```python
"""
MNIST æ‰‹å†™æ•°å­—è¯†åˆ«é¡¹ç›®
ç›®æ ‡ï¼šç†Ÿæ‚‰ PyTorch å®Œæ•´è®­ç»ƒæµç¨‹
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import DataLoader
from torchvision import datasets, transforms
import matplotlib.pyplot as plt
import numpy as np

# è®¾ç½®éšæœºç§å­
torch.manual_seed(42)

# è®¾å¤‡é€‰æ‹©
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"ä½¿ç”¨è®¾å¤‡: {device}")

# ============================================================
# 2. æ•°æ®å‡†å¤‡
# ============================================================
print("\n" + "=" * 60)
print("1. æ•°æ®å‡†å¤‡")
print("=" * 60)

# æ•°æ®é¢„å¤„ç†
transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.1307,), (0.3081,))  # MNIST çš„å‡å€¼å’Œæ ‡å‡†å·®
])

# ä¸‹è½½å¹¶åŠ è½½æ•°æ®
train_dataset = datasets.MNIST(
    root='./data',
    train=True,
    download=True,
    transform=transform
)

test_dataset = datasets.MNIST(
    root='./data',
    train=False,
    download=True,
    transform=transform
)

# åˆ›å»º DataLoader
train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=1000, shuffle=False)

print(f"è®­ç»ƒé›†å¤§å°: {len(train_dataset)}")
print(f"æµ‹è¯•é›†å¤§å°: {len(test_dataset)}")
print(f"å›¾åƒå°ºå¯¸: {train_dataset[0][0].shape}")
print(f"ç±»åˆ«æ•°: {len(train_dataset.classes)}")

# å¯è§†åŒ–æ ·æœ¬
fig, axes = plt.subplots(2, 5, figsize=(12, 5))
for idx, ax in enumerate(axes.flatten()):
    image, label = train_dataset[idx]
    ax.imshow(image.squeeze().numpy(), cmap='gray')
    ax.set_title(f'Label: {label}')
    ax.axis('off')

plt.suptitle('MNIST Samples', fontsize=14)
plt.tight_layout()
plt.savefig('mnist_samples.png', dpi=150)
plt.show()

# ç±»åˆ«åˆ†å¸ƒ
labels = [train_dataset[i][1] for i in range(len(train_dataset))]
plt.figure(figsize=(10, 5))
plt.hist(labels, bins=10, edgecolor='black', alpha=0.7)
plt.xlabel('Digit')
plt.ylabel('Count')
plt.title('Class Distribution in Training Set')
plt.xticks(range(10))
plt.grid(True, alpha=0.3)
plt.savefig('mnist_distribution.png', dpi=150)
plt.show()
```

---

## 3. MLP æ¨¡å‹

### 3.1 æ¨¡å‹å®šä¹‰

```python
# ============================================================
# 3. MLP æ¨¡å‹
# ============================================================
print("\n" + "=" * 60)
print("2. MLP æ¨¡å‹")
print("=" * 60)

class MLP(nn.Module):
    """å¤šå±‚æ„ŸçŸ¥æœº"""
    def __init__(self, input_dim=784, hidden_dim=256, num_classes=10):
        super().__init__()
        self.flatten = nn.Flatten()
        self.layers = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(hidden_dim, num_classes)
        )

    def forward(self, x):
        x = self.flatten(x)  # [B, 1, 28, 28] -> [B, 784]
        return self.layers(x)

# åˆ›å»ºæ¨¡å‹
mlp_model = MLP()
print(mlp_model)

# å‚æ•°ç»Ÿè®¡
total_params = sum(p.numel() for p in mlp_model.parameters())
print(f"\nMLP å‚æ•°é‡: {total_params:,}")

# æµ‹è¯•å‰å‘ä¼ æ’­
sample_input = torch.randn(2, 1, 28, 28)
sample_output = mlp_model(sample_input)
print(f"è¾“å…¥å½¢çŠ¶: {sample_input.shape}")
print(f"è¾“å‡ºå½¢çŠ¶: {sample_output.shape}")
```

---

## 4. CNN æ¨¡å‹

### 4.1 æ¨¡å‹å®šä¹‰

```python
# ============================================================
# 4. CNN æ¨¡å‹
# ============================================================
print("\n" + "=" * 60)
print("3. CNN æ¨¡å‹")
print("=" * 60)

class SimpleCNN(nn.Module):
    """ç®€å•çš„ CNN"""
    def __init__(self, num_classes=10):
        super().__init__()
        # å·ç§¯å±‚
        self.conv_layers = nn.Sequential(
            # ç¬¬ä¸€å±‚å·ç§¯ï¼š1 -> 32 é€šé“
            nn.Conv2d(1, 32, kernel_size=3, padding=1),  # 28x28 -> 28x28
            nn.BatchNorm2d(32),
            nn.ReLU(),
            nn.MaxPool2d(2),  # 28x28 -> 14x14

            # ç¬¬äºŒå±‚å·ç§¯ï¼š32 -> 64 é€šé“
            nn.Conv2d(32, 64, kernel_size=3, padding=1),  # 14x14 -> 14x14
            nn.BatchNorm2d(64),
            nn.ReLU(),
            nn.MaxPool2d(2),  # 14x14 -> 7x7
        )

        # å…¨è¿æ¥å±‚
        self.fc_layers = nn.Sequential(
            nn.Flatten(),
            nn.Linear(64 * 7 * 7, 128),
            nn.ReLU(),
            nn.Dropout(0.5),
            nn.Linear(128, num_classes)
        )

    def forward(self, x):
        x = self.conv_layers(x)
        x = self.fc_layers(x)
        return x

# åˆ›å»ºæ¨¡å‹
cnn_model = SimpleCNN()
print(cnn_model)

# å‚æ•°ç»Ÿè®¡
total_params_cnn = sum(p.numel() for p in cnn_model.parameters())
print(f"\nCNN å‚æ•°é‡: {total_params_cnn:,}")

# æµ‹è¯•å‰å‘ä¼ æ’­
sample_output_cnn = cnn_model(sample_input)
print(f"è¾“å…¥å½¢çŠ¶: {sample_input.shape}")
print(f"è¾“å‡ºå½¢çŠ¶: {sample_output_cnn.shape}")
```

---

## 5. è®­ç»ƒä¸è¯„ä¼°

### 5.1 è®­ç»ƒå‡½æ•°

```python
# ============================================================
# 5. è®­ç»ƒä¸è¯„ä¼°
# ============================================================
print("\n" + "=" * 60)
print("4. è®­ç»ƒä¸è¯„ä¼°")
print("=" * 60)

def train_one_epoch(model, train_loader, criterion, optimizer, device):
    """è®­ç»ƒä¸€ä¸ª epoch"""
    model.train()
    total_loss = 0
    correct = 0
    total = 0

    for batch_idx, (data, target) in enumerate(train_loader):
        data, target = data.to(device), target.to(device)

        # å‰å‘ä¼ æ’­
        optimizer.zero_grad()
        output = model(data)
        loss = criterion(output, target)

        # åå‘ä¼ æ’­
        loss.backward()
        optimizer.step()

        # ç»Ÿè®¡
        total_loss += loss.item()
        pred = output.argmax(dim=1)
        correct += pred.eq(target).sum().item()
        total += target.size(0)

    avg_loss = total_loss / len(train_loader)
    accuracy = correct / total
    return avg_loss, accuracy


def evaluate(model, test_loader, criterion, device):
    """è¯„ä¼°æ¨¡å‹"""
    model.eval()
    total_loss = 0
    correct = 0
    total = 0

    with torch.no_grad():
        for data, target in test_loader:
            data, target = data.to(device), target.to(device)
            output = model(data)
            loss = criterion(output, target)

            total_loss += loss.item()
            pred = output.argmax(dim=1)
            correct += pred.eq(target).sum().item()
            total += target.size(0)

    avg_loss = total_loss / len(test_loader)
    accuracy = correct / total
    return avg_loss, accuracy


def train_model(model, train_loader, test_loader, num_epochs, device, model_name="model"):
    """å®Œæ•´è®­ç»ƒæµç¨‹"""
    model = model.to(device)
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=0.001)

    train_losses, test_losses = [], []
    train_accs, test_accs = [], []

    print(f"\nå¼€å§‹è®­ç»ƒ {model_name}...")
    print("-" * 50)

    for epoch in range(num_epochs):
        train_loss, train_acc = train_one_epoch(model, train_loader, criterion, optimizer, device)
        test_loss, test_acc = evaluate(model, test_loader, criterion, device)

        train_losses.append(train_loss)
        test_losses.append(test_loss)
        train_accs.append(train_acc)
        test_accs.append(test_acc)

        print(f"Epoch {epoch+1:2d}/{num_epochs}: "
              f"Train Loss={train_loss:.4f}, Train Acc={train_acc:.4f}, "
              f"Test Loss={test_loss:.4f}, Test Acc={test_acc:.4f}")

    return {
        'train_losses': train_losses,
        'test_losses': test_losses,
        'train_accs': train_accs,
        'test_accs': test_accs
    }
```

### 5.2 è®­ç»ƒä¸¤ä¸ªæ¨¡å‹

```python
# è®­ç»ƒ MLP
mlp_model = MLP().to(device)
mlp_results = train_model(mlp_model, train_loader, test_loader, num_epochs=10, device=device, model_name="MLP")

# è®­ç»ƒ CNN
cnn_model = SimpleCNN().to(device)
cnn_results = train_model(cnn_model, train_loader, test_loader, num_epochs=10, device=device, model_name="CNN")
```

---

## 6. ç»“æœåˆ†æ

### 6.1 è®­ç»ƒæ›²çº¿

```python
# ============================================================
# 6. ç»“æœåˆ†æ
# ============================================================
print("\n" + "=" * 60)
print("5. ç»“æœåˆ†æ")
print("=" * 60)

# ç»˜åˆ¶è®­ç»ƒæ›²çº¿
fig, axes = plt.subplots(2, 2, figsize=(14, 10))

# MLP Loss
axes[0, 0].plot(mlp_results['train_losses'], label='Train', linewidth=2)
axes[0, 0].plot(mlp_results['test_losses'], label='Test', linewidth=2)
axes[0, 0].set_xlabel('Epoch')
axes[0, 0].set_ylabel('Loss')
axes[0, 0].set_title('MLP - Loss Curves')
axes[0, 0].legend()
axes[0, 0].grid(True)

# MLP Accuracy
axes[0, 1].plot(mlp_results['train_accs'], label='Train', linewidth=2)
axes[0, 1].plot(mlp_results['test_accs'], label='Test', linewidth=2)
axes[0, 1].set_xlabel('Epoch')
axes[0, 1].set_ylabel('Accuracy')
axes[0, 1].set_title('MLP - Accuracy Curves')
axes[0, 1].legend()
axes[0, 1].grid(True)

# CNN Loss
axes[1, 0].plot(cnn_results['train_losses'], label='Train', linewidth=2)
axes[1, 0].plot(cnn_results['test_losses'], label='Test', linewidth=2)
axes[1, 0].set_xlabel('Epoch')
axes[1, 0].set_ylabel('Loss')
axes[1, 0].set_title('CNN - Loss Curves')
axes[1, 0].legend()
axes[1, 0].grid(True)

# CNN Accuracy
axes[1, 1].plot(cnn_results['train_accs'], label='Train', linewidth=2)
axes[1, 1].plot(cnn_results['test_accs'], label='Test', linewidth=2)
axes[1, 1].set_xlabel('Epoch')
axes[1, 1].set_ylabel('Accuracy')
axes[1, 1].set_title('CNN - Accuracy Curves')
axes[1, 1].legend()
axes[1, 1].grid(True)

plt.tight_layout()
plt.savefig('mnist_training_curves.png', dpi=150)
plt.show()

# æ¨¡å‹å¯¹æ¯”
print("\næ¨¡å‹å¯¹æ¯”:")
print("-" * 40)
print(f"MLP æœ€ç»ˆæµ‹è¯•å‡†ç¡®ç‡: {mlp_results['test_accs'][-1]:.4f}")
print(f"CNN æœ€ç»ˆæµ‹è¯•å‡†ç¡®ç‡: {cnn_results['test_accs'][-1]:.4f}")
```

### 6.2 æ··æ·†çŸ©é˜µ

```python
from sklearn.metrics import confusion_matrix, classification_report
import seaborn as sns

def plot_confusion_matrix(model, test_loader, device, title="Confusion Matrix"):
    """ç»˜åˆ¶æ··æ·†çŸ©é˜µ"""
    model.eval()
    all_preds = []
    all_targets = []

    with torch.no_grad():
        for data, target in test_loader:
            data = data.to(device)
            output = model(data)
            pred = output.argmax(dim=1)
            all_preds.extend(pred.cpu().numpy())
            all_targets.extend(target.numpy())

    cm = confusion_matrix(all_targets, all_preds)

    plt.figure(figsize=(10, 8))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                xticklabels=range(10), yticklabels=range(10))
    plt.xlabel('Predicted')
    plt.ylabel('Actual')
    plt.title(title)
    plt.tight_layout()
    plt.savefig(f'{title.lower().replace(" ", "_")}.png', dpi=150)
    plt.show()

    print(f"\n{title} - åˆ†ç±»æŠ¥å‘Š:")
    print(classification_report(all_targets, all_preds))

# ç»˜åˆ¶ CNN æ··æ·†çŸ©é˜µ
plot_confusion_matrix(cnn_model, test_loader, device, "CNN Confusion Matrix")
```

### 6.3 é”™è¯¯æ ·æœ¬åˆ†æ

```python
def analyze_errors(model, test_loader, device, num_samples=10):
    """åˆ†æé”™è¯¯é¢„æµ‹çš„æ ·æœ¬"""
    model.eval()
    errors = []

    with torch.no_grad():
        for data, target in test_loader:
            data, target = data.to(device), target.to(device)
            output = model(data)
            pred = output.argmax(dim=1)

            # æ‰¾å‡ºé”™è¯¯é¢„æµ‹
            wrong_mask = pred != target
            if wrong_mask.any():
                wrong_indices = wrong_mask.nonzero(as_tuple=True)[0]
                for idx in wrong_indices:
                    errors.append({
                        'image': data[idx].cpu(),
                        'true_label': target[idx].item(),
                        'pred_label': pred[idx].item(),
                        'confidence': F.softmax(output[idx], dim=0).max().item()
                    })
                    if len(errors) >= num_samples:
                        break
            if len(errors) >= num_samples:
                break

    # å¯è§†åŒ–é”™è¯¯æ ·æœ¬
    fig, axes = plt.subplots(2, 5, figsize=(15, 6))
    for idx, ax in enumerate(axes.flatten()):
        if idx < len(errors):
            err = errors[idx]
            ax.imshow(err['image'].squeeze().numpy(), cmap='gray')
            ax.set_title(f"True: {err['true_label']}, Pred: {err['pred_label']}\n"
                        f"Conf: {err['confidence']:.2f}", fontsize=10)
        ax.axis('off')

    plt.suptitle('Misclassified Samples', fontsize=14)
    plt.tight_layout()
    plt.savefig('mnist_errors.png', dpi=150)
    plt.show()

# åˆ†æé”™è¯¯æ ·æœ¬
analyze_errors(cnn_model, test_loader, device)
```

### 6.4 ç‰¹å¾å¯è§†åŒ–

```python
def visualize_conv_features(model, test_loader, device):
    """å¯è§†åŒ–å·ç§¯å±‚çš„ç‰¹å¾å›¾"""
    model.eval()

    # è·å–ä¸€å¼ å›¾åƒ
    data, target = next(iter(test_loader))
    image = data[0:1].to(device)
    label = target[0].item()

    # æå–ç‰¹å¾
    activations = {}
    def hook_fn(name):
        def hook(module, input, output):
            activations[name] = output.detach()
        return hook

    # æ³¨å†Œé’©å­
    model.conv_layers[0].register_forward_hook(hook_fn('conv1'))
    model.conv_layers[4].register_forward_hook(hook_fn('conv2'))

    # å‰å‘ä¼ æ’­
    with torch.no_grad():
        _ = model(image)

    # å¯è§†åŒ–
    fig, axes = plt.subplots(3, 8, figsize=(16, 6))

    # åŸå›¾
    axes[0, 0].imshow(image.squeeze().cpu().numpy(), cmap='gray')
    axes[0, 0].set_title(f'Original (Label: {label})')
    axes[0, 0].axis('off')
    for i in range(1, 8):
        axes[0, i].axis('off')

    # Conv1 ç‰¹å¾å›¾
    conv1_feat = activations['conv1'].squeeze().cpu().numpy()
    for i in range(8):
        axes[1, i].imshow(conv1_feat[i], cmap='viridis')
        axes[1, i].set_title(f'Conv1-{i}', fontsize=8)
        axes[1, i].axis('off')

    # Conv2 ç‰¹å¾å›¾
    conv2_feat = activations['conv2'].squeeze().cpu().numpy()
    for i in range(8):
        axes[2, i].imshow(conv2_feat[i], cmap='viridis')
        axes[2, i].set_title(f'Conv2-{i}', fontsize=8)
        axes[2, i].axis('off')

    plt.suptitle('CNN Feature Maps', fontsize=14)
    plt.tight_layout()
    plt.savefig('mnist_features.png', dpi=150)
    plt.show()

visualize_conv_features(cnn_model, test_loader, device)
```

---

## 7. æ‰©å±•ä»»åŠ¡

### 7.1 æ¨¡å‹ä¿å­˜ä¸åŠ è½½

```python
# ä¿å­˜æ¨¡å‹
torch.save(cnn_model.state_dict(), 'mnist_cnn_model.pth')
print("æ¨¡å‹å·²ä¿å­˜åˆ° mnist_cnn_model.pth")

# åŠ è½½æ¨¡å‹
loaded_model = SimpleCNN()
loaded_model.load_state_dict(torch.load('mnist_cnn_model.pth'))
loaded_model.to(device)
loaded_model.eval()

# éªŒè¯
test_loss, test_acc = evaluate(loaded_model, test_loader, nn.CrossEntropyLoss(), device)
print(f"åŠ è½½æ¨¡å‹çš„æµ‹è¯•å‡†ç¡®ç‡: {test_acc:.4f}")
```

### 7.2 é¢„æµ‹æ–°å›¾åƒ

```python
def predict_digit(model, image, device):
    """é¢„æµ‹å•å¼ å›¾åƒ"""
    model.eval()

    # é¢„å¤„ç†
    if isinstance(image, np.ndarray):
        image = torch.from_numpy(image).float()

    if image.dim() == 2:
        image = image.unsqueeze(0).unsqueeze(0)  # æ·»åŠ  batch å’Œ channel ç»´åº¦
    elif image.dim() == 3:
        image = image.unsqueeze(0)

    # å½’ä¸€åŒ–
    image = (image - 0.1307) / 0.3081

    image = image.to(device)

    with torch.no_grad():
        output = model(image)
        probs = F.softmax(output, dim=1)
        pred = output.argmax(dim=1).item()
        confidence = probs[0, pred].item()

    return pred, confidence, probs.squeeze().cpu().numpy()

# æµ‹è¯•
sample_image = test_dataset[42][0]
pred, conf, probs = predict_digit(cnn_model, sample_image, device)

print(f"é¢„æµ‹ç»“æœ: {pred}")
print(f"ç½®ä¿¡åº¦: {conf:.4f}")
print(f"å„ç±»åˆ«æ¦‚ç‡: {probs.round(3)}")

# å¯è§†åŒ–
plt.figure(figsize=(12, 4))

plt.subplot(1, 2, 1)
plt.imshow(sample_image.squeeze().numpy(), cmap='gray')
plt.title(f'Prediction: {pred} (Confidence: {conf:.2%})')
plt.axis('off')

plt.subplot(1, 2, 2)
plt.bar(range(10), probs)
plt.xlabel('Digit')
plt.ylabel('Probability')
plt.title('Class Probabilities')
plt.xticks(range(10))

plt.tight_layout()
plt.show()
```

### 7.3 è¿›é˜¶æŒ‘æˆ˜

```python
"""
è¿›é˜¶ä»»åŠ¡æ¸…å•ï¼š

1. æ•°æ®å¢å¼º
   - éšæœºæ—‹è½¬ï¼ˆÂ±15åº¦ï¼‰
   - éšæœºå¹³ç§»
   - å¼¹æ€§å˜å½¢

2. æ›´æ·±çš„ç½‘ç»œ
   - å¢åŠ å·ç§¯å±‚
   - ä½¿ç”¨æ®‹å·®è¿æ¥
   - å°è¯• 3x3 å·ç§¯å †å 

3. æ­£åˆ™åŒ–æŠ€å·§
   - å¢åŠ  Dropout
   - ä½¿ç”¨ Label Smoothing
   - æ·»åŠ  weight_decay

4. å­¦ä¹ ç‡è°ƒåº¦
   - StepLR
   - CosineAnnealingLR
   - OneCycleLR

5. ç›®æ ‡ï¼šè¾¾åˆ° 99.5%+ å‡†ç¡®ç‡
"""

# è¿›é˜¶ CNN ç¤ºä¾‹
class AdvancedCNN(nn.Module):
    def __init__(self, num_classes=10):
        super().__init__()
        self.features = nn.Sequential(
            nn.Conv2d(1, 32, 3, padding=1),
            nn.BatchNorm2d(32),
            nn.ReLU(),
            nn.Conv2d(32, 32, 3, padding=1),
            nn.BatchNorm2d(32),
            nn.ReLU(),
            nn.MaxPool2d(2),
            nn.Dropout(0.25),

            nn.Conv2d(32, 64, 3, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(),
            nn.Conv2d(64, 64, 3, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(),
            nn.MaxPool2d(2),
            nn.Dropout(0.25),

            nn.Conv2d(64, 128, 3, padding=1),
            nn.BatchNorm2d(128),
            nn.ReLU(),
            nn.Dropout(0.25),
        )

        self.classifier = nn.Sequential(
            nn.Flatten(),
            nn.Linear(128 * 7 * 7, 256),
            nn.ReLU(),
            nn.Dropout(0.5),
            nn.Linear(256, num_classes)
        )

    def forward(self, x):
        x = self.features(x)
        x = self.classifier(x)
        return x

# è®­ç»ƒè¿›é˜¶æ¨¡å‹
advanced_model = AdvancedCNN().to(device)
print(f"è¿›é˜¶ CNN å‚æ•°é‡: {sum(p.numel() for p in advanced_model.parameters()):,}")
```

---

## é¡¹ç›®æ€»ç»“

```
ğŸ¯ æœ¬é¡¹ç›®å®Œæˆçš„ä»»åŠ¡ï¼š

1. âœ… åŠ è½½ MNIST æ•°æ®é›†
2. âœ… å®ç° MLP å’Œ CNN ä¸¤ç§æ¨¡å‹
3. âœ… å®Œæˆè®­ç»ƒå’Œè¯„ä¼°æµç¨‹
4. âœ… å¯è§†åŒ–è®­ç»ƒæ›²çº¿å’Œæ··æ·†çŸ©é˜µ
5. âœ… åˆ†æé”™è¯¯æ ·æœ¬
6. âœ… å¯è§†åŒ– CNN ç‰¹å¾å›¾
7. âœ… æ¨¡å‹ä¿å­˜ä¸åŠ è½½

ğŸ“Š å…¸å‹ç»“æœï¼š
- MLPï¼š~97-98% å‡†ç¡®ç‡
- ç®€å• CNNï¼š~99% å‡†ç¡®ç‡
- è¿›é˜¶ CNNï¼š~99.5% å‡†ç¡®ç‡

ğŸ“ å­¦åˆ°çš„çŸ¥è¯†ç‚¹ï¼š
- PyTorch å®Œæ•´è®­ç»ƒæµç¨‹
- MLP vs CNN çš„åŒºåˆ«
- BatchNorm å’Œ Dropout çš„ä½¿ç”¨
- æ¨¡å‹è¯„ä¼°å’Œé”™è¯¯åˆ†æ
```

---

## â¡ï¸ ä¸‹ä¸€æ­¥

å®Œæˆæœ¬å…¥é—¨é¡¹ç›®åï¼Œç»§ç»­æŒ‘æˆ˜ [12-é¡¹ç›®-CIFAR10å›¾åƒåˆ†ç±».md](./12-é¡¹ç›®-CIFAR10å›¾åƒåˆ†ç±».md)

