# ğŸ“ 13 - é¡¹ç›®ï¼šæ–‡æœ¬æƒ…æ„Ÿåˆ†æ

> ä½¿ç”¨ LSTM/GRU å¯¹ç”µå½±è¯„è®ºè¿›è¡Œæƒ…æ„Ÿåˆ†ç±»

---

## ç›®å½•

1. [é¡¹ç›®æ¦‚è¿°](#1-é¡¹ç›®æ¦‚è¿°)
2. [æ•°æ®å‡†å¤‡](#2-æ•°æ®å‡†å¤‡)
3. [æ¨¡å‹å®ç°](#3-æ¨¡å‹å®ç°)
4. [è®­ç»ƒä¸è¯„ä¼°](#4-è®­ç»ƒä¸è¯„ä¼°)
5. [ç»“æœåˆ†æ](#5-ç»“æœåˆ†æ)
6. [ä¼˜åŒ–æ–¹å‘](#6-ä¼˜åŒ–æ–¹å‘)

---

## 1. é¡¹ç›®æ¦‚è¿°

### 1.1 ä»»åŠ¡è¯´æ˜

```
æ•°æ®é›†ï¼šIMDB ç”µå½±è¯„è®º
- 25,000 æ¡è®­ç»ƒæ ·æœ¬
- 25,000 æ¡æµ‹è¯•æ ·æœ¬
- äºŒåˆ†ç±»ï¼šæ­£é¢ (1) / è´Ÿé¢ (0)

ç›®æ ‡ï¼šæ ¹æ®è¯„è®ºæ–‡æœ¬é¢„æµ‹æƒ…æ„Ÿå€¾å‘

æ–¹æ¡ˆï¼š
1. åŸºç¡€ LSTM
2. åŒå‘ LSTM
3. å¸¦æ³¨æ„åŠ›æœºåˆ¶çš„ LSTM
```

### 1.2 é¡¹ç›®ç»“æ„

```
sentiment_project/
â”œâ”€â”€ data/              # æ•°æ®ç›®å½•
â”œâ”€â”€ models.py          # æ¨¡å‹å®šä¹‰
â”œâ”€â”€ dataset.py         # æ•°æ®å¤„ç†
â”œâ”€â”€ train.py           # è®­ç»ƒè„šæœ¬
â”œâ”€â”€ evaluate.py        # è¯„ä¼°è„šæœ¬
â””â”€â”€ checkpoints/       # ä¿å­˜çš„æ¨¡å‹
```

---

## 2. æ•°æ®å‡†å¤‡

```python
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from torchtext.datasets import IMDB
from torchtext.data.utils import get_tokenizer
from torchtext.vocab import build_vocab_from_iterator
from torch.nn.utils.rnn import pad_sequence
import numpy as np

# ========== ä¸‹è½½æ•°æ® ==========
# torchtext æ–¹å¼
train_iter, test_iter = IMDB(split=('train', 'test'))

# è½¬ä¸ºåˆ—è¡¨æ–¹ä¾¿å¤„ç†
train_data = list(train_iter)
test_data = list(test_iter)

print(f"è®­ç»ƒæ ·æœ¬æ•°: {len(train_data)}")
print(f"æµ‹è¯•æ ·æœ¬æ•°: {len(test_data)}")
print(f"æ ·æœ¬ç¤ºä¾‹: {train_data[0]}")

# ========== åˆ†è¯å™¨ ==========
tokenizer = get_tokenizer('basic_english')

# æµ‹è¯•åˆ†è¯
sample_text = "This movie is great! I really love it."
tokens = tokenizer(sample_text)
print(f"åˆ†è¯ç»“æœ: {tokens}")

# ========== æ„å»ºè¯è¡¨ ==========
def yield_tokens(data_iter):
    for label, text in data_iter:
        yield tokenizer(text)

# æ„å»ºè¯è¡¨
vocab = build_vocab_from_iterator(
    yield_tokens(train_data),
    min_freq=5,
    specials=['<pad>', '<unk>']
)
vocab.set_default_index(vocab['<unk>'])

print(f"è¯è¡¨å¤§å°: {len(vocab)}")
print(f"'great' çš„ç´¢å¼•: {vocab['great']}")
print(f"'<unk>' çš„ç´¢å¼•: {vocab['<unk>']}")

# ========== æ•°æ®å¤„ç† ==========
class IMDBDataset(Dataset):
    def __init__(self, data, vocab, tokenizer, max_len=256):
        self.data = data
        self.vocab = vocab
        self.tokenizer = tokenizer
        self.max_len = max_len

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        label, text = self.data[idx]

        # æ ‡ç­¾è½¬æ¢ï¼š'pos' -> 1, 'neg' -> 0
        label = 1 if label == 'pos' else 0

        # åˆ†è¯
        tokens = self.tokenizer(text)

        # æˆªæ–­
        if len(tokens) > self.max_len:
            tokens = tokens[:self.max_len]

        # è½¬ä¸ºç´¢å¼•
        token_ids = [self.vocab[token] for token in tokens]

        return torch.tensor(token_ids, dtype=torch.long), label, len(token_ids)

def collate_fn(batch):
    """å¤„ç†å˜é•¿åºåˆ—"""
    texts, labels, lengths = zip(*batch)

    # å¡«å……
    texts_padded = pad_sequence(texts, batch_first=True, padding_value=0)

    # è½¬ä¸ºå¼ é‡
    labels = torch.tensor(labels, dtype=torch.long)
    lengths = torch.tensor(lengths, dtype=torch.long)

    return texts_padded, labels, lengths

# åˆ›å»ºæ•°æ®é›†å’Œ DataLoader
train_dataset = IMDBDataset(train_data, vocab, tokenizer)
test_dataset = IMDBDataset(test_data, vocab, tokenizer)

train_loader = DataLoader(
    train_dataset, batch_size=64, shuffle=True,
    collate_fn=collate_fn, num_workers=2
)
test_loader = DataLoader(
    test_dataset, batch_size=128, shuffle=False,
    collate_fn=collate_fn, num_workers=2
)

# æŸ¥çœ‹ä¸€ä¸ªæ‰¹æ¬¡
texts, labels, lengths = next(iter(train_loader))
print(f"æ‰¹æ¬¡æ–‡æœ¬å½¢çŠ¶: {texts.shape}")
print(f"æ‰¹æ¬¡æ ‡ç­¾å½¢çŠ¶: {labels.shape}")
print(f"åºåˆ—é•¿åº¦: {lengths[:5]}")
```

---

## 3. æ¨¡å‹å®ç°

### 3.1 åŸºç¡€ LSTM

```python
class LSTMClassifier(nn.Module):
    def __init__(self, vocab_size, embed_dim, hidden_dim, num_layers,
                 num_classes, dropout=0.5, pad_idx=0):
        super().__init__()

        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=pad_idx)

        self.lstm = nn.LSTM(
            embed_dim, hidden_dim, num_layers,
            batch_first=True, dropout=dropout if num_layers > 1 else 0,
            bidirectional=False
        )

        self.dropout = nn.Dropout(dropout)
        self.fc = nn.Linear(hidden_dim, num_classes)

    def forward(self, text, lengths):
        # text: [batch, seq_len]
        embedded = self.embedding(text)  # [batch, seq_len, embed_dim]
        embedded = self.dropout(embedded)

        # Pack åºåˆ—
        packed = nn.utils.rnn.pack_padded_sequence(
            embedded, lengths.cpu(), batch_first=True, enforce_sorted=False
        )

        output, (hidden, cell) = self.lstm(packed)
        # hidden: [num_layers, batch, hidden_dim]

        # å–æœ€åä¸€å±‚çš„éšè—çŠ¶æ€
        hidden = self.dropout(hidden[-1])  # [batch, hidden_dim]

        return self.fc(hidden)  # [batch, num_classes]
```

### 3.2 åŒå‘ LSTM

```python
class BiLSTMClassifier(nn.Module):
    def __init__(self, vocab_size, embed_dim, hidden_dim, num_layers,
                 num_classes, dropout=0.5, pad_idx=0):
        super().__init__()

        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=pad_idx)

        self.lstm = nn.LSTM(
            embed_dim, hidden_dim, num_layers,
            batch_first=True, dropout=dropout if num_layers > 1 else 0,
            bidirectional=True
        )

        self.dropout = nn.Dropout(dropout)
        self.fc = nn.Linear(hidden_dim * 2, num_classes)  # *2 å› ä¸ºåŒå‘

    def forward(self, text, lengths):
        embedded = self.embedding(text)
        embedded = self.dropout(embedded)

        packed = nn.utils.rnn.pack_padded_sequence(
            embedded, lengths.cpu(), batch_first=True, enforce_sorted=False
        )

        output, (hidden, cell) = self.lstm(packed)
        # hidden: [num_layers * 2, batch, hidden_dim]

        # æ‹¼æ¥å‰å‘å’Œåå‘çš„æœ€åä¸€å±‚éšè—çŠ¶æ€
        hidden_forward = hidden[-2]  # æœ€åä¸€å±‚å‰å‘
        hidden_backward = hidden[-1]  # æœ€åä¸€å±‚åå‘
        hidden = torch.cat([hidden_forward, hidden_backward], dim=1)
        hidden = self.dropout(hidden)

        return self.fc(hidden)
```

### 3.3 å¸¦æ³¨æ„åŠ›æœºåˆ¶çš„ LSTM

```python
class AttentionLSTM(nn.Module):
    def __init__(self, vocab_size, embed_dim, hidden_dim, num_layers,
                 num_classes, dropout=0.5, pad_idx=0):
        super().__init__()

        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=pad_idx)

        self.lstm = nn.LSTM(
            embed_dim, hidden_dim, num_layers,
            batch_first=True, dropout=dropout if num_layers > 1 else 0,
            bidirectional=True
        )

        # æ³¨æ„åŠ›å±‚
        self.attention = nn.Linear(hidden_dim * 2, 1)

        self.dropout = nn.Dropout(dropout)
        self.fc = nn.Linear(hidden_dim * 2, num_classes)

    def forward(self, text, lengths):
        embedded = self.embedding(text)  # [batch, seq_len, embed_dim]
        embedded = self.dropout(embedded)

        packed = nn.utils.rnn.pack_padded_sequence(
            embedded, lengths.cpu(), batch_first=True, enforce_sorted=False
        )

        output, (hidden, cell) = self.lstm(packed)

        # Unpack
        output, _ = nn.utils.rnn.pad_packed_sequence(output, batch_first=True)
        # output: [batch, seq_len, hidden_dim * 2]

        # è®¡ç®—æ³¨æ„åŠ›æƒé‡
        attn_weights = self.attention(output).squeeze(-1)  # [batch, seq_len]

        # åˆ›å»º maskï¼ˆå¿½ç•¥ padding ä½ç½®ï¼‰
        max_len = text.size(1)
        mask = torch.arange(max_len, device=text.device).expand(len(lengths), max_len) < lengths.unsqueeze(1)
        attn_weights = attn_weights.masked_fill(~mask, float('-inf'))

        # Softmax
        attn_weights = torch.softmax(attn_weights, dim=1)  # [batch, seq_len]

        # åŠ æƒæ±‚å’Œ
        weighted = torch.bmm(attn_weights.unsqueeze(1), output).squeeze(1)
        # [batch, hidden_dim * 2]

        weighted = self.dropout(weighted)

        return self.fc(weighted)

# åˆ›å»ºæ¨¡å‹
vocab_size = len(vocab)
embed_dim = 128
hidden_dim = 128
num_layers = 2
num_classes = 2

model = AttentionLSTM(vocab_size, embed_dim, hidden_dim, num_layers, num_classes)

# å‚æ•°ç»Ÿè®¡
total_params = sum(p.numel() for p in model.parameters())
print(f"æ¨¡å‹å‚æ•°é‡: {total_params:,}")
```

---

## 4. è®­ç»ƒä¸è¯„ä¼°

```python
import torch.optim as optim
from tqdm import tqdm
import time

# ========== è®¾ç½® ==========
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"ä½¿ç”¨è®¾å¤‡: {device}")

model = BiLSTMClassifier(vocab_size, embed_dim, hidden_dim, num_layers, num_classes)
model = model.to(device)

# æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)
scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=2)

# ========== è®­ç»ƒå‡½æ•° ==========
def train_one_epoch(model, loader, criterion, optimizer, device):
    model.train()
    running_loss = 0.0
    correct = 0
    total = 0

    pbar = tqdm(loader, desc='Training')
    for texts, labels, lengths in pbar:
        texts = texts.to(device)
        labels = labels.to(device)
        lengths = lengths.to(device)

        optimizer.zero_grad()
        outputs = model(texts, lengths)
        loss = criterion(outputs, labels)
        loss.backward()

        # æ¢¯åº¦è£å‰ª
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)

        optimizer.step()

        running_loss += loss.item()
        _, predicted = outputs.max(1)
        total += labels.size(0)
        correct += predicted.eq(labels).sum().item()

        pbar.set_postfix({
            'loss': f'{running_loss/total*labels.size(0):.4f}',
            'acc': f'{100.*correct/total:.2f}%'
        })

    return running_loss / len(loader), correct / total

def evaluate(model, loader, criterion, device):
    model.eval()
    running_loss = 0.0
    correct = 0
    total = 0

    with torch.no_grad():
        for texts, labels, lengths in loader:
            texts = texts.to(device)
            labels = labels.to(device)
            lengths = lengths.to(device)

            outputs = model(texts, lengths)
            loss = criterion(outputs, labels)

            running_loss += loss.item()
            _, predicted = outputs.max(1)
            total += labels.size(0)
            correct += predicted.eq(labels).sum().item()

    return running_loss / len(loader), correct / total

# ========== è®­ç»ƒå¾ªç¯ ==========
num_epochs = 10
best_val_acc = 0
train_losses, val_losses = [], []
train_accs, val_accs = [], []

print(f"\nå¼€å§‹è®­ç»ƒï¼Œå…± {num_epochs} ä¸ª epoch")
print("=" * 60)

for epoch in range(num_epochs):
    start_time = time.time()

    # è®­ç»ƒ
    train_loss, train_acc = train_one_epoch(model, train_loader, criterion, optimizer, device)

    # éªŒè¯ï¼ˆä½¿ç”¨æµ‹è¯•é›†ä½œä¸ºéªŒè¯ï¼Œå®é™…é¡¹ç›®åº”è¯¥åˆ’åˆ†éªŒè¯é›†ï¼‰
    val_loss, val_acc = evaluate(model, test_loader, criterion, device)

    # å­¦ä¹ ç‡è°ƒåº¦
    scheduler.step(val_loss)

    # è®°å½•
    train_losses.append(train_loss)
    val_losses.append(val_loss)
    train_accs.append(train_acc)
    val_accs.append(val_acc)

    # ä¿å­˜æœ€ä½³æ¨¡å‹
    if val_acc > best_val_acc:
        best_val_acc = val_acc
        torch.save(model.state_dict(), 'best_sentiment_model.pth')
        print(f"  âœ“ ä¿å­˜æœ€ä½³æ¨¡å‹ï¼ŒéªŒè¯å‡†ç¡®ç‡: {val_acc:.4f}")

    elapsed = time.time() - start_time

    print(f"Epoch {epoch+1:2d}/{num_epochs} | "
          f"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f} | "
          f"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f} | "
          f"LR: {optimizer.param_groups[0]['lr']:.6f} | Time: {elapsed:.1f}s")

print("=" * 60)
print(f"è®­ç»ƒå®Œæˆï¼æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {best_val_acc:.4f}")
```

---

## 5. ç»“æœåˆ†æ

```python
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc

# ========== è®­ç»ƒæ›²çº¿ ==========
fig, axes = plt.subplots(1, 2, figsize=(14, 5))

axes[0].plot(train_losses, label='Train', linewidth=2)
axes[0].plot(val_losses, label='Validation', linewidth=2)
axes[0].set_xlabel('Epoch')
axes[0].set_ylabel('Loss')
axes[0].set_title('Loss Curves')
axes[0].legend()
axes[0].grid(True, alpha=0.3)

axes[1].plot(train_accs, label='Train', linewidth=2)
axes[1].plot(val_accs, label='Validation', linewidth=2)
axes[1].set_xlabel('Epoch')
axes[1].set_ylabel('Accuracy')
axes[1].set_title('Accuracy Curves')
axes[1].legend()
axes[1].grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig('sentiment_training_curves.png', dpi=150)
plt.show()

# ========== è·å–é¢„æµ‹ç»“æœ ==========
def get_predictions(model, loader, device):
    model.eval()
    all_preds = []
    all_labels = []
    all_probs = []

    with torch.no_grad():
        for texts, labels, lengths in loader:
            texts = texts.to(device)
            lengths = lengths.to(device)

            outputs = model(texts, lengths)
            probs = torch.softmax(outputs, dim=1)
            _, preds = outputs.max(1)

            all_preds.extend(preds.cpu().numpy())
            all_labels.extend(labels.numpy())
            all_probs.extend(probs[:, 1].cpu().numpy())  # æ­£ç±»æ¦‚ç‡

    return np.array(all_preds), np.array(all_labels), np.array(all_probs)

model.load_state_dict(torch.load('best_sentiment_model.pth'))
predictions, labels, probs = get_predictions(model, test_loader, device)

# ========== æ··æ·†çŸ©é˜µ ==========
cm = confusion_matrix(labels, predictions)

plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=['Negative', 'Positive'],
            yticklabels=['Negative', 'Positive'])
plt.xlabel('Predicted')
plt.ylabel('True')
plt.title('Confusion Matrix')
plt.tight_layout()
plt.savefig('sentiment_confusion_matrix.png', dpi=150)
plt.show()

# ========== åˆ†ç±»æŠ¥å‘Š ==========
print("\nåˆ†ç±»æŠ¥å‘Š:")
print(classification_report(labels, predictions, target_names=['Negative', 'Positive']))

# ========== ROC æ›²çº¿ ==========
fpr, tpr, thresholds = roc_curve(labels, probs)
roc_auc = auc(fpr, tpr)

plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.4f})')
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve')
plt.legend(loc="lower right")
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.savefig('sentiment_roc_curve.png', dpi=150)
plt.show()

# ========== æ¨ç†æµ‹è¯• ==========
def predict_sentiment(model, text, vocab, tokenizer, device):
    """é¢„æµ‹å•æ¡æ–‡æœ¬çš„æƒ…æ„Ÿ"""
    model.eval()

    # åˆ†è¯å’Œç¼–ç 
    tokens = tokenizer(text)
    token_ids = [vocab[token] for token in tokens]

    # è½¬ä¸ºå¼ é‡
    text_tensor = torch.tensor([token_ids], dtype=torch.long).to(device)
    length = torch.tensor([len(token_ids)]).to(device)

    with torch.no_grad():
        output = model(text_tensor, length)
        probs = torch.softmax(output, dim=1)
        pred = output.argmax(1).item()

    sentiment = 'Positive' if pred == 1 else 'Negative'
    confidence = probs[0][pred].item()

    return sentiment, confidence

# æµ‹è¯•
test_texts = [
    "This movie is absolutely wonderful! I loved every minute of it.",
    "Terrible film. Waste of time and money. Don't watch it.",
    "The acting was okay but the story was quite boring.",
    "Best movie I've seen this year! Highly recommended!",
]

print("\næ¨ç†æµ‹è¯•:")
print("-" * 60)
for text in test_texts:
    sentiment, confidence = predict_sentiment(model, text, vocab, tokenizer, device)
    print(f"Text: {text[:50]}...")
    print(f"Sentiment: {sentiment} (confidence: {confidence:.4f})")
    print("-" * 60)
```

---

## 6. ä¼˜åŒ–æ–¹å‘

### 6.1 æå‡æ€§èƒ½

```python
# 1. ä½¿ç”¨é¢„è®­ç»ƒè¯å‘é‡
from torchtext.vocab import GloVe

# åŠ è½½ GloVe
glove = GloVe(name='6B', dim=100)

# åˆ›å»ºåµŒå…¥çŸ©é˜µ
def create_embedding_matrix(vocab, glove, embed_dim):
    embedding_matrix = torch.zeros(len(vocab), embed_dim)
    for word, idx in vocab.get_stoi().items():
        if word in glove.stoi:
            embedding_matrix[idx] = glove[word]
    return embedding_matrix

embedding_matrix = create_embedding_matrix(vocab, glove, 100)

# ç”¨äºæ¨¡å‹
class PretrainedLSTM(nn.Module):
    def __init__(self, embedding_matrix, hidden_dim, num_classes, freeze_embed=True):
        super().__init__()
        vocab_size, embed_dim = embedding_matrix.shape

        self.embedding = nn.Embedding.from_pretrained(
            embedding_matrix, freeze=freeze_embed, padding_idx=0
        )
        # ... å…¶ä»–å±‚

# 2. ä½¿ç”¨æ›´å¼ºçš„æ¨¡å‹æ¶æ„
# ä¾‹å¦‚ï¼šTextCNNã€Transformer

# 3. æ•°æ®å¢å¼º
# - åŒä¹‰è¯æ›¿æ¢
# - éšæœºåˆ é™¤
# - å›è¯‘

# 4. é›†æˆå­¦ä¹ 
def ensemble_predict(models, texts, lengths):
    outputs = [model(texts, lengths) for model in models]
    return torch.stack(outputs).mean(dim=0)
```

### 6.2 æœŸæœ›æ•ˆæœ

```
æ¨¡å‹                    IMDB æµ‹è¯•å‡†ç¡®ç‡
--------------------------------------------
åŸºç¡€ LSTM              ~85%
åŒå‘ LSTM              ~88%
+ æ³¨æ„åŠ›æœºåˆ¶            ~89%
+ é¢„è®­ç»ƒè¯å‘é‡          ~90%
Transformer (BERT)     ~93%+
```

### 6.3 å®Œæ•´æ¨ç†æ¥å£

```python
class SentimentAnalyzer:
    """æƒ…æ„Ÿåˆ†æå°è£…ç±»"""
    def __init__(self, model_path, vocab, tokenizer, device='cpu'):
        self.vocab = vocab
        self.tokenizer = tokenizer
        self.device = torch.device(device)

        # åŠ è½½æ¨¡å‹
        self.model = BiLSTMClassifier(
            vocab_size=len(vocab),
            embed_dim=128,
            hidden_dim=128,
            num_layers=2,
            num_classes=2
        )
        self.model.load_state_dict(torch.load(model_path, map_location=device))
        self.model.to(self.device)
        self.model.eval()

    def predict(self, text):
        """é¢„æµ‹å•æ¡æ–‡æœ¬"""
        tokens = self.tokenizer(text)
        token_ids = [self.vocab[token] for token in tokens]

        text_tensor = torch.tensor([token_ids], dtype=torch.long).to(self.device)
        length = torch.tensor([len(token_ids)]).to(self.device)

        with torch.no_grad():
            output = self.model(text_tensor, length)
            probs = torch.softmax(output, dim=1)

        return {
            'sentiment': 'positive' if output.argmax(1).item() == 1 else 'negative',
            'confidence': probs[0].max().item(),
            'probabilities': {
                'negative': probs[0][0].item(),
                'positive': probs[0][1].item()
            }
        }

    def predict_batch(self, texts):
        """æ‰¹é‡é¢„æµ‹"""
        return [self.predict(text) for text in texts]

# ä½¿ç”¨
analyzer = SentimentAnalyzer('best_sentiment_model.pth', vocab, tokenizer)
result = analyzer.predict("This is a great movie!")
print(result)
```

---

## â¡ï¸ ä¸‹ä¸€æ­¥

å®Œæˆæ‰€æœ‰é¡¹ç›®åï¼Œè¿›è¡Œ [14-è‡ªæµ‹æ¸…å•.md](./14-è‡ªæµ‹æ¸…å•.md) æ£€éªŒå­¦ä¹ æ•ˆæœï¼

