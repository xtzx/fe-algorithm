# ğŸ–¼ï¸ 06 - å·ç§¯ç¥ç»ç½‘ç»œ CNN

> CNN æ˜¯å¤„ç†å›¾åƒçš„åˆ©å™¨ï¼Œé€šè¿‡å·ç§¯æ“ä½œæå–ç©ºé—´ç‰¹å¾

---

## ç›®å½•

1. [å·ç§¯æ“ä½œ](#1-å·ç§¯æ“ä½œ)
2. [æ± åŒ–å±‚](#2-æ± åŒ–å±‚)
3. [æ„å»º CNN](#3-æ„å»º-cnn)
4. [ç»å…¸æ¶æ„æ¼”è¿›](#4-ç»å…¸æ¶æ„æ¼”è¿›)
5. [å®æˆ˜ï¼šMNIST åˆ†ç±»](#5-å®æˆ˜mnist-åˆ†ç±»)
6. [ç»ƒä¹ é¢˜](#6-ç»ƒä¹ é¢˜)

---

## 1. å·ç§¯æ“ä½œ

### 1.1 å·ç§¯çš„ç›´è§‚ç†è§£

```
å·ç§¯æ ¸åœ¨è¾“å…¥ä¸Šæ»‘åŠ¨ï¼Œè®¡ç®—åŠ æƒå’Œ

è¾“å…¥å›¾åƒ (5x5)          å·ç§¯æ ¸ (3x3)        è¾“å‡º (3x3)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”           â”Œâ”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1 2 3 4 5   â”‚         â”‚ 1 0 1 â”‚           â”‚ ? ? ? â”‚
â”‚ 2 3 4 5 6   â”‚    *    â”‚ 0 1 0 â”‚     =     â”‚ ? ? ? â”‚
â”‚ 3 4 5 6 7   â”‚         â”‚ 1 0 1 â”‚           â”‚ ? ? ? â”‚
â”‚ 4 5 6 7 8   â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”˜           â””â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚ 5 6 7 8 9   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

å·ç§¯æ ¸å­¦ä¹ æ£€æµ‹ç‰¹å®šæ¨¡å¼ï¼ˆè¾¹ç¼˜ã€çº¹ç†ã€å½¢çŠ¶ç­‰ï¼‰
```

### 1.2 Conv2d åŸºç¡€

```python
import torch
import torch.nn as nn

# å·ç§¯å±‚
conv = nn.Conv2d(
    in_channels=3,    # è¾“å…¥é€šé“æ•°ï¼ˆRGB=3ï¼‰
    out_channels=64,  # è¾“å‡ºé€šé“æ•°ï¼ˆå·ç§¯æ ¸æ•°é‡ï¼‰
    kernel_size=3,    # å·ç§¯æ ¸å¤§å°
    stride=1,         # æ­¥é•¿
    padding=1,        # å¡«å……
    bias=True         # æ˜¯å¦æœ‰åç½®
)

# è¾“å…¥ï¼š[batch, channels, height, width]
x = torch.randn(32, 3, 224, 224)
y = conv(x)
print(f"è¾“å…¥: {x.shape} â†’ è¾“å‡º: {y.shape}")
# [32, 3, 224, 224] â†’ [32, 64, 224, 224]

# æŸ¥çœ‹å‚æ•°
print(f"æƒé‡å½¢çŠ¶: {conv.weight.shape}")  # [out_c, in_c, kH, kW] = [64, 3, 3, 3]
print(f"åç½®å½¢çŠ¶: {conv.bias.shape}")    # [64]
```

### 1.3 è¾“å‡ºå°ºå¯¸è®¡ç®—

```python
# å…¬å¼ï¼šoutput_size = (input_size + 2*padding - kernel_size) / stride + 1

def calc_output_size(input_size, kernel_size, stride=1, padding=0):
    return (input_size + 2*padding - kernel_size) // stride + 1

# ç¤ºä¾‹
print(calc_output_size(224, kernel_size=3, stride=1, padding=1))  # 224
print(calc_output_size(224, kernel_size=3, stride=2, padding=1))  # 112
print(calc_output_size(224, kernel_size=7, stride=2, padding=3))  # 112

# å¸¸ç”¨é…ç½®
# ä¿æŒå°ºå¯¸ä¸å˜ï¼škernel=3, stride=1, padding=1
# å‡åŠå°ºå¯¸ï¼škernel=3, stride=2, padding=1
# å‡åŠå°ºå¯¸ï¼škernel=2, stride=2, padding=0 (æ± åŒ–å¸¸ç”¨)
```

### 1.4 ä¸åŒå·ç§¯ç±»å‹

```python
# æ ‡å‡†å·ç§¯
conv_standard = nn.Conv2d(64, 128, kernel_size=3, padding=1)

# æ·±åº¦å¯åˆ†ç¦»å·ç§¯ï¼ˆMobileNetï¼‰
# 1. æ·±åº¦å·ç§¯ï¼šæ¯ä¸ªé€šé“å•ç‹¬å·ç§¯
conv_depthwise = nn.Conv2d(64, 64, kernel_size=3, padding=1, groups=64)
# 2. é€ç‚¹å·ç§¯ï¼š1x1 å·ç§¯èåˆé€šé“
conv_pointwise = nn.Conv2d(64, 128, kernel_size=1)

# åˆ†ç»„å·ç§¯
conv_grouped = nn.Conv2d(64, 128, kernel_size=3, padding=1, groups=4)

# 1x1 å·ç§¯ï¼ˆé€šé“å˜æ¢ï¼‰
conv_1x1 = nn.Conv2d(64, 32, kernel_size=1)

# ç©ºæ´å·ç§¯ï¼ˆæ‰©å¤§æ„Ÿå—é‡ï¼‰
conv_dilated = nn.Conv2d(64, 64, kernel_size=3, padding=2, dilation=2)

# è½¬ç½®å·ç§¯ï¼ˆä¸Šé‡‡æ ·ï¼‰
conv_transpose = nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1)
# è¾“å…¥ [B, 64, 14, 14] â†’ è¾“å‡º [B, 32, 28, 28]
```

### 1.5 å¯è§†åŒ–å·ç§¯æ ¸

```python
import matplotlib.pyplot as plt
import torch
import torch.nn as nn

# è®­ç»ƒåçš„å·ç§¯æ ¸
conv = nn.Conv2d(3, 16, kernel_size=3)

# å¯è§†åŒ–ç¬¬ä¸€å±‚å·ç§¯æ ¸
fig, axes = plt.subplots(4, 4, figsize=(8, 8))
for i, ax in enumerate(axes.flat):
    if i < 16:
        # å–å‡ºä¸€ä¸ªå·ç§¯æ ¸ï¼Œå°† 3 é€šé“è½¬ä¸ºç°åº¦æ˜¾ç¤º
        kernel = conv.weight[i].detach().cpu()
        # ç®€å•å¤„ç†ï¼šå–å‡å€¼
        kernel_gray = kernel.mean(dim=0)
        ax.imshow(kernel_gray, cmap='gray')
        ax.axis('off')
        ax.set_title(f'Kernel {i}')
plt.tight_layout()
plt.show()
```

---

## 2. æ± åŒ–å±‚

### 2.1 æœ€å¤§æ± åŒ–

```python
# æœ€å¤§æ± åŒ–ï¼šå–åŒºåŸŸå†…æœ€å¤§å€¼
max_pool = nn.MaxPool2d(kernel_size=2, stride=2)

x = torch.randn(1, 64, 28, 28)
y = max_pool(x)
print(f"MaxPool: {x.shape} â†’ {y.shape}")  # [1, 64, 28, 28] â†’ [1, 64, 14, 14]

# å¯è§†åŒ–
x = torch.tensor([[[[1., 2., 3., 4.],
                    [5., 6., 7., 8.],
                    [9., 10., 11., 12.],
                    [13., 14., 15., 16.]]]])

pool = nn.MaxPool2d(2, 2)
y = pool(x)
print(f"è¾“å…¥:\n{x[0, 0]}")
print(f"MaxPool è¾“å‡º:\n{y[0, 0]}")
# [[6, 8],
#  [14, 16]]
```

### 2.2 å¹³å‡æ± åŒ–

```python
# å¹³å‡æ± åŒ–ï¼šå–åŒºåŸŸå†…å¹³å‡å€¼
avg_pool = nn.AvgPool2d(kernel_size=2, stride=2)

x = torch.randn(1, 64, 28, 28)
y = avg_pool(x)
print(f"AvgPool: {x.shape} â†’ {y.shape}")  # [1, 64, 14, 14]
```

### 2.3 å…¨å±€æ± åŒ–

```python
# å…¨å±€å¹³å‡æ± åŒ–ï¼ˆGAPï¼‰ï¼šæŠŠæ¯ä¸ªé€šé“å‹ç¼©æˆä¸€ä¸ªæ•°
gap = nn.AdaptiveAvgPool2d(1)

x = torch.randn(32, 512, 7, 7)
y = gap(x)
print(f"GAP: {x.shape} â†’ {y.shape}")  # [32, 512, 7, 7] â†’ [32, 512, 1, 1]

# ç­‰ä»·äº
y = x.mean(dim=(2, 3), keepdim=True)

# è‡ªé€‚åº”æ± åŒ–ï¼šè¾“å‡ºæŒ‡å®šå¤§å°
adaptive_pool = nn.AdaptiveAvgPool2d((7, 7))  # è¾“å‡ºå›ºå®šä¸º 7x7
x = torch.randn(32, 512, 14, 14)
y = adaptive_pool(x)
print(f"Adaptive: {x.shape} â†’ {y.shape}")  # [32, 512, 7, 7]
```

---

## 3. æ„å»º CNN

### 3.1 åŸºæœ¬ CNN ç»“æ„

```python
import torch.nn as nn
import torch.nn.functional as F

class SimpleCNN(nn.Module):
    def __init__(self, num_classes=10):
        super().__init__()

        # ç‰¹å¾æå–å™¨
        self.features = nn.Sequential(
            # å·ç§¯å— 1
            nn.Conv2d(3, 32, kernel_size=3, padding=1),
            nn.BatchNorm2d(32),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2, 2),  # 224 â†’ 112

            # å·ç§¯å— 2
            nn.Conv2d(32, 64, kernel_size=3, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2, 2),  # 112 â†’ 56

            # å·ç§¯å— 3
            nn.Conv2d(64, 128, kernel_size=3, padding=1),
            nn.BatchNorm2d(128),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2, 2),  # 56 â†’ 28

            # å·ç§¯å— 4
            nn.Conv2d(128, 256, kernel_size=3, padding=1),
            nn.BatchNorm2d(256),
            nn.ReLU(inplace=True),
            nn.AdaptiveAvgPool2d(1),  # å…¨å±€å¹³å‡æ± åŒ–
        )

        # åˆ†ç±»å™¨
        self.classifier = nn.Sequential(
            nn.Flatten(),
            nn.Dropout(0.5),
            nn.Linear(256, num_classes)
        )

    def forward(self, x):
        x = self.features(x)
        x = self.classifier(x)
        return x

# æµ‹è¯•
model = SimpleCNN(num_classes=10)
x = torch.randn(2, 3, 224, 224)
y = model(x)
print(f"è¾“å‡ºå½¢çŠ¶: {y.shape}")  # [2, 10]

# å‚æ•°ç»Ÿè®¡
total = sum(p.numel() for p in model.parameters())
print(f"å‚æ•°é‡: {total:,}")
```

### 3.2 å·ç§¯å—å°è£…

```python
def conv_block(in_channels, out_channels, pool=True):
    """å·ç§¯å—ï¼šConv -> BN -> ReLU (-> Pool)"""
    layers = [
        nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),
        nn.BatchNorm2d(out_channels),
        nn.ReLU(inplace=True)
    ]
    if pool:
        layers.append(nn.MaxPool2d(2, 2))
    return nn.Sequential(*layers)

class CNN(nn.Module):
    def __init__(self, in_channels=3, num_classes=10):
        super().__init__()

        self.conv1 = conv_block(in_channels, 64, pool=True)   # /2
        self.conv2 = conv_block(64, 128, pool=True)           # /2
        self.conv3 = conv_block(128, 256, pool=True)          # /2
        self.conv4 = conv_block(256, 512, pool=True)          # /2

        self.gap = nn.AdaptiveAvgPool2d(1)
        self.fc = nn.Linear(512, num_classes)

    def forward(self, x):
        x = self.conv1(x)
        x = self.conv2(x)
        x = self.conv3(x)
        x = self.conv4(x)
        x = self.gap(x)
        x = x.view(x.size(0), -1)
        x = self.fc(x)
        return x
```

---

## 4. ç»å…¸æ¶æ„æ¼”è¿›

### 4.1 LeNet (1998)

```python
class LeNet(nn.Module):
    """ç¬¬ä¸€ä¸ªæˆåŠŸçš„ CNNï¼Œç”¨äºæ‰‹å†™æ•°å­—è¯†åˆ«"""
    def __init__(self):
        super().__init__()
        self.conv1 = nn.Conv2d(1, 6, 5)    # 28â†’24
        self.pool = nn.AvgPool2d(2, 2)     # 24â†’12
        self.conv2 = nn.Conv2d(6, 16, 5)   # 12â†’8
        # pool: 8â†’4
        self.fc1 = nn.Linear(16 * 4 * 4, 120)
        self.fc2 = nn.Linear(120, 84)
        self.fc3 = nn.Linear(84, 10)

    def forward(self, x):
        x = self.pool(torch.relu(self.conv1(x)))
        x = self.pool(torch.relu(self.conv2(x)))
        x = x.view(-1, 16 * 4 * 4)
        x = torch.relu(self.fc1(x))
        x = torch.relu(self.fc2(x))
        x = self.fc3(x)
        return x
```

### 4.2 VGG (2014)

```python
class VGG16(nn.Module):
    """
    æ ¸å¿ƒæ€æƒ³ï¼šä½¿ç”¨å¤šä¸ªå°å·ç§¯æ ¸ï¼ˆ3x3ï¼‰ä»£æ›¿å¤§å·ç§¯æ ¸
    ä¸¤ä¸ª 3x3 å·ç§¯çš„æ„Ÿå—é‡ç­‰äºä¸€ä¸ª 5x5ï¼Œä½†å‚æ•°æ›´å°‘
    """
    def __init__(self, num_classes=1000):
        super().__init__()

        self.features = nn.Sequential(
            # Block 1
            nn.Conv2d(3, 64, 3, padding=1), nn.ReLU(inplace=True),
            nn.Conv2d(64, 64, 3, padding=1), nn.ReLU(inplace=True),
            nn.MaxPool2d(2, 2),

            # Block 2
            nn.Conv2d(64, 128, 3, padding=1), nn.ReLU(inplace=True),
            nn.Conv2d(128, 128, 3, padding=1), nn.ReLU(inplace=True),
            nn.MaxPool2d(2, 2),

            # Block 3
            nn.Conv2d(128, 256, 3, padding=1), nn.ReLU(inplace=True),
            nn.Conv2d(256, 256, 3, padding=1), nn.ReLU(inplace=True),
            nn.Conv2d(256, 256, 3, padding=1), nn.ReLU(inplace=True),
            nn.MaxPool2d(2, 2),

            # Block 4
            nn.Conv2d(256, 512, 3, padding=1), nn.ReLU(inplace=True),
            nn.Conv2d(512, 512, 3, padding=1), nn.ReLU(inplace=True),
            nn.Conv2d(512, 512, 3, padding=1), nn.ReLU(inplace=True),
            nn.MaxPool2d(2, 2),

            # Block 5
            nn.Conv2d(512, 512, 3, padding=1), nn.ReLU(inplace=True),
            nn.Conv2d(512, 512, 3, padding=1), nn.ReLU(inplace=True),
            nn.Conv2d(512, 512, 3, padding=1), nn.ReLU(inplace=True),
            nn.MaxPool2d(2, 2),
        )

        self.classifier = nn.Sequential(
            nn.Linear(512 * 7 * 7, 4096),
            nn.ReLU(inplace=True),
            nn.Dropout(0.5),
            nn.Linear(4096, 4096),
            nn.ReLU(inplace=True),
            nn.Dropout(0.5),
            nn.Linear(4096, num_classes),
        )

    def forward(self, x):
        x = self.features(x)
        x = x.view(x.size(0), -1)
        x = self.classifier(x)
        return x
```

### 4.3 ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹

```python
import torchvision.models as models

# åŠ è½½é¢„è®­ç»ƒ VGG16
vgg = models.vgg16(pretrained=True)

# æ›¿æ¢æœ€åä¸€å±‚é€‚åº”æ–°ä»»åŠ¡
vgg.classifier[6] = nn.Linear(4096, 10)

# å†»ç»“ç‰¹å¾æå–å™¨
for param in vgg.features.parameters():
    param.requires_grad = False

# åªè®­ç»ƒåˆ†ç±»å™¨
optimizer = torch.optim.Adam(vgg.classifier.parameters(), lr=0.001)
```

---

## 5. å®æˆ˜ï¼šMNIST åˆ†ç±»

```python
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from torchvision import datasets, transforms

# ========== æ•°æ®å‡†å¤‡ ==========
transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.1307,), (0.3081,))  # MNIST å‡å€¼å’Œæ ‡å‡†å·®
])

train_dataset = datasets.MNIST('./data', train=True, download=True, transform=transform)
test_dataset = datasets.MNIST('./data', train=False, transform=transform)

train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=1000, shuffle=False)

# ========== æ¨¡å‹ ==========
class MNISTNet(nn.Module):
    def __init__(self):
        super().__init__()
        self.conv1 = nn.Conv2d(1, 32, 3, 1)
        self.conv2 = nn.Conv2d(32, 64, 3, 1)
        self.dropout1 = nn.Dropout(0.25)
        self.dropout2 = nn.Dropout(0.5)
        self.fc1 = nn.Linear(9216, 128)
        self.fc2 = nn.Linear(128, 10)

    def forward(self, x):
        x = torch.relu(self.conv1(x))  # 28â†’26
        x = torch.relu(self.conv2(x))  # 26â†’24
        x = nn.functional.max_pool2d(x, 2)  # 24â†’12
        x = self.dropout1(x)
        x = torch.flatten(x, 1)  # 64*12*12 = 9216
        x = torch.relu(self.fc1(x))
        x = self.dropout2(x)
        x = self.fc2(x)
        return x

# ========== è®­ç»ƒ ==========
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model = MNISTNet().to(device)
optimizer = optim.Adam(model.parameters(), lr=0.001)
criterion = nn.CrossEntropyLoss()

def train_epoch(model, loader, optimizer, criterion, device):
    model.train()
    total_loss = 0
    correct = 0

    for batch_idx, (data, target) in enumerate(loader):
        data, target = data.to(device), target.to(device)

        optimizer.zero_grad()
        output = model(data)
        loss = criterion(output, target)
        loss.backward()
        optimizer.step()

        total_loss += loss.item()
        pred = output.argmax(dim=1)
        correct += pred.eq(target).sum().item()

    return total_loss / len(loader), correct / len(loader.dataset)

def test(model, loader, device):
    model.eval()
    correct = 0

    with torch.no_grad():
        for data, target in loader:
            data, target = data.to(device), target.to(device)
            output = model(data)
            pred = output.argmax(dim=1)
            correct += pred.eq(target).sum().item()

    return correct / len(loader.dataset)

# è®­ç»ƒå¾ªç¯
for epoch in range(1, 11):
    train_loss, train_acc = train_epoch(model, train_loader, optimizer, criterion, device)
    test_acc = test(model, test_loader, device)
    print(f'Epoch {epoch}: Train Loss={train_loss:.4f}, Train Acc={train_acc:.4f}, Test Acc={test_acc:.4f}')

# é¢„æœŸï¼š10 epoch å Test Acc > 99%
```

---

## 6. ç»ƒä¹ é¢˜

### åŸºç¡€ç»ƒä¹ 

1. è®¡ç®—ï¼šè¾“å…¥ 32x32ï¼Œkernel=5, stride=2, padding=2 åçš„è¾“å‡ºå°ºå¯¸
2. å®ç°ä¸€ä¸ª CNN ç”¨äº CIFAR-10 åˆ†ç±»
3. å°è¯•ç”¨æ·±åº¦å¯åˆ†ç¦»å·ç§¯æ›¿æ¢æ ‡å‡†å·ç§¯ï¼Œæ¯”è¾ƒå‚æ•°é‡

### å‚è€ƒç­”æ¡ˆ

<details>
<summary>ç‚¹å‡»æŸ¥çœ‹ç­”æ¡ˆ</summary>

```python
# 1. è¾“å‡ºå°ºå¯¸è®¡ç®—
# (32 + 2*2 - 5) / 2 + 1 = 31/2 + 1 = 15.5 â†’ 15
print("è¾“å‡ºå°ºå¯¸:", (32 + 4 - 5) // 2 + 1)  # 16


# 2. CIFAR-10 CNN
class CIFAR10Net(nn.Module):
    def __init__(self):
        super().__init__()
        self.features = nn.Sequential(
            nn.Conv2d(3, 64, 3, padding=1), nn.BatchNorm2d(64), nn.ReLU(),
            nn.Conv2d(64, 64, 3, padding=1), nn.BatchNorm2d(64), nn.ReLU(),
            nn.MaxPool2d(2, 2),  # 32â†’16

            nn.Conv2d(64, 128, 3, padding=1), nn.BatchNorm2d(128), nn.ReLU(),
            nn.Conv2d(128, 128, 3, padding=1), nn.BatchNorm2d(128), nn.ReLU(),
            nn.MaxPool2d(2, 2),  # 16â†’8

            nn.Conv2d(128, 256, 3, padding=1), nn.BatchNorm2d(256), nn.ReLU(),
            nn.Conv2d(256, 256, 3, padding=1), nn.BatchNorm2d(256), nn.ReLU(),
            nn.MaxPool2d(2, 2),  # 8â†’4

            nn.AdaptiveAvgPool2d(1)
        )
        self.classifier = nn.Linear(256, 10)

    def forward(self, x):
        x = self.features(x)
        x = x.view(x.size(0), -1)
        return self.classifier(x)


# 3. æ·±åº¦å¯åˆ†ç¦»å·ç§¯å¯¹æ¯”
# æ ‡å‡†å·ç§¯å‚æ•°ï¼šin_c * out_c * k * k = 64 * 128 * 3 * 3 = 73,728
conv_standard = nn.Conv2d(64, 128, 3, padding=1)
print(f"æ ‡å‡†å·ç§¯å‚æ•°: {sum(p.numel() for p in conv_standard.parameters())}")

# æ·±åº¦å¯åˆ†ç¦»å·ç§¯å‚æ•°ï¼šin_c * k * k + in_c * out_c = 64*9 + 64*128 = 8,768
class DepthwiseSeparable(nn.Module):
    def __init__(self, in_c, out_c, k=3):
        super().__init__()
        self.depthwise = nn.Conv2d(in_c, in_c, k, padding=k//2, groups=in_c)
        self.pointwise = nn.Conv2d(in_c, out_c, 1)

    def forward(self, x):
        return self.pointwise(self.depthwise(x))

conv_dw = DepthwiseSeparable(64, 128)
print(f"æ·±åº¦å¯åˆ†ç¦»å‚æ•°: {sum(p.numel() for p in conv_dw.parameters())}")
# å‚æ•°å‡å°‘çº¦ 8 å€ï¼
```

</details>

---

## â¡ï¸ ä¸‹ä¸€æ­¥

å­¦å®Œæœ¬èŠ‚åï¼Œç»§ç»­å­¦ä¹  [07-ResNetä¸ViT.md](./07-ResNetä¸ViT.md)

