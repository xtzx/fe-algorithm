# ğŸ§  01 - æ·±åº¦å­¦ä¹ åŸºç¡€

> ç†è§£ç¥ç»ç½‘ç»œçš„æ ¸å¿ƒç»„ä»¶ï¼šå±‚ã€æ¿€æ´»å‡½æ•°ã€æŸå¤±å‡½æ•°ã€ä¼˜åŒ–å™¨

---

## ç›®å½•

1. [ç¥ç»ç½‘ç»œåŸºæœ¬æ¦‚å¿µ](#1-ç¥ç»ç½‘ç»œåŸºæœ¬æ¦‚å¿µ)
2. [æ¿€æ´»å‡½æ•°](#2-æ¿€æ´»å‡½æ•°)
3. [æŸå¤±å‡½æ•°](#3-æŸå¤±å‡½æ•°)
4. [ä¼˜åŒ–å™¨](#4-ä¼˜åŒ–å™¨)
5. [æ­£åˆ™åŒ–](#5-æ­£åˆ™åŒ–)
6. [æ¢¯åº¦é—®é¢˜ä¸è§£å†³æ–¹æ¡ˆ](#6-æ¢¯åº¦é—®é¢˜ä¸è§£å†³æ–¹æ¡ˆ)
7. [ç»ƒä¹ é¢˜](#7-ç»ƒä¹ é¢˜)

---

## 1. ç¥ç»ç½‘ç»œåŸºæœ¬æ¦‚å¿µ

### 1.1 ä»æ„ŸçŸ¥æœºåˆ°ç¥ç»ç½‘ç»œ

```
å•ä¸ªç¥ç»å…ƒï¼ˆæ„ŸçŸ¥æœºï¼‰ï¼š
è¾“å…¥ xâ‚, xâ‚‚, ..., xâ‚™
     â†“
åŠ æƒæ±‚å’Œï¼šz = wâ‚xâ‚ + wâ‚‚xâ‚‚ + ... + wâ‚™xâ‚™ + b
     â†“
æ¿€æ´»å‡½æ•°ï¼ša = f(z)
     â†“
è¾“å‡º
```

```python
import numpy as np

def neuron(x, weights, bias, activation):
    """å•ä¸ªç¥ç»å…ƒ"""
    z = np.dot(weights, x) + bias  # çº¿æ€§å˜æ¢
    a = activation(z)               # éçº¿æ€§æ¿€æ´»
    return a

# ReLU æ¿€æ´»
relu = lambda z: np.maximum(0, z)

# ç¤ºä¾‹
x = np.array([1.0, 2.0, 3.0])
w = np.array([0.5, -0.3, 0.8])
b = 0.1

output = neuron(x, w, b, relu)
print(f"è¾“å‡º: {output}")  # 2.5
```

### 1.2 å¤šå±‚ç¥ç»ç½‘ç»œ

```
è¾“å…¥å±‚ â†’ éšè—å±‚ 1 â†’ éšè—å±‚ 2 â†’ ... â†’ è¾“å‡ºå±‚
  â†“         â†“           â†“              â†“
 (x)      (hâ‚)        (hâ‚‚)           (Å·)

æ¯ä¸€å±‚ï¼šh = f(W @ x + b)
```

```python
import torch
import torch.nn as nn

# ä¸€ä¸ªç®€å•çš„ 3 å±‚ç½‘ç»œ
class SimpleNet(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim):
        super().__init__()
        self.layer1 = nn.Linear(input_dim, hidden_dim)   # è¾“å…¥å±‚ â†’ éšè—å±‚
        self.layer2 = nn.Linear(hidden_dim, hidden_dim)  # éšè—å±‚ â†’ éšè—å±‚
        self.layer3 = nn.Linear(hidden_dim, output_dim)  # éšè—å±‚ â†’ è¾“å‡ºå±‚
        self.relu = nn.ReLU()

    def forward(self, x):
        x = self.relu(self.layer1(x))
        x = self.relu(self.layer2(x))
        x = self.layer3(x)  # è¾“å‡ºå±‚é€šå¸¸ä¸åŠ æ¿€æ´»ï¼ˆæˆ–æ ¹æ®ä»»åŠ¡åŠ  softmax/sigmoidï¼‰
        return x

# åˆ›å»ºç½‘ç»œ
model = SimpleNet(input_dim=10, hidden_dim=64, output_dim=2)
print(model)

# å‰å‘ä¼ æ’­
x = torch.randn(32, 10)  # batch_size=32, features=10
output = model(x)
print(f"è¾“å‡ºå½¢çŠ¶: {output.shape}")  # [32, 2]
```

### 1.3 å‚æ•°ä¸è¶…å‚æ•°

| ç±»å‹ | è¯´æ˜ | ç¤ºä¾‹ |
|------|------|------|
| **å‚æ•°** | æ¨¡å‹ä»æ•°æ®å­¦ä¹  | æƒé‡ Wã€åç½® b |
| **è¶…å‚æ•°** | è®­ç»ƒå‰è®¾ç½® | å­¦ä¹ ç‡ã€å±‚æ•°ã€éšè—å•å…ƒæ•° |

```python
# æŸ¥çœ‹æ¨¡å‹å‚æ•°
for name, param in model.named_parameters():
    print(f"{name}: {param.shape}")

# ç»Ÿè®¡å‚æ•°æ•°é‡
total_params = sum(p.numel() for p in model.parameters())
trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
print(f"æ€»å‚æ•°: {total_params:,}")
print(f"å¯è®­ç»ƒå‚æ•°: {trainable_params:,}")
```

---

## 2. æ¿€æ´»å‡½æ•°

### 2.1 ä¸ºä»€ä¹ˆéœ€è¦æ¿€æ´»å‡½æ•°ï¼Ÿ

```
æ²¡æœ‰æ¿€æ´»å‡½æ•°ï¼š
hâ‚ = Wâ‚ @ x + bâ‚
hâ‚‚ = Wâ‚‚ @ hâ‚ + bâ‚‚ = Wâ‚‚ @ (Wâ‚ @ x + bâ‚) + bâ‚‚ = (Wâ‚‚Wâ‚) @ x + (Wâ‚‚bâ‚ + bâ‚‚)

å¤šå±‚çº¿æ€§å˜æ¢ = ä¸€å±‚çº¿æ€§å˜æ¢ï¼
æ¿€æ´»å‡½æ•°å¼•å…¥éçº¿æ€§ï¼Œè®©ç½‘ç»œèƒ½å­¦ä¹ å¤æ‚æ¨¡å¼
```

### 2.2 å¸¸è§æ¿€æ´»å‡½æ•°

```python
import torch
import torch.nn.functional as F
import matplotlib.pyplot as plt
import numpy as np

x = torch.linspace(-5, 5, 100)

fig, axes = plt.subplots(2, 3, figsize=(15, 10))

# 1. Sigmoid
y_sigmoid = torch.sigmoid(x)
axes[0, 0].plot(x.numpy(), y_sigmoid.numpy())
axes[0, 0].set_title('Sigmoid: Ïƒ(x) = 1/(1+e^(-x))')
axes[0, 0].set_xlabel('x')
axes[0, 0].axhline(y=0.5, color='r', linestyle='--', alpha=0.5)
axes[0, 0].grid(True)

# 2. Tanh
y_tanh = torch.tanh(x)
axes[0, 1].plot(x.numpy(), y_tanh.numpy())
axes[0, 1].set_title('Tanh: (e^x - e^(-x))/(e^x + e^(-x))')
axes[0, 1].axhline(y=0, color='r', linestyle='--', alpha=0.5)
axes[0, 1].grid(True)

# 3. ReLU
y_relu = F.relu(x)
axes[0, 2].plot(x.numpy(), y_relu.numpy())
axes[0, 2].set_title('ReLU: max(0, x)')
axes[0, 2].grid(True)

# 4. LeakyReLU
y_leaky = F.leaky_relu(x, negative_slope=0.1)
axes[1, 0].plot(x.numpy(), y_leaky.numpy())
axes[1, 0].set_title('LeakyReLU: max(0.1x, x)')
axes[1, 0].grid(True)

# 5. GELU
y_gelu = F.gelu(x)
axes[1, 1].plot(x.numpy(), y_gelu.numpy())
axes[1, 1].set_title('GELU (Transformer å¸¸ç”¨)')
axes[1, 1].grid(True)

# 6. SiLU/Swish
y_silu = F.silu(x)
axes[1, 2].plot(x.numpy(), y_silu.numpy())
axes[1, 2].set_title('SiLU/Swish: x * sigmoid(x)')
axes[1, 2].grid(True)

plt.tight_layout()
plt.show()
```

### 2.3 æ¿€æ´»å‡½æ•°å¯¹æ¯”

| æ¿€æ´»å‡½æ•° | å…¬å¼ | ä¼˜ç‚¹ | ç¼ºç‚¹ | ä½¿ç”¨åœºæ™¯ |
|---------|------|------|------|---------|
| **Sigmoid** | 1/(1+e^(-x)) | è¾“å‡º (0,1) | æ¢¯åº¦æ¶ˆå¤±ã€éé›¶ä¸­å¿ƒ | äºŒåˆ†ç±»è¾“å‡ºå±‚ |
| **Tanh** | (e^x-e^(-x))/(e^x+e^(-x)) | é›¶ä¸­å¿ƒ | æ¢¯åº¦æ¶ˆå¤± | è¾ƒå°‘ä½¿ç”¨ |
| **ReLU** | max(0,x) | è®¡ç®—ç®€å•ã€ç¼“è§£æ¢¯åº¦æ¶ˆå¤± | æ­»ç¥ç»å…ƒ | éšè—å±‚é»˜è®¤é€‰æ‹© |
| **LeakyReLU** | max(Î±x,x) | è§£å†³æ­»ç¥ç»å…ƒ | Î± éœ€è¦è°ƒ | ReLU æ›¿ä»£ |
| **GELU** | xÂ·Î¦(x) | å¹³æ»‘ã€æ•ˆæœå¥½ | è®¡ç®—ç¨å¤æ‚ | Transformer |
| **SiLU** | xÂ·Ïƒ(x) | å¹³æ»‘ã€è‡ªé—¨æ§ | è®¡ç®—ç¨å¤æ‚ | ç°ä»£ç½‘ç»œ |

### 2.4 PyTorch ä¸­ä½¿ç”¨æ¿€æ´»å‡½æ•°

```python
import torch.nn as nn

# æ–¹å¼ 1ï¼šä½œä¸ºå±‚
model = nn.Sequential(
    nn.Linear(10, 64),
    nn.ReLU(),           # æ¿€æ´»å‡½æ•°ä½œä¸ºå•ç‹¬çš„å±‚
    nn.Linear(64, 2)
)

# æ–¹å¼ 2ï¼šä½œä¸ºå‡½æ•°
import torch.nn.functional as F

class MyModel(nn.Module):
    def __init__(self):
        super().__init__()
        self.fc1 = nn.Linear(10, 64)
        self.fc2 = nn.Linear(64, 2)

    def forward(self, x):
        x = F.relu(self.fc1(x))  # å‡½æ•°å¼è°ƒç”¨
        return self.fc2(x)
```

---

## 3. æŸå¤±å‡½æ•°

### 3.1 å›å½’ä»»åŠ¡

```python
import torch
import torch.nn as nn

# å‡æ–¹è¯¯å·® (MSE)
mse_loss = nn.MSELoss()

y_true = torch.tensor([1.0, 2.0, 3.0])
y_pred = torch.tensor([1.1, 2.2, 2.8])

loss = mse_loss(y_pred, y_true)
print(f"MSE Loss: {loss.item():.4f}")

# æ‰‹åŠ¨è®¡ç®—
mse_manual = ((y_pred - y_true) ** 2).mean()
print(f"MSE (manual): {mse_manual.item():.4f}")

# L1 Lossï¼ˆå¹³å‡ç»å¯¹è¯¯å·®ï¼‰
l1_loss = nn.L1Loss()
loss_l1 = l1_loss(y_pred, y_true)
print(f"L1 Loss: {loss_l1.item():.4f}")

# Smooth L1 Lossï¼ˆHuber Lossï¼‰- å¯¹å¼‚å¸¸å€¼é²æ£’
smooth_l1 = nn.SmoothL1Loss()
loss_smooth = smooth_l1(y_pred, y_true)
print(f"Smooth L1 Loss: {loss_smooth.item():.4f}")
```

### 3.2 åˆ†ç±»ä»»åŠ¡

```python
# äºŒåˆ†ç±»ï¼šBinary Cross Entropy
bce_loss = nn.BCELoss()
bce_with_logits = nn.BCEWithLogitsLoss()  # å†…ç½® sigmoidï¼Œæ›´ç¨³å®š

y_true = torch.tensor([1.0, 0.0, 1.0])
y_pred_probs = torch.tensor([0.9, 0.1, 0.8])  # æ¦‚ç‡
y_pred_logits = torch.tensor([2.0, -2.0, 1.5])  # logits

loss_bce = bce_loss(y_pred_probs, y_true)
loss_bce_logits = bce_with_logits(y_pred_logits, y_true)
print(f"BCE Loss: {loss_bce.item():.4f}")
print(f"BCE with Logits: {loss_bce_logits.item():.4f}")

# å¤šåˆ†ç±»ï¼šCross Entropy Loss
ce_loss = nn.CrossEntropyLoss()  # å†…ç½® softmax

y_true = torch.tensor([0, 2, 1])  # ç±»åˆ«ç´¢å¼•
y_pred = torch.tensor([
    [2.0, 0.5, 0.3],  # æ ·æœ¬ 1 çš„ logits
    [0.1, 0.2, 3.0],  # æ ·æœ¬ 2 çš„ logits
    [0.5, 2.5, 0.2]   # æ ·æœ¬ 3 çš„ logits
])

loss_ce = ce_loss(y_pred, y_true)
print(f"CrossEntropy Loss: {loss_ce.item():.4f}")

# æ‰‹åŠ¨è®¡ç®—
import torch.nn.functional as F
probs = F.softmax(y_pred, dim=1)
log_probs = torch.log(probs)
# å–å‡ºæ­£ç¡®ç±»åˆ«çš„ log æ¦‚ç‡
nll = -log_probs[range(3), y_true].mean()
print(f"CrossEntropy (manual): {nll.item():.4f}")
```

### 3.3 æŸå¤±å‡½æ•°é€‰æ‹©æŒ‡å—

| ä»»åŠ¡ | æŸå¤±å‡½æ•° | è¯´æ˜ |
|------|---------|------|
| å›å½’ | MSELoss | å‡æ–¹è¯¯å·® |
| å›å½’ï¼ˆæœ‰å¼‚å¸¸å€¼ï¼‰ | SmoothL1Loss | å¯¹å¼‚å¸¸å€¼é²æ£’ |
| äºŒåˆ†ç±» | BCEWithLogitsLoss | è¾“å…¥æ˜¯ logits |
| å¤šåˆ†ç±» | CrossEntropyLoss | è¾“å…¥æ˜¯ logits |
| å¤šæ ‡ç­¾åˆ†ç±» | BCEWithLogitsLoss | æ¯ä¸ªç±»åˆ«ç‹¬ç«‹äºŒåˆ†ç±» |

---

## 4. ä¼˜åŒ–å™¨

### 4.1 ä¼˜åŒ–å™¨çš„ä½œç”¨

```
è®­ç»ƒå¾ªç¯ï¼š
1. å‰å‘ä¼ æ’­ï¼šy_pred = model(x)
2. è®¡ç®—æŸå¤±ï¼šloss = loss_fn(y_pred, y_true)
3. åå‘ä¼ æ’­ï¼šloss.backward()  # è®¡ç®—æ¢¯åº¦
4. æ›´æ–°å‚æ•°ï¼šoptimizer.step()  # ç”¨æ¢¯åº¦æ›´æ–°å‚æ•°
5. æ¸…é›¶æ¢¯åº¦ï¼šoptimizer.zero_grad()
```

### 4.2 SGD

```python
import torch
import torch.nn as nn
import torch.optim as optim

model = nn.Linear(10, 2)

# åŸºç¡€ SGD
optimizer = optim.SGD(model.parameters(), lr=0.01)

# å¸¦åŠ¨é‡çš„ SGDï¼ˆæ›´å¸¸ç”¨ï¼‰
optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)

# å¸¦ weight decayï¼ˆL2 æ­£åˆ™åŒ–ï¼‰
optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=1e-4)
```

**SGD æ›´æ–°å…¬å¼**ï¼š
```
v = momentum * v + gradient
param = param - lr * v
```

### 4.3 Adam

```python
# Adamï¼šè‡ªé€‚åº”å­¦ä¹ ç‡
optimizer = optim.Adam(model.parameters(), lr=0.001)

# å¸¸ç”¨å‚æ•°
optimizer = optim.Adam(
    model.parameters(),
    lr=0.001,
    betas=(0.9, 0.999),  # ä¸€é˜¶å’ŒäºŒé˜¶åŠ¨é‡è¡°å‡
    eps=1e-8,            # é˜²æ­¢é™¤é›¶
    weight_decay=0       # L2 æ­£åˆ™åŒ–
)
```

### 4.4 AdamW

```python
# AdamWï¼šä¿®æ­£äº† weight decay çš„å®ç°
# åœ¨ Transformer ç­‰æ¨¡å‹ä¸­æ›´å¸¸ç”¨
optimizer = optim.AdamW(
    model.parameters(),
    lr=0.001,
    weight_decay=0.01  # ç›´æ¥åº”ç”¨ weight decayï¼Œè€Œä¸æ˜¯ L2
)
```

**Adam vs AdamW**ï¼š
- Adam çš„ weight_decay å®é™…ä¸Šæ˜¯ L2 æ­£åˆ™åŒ–ï¼Œä¸è‡ªé€‚åº”å­¦ä¹ ç‡è€¦åˆ
- AdamW è§£è€¦äº† weight decayï¼Œæ•ˆæœæ›´å¥½

### 4.5 å­¦ä¹ ç‡è°ƒåº¦

```python
import torch.optim.lr_scheduler as lr_scheduler

optimizer = optim.Adam(model.parameters(), lr=0.001)

# StepLRï¼šæ¯ step_size ä¸ª epoch è¡°å‡ä¸€æ¬¡
scheduler = lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)

# CosineAnnealingï¼šä½™å¼¦é€€ç«
scheduler = lr_scheduler.CosineAnnealingLR(optimizer, T_max=100)

# ReduceLROnPlateauï¼šéªŒè¯é›†æŒ‡æ ‡ä¸ä¸‹é™æ—¶è¡°å‡
scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=5)

# Warmup + Cosineï¼ˆå¸¸ç”¨ç»„åˆï¼‰
def get_scheduler(optimizer, num_warmup_steps, num_training_steps):
    def lr_lambda(step):
        if step < num_warmup_steps:
            return step / num_warmup_steps
        progress = (step - num_warmup_steps) / (num_training_steps - num_warmup_steps)
        return 0.5 * (1 + np.cos(np.pi * progress))
    return lr_scheduler.LambdaLR(optimizer, lr_lambda)

# è®­ç»ƒå¾ªç¯ä¸­ä½¿ç”¨
for epoch in range(num_epochs):
    train_one_epoch()
    scheduler.step()  # StepLR, CosineAnnealing
    # scheduler.step(val_loss)  # ReduceLROnPlateau
```

### 4.6 ä¼˜åŒ–å™¨é€‰æ‹©æŒ‡å—

| ä¼˜åŒ–å™¨ | ä½¿ç”¨åœºæ™¯ | å»ºè®®å­¦ä¹ ç‡ |
|--------|---------|-----------|
| **SGD + momentum** | è§†è§‰ä»»åŠ¡ã€éœ€è¦ç²¾è°ƒ | 0.01-0.1 |
| **Adam** | å¿«é€Ÿå®éªŒã€NLP ä»»åŠ¡ | 0.001-0.0001 |
| **AdamW** | Transformerã€ç°ä»£ç½‘ç»œ | 0.001-0.00001 |

---

## 5. æ­£åˆ™åŒ–

### 5.1 Dropout

```python
import torch.nn as nn

# è®­ç»ƒæ—¶éšæœºä¸¢å¼ƒç¥ç»å…ƒï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ
class ModelWithDropout(nn.Module):
    def __init__(self):
        super().__init__()
        self.fc1 = nn.Linear(100, 256)
        self.dropout = nn.Dropout(p=0.5)  # 50% æ¦‚ç‡ä¸¢å¼ƒ
        self.fc2 = nn.Linear(256, 10)

    def forward(self, x):
        x = F.relu(self.fc1(x))
        x = self.dropout(x)  # è®­ç»ƒæ—¶ç”Ÿæ•ˆï¼Œeval æ—¶è‡ªåŠ¨å…³é—­
        x = self.fc2(x)
        return x

model = ModelWithDropout()

# è®­ç»ƒæ¨¡å¼
model.train()
# Dropout ç”Ÿæ•ˆ

# è¯„ä¼°æ¨¡å¼
model.eval()
# Dropout å…³é—­
```

### 5.2 BatchNorm vs LayerNorm

```python
import torch.nn as nn

# BatchNormï¼šåœ¨ batch ç»´åº¦ä¸Šå½’ä¸€åŒ–
# é€‚ç”¨äºï¼šCNNã€å›ºå®š batch size
batch_norm = nn.BatchNorm1d(num_features=64)
# æˆ–
batch_norm_2d = nn.BatchNorm2d(num_features=64)  # ç”¨äºå›¾åƒ

# LayerNormï¼šåœ¨ç‰¹å¾ç»´åº¦ä¸Šå½’ä¸€åŒ–
# é€‚ç”¨äºï¼šTransformerã€RNNã€å˜é•¿åºåˆ—
layer_norm = nn.LayerNorm(normalized_shape=64)

# ç¤ºä¾‹
x = torch.randn(32, 64)  # [batch, features]

# BatchNormï¼šå¯¹æ¯ä¸ªç‰¹å¾ï¼Œåœ¨ batch ç»´åº¦ä¸Šå½’ä¸€åŒ–
out_bn = batch_norm(x)  # å‡å€¼å’Œæ–¹å·®æ˜¯åœ¨ 32 ä¸ªæ ·æœ¬ä¸Šè®¡ç®—çš„

# LayerNormï¼šå¯¹æ¯ä¸ªæ ·æœ¬ï¼Œåœ¨ç‰¹å¾ç»´åº¦ä¸Šå½’ä¸€åŒ–
out_ln = layer_norm(x)  # å‡å€¼å’Œæ–¹å·®æ˜¯åœ¨ 64 ä¸ªç‰¹å¾ä¸Šè®¡ç®—çš„
```

**å…³é”®åŒºåˆ«**ï¼š

| ç‰¹æ€§ | BatchNorm | LayerNorm |
|------|-----------|-----------|
| å½’ä¸€åŒ–ç»´åº¦ | batch | ç‰¹å¾ |
| è®­ç»ƒ/æ¨ç† | è¡Œä¸ºä¸åŒ | è¡Œä¸ºç›¸åŒ |
| batch size ä¾èµ– | æ˜¯ | å¦ |
| å¸¸ç”¨åœºæ™¯ | CNN | Transformer, RNN |

---

## 6. æ¢¯åº¦é—®é¢˜ä¸è§£å†³æ–¹æ¡ˆ

### 6.1 æ¢¯åº¦æ¶ˆå¤±ä¸çˆ†ç‚¸

```python
import torch

# æ·±å±‚ç½‘ç»œä¸­çš„æ¢¯åº¦é—®é¢˜
def show_gradient_problem():
    # æ¨¡æ‹Ÿæ·±å±‚ç½‘ç»œçš„æ¢¯åº¦ä¼ æ’­
    x = torch.tensor(1.0, requires_grad=True)

    # æ¢¯åº¦æ¶ˆå¤±ï¼ˆsigmoid/tanh çš„é—®é¢˜ï¼‰
    y = x
    for _ in range(50):
        y = torch.sigmoid(y)
    y.backward()
    print(f"Sigmoid ç»è¿‡ 50 å±‚åçš„æ¢¯åº¦: {x.grad}")

    x = torch.tensor(1.0, requires_grad=True)
    # æ¢¯åº¦çˆ†ç‚¸
    y = x
    for _ in range(50):
        y = y * 1.5  # æ¯å±‚æ”¾å¤§ 1.5 å€
    # y.backward()  # ä¼šçˆ†ç‚¸

show_gradient_problem()
```

### 6.2 è§£å†³æ–¹æ¡ˆ

```python
# 1. ä½¿ç”¨ ReLU æ¿€æ´»å‡½æ•°
# æ­£åŒºé—´æ¢¯åº¦æ’ä¸º 1ï¼Œä¸ä¼šæ¶ˆå¤±

# 2. æ®‹å·®è¿æ¥ï¼ˆResNetï¼‰
class ResidualBlock(nn.Module):
    def __init__(self, dim):
        super().__init__()
        self.fc1 = nn.Linear(dim, dim)
        self.fc2 = nn.Linear(dim, dim)

    def forward(self, x):
        residual = x
        out = F.relu(self.fc1(x))
        out = self.fc2(out)
        return F.relu(out + residual)  # æ®‹å·®è¿æ¥

# 3. æ¢¯åº¦è£å‰ª
optimizer.zero_grad()
loss.backward()
torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
optimizer.step()

# 4. åˆé€‚çš„æƒé‡åˆå§‹åŒ–
# PyTorch é»˜è®¤ä½¿ç”¨ Kaiming åˆå§‹åŒ–ï¼Œé€šå¸¸ä¸éœ€è¦æ‰‹åŠ¨è®¾ç½®
nn.init.kaiming_normal_(layer.weight, mode='fan_out', nonlinearity='relu')
nn.init.xavier_uniform_(layer.weight)
```

---

## 7. ç»ƒä¹ é¢˜

### åŸºç¡€ç»ƒä¹ 

1. å®ç°ä¸€ä¸ª 3 å±‚ MLPï¼Œä½¿ç”¨ä¸åŒçš„æ¿€æ´»å‡½æ•°ï¼Œè§‚å¯Ÿè¾“å‡ºåˆ†å¸ƒ
2. æ¯”è¾ƒ MSELoss å’Œ L1Loss å¯¹å¼‚å¸¸å€¼çš„æ•æ„Ÿåº¦
3. ç”¨ Adam å’Œ SGD è®­ç»ƒåŒä¸€ä¸ªæ¨¡å‹ï¼Œæ¯”è¾ƒæ”¶æ•›é€Ÿåº¦

### å‚è€ƒç­”æ¡ˆ

<details>
<summary>ç‚¹å‡»æŸ¥çœ‹ç­”æ¡ˆ</summary>

```python
import torch
import torch.nn as nn
import torch.nn.functional as F

# 1. ä¸åŒæ¿€æ´»å‡½æ•°çš„ MLP
class MLP(nn.Module):
    def __init__(self, activation='relu'):
        super().__init__()
        self.fc1 = nn.Linear(10, 64)
        self.fc2 = nn.Linear(64, 64)
        self.fc3 = nn.Linear(64, 2)

        if activation == 'relu':
            self.act = nn.ReLU()
        elif activation == 'gelu':
            self.act = nn.GELU()
        elif activation == 'silu':
            self.act = nn.SiLU()

    def forward(self, x):
        x = self.act(self.fc1(x))
        x = self.act(self.fc2(x))
        return self.fc3(x)

# æµ‹è¯•
x = torch.randn(32, 10)
for act in ['relu', 'gelu', 'silu']:
    model = MLP(activation=act)
    out = model(x)
    print(f"{act}: è¾“å‡ºå‡å€¼={out.mean():.4f}, æ ‡å‡†å·®={out.std():.4f}")

# 2. æŸå¤±å‡½æ•°å¯¹å¼‚å¸¸å€¼çš„æ•æ„Ÿåº¦
y_true = torch.tensor([1.0, 2.0, 3.0, 4.0, 5.0])
y_pred_normal = torch.tensor([1.1, 2.1, 2.9, 4.1, 5.0])
y_pred_outlier = torch.tensor([1.1, 2.1, 2.9, 4.1, 50.0])  # ä¸€ä¸ªå¼‚å¸¸å€¼

mse = nn.MSELoss()
l1 = nn.L1Loss()

print(f"æ­£å¸¸é¢„æµ‹ - MSE: {mse(y_pred_normal, y_true):.4f}, L1: {l1(y_pred_normal, y_true):.4f}")
print(f"æœ‰å¼‚å¸¸å€¼ - MSE: {mse(y_pred_outlier, y_true):.4f}, L1: {l1(y_pred_outlier, y_true):.4f}")
# MSE å¯¹å¼‚å¸¸å€¼æ›´æ•æ„Ÿï¼ˆå¹³æ–¹æ”¾å¤§äº†è¯¯å·®ï¼‰
```

</details>

---

## â¡ï¸ ä¸‹ä¸€æ­¥

å­¦å®Œæœ¬èŠ‚åï¼Œç»§ç»­å­¦ä¹  [02-PyTorchåŸºç¡€-Tensor.md](./02-PyTorchåŸºç¡€-Tensor.md)

