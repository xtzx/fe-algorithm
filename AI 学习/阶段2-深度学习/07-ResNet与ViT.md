# ğŸ—ï¸ 07 - ResNet ä¸ Vision Transformer

> æ®‹å·®è¿æ¥è®©ç½‘ç»œå¯ä»¥æ›´æ·±ï¼ŒViT è¯æ˜å›¾åƒä¹Ÿèƒ½ç”¨ Transformer

---

## ç›®å½•

1. [æ®‹å·®è¿æ¥åŸç†](#1-æ®‹å·®è¿æ¥åŸç†)
2. [ResNet å®ç°](#2-resnet-å®ç°)
3. [Vision Transformer (ViT)](#3-vision-transformer-vit)
4. [è¿ç§»å­¦ä¹ ](#4-è¿ç§»å­¦ä¹ )
5. [ç»ƒä¹ é¢˜](#5-ç»ƒä¹ é¢˜)

---

## 1. æ®‹å·®è¿æ¥åŸç†

### 1.1 ä¸ºä»€ä¹ˆéœ€è¦æ®‹å·®è¿æ¥ï¼Ÿ

```
é—®é¢˜ï¼šç½‘ç»œè¶Šæ·±ï¼Œè®­ç»ƒè¶Šéš¾
- æ¢¯åº¦æ¶ˆå¤±ï¼šæ¢¯åº¦åœ¨åå‘ä¼ æ’­ä¸­é€å±‚è¡°å‡
- é€€åŒ–é—®é¢˜ï¼šæ›´æ·±çš„ç½‘ç»œåè€Œå‡†ç¡®ç‡æ›´ä½

ç›´è§‰ï¼šå¦‚æœæ–°å¢çš„å±‚æ˜¯"å¤šä½™çš„"ï¼Œç½‘ç»œè‡³å°‘åº”è¯¥èƒ½å­¦åˆ°æ’ç­‰æ˜ å°„
ä½†å®é™…ä¸Šï¼Œè®©ç½‘ç»œå­¦ä¹  H(x) = x å¾ˆéš¾

æ®‹å·®è¿æ¥çš„è§£å†³æ–¹æ¡ˆï¼š
- è®©ç½‘ç»œå­¦ä¹ æ®‹å·® F(x) = H(x) - x
- è¾“å‡ºå˜æˆ H(x) = F(x) + x
- å­¦ä¹ "æ’ç­‰"å°±æ˜¯è®© F(x) = 0ï¼Œè¿™æ›´å®¹æ˜“ï¼
```

### 1.2 æ®‹å·®å—

```python
import torch
import torch.nn as nn

class BasicBlock(nn.Module):
    """ResNet åŸºæœ¬æ®‹å·®å—ï¼ˆç”¨äº ResNet-18/34ï¼‰"""
    expansion = 1

    def __init__(self, in_channels, out_channels, stride=1, downsample=None):
        super().__init__()

        # ç¬¬ä¸€ä¸ªå·ç§¯
        self.conv1 = nn.Conv2d(in_channels, out_channels, 3, stride, padding=1, bias=False)
        self.bn1 = nn.BatchNorm2d(out_channels)

        # ç¬¬äºŒä¸ªå·ç§¯
        self.conv2 = nn.Conv2d(out_channels, out_channels, 3, 1, padding=1, bias=False)
        self.bn2 = nn.BatchNorm2d(out_channels)

        self.relu = nn.ReLU(inplace=True)
        self.downsample = downsample  # ç”¨äºè°ƒæ•´æ®‹å·®ç»´åº¦

    def forward(self, x):
        identity = x

        out = self.relu(self.bn1(self.conv1(x)))
        out = self.bn2(self.conv2(out))

        # å¦‚æœç»´åº¦ä¸åŒ¹é…ï¼Œéœ€è¦è°ƒæ•´
        if self.downsample is not None:
            identity = self.downsample(x)

        out += identity  # æ®‹å·®è¿æ¥ï¼
        out = self.relu(out)

        return out

# æµ‹è¯•
block = BasicBlock(64, 64)
x = torch.randn(2, 64, 56, 56)
y = block(x)
print(f"BasicBlock: {x.shape} â†’ {y.shape}")
```

### 1.3 ç“¶é¢ˆå—

```python
class Bottleneck(nn.Module):
    """ResNet ç“¶é¢ˆå—ï¼ˆç”¨äº ResNet-50/101/152ï¼‰

    ä½¿ç”¨ 1x1 å·ç§¯é™ç»´å’Œå‡ç»´ï¼Œå‡å°‘è®¡ç®—é‡
    ç»“æ„ï¼š1x1(é™ç»´) â†’ 3x3 â†’ 1x1(å‡ç»´)
    """
    expansion = 4  # è¾“å‡ºé€šé“ = out_channels * expansion

    def __init__(self, in_channels, out_channels, stride=1, downsample=None):
        super().__init__()

        # 1x1 å·ç§¯é™ç»´
        self.conv1 = nn.Conv2d(in_channels, out_channels, 1, bias=False)
        self.bn1 = nn.BatchNorm2d(out_channels)

        # 3x3 å·ç§¯
        self.conv2 = nn.Conv2d(out_channels, out_channels, 3, stride, padding=1, bias=False)
        self.bn2 = nn.BatchNorm2d(out_channels)

        # 1x1 å·ç§¯å‡ç»´
        self.conv3 = nn.Conv2d(out_channels, out_channels * self.expansion, 1, bias=False)
        self.bn3 = nn.BatchNorm2d(out_channels * self.expansion)

        self.relu = nn.ReLU(inplace=True)
        self.downsample = downsample

    def forward(self, x):
        identity = x

        out = self.relu(self.bn1(self.conv1(x)))
        out = self.relu(self.bn2(self.conv2(out)))
        out = self.bn3(self.conv3(out))

        if self.downsample is not None:
            identity = self.downsample(x)

        out += identity
        out = self.relu(out)

        return out

# æµ‹è¯•
downsample = nn.Sequential(
    nn.Conv2d(64, 256, 1, bias=False),
    nn.BatchNorm2d(256)
)
block = Bottleneck(64, 64, downsample=downsample)
x = torch.randn(2, 64, 56, 56)
y = block(x)
print(f"Bottleneck: {x.shape} â†’ {y.shape}")  # [2, 64, 56, 56] â†’ [2, 256, 56, 56]
```

---

## 2. ResNet å®ç°

### 2.1 å®Œæ•´ ResNet

```python
class ResNet(nn.Module):
    def __init__(self, block, layers, num_classes=1000):
        super().__init__()

        self.in_channels = 64

        # Stem: åˆå§‹å·ç§¯
        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)
        self.bn1 = nn.BatchNorm2d(64)
        self.relu = nn.ReLU(inplace=True)
        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)

        # 4 ä¸ª Stage
        self.layer1 = self._make_layer(block, 64, layers[0])
        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)
        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)
        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)

        # åˆ†ç±»å¤´
        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))
        self.fc = nn.Linear(512 * block.expansion, num_classes)

        # åˆå§‹åŒ–
        self._init_weights()

    def _make_layer(self, block, out_channels, num_blocks, stride=1):
        downsample = None

        # å¦‚æœç»´åº¦ä¸åŒ¹é…ï¼Œåˆ›å»º downsample
        if stride != 1 or self.in_channels != out_channels * block.expansion:
            downsample = nn.Sequential(
                nn.Conv2d(self.in_channels, out_channels * block.expansion,
                          1, stride, bias=False),
                nn.BatchNorm2d(out_channels * block.expansion),
            )

        layers = []
        # ç¬¬ä¸€ä¸ªå—å¯èƒ½éœ€è¦ downsample
        layers.append(block(self.in_channels, out_channels, stride, downsample))
        self.in_channels = out_channels * block.expansion

        # å‰©ä½™å—
        for _ in range(1, num_blocks):
            layers.append(block(self.in_channels, out_channels))

        return nn.Sequential(*layers)

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.constant_(m.bias, 0)

    def forward(self, x):
        # Stem
        x = self.conv1(x)   # 224 â†’ 112
        x = self.bn1(x)
        x = self.relu(x)
        x = self.maxpool(x) # 112 â†’ 56

        # 4 Stages
        x = self.layer1(x)  # 56 â†’ 56
        x = self.layer2(x)  # 56 â†’ 28
        x = self.layer3(x)  # 28 â†’ 14
        x = self.layer4(x)  # 14 â†’ 7

        # åˆ†ç±»
        x = self.avgpool(x) # 7 â†’ 1
        x = torch.flatten(x, 1)
        x = self.fc(x)

        return x

# ä¸åŒæ·±åº¦çš„ ResNet
def resnet18(num_classes=1000):
    return ResNet(BasicBlock, [2, 2, 2, 2], num_classes)

def resnet34(num_classes=1000):
    return ResNet(BasicBlock, [3, 4, 6, 3], num_classes)

def resnet50(num_classes=1000):
    return ResNet(Bottleneck, [3, 4, 6, 3], num_classes)

def resnet101(num_classes=1000):
    return ResNet(Bottleneck, [3, 4, 23, 3], num_classes)

# æµ‹è¯•
model = resnet18(num_classes=10)
x = torch.randn(2, 3, 224, 224)
y = model(x)
print(f"ResNet-18 è¾“å‡º: {y.shape}")  # [2, 10]

total_params = sum(p.numel() for p in model.parameters())
print(f"å‚æ•°é‡: {total_params:,}")  # ~11M
```

### 2.2 ä½¿ç”¨é¢„è®­ç»ƒ ResNet

```python
import torchvision.models as models

# åŠ è½½é¢„è®­ç»ƒæƒé‡
resnet = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V2)
# æˆ–æ—§ç‰ˆå†™æ³•
# resnet = models.resnet50(pretrained=True)

# ä¿®æ”¹æœ€åä¸€å±‚
num_classes = 10
resnet.fc = nn.Linear(resnet.fc.in_features, num_classes)

# å†»ç»“é™¤æœ€åä¸€å±‚å¤–çš„æ‰€æœ‰å±‚
for param in resnet.parameters():
    param.requires_grad = False
for param in resnet.fc.parameters():
    param.requires_grad = True

# æŸ¥çœ‹å¯è®­ç»ƒå‚æ•°
trainable = sum(p.numel() for p in resnet.parameters() if p.requires_grad)
print(f"å¯è®­ç»ƒå‚æ•°: {trainable:,}")
```

---

## 3. Vision Transformer (ViT)

### 3.1 ViT æ ¸å¿ƒæ€æƒ³

```
ä¼ ç»Ÿï¼šCNN ç”¨å·ç§¯æå–å±€éƒ¨ç‰¹å¾
ViTï¼šæŠŠå›¾åƒåˆ†æˆ patchï¼Œå½“ä½œåºåˆ—ç”¨ Transformer å¤„ç†

1. å›¾åƒåˆ‡åˆ†ä¸º patch
   224x224 å›¾åƒ â†’ 14x14 ä¸ª 16x16 çš„ patch â†’ 196 ä¸ª patch

2. æ¯ä¸ª patch å±•å¹³å¹¶æ˜ å°„åˆ° embedding
   16x16x3 = 768 â†’ Linear â†’ D ç»´å‘é‡

3. åŠ ä¸Šä½ç½®ç¼–ç å’Œ [CLS] token

4. é€å…¥ Transformer Encoder

5. ç”¨ [CLS] token çš„è¾“å‡ºåšåˆ†ç±»
```

### 3.2 Patch Embedding

```python
class PatchEmbedding(nn.Module):
    """å°†å›¾åƒåˆ†æˆ patch å¹¶åµŒå…¥"""
    def __init__(self, img_size=224, patch_size=16, in_channels=3, embed_dim=768):
        super().__init__()
        self.img_size = img_size
        self.patch_size = patch_size
        self.num_patches = (img_size // patch_size) ** 2

        # ç”¨å·ç§¯å®ç° patch åˆ†å‰² + çº¿æ€§æ˜ å°„
        self.proj = nn.Conv2d(in_channels, embed_dim,
                               kernel_size=patch_size, stride=patch_size)

    def forward(self, x):
        # x: [B, C, H, W] â†’ [B, embed_dim, H/P, W/P]
        x = self.proj(x)
        # [B, embed_dim, num_patches_h, num_patches_w] â†’ [B, num_patches, embed_dim]
        x = x.flatten(2).transpose(1, 2)
        return x

# æµ‹è¯•
patch_embed = PatchEmbedding()
x = torch.randn(2, 3, 224, 224)
patches = patch_embed(x)
print(f"Patch Embedding: {x.shape} â†’ {patches.shape}")
# [2, 3, 224, 224] â†’ [2, 196, 768]
```

### 3.3 ç®€åŒ–ç‰ˆ ViT

```python
class ViT(nn.Module):
    def __init__(
        self,
        img_size=224,
        patch_size=16,
        in_channels=3,
        num_classes=1000,
        embed_dim=768,
        num_heads=12,
        num_layers=12,
        mlp_ratio=4,
        dropout=0.1,
    ):
        super().__init__()

        # Patch Embedding
        self.patch_embed = PatchEmbedding(img_size, patch_size, in_channels, embed_dim)
        num_patches = self.patch_embed.num_patches

        # [CLS] token
        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))

        # ä½ç½®ç¼–ç 
        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim))
        self.pos_drop = nn.Dropout(dropout)

        # Transformer Encoder
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=embed_dim,
            nhead=num_heads,
            dim_feedforward=int(embed_dim * mlp_ratio),
            dropout=dropout,
            activation='gelu',
            batch_first=True
        )
        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers)

        # åˆ†ç±»å¤´
        self.norm = nn.LayerNorm(embed_dim)
        self.head = nn.Linear(embed_dim, num_classes)

        # åˆå§‹åŒ–
        nn.init.trunc_normal_(self.pos_embed, std=0.02)
        nn.init.trunc_normal_(self.cls_token, std=0.02)

    def forward(self, x):
        B = x.shape[0]

        # Patch embedding
        x = self.patch_embed(x)  # [B, num_patches, embed_dim]

        # æ·»åŠ  [CLS] token
        cls_tokens = self.cls_token.expand(B, -1, -1)  # [B, 1, embed_dim]
        x = torch.cat([cls_tokens, x], dim=1)  # [B, num_patches+1, embed_dim]

        # æ·»åŠ ä½ç½®ç¼–ç 
        x = x + self.pos_embed
        x = self.pos_drop(x)

        # Transformer
        x = self.transformer(x)

        # å– [CLS] token çš„è¾“å‡ºåšåˆ†ç±»
        x = self.norm(x[:, 0])  # [B, embed_dim]
        x = self.head(x)        # [B, num_classes]

        return x

# æµ‹è¯•
model = ViT(
    img_size=224,
    patch_size=16,
    num_classes=10,
    embed_dim=384,  # ViT-Small
    num_heads=6,
    num_layers=6,
)

x = torch.randn(2, 3, 224, 224)
y = model(x)
print(f"ViT è¾“å‡º: {y.shape}")  # [2, 10]

total_params = sum(p.numel() for p in model.parameters())
print(f"å‚æ•°é‡: {total_params:,}")
```

### 3.4 ä½¿ç”¨é¢„è®­ç»ƒ ViT

```python
import torchvision.models as models

# åŠ è½½é¢„è®­ç»ƒ ViT
vit = models.vit_b_16(weights=models.ViT_B_16_Weights.IMAGENET1K_V1)

# ä¿®æ”¹åˆ†ç±»å¤´
vit.heads.head = nn.Linear(vit.heads.head.in_features, 10)

# æµ‹è¯•
x = torch.randn(2, 3, 224, 224)
y = vit(x)
print(f"é¢„è®­ç»ƒ ViT è¾“å‡º: {y.shape}")

# æˆ–ä½¿ç”¨ timm åº“ï¼ˆæ›´å¤šæ¨¡å‹é€‰æ‹©ï¼‰
# pip install timm
import timm

# åˆ—å‡ºå¯ç”¨çš„ ViT æ¨¡å‹
# print(timm.list_models('vit*'))

# åŠ è½½é¢„è®­ç»ƒæ¨¡å‹
model = timm.create_model('vit_base_patch16_224', pretrained=True, num_classes=10)
```

---

## 4. è¿ç§»å­¦ä¹ 

### 4.1 è¿ç§»å­¦ä¹ ç­–ç•¥

```python
def create_transfer_model(model_name='resnet50', num_classes=10, freeze_backbone=True):
    """åˆ›å»ºè¿ç§»å­¦ä¹ æ¨¡å‹"""

    if model_name == 'resnet50':
        model = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)
        in_features = model.fc.in_features
        model.fc = nn.Linear(in_features, num_classes)

        if freeze_backbone:
            for param in model.parameters():
                param.requires_grad = False
            for param in model.fc.parameters():
                param.requires_grad = True

    elif model_name == 'vit_b_16':
        model = models.vit_b_16(weights=models.ViT_B_16_Weights.DEFAULT)
        in_features = model.heads.head.in_features
        model.heads.head = nn.Linear(in_features, num_classes)

        if freeze_backbone:
            for param in model.parameters():
                param.requires_grad = False
            for param in model.heads.parameters():
                param.requires_grad = True

    return model

# ä½¿ç”¨
model = create_transfer_model('resnet50', num_classes=10, freeze_backbone=True)
```

### 4.2 å¾®è°ƒç­–ç•¥

```python
class FineTuner:
    """åˆ†é˜¶æ®µå¾®è°ƒ"""

    def __init__(self, model, num_stages=3):
        self.model = model
        self.num_stages = num_stages

        # è·å–æ‰€æœ‰å‚æ•°ç»„
        if hasattr(model, 'layer1'):  # ResNet
            self.param_groups = [
                model.conv1, model.bn1,
                model.layer1, model.layer2,
                model.layer3, model.layer4,
                model.fc
            ]
        else:  # ViT
            # ç®€åŒ–å¤„ç†
            self.param_groups = [model]

    def unfreeze_stage(self, stage):
        """è§£å†»ç‰¹å®šé˜¶æ®µçš„å‚æ•°"""
        # ä»åå¾€å‰è§£å†»
        start_idx = len(self.param_groups) - stage - 1
        start_idx = max(0, start_idx)

        for i, group in enumerate(self.param_groups):
            for param in group.parameters():
                param.requires_grad = (i >= start_idx)

        trainable = sum(p.numel() for p in self.model.parameters() if p.requires_grad)
        print(f"Stage {stage}: å¯è®­ç»ƒå‚æ•° {trainable:,}")

# ä½¿ç”¨ç¤ºä¾‹
# Stage 0: åªè®­ç»ƒåˆ†ç±»å¤´
# Stage 1: è®­ç»ƒæœ€åå‡ å±‚ + åˆ†ç±»å¤´
# Stage 2: è®­ç»ƒå…¨éƒ¨
```

### 4.3 å®Œæ•´è¿ç§»å­¦ä¹ æµç¨‹

```python
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from torchvision import datasets, transforms, models

# ========== æ•°æ®å‡†å¤‡ ==========
train_transform = transforms.Compose([
    transforms.Resize(256),
    transforms.RandomCrop(224),
    transforms.RandomHorizontalFlip(),
    transforms.ToTensor(),
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
])

val_transform = transforms.Compose([
    transforms.Resize(256),
    transforms.CenterCrop(224),
    transforms.ToTensor(),
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
])

# ä½¿ç”¨ CIFAR-10 æ¼”ç¤º
train_dataset = datasets.CIFAR10('./data', train=True, download=True, transform=train_transform)
val_dataset = datasets.CIFAR10('./data', train=False, transform=val_transform)

train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=2)
val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=2)

# ========== æ¨¡å‹ ==========
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

model = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)
model.fc = nn.Linear(model.fc.in_features, 10)

# å†»ç»“ backbone
for name, param in model.named_parameters():
    if 'fc' not in name:
        param.requires_grad = False

model = model.to(device)

# ========== è®­ç»ƒ ==========
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=0.001)

def train_one_epoch(model, loader, criterion, optimizer, device):
    model.train()
    running_loss = 0
    correct = 0
    total = 0

    for inputs, labels in loader:
        inputs, labels = inputs.to(device), labels.to(device)

        optimizer.zero_grad()
        outputs = model(inputs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        running_loss += loss.item()
        _, predicted = outputs.max(1)
        total += labels.size(0)
        correct += predicted.eq(labels).sum().item()

    return running_loss / len(loader), correct / total

def evaluate(model, loader, device):
    model.eval()
    correct = 0
    total = 0

    with torch.no_grad():
        for inputs, labels in loader:
            inputs, labels = inputs.to(device), labels.to(device)
            outputs = model(inputs)
            _, predicted = outputs.max(1)
            total += labels.size(0)
            correct += predicted.eq(labels).sum().item()

    return correct / total

# è®­ç»ƒ
for epoch in range(5):
    train_loss, train_acc = train_one_epoch(model, train_loader, criterion, optimizer, device)
    val_acc = evaluate(model, val_loader, device)
    print(f'Epoch {epoch+1}: Loss={train_loss:.4f}, Train Acc={train_acc:.4f}, Val Acc={val_acc:.4f}')
```

---

## 5. ç»ƒä¹ é¢˜

### åŸºç¡€ç»ƒä¹ 

1. æ‰‹å†™ä¸€ä¸ª BasicBlockï¼Œç†è§£æ®‹å·®è¿æ¥
2. ç”¨é¢„è®­ç»ƒ ResNet åš CIFAR-10 åˆ†ç±»ï¼Œå¯¹æ¯”ä»å¤´è®­ç»ƒ
3. ç†è§£ ViT çš„ patch embedding è¿‡ç¨‹

### å‚è€ƒç­”æ¡ˆ

<details>
<summary>ç‚¹å‡»æŸ¥çœ‹ç­”æ¡ˆ</summary>

```python
# 1. æ‰‹å†™ BasicBlock
class MyBasicBlock(nn.Module):
    def __init__(self, channels):
        super().__init__()
        self.conv1 = nn.Conv2d(channels, channels, 3, padding=1)
        self.bn1 = nn.BatchNorm2d(channels)
        self.conv2 = nn.Conv2d(channels, channels, 3, padding=1)
        self.bn2 = nn.BatchNorm2d(channels)

    def forward(self, x):
        identity = x  # ä¿å­˜è¾“å…¥

        out = torch.relu(self.bn1(self.conv1(x)))
        out = self.bn2(self.conv2(out))

        out = out + identity  # æ®‹å·®è¿æ¥
        out = torch.relu(out)

        return out

# éªŒè¯æ®‹å·®è¿æ¥
block = MyBasicBlock(64)
x = torch.randn(2, 64, 32, 32)
y = block(x)
print(f"è¾“å…¥è¾“å‡ºå½¢çŠ¶: {x.shape} â†’ {y.shape}")


# 2. é¢„è®­ç»ƒ vs ä»å¤´è®­ç»ƒå¯¹æ¯”
# é¢„è®­ç»ƒæ¨¡å‹é€šå¸¸åœ¨å°‘é‡æ•°æ®ä¸Šå°±èƒ½è¾¾åˆ°å¾ˆé«˜å‡†ç¡®ç‡
# ä»å¤´è®­ç»ƒéœ€è¦æ›´å¤šæ•°æ®å’Œ epoch


# 3. Patch Embedding ç†è§£
def manual_patch_embed(img, patch_size=16, embed_dim=768):
    """æ‰‹åŠ¨å®ç° patch embedding"""
    B, C, H, W = img.shape
    assert H % patch_size == 0 and W % patch_size == 0

    # åˆ†å‰²æˆ patch
    num_patches_h = H // patch_size
    num_patches_w = W // patch_size

    # é‡æ’: [B, C, H, W] â†’ [B, num_patches, patch_size*patch_size*C]
    patches = img.unfold(2, patch_size, patch_size).unfold(3, patch_size, patch_size)
    # [B, C, num_h, num_w, patch_size, patch_size]
    patches = patches.permute(0, 2, 3, 1, 4, 5).contiguous()
    # [B, num_h, num_w, C, patch_size, patch_size]
    patches = patches.view(B, num_patches_h * num_patches_w, -1)
    # [B, num_patches, C*patch_size*patch_size]

    # çº¿æ€§æŠ•å½±
    proj = nn.Linear(C * patch_size * patch_size, embed_dim)
    embedded = proj(patches)

    return embedded

img = torch.randn(2, 3, 224, 224)
embedded = manual_patch_embed(img)
print(f"æ‰‹åŠ¨ Patch Embedding: {embedded.shape}")  # [2, 196, 768]
```

</details>

---

## â¡ï¸ ä¸‹ä¸€æ­¥

å­¦å®Œæœ¬èŠ‚åï¼Œç»§ç»­å­¦ä¹  [08-å¾ªç¯ç¥ç»ç½‘ç»œRNN.md](./08-å¾ªç¯ç¥ç»ç½‘ç»œRNN.md)

