# ğŸ”¥ 05 - æ•°æ®åŠ è½½ä¸æ¨¡å‹ä¿å­˜

> Dataset å’Œ DataLoader æ˜¯ PyTorch æ•°æ®å¤„ç†çš„æ ¸å¿ƒï¼Œæ¨¡å‹ä¿å­˜/åŠ è½½æ˜¯å·¥ç¨‹å¿…å¤‡æŠ€èƒ½

---

## ç›®å½•

1. [Dataset åŸºç¡€](#1-dataset-åŸºç¡€)
2. [DataLoader è¯¦è§£](#2-dataloader-è¯¦è§£)
3. [æ•°æ®å¢å¼º](#3-æ•°æ®å¢å¼º)
4. [æ¨¡å‹ä¿å­˜ä¸åŠ è½½](#4-æ¨¡å‹ä¿å­˜ä¸åŠ è½½)
5. [Checkpoint ç®¡ç†](#5-checkpoint-ç®¡ç†)
6. [ç»ƒä¹ é¢˜](#6-ç»ƒä¹ é¢˜)

---

## 1. Dataset åŸºç¡€

### 1.1 å†…ç½® Dataset

```python
import torch
from torch.utils.data import Dataset, TensorDataset

# TensorDatasetï¼šä» Tensor åˆ›å»º
X = torch.randn(1000, 10)
y = torch.randint(0, 2, (1000,))

dataset = TensorDataset(X, y)
print(f"æ•°æ®é›†å¤§å°: {len(dataset)}")
print(f"å•ä¸ªæ ·æœ¬: {dataset[0]}")  # è¿”å›å…ƒç»„ (x, y)

# å¸¸ç”¨å†…ç½®æ•°æ®é›†
from torchvision import datasets

# MNIST
mnist = datasets.MNIST(root='./data', train=True, download=True)

# CIFAR-10
cifar = datasets.CIFAR10(root='./data', train=True, download=True)

# ImageNet, COCO ç­‰éœ€è¦æ‰‹åŠ¨ä¸‹è½½
```

### 1.2 è‡ªå®šä¹‰ Dataset

```python
from torch.utils.data import Dataset

class CustomDataset(Dataset):
    def __init__(self, data, labels, transform=None):
        """
        data: ç‰¹å¾æ•°æ®
        labels: æ ‡ç­¾
        transform: æ•°æ®å˜æ¢
        """
        self.data = data
        self.labels = labels
        self.transform = transform

    def __len__(self):
        """è¿”å›æ•°æ®é›†å¤§å°"""
        return len(self.data)

    def __getitem__(self, idx):
        """è¿”å›å•ä¸ªæ ·æœ¬"""
        x = self.data[idx]
        y = self.labels[idx]

        if self.transform:
            x = self.transform(x)

        return x, y

# ä½¿ç”¨
X = torch.randn(1000, 10)
y = torch.randint(0, 2, (1000,))
dataset = CustomDataset(X, y)

print(f"æ•°æ®é›†å¤§å°: {len(dataset)}")
sample_x, sample_y = dataset[0]
print(f"æ ·æœ¬ç‰¹å¾å½¢çŠ¶: {sample_x.shape}, æ ‡ç­¾: {sample_y}")
```

### 1.3 å›¾åƒæ•°æ®é›†ç¤ºä¾‹

```python
import os
from PIL import Image
from torch.utils.data import Dataset
from torchvision import transforms

class ImageDataset(Dataset):
    def __init__(self, image_dir, transform=None):
        self.image_dir = image_dir
        self.image_files = [f for f in os.listdir(image_dir) if f.endswith(('.jpg', '.png'))]
        self.transform = transform or transforms.ToTensor()

    def __len__(self):
        return len(self.image_files)

    def __getitem__(self, idx):
        img_name = self.image_files[idx]
        img_path = os.path.join(self.image_dir, img_name)

        # åŠ è½½å›¾åƒ
        image = Image.open(img_path).convert('RGB')

        # ä»æ–‡ä»¶åæå–æ ‡ç­¾ï¼ˆå‡è®¾æ ¼å¼ï¼šlabel_xxx.jpgï¼‰
        label = int(img_name.split('_')[0])

        if self.transform:
            image = self.transform(image)

        return image, label

# ä½¿ç”¨ç¤ºä¾‹
# dataset = ImageDataset('./images', transform=my_transform)
```

### 1.4 æ–‡æœ¬æ•°æ®é›†ç¤ºä¾‹

```python
class TextDataset(Dataset):
    def __init__(self, texts, labels, vocab, max_len=128):
        self.texts = texts
        self.labels = labels
        self.vocab = vocab  # è¯è¡¨ï¼šword -> id
        self.max_len = max_len

    def __len__(self):
        return len(self.texts)

    def tokenize(self, text):
        """ç®€å•åˆ†è¯"""
        tokens = text.lower().split()
        ids = [self.vocab.get(t, self.vocab['<UNK>']) for t in tokens]

        # æˆªæ–­æˆ–å¡«å……
        if len(ids) > self.max_len:
            ids = ids[:self.max_len]
        else:
            ids = ids + [self.vocab['<PAD>']] * (self.max_len - len(ids))

        return torch.tensor(ids)

    def __getitem__(self, idx):
        text = self.texts[idx]
        label = self.labels[idx]
        token_ids = self.tokenize(text)
        return token_ids, label

# æ„å»ºè¯è¡¨
def build_vocab(texts, min_freq=2):
    from collections import Counter
    word_freq = Counter()
    for text in texts:
        word_freq.update(text.lower().split())

    vocab = {'<PAD>': 0, '<UNK>': 1}
    for word, freq in word_freq.items():
        if freq >= min_freq:
            vocab[word] = len(vocab)
    return vocab

# ä½¿ç”¨
texts = ['hello world', 'deep learning is fun', 'pytorch is great']
labels = [0, 1, 1]
vocab = build_vocab(texts, min_freq=1)
dataset = TextDataset(texts, labels, vocab, max_len=10)

token_ids, label = dataset[0]
print(f"Token IDs: {token_ids}, æ ‡ç­¾: {label}")
```

---

## 2. DataLoader è¯¦è§£

### 2.1 åŸºæœ¬ä½¿ç”¨

```python
from torch.utils.data import DataLoader

dataset = TensorDataset(torch.randn(1000, 10), torch.randint(0, 2, (1000,)))

# åŸºæœ¬ DataLoader
loader = DataLoader(dataset, batch_size=32, shuffle=True)

# éå†
for batch_x, batch_y in loader:
    print(f"æ‰¹æ¬¡å½¢çŠ¶: {batch_x.shape}, {batch_y.shape}")
    break  # åªçœ‹ç¬¬ä¸€æ‰¹

print(f"æ‰¹æ¬¡æ•°é‡: {len(loader)}")  # ceil(1000/32) = 32
```

### 2.2 å¸¸ç”¨å‚æ•°

```python
loader = DataLoader(
    dataset,
    batch_size=32,      # æ‰¹æ¬¡å¤§å°
    shuffle=True,       # æ˜¯å¦æ‰“ä¹±
    num_workers=4,      # å¤šè¿›ç¨‹åŠ è½½ï¼ˆ0 è¡¨ç¤ºä¸»è¿›ç¨‹ï¼‰
    pin_memory=True,    # é”é¡µå†…å­˜ï¼ŒåŠ é€Ÿ GPU ä¼ è¾“
    drop_last=True,     # ä¸¢å¼ƒæœ€åä¸å®Œæ•´çš„æ‰¹æ¬¡
    collate_fn=None,    # è‡ªå®šä¹‰æ‰¹æ¬¡ç»„åˆå‡½æ•°
)

# è®­ç»ƒé›†é€šå¸¸ shuffle=True, drop_last=True
train_loader = DataLoader(train_set, batch_size=32, shuffle=True, drop_last=True)

# éªŒè¯/æµ‹è¯•é›†é€šå¸¸ shuffle=False, drop_last=False
val_loader = DataLoader(val_set, batch_size=32, shuffle=False)
```

### 2.3 è‡ªå®šä¹‰ collate_fn

```python
def custom_collate(batch):
    """
    å¤„ç†å˜é•¿åºåˆ—ç­‰æƒ…å†µ
    batch: [(x1, y1), (x2, y2), ...]
    """
    xs, ys = zip(*batch)

    # è·å–æœ€å¤§é•¿åº¦
    max_len = max(len(x) for x in xs)

    # å¡«å……åˆ°ç›¸åŒé•¿åº¦
    padded_xs = []
    for x in xs:
        padding = torch.zeros(max_len - len(x))
        padded_xs.append(torch.cat([x, padding]))

    return torch.stack(padded_xs), torch.tensor(ys)

# ä½¿ç”¨
loader = DataLoader(dataset, batch_size=32, collate_fn=custom_collate)
```

### 2.4 æ•°æ®é›†åˆ’åˆ†

```python
from torch.utils.data import random_split, Subset

# éšæœºåˆ’åˆ†
dataset = TensorDataset(torch.randn(1000, 10), torch.randint(0, 2, (1000,)))

train_size = int(0.8 * len(dataset))
val_size = len(dataset) - train_size
train_set, val_set = random_split(dataset, [train_size, val_size])

print(f"è®­ç»ƒé›†: {len(train_set)}, éªŒè¯é›†: {len(val_set)}")

# æŒ‰ç´¢å¼•åˆ’åˆ†
indices = list(range(len(dataset)))
train_indices = indices[:800]
val_indices = indices[800:]

train_set = Subset(dataset, train_indices)
val_set = Subset(dataset, val_indices)

# ä½¿ç”¨ sklearn çš„ train_test_split
from sklearn.model_selection import train_test_split

X = torch.randn(1000, 10)
y = torch.randint(0, 2, (1000,))

X_train, X_val, y_train, y_val = train_test_split(
    X.numpy(), y.numpy(), test_size=0.2, random_state=42, stratify=y.numpy()
)

train_set = TensorDataset(torch.tensor(X_train), torch.tensor(y_train))
val_set = TensorDataset(torch.tensor(X_val), torch.tensor(y_val))
```

---

## 3. æ•°æ®å¢å¼º

### 3.1 å›¾åƒå¢å¼º

```python
from torchvision import transforms

# è®­ç»ƒé›†å¢å¼º
train_transform = transforms.Compose([
    transforms.Resize(256),                    # è°ƒæ•´å¤§å°
    transforms.RandomCrop(224),                # éšæœºè£å‰ª
    transforms.RandomHorizontalFlip(p=0.5),    # éšæœºæ°´å¹³ç¿»è½¬
    transforms.RandomRotation(15),             # éšæœºæ—‹è½¬
    transforms.ColorJitter(                    # é¢œè‰²æŠ–åŠ¨
        brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1
    ),
    transforms.RandomAffine(                   # éšæœºä»¿å°„å˜æ¢
        degrees=15, translate=(0.1, 0.1), scale=(0.9, 1.1)
    ),
    transforms.ToTensor(),                     # è½¬ä¸º Tensor
    transforms.Normalize(                      # æ ‡å‡†åŒ–
        mean=[0.485, 0.456, 0.406],
        std=[0.229, 0.224, 0.225]
    ),
])

# éªŒè¯/æµ‹è¯•é›†ï¼ˆä¸å¢å¼ºï¼‰
val_transform = transforms.Compose([
    transforms.Resize(256),
    transforms.CenterCrop(224),
    transforms.ToTensor(),
    transforms.Normalize(
        mean=[0.485, 0.456, 0.406],
        std=[0.229, 0.224, 0.225]
    ),
])

# ä½¿ç”¨
from torchvision.datasets import CIFAR10

train_dataset = CIFAR10(root='./data', train=True, transform=train_transform, download=True)
val_dataset = CIFAR10(root='./data', train=False, transform=val_transform, download=True)
```

### 3.2 é«˜çº§å¢å¼º (AutoAugment, RandAugment)

```python
from torchvision.transforms import autoaugment

# AutoAugmentï¼ˆè‡ªåŠ¨æœç´¢çš„å¢å¼ºç­–ç•¥ï¼‰
auto_aug = transforms.Compose([
    transforms.Resize(256),
    transforms.RandomCrop(224),
    autoaugment.AutoAugment(autoaugment.AutoAugmentPolicy.IMAGENET),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])

# RandAugmentï¼ˆéšæœºå¢å¼ºï¼‰
rand_aug = transforms.Compose([
    transforms.Resize(256),
    transforms.RandomCrop(224),
    autoaugment.RandAugment(num_ops=2, magnitude=9),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])
```

### 3.3 MixUp å’Œ CutMix

```python
def mixup_data(x, y, alpha=0.2):
    """MixUp æ•°æ®å¢å¼º"""
    if alpha > 0:
        lam = np.random.beta(alpha, alpha)
    else:
        lam = 1

    batch_size = x.size(0)
    index = torch.randperm(batch_size)

    mixed_x = lam * x + (1 - lam) * x[index, :]
    y_a, y_b = y, y[index]

    return mixed_x, y_a, y_b, lam

def mixup_criterion(criterion, pred, y_a, y_b, lam):
    """MixUp æŸå¤±"""
    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)

# è®­ç»ƒä¸­ä½¿ç”¨
for x, y in train_loader:
    x, y_a, y_b, lam = mixup_data(x, y, alpha=0.2)
    output = model(x)
    loss = mixup_criterion(criterion, output, y_a, y_b, lam)
```

---

## 4. æ¨¡å‹ä¿å­˜ä¸åŠ è½½

### 4.1 ä¿å­˜/åŠ è½½ state_dictï¼ˆæ¨èï¼‰

```python
import torch

# ä¿å­˜
torch.save(model.state_dict(), 'model.pth')

# åŠ è½½
model = MyModel()  # å…ˆåˆ›å»ºæ¨¡å‹å®ä¾‹
model.load_state_dict(torch.load('model.pth'))
model.eval()  # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼

# åŠ è½½åˆ°æŒ‡å®šè®¾å¤‡
model.load_state_dict(torch.load('model.pth', map_location='cpu'))
model.load_state_dict(torch.load('model.pth', map_location='cuda:0'))

# è‡ªåŠ¨é€‰æ‹©è®¾å¤‡
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model.load_state_dict(torch.load('model.pth', map_location=device))
```

### 4.2 ä¿å­˜æ•´ä¸ªæ¨¡å‹

```python
# ä¿å­˜ï¼ˆåŒ…å«æ¨¡å‹ç»“æ„å’Œå‚æ•°ï¼‰
torch.save(model, 'model_full.pth')

# åŠ è½½
model = torch.load('model_full.pth')
model.eval()

# æ³¨æ„ï¼šè¿™ç§æ–¹å¼ä¾èµ–äºæ¨¡å‹ç±»çš„å®šä¹‰ï¼Œè·¨æ–‡ä»¶ä½¿ç”¨å¯èƒ½æœ‰é—®é¢˜
```

### 4.3 ä¿å­˜è®­ç»ƒçŠ¶æ€

```python
# ä¿å­˜å®Œæ•´è®­ç»ƒçŠ¶æ€
checkpoint = {
    'epoch': epoch,
    'model_state_dict': model.state_dict(),
    'optimizer_state_dict': optimizer.state_dict(),
    'scheduler_state_dict': scheduler.state_dict() if scheduler else None,
    'loss': loss,
    'best_accuracy': best_accuracy,
}
torch.save(checkpoint, 'checkpoint.pth')

# æ¢å¤è®­ç»ƒ
checkpoint = torch.load('checkpoint.pth')
model.load_state_dict(checkpoint['model_state_dict'])
optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
if scheduler and checkpoint['scheduler_state_dict']:
    scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
start_epoch = checkpoint['epoch'] + 1
best_accuracy = checkpoint['best_accuracy']

print(f"ä» epoch {start_epoch} æ¢å¤è®­ç»ƒï¼Œæœ€ä½³å‡†ç¡®ç‡: {best_accuracy:.2%}")
```

---

## 5. Checkpoint ç®¡ç†

### 5.1 åªä¿å­˜æœ€ä½³æ¨¡å‹

```python
class BestModelSaver:
    def __init__(self, path='best_model.pth', mode='max'):
        self.path = path
        self.mode = mode  # 'max' for accuracy, 'min' for loss
        self.best = float('-inf') if mode == 'max' else float('inf')

    def __call__(self, metric, model):
        is_best = (self.mode == 'max' and metric > self.best) or \
                  (self.mode == 'min' and metric < self.best)

        if is_best:
            self.best = metric
            torch.save(model.state_dict(), self.path)
            print(f"ä¿å­˜æœ€ä½³æ¨¡å‹ï¼ŒæŒ‡æ ‡: {metric:.4f}")

        return is_best

# ä½¿ç”¨
saver = BestModelSaver('best_model.pth', mode='max')

for epoch in range(num_epochs):
    train(...)
    val_acc = validate(...)
    saver(val_acc, model)
```

### 5.2 ä¿å­˜å¤šä¸ª Checkpoint

```python
import os
from collections import deque

class CheckpointManager:
    def __init__(self, save_dir='checkpoints', max_to_keep=5):
        self.save_dir = save_dir
        self.max_to_keep = max_to_keep
        self.checkpoints = deque()
        os.makedirs(save_dir, exist_ok=True)

    def save(self, model, optimizer, epoch, metric):
        path = os.path.join(self.save_dir, f'checkpoint_epoch{epoch}_{metric:.4f}.pth')

        torch.save({
            'epoch': epoch,
            'model_state_dict': model.state_dict(),
            'optimizer_state_dict': optimizer.state_dict(),
            'metric': metric,
        }, path)

        self.checkpoints.append(path)

        # åˆ é™¤æ—§çš„
        while len(self.checkpoints) > self.max_to_keep:
            old_path = self.checkpoints.popleft()
            if os.path.exists(old_path):
                os.remove(old_path)
                print(f"åˆ é™¤æ—§ checkpoint: {old_path}")

        print(f"ä¿å­˜ checkpoint: {path}")

    def load_latest(self):
        if self.checkpoints:
            return torch.load(self.checkpoints[-1])
        return None

# ä½¿ç”¨
ckpt_manager = CheckpointManager('checkpoints', max_to_keep=3)

for epoch in range(num_epochs):
    train(...)
    val_acc = validate(...)
    ckpt_manager.save(model, optimizer, epoch, val_acc)
```

### 5.3 Early Stopping

```python
class EarlyStopping:
    def __init__(self, patience=10, min_delta=0, mode='min'):
        self.patience = patience
        self.min_delta = min_delta
        self.mode = mode
        self.counter = 0
        self.best = float('inf') if mode == 'min' else float('-inf')
        self.should_stop = False

    def __call__(self, metric):
        if self.mode == 'min':
            improved = metric < self.best - self.min_delta
        else:
            improved = metric > self.best + self.min_delta

        if improved:
            self.best = metric
            self.counter = 0
        else:
            self.counter += 1
            if self.counter >= self.patience:
                self.should_stop = True
                print(f"Early stopping: {self.patience} epochs æ²¡æœ‰æå‡")

        return self.should_stop

# ä½¿ç”¨
early_stopping = EarlyStopping(patience=10, mode='min')

for epoch in range(num_epochs):
    train(...)
    val_loss = validate(...)

    if early_stopping(val_loss):
        break
```

---

## 6. ç»ƒä¹ é¢˜

### åŸºç¡€ç»ƒä¹ 

1. åˆ›å»ºä¸€ä¸ªè‡ªå®šä¹‰ Datasetï¼Œä» CSV æ–‡ä»¶åŠ è½½æ•°æ®
2. å®ç°ä¸€ä¸ª DataLoaderï¼Œç”¨ collate_fn å¤„ç†å˜é•¿æ–‡æœ¬
3. å®ç°å®Œæ•´çš„è®­ç»ƒå¾ªç¯ï¼ŒåŒ…å«æ¨¡å‹ä¿å­˜å’Œ early stopping

### å‚è€ƒç­”æ¡ˆ

<details>
<summary>ç‚¹å‡»æŸ¥çœ‹ç­”æ¡ˆ</summary>

```python
import torch
from torch.utils.data import Dataset, DataLoader
import pandas as pd

# 1. CSV æ•°æ®é›†
class CSVDataset(Dataset):
    def __init__(self, csv_path, feature_cols, label_col):
        self.df = pd.read_csv(csv_path)
        self.features = torch.tensor(self.df[feature_cols].values, dtype=torch.float32)
        self.labels = torch.tensor(self.df[label_col].values, dtype=torch.long)

    def __len__(self):
        return len(self.labels)

    def __getitem__(self, idx):
        return self.features[idx], self.labels[idx]


# 2. å˜é•¿æ–‡æœ¬ collate_fn
def text_collate_fn(batch):
    texts, labels = zip(*batch)

    # æ‰¾æœ€å¤§é•¿åº¦
    max_len = max(len(t) for t in texts)

    # å¡«å……
    padded = []
    masks = []
    for t in texts:
        pad_len = max_len - len(t)
        padded.append(torch.cat([t, torch.zeros(pad_len)]))
        masks.append(torch.cat([torch.ones(len(t)), torch.zeros(pad_len)]))

    return torch.stack(padded).long(), torch.stack(masks), torch.tensor(labels)


# 3. å®Œæ•´è®­ç»ƒå¾ªç¯
import torch.nn as nn
import torch.optim as optim

def train_with_checkpoint(
    model, train_loader, val_loader,
    num_epochs=100, patience=10, save_path='best_model.pth'
):
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model = model.to(device)

    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=0.001)

    best_val_acc = 0
    no_improve = 0

    for epoch in range(num_epochs):
        # è®­ç»ƒ
        model.train()
        train_loss = 0
        for x, y in train_loader:
            x, y = x.to(device), y.to(device)

            optimizer.zero_grad()
            output = model(x)
            loss = criterion(output, y)
            loss.backward()
            optimizer.step()

            train_loss += loss.item()

        # éªŒè¯
        model.eval()
        correct = 0
        total = 0
        with torch.no_grad():
            for x, y in val_loader:
                x, y = x.to(device), y.to(device)
                output = model(x)
                pred = output.argmax(1)
                correct += (pred == y).sum().item()
                total += len(y)

        val_acc = correct / total
        train_loss /= len(train_loader)

        print(f"Epoch {epoch+1}: Train Loss = {train_loss:.4f}, Val Acc = {val_acc:.4f}")

        # ä¿å­˜æœ€ä½³
        if val_acc > best_val_acc:
            best_val_acc = val_acc
            torch.save(model.state_dict(), save_path)
            no_improve = 0
            print(f"  ä¿å­˜æœ€ä½³æ¨¡å‹ï¼Œå‡†ç¡®ç‡: {val_acc:.4f}")
        else:
            no_improve += 1

        # Early stopping
        if no_improve >= patience:
            print(f"Early stopping at epoch {epoch+1}")
            break

    # åŠ è½½æœ€ä½³æ¨¡å‹
    model.load_state_dict(torch.load(save_path))
    return model, best_val_acc
```

</details>

---

## â¡ï¸ ä¸‹ä¸€æ­¥

å­¦å®Œæœ¬èŠ‚åï¼Œç»§ç»­å­¦ä¹  [06-å·ç§¯ç¥ç»ç½‘ç»œCNN.md](./06-å·ç§¯ç¥ç»ç½‘ç»œCNN.md)

