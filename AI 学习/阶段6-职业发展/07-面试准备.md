# ğŸ¤ 07 - é¢è¯•å‡†å¤‡

> å…«è‚¡æ–‡ã€é¡¹ç›®æ·±æŒ–ã€æ‰‹æ’•ä»£ç 

---

## é¢è¯•ç±»å‹

```
1. æŠ€æœ¯ä¸€é¢ï¼šåŸºç¡€çŸ¥è¯† + é¡¹ç›®ä»‹ç»
2. æŠ€æœ¯äºŒé¢ï¼šé¡¹ç›®æ·±æŒ– + ç³»ç»Ÿè®¾è®¡
3. æŠ€æœ¯ä¸‰é¢ï¼šç®—æ³•/æ‰‹æ’• + æŠ€æœ¯å¹¿åº¦
4. HR é¢ï¼šè–ªèµ„ã€ç¨³å®šæ€§ã€èŒä¸šè§„åˆ’
```

---

## å…«è‚¡æ–‡æ¸…å•

### Transformer æ¶æ„

```
Q: Transformer çš„æ ¸å¿ƒç»„ä»¶æœ‰å“ªäº›ï¼Ÿå„æœ‰ä»€ä¹ˆä½œç”¨ï¼Ÿ

A:
1. Self-Attentionï¼šæ•æ‰åºåˆ—å†…éƒ¨çš„ä¾èµ–å…³ç³»
2. Multi-Head Attentionï¼šå¤šè§’åº¦å…³æ³¨ä¸åŒç‰¹å¾
3. Feed Forward Networkï¼šéçº¿æ€§å˜æ¢ï¼Œå¢åŠ è¡¨è¾¾èƒ½åŠ›
4. Residual Connectionï¼šç¼“è§£æ¢¯åº¦æ¶ˆå¤±
5. Layer Normalizationï¼šç¨³å®šè®­ç»ƒ
6. Positional Encodingï¼šæ³¨å…¥ä½ç½®ä¿¡æ¯

Q: Self-Attention çš„è®¡ç®—å…¬å¼æ˜¯ä»€ä¹ˆï¼Ÿå¤æ‚åº¦æ˜¯å¤šå°‘ï¼Ÿ

A:
Attention(Q,K,V) = softmax(QK^T / âˆšd_k) V

å¤æ‚åº¦ï¼šO(nÂ²Â·d)ï¼Œn æ˜¯åºåˆ—é•¿åº¦ï¼Œd æ˜¯ç»´åº¦
```

### Attention æœºåˆ¶

```
Q: ä¸ºä»€ä¹ˆè¦é™¤ä»¥ âˆšd_kï¼Ÿ

A:
å½“ d_k è¾ƒå¤§æ—¶ï¼ŒQK^T çš„å€¼ä¼šå¾ˆå¤§ï¼Œ
softmax åä¼šè¶‹å‘äº one-hotï¼Œæ¢¯åº¦æ¶ˆå¤±ã€‚
é™¤ä»¥ âˆšd_k å¯ä»¥ç¨³å®šæ–¹å·®ï¼Œè®© softmax æ›´å¹³æ»‘ã€‚

Q: Multi-Head Attention ç›¸æ¯” Single-Head æœ‰ä»€ä¹ˆä¼˜åŠ¿ï¼Ÿ

A:
1. å¯ä»¥å…³æ³¨ä¸åŒä½ç½®çš„ä¸åŒç‰¹å¾
2. ç±»ä¼¼ CNN çš„å¤šé€šé“ï¼Œå¢åŠ è¡¨è¾¾èƒ½åŠ›
3. è®¡ç®—é‡ç›¸åŒï¼ˆåˆ†å‰²ç»´åº¦ï¼‰

Q: KV Cache æ˜¯ä»€ä¹ˆï¼Ÿä¸ºä»€ä¹ˆèƒ½åŠ é€Ÿæ¨ç†ï¼Ÿ

A:
- ç”Ÿæˆæ—¶ï¼Œå·²ç”Ÿæˆ token çš„ Kã€V ä¸å˜
- ç¼“å­˜èµ·æ¥ï¼Œåªè®¡ç®—æ–° token çš„ Kã€V
- æ—¶é—´å¤æ‚åº¦ä» O(nÂ²) é™åˆ° O(n)
```

### RAG åŸç†

```
Q: RAG çš„åŸºæœ¬æµç¨‹æ˜¯ä»€ä¹ˆï¼Ÿ

A:
1. æ–‡æ¡£å¤„ç†ï¼šåŠ è½½ã€åˆ‡åˆ†ã€æ¸…æ´—
2. å‘é‡åŒ–ï¼šç”¨ Embedding æ¨¡å‹è½¬ä¸ºå‘é‡
3. å­˜å‚¨ï¼šå­˜å…¥å‘é‡æ•°æ®åº“
4. æ£€ç´¢ï¼šæŸ¥è¯¢å‘é‡åŒ–ï¼Œç›¸ä¼¼åº¦æ£€ç´¢
5. ç”Ÿæˆï¼šæ£€ç´¢ç»“æœ + é—®é¢˜ â†’ LLM ç”Ÿæˆç­”æ¡ˆ

Q: RAG æ£€ç´¢æ•ˆæœä¸å¥½æ€ä¹ˆä¼˜åŒ–ï¼Ÿ

A:
1. ä¼˜åŒ–åˆ‡åˆ†ï¼šè°ƒæ•´ chunk_sizeã€å¢åŠ  overlap
2. ä¼˜åŒ– Embeddingï¼šæ¢æ¨¡å‹ã€Fine-tune
3. ä¼˜åŒ–æ£€ç´¢ï¼šHybrid Searchã€Multi-Query
4. å¢åŠ ç²¾æ’ï¼šRerank æ¨¡å‹
5. ä¼˜åŒ– Promptï¼šæ·»åŠ æŒ‡å¼•ã€Few-shot

Q: å‘é‡æ£€ç´¢æœ‰å“ªäº›ç›¸ä¼¼åº¦è®¡ç®—æ–¹å¼ï¼Ÿ

A:
1. ä½™å¼¦ç›¸ä¼¼åº¦ï¼šæœ€å¸¸ç”¨ï¼Œä¸å—å‘é‡é•¿åº¦å½±å“
2. ç‚¹ç§¯ï¼šè®¡ç®—å¿«ï¼Œä½†å—é•¿åº¦å½±å“
3. æ¬§æ°è·ç¦»ï¼šL2 è·ç¦»ï¼Œé€‚åˆè¿ç»­æ•°æ®
4. æ›¼å“ˆé¡¿è·ç¦»ï¼šL1 è·ç¦»ï¼Œå¯¹å¼‚å¸¸å€¼é²æ£’
```

### Agent åŸç†

```
Q: Agent çš„æ ¸å¿ƒç»„ä»¶æœ‰å“ªäº›ï¼Ÿ

A:
1. Planningï¼šä»»åŠ¡è§„åˆ’å’Œåˆ†è§£
2. Memoryï¼šçŸ­æœŸï¼ˆå¯¹è¯ï¼‰å’Œé•¿æœŸï¼ˆçŸ¥è¯†åº“ï¼‰
3. Toolsï¼šå¤–éƒ¨å·¥å…·è°ƒç”¨èƒ½åŠ›
4. Reflectionï¼šè‡ªæˆ‘åæ€å’Œçº é”™

Q: ReAct æ˜¯ä»€ä¹ˆï¼Ÿ

A:
Reasoning + Acting çš„ç»“åˆï¼š
1. Thoughtï¼šæ¨ç†å½“å‰åº”è¯¥åšä»€ä¹ˆ
2. Actionï¼šé€‰æ‹©å¹¶æ‰§è¡Œå·¥å…·
3. Observationï¼šè·å–æ‰§è¡Œç»“æœ
4. å¾ªç¯ç›´åˆ°å®Œæˆä»»åŠ¡

Q: Function Calling æ˜¯æ€ä¹ˆå®ç°çš„ï¼Ÿ

A:
1. å®šä¹‰å‡½æ•° Schemaï¼ˆJSON Schemaï¼‰
2. LLM æ ¹æ® Schema ç”Ÿæˆå‚æ•°
3. åç«¯è§£æå¹¶æ‰§è¡Œå‡½æ•°
4. ç»“æœè¿”å›ç»™ LLM ç»§ç»­
```

### å¾®è°ƒç›¸å…³

```
Q: LoRA çš„åŸç†æ˜¯ä»€ä¹ˆï¼Ÿ

A:
ä½ç§©é€‚åº”ï¼š
- å†»ç»“åŸå§‹æƒé‡ W
- æ·»åŠ ä½ç§©åˆ†è§£ Î”W = BA
- B: dÃ—r, A: rÃ—k, r << min(d,k)
- åªè®­ç»ƒ Aã€Bï¼Œå‚æ•°é‡å¤§å¹…å‡å°‘

Q: QLoRA ç›¸æ¯” LoRA æœ‰ä»€ä¹ˆæ”¹è¿›ï¼Ÿ

A:
1. åŸºåº§æ¨¡å‹ 4-bit é‡åŒ–ï¼ˆNF4ï¼‰
2. åŒé‡é‡åŒ–ï¼ˆé‡åŒ–é‡åŒ–å‚æ•°ï¼‰
3. åˆ†é¡µä¼˜åŒ–å™¨ï¼ˆé˜²æ­¢ OOMï¼‰
4. å¯åœ¨æ¶ˆè´¹çº§ GPU å¾®è°ƒå¤§æ¨¡å‹

Q: å¾®è°ƒæ•°æ®è´¨é‡çš„é‡è¦æ€§ï¼Ÿ

A:
- æ•°æ®è´¨é‡ > æ•°æ®æ•°é‡
- é«˜è´¨é‡ 1000 æ¡å¯èƒ½æ¯”ä½è´¨é‡ 10000 æ¡æ•ˆæœå¥½
- éœ€è¦è¦†ç›–ç›®æ ‡åœºæ™¯
- æ ¼å¼è¦ä¸æ¨ç†æ—¶ä¸€è‡´
```

---

## é¡¹ç›®æ·±æŒ–å‡†å¤‡

### å‡†å¤‡æ¡†æ¶

```
å¯¹æ¯ä¸ªé¡¹ç›®ï¼Œå‡†å¤‡ä»¥ä¸‹é—®é¢˜çš„å›ç­”ï¼š

1. é¡¹ç›®èƒŒæ™¯
   - ä¸ºä»€ä¹ˆè¦åšè¿™ä¸ªé¡¹ç›®ï¼Ÿ
   - è§£å†³ä»€ä¹ˆä¸šåŠ¡é—®é¢˜ï¼Ÿ

2. æŠ€æœ¯é€‰å‹
   - ä¸ºä»€ä¹ˆé€‰æ‹©è¿™ä¸ªæŠ€æœ¯æ ˆï¼Ÿ
   - è€ƒè™‘è¿‡å“ªäº›å¤‡é€‰æ–¹æ¡ˆï¼Ÿ
   - æƒè¡¡äº†å“ªäº›å› ç´ ï¼Ÿ

3. æ ¸å¿ƒéš¾ç‚¹
   - é‡åˆ°çš„æœ€å¤§æŠ€æœ¯æŒ‘æˆ˜æ˜¯ä»€ä¹ˆï¼Ÿ
   - æ€ä¹ˆè§£å†³çš„ï¼Ÿ
   - æœ‰æ²¡æœ‰å…¶ä»–æ–¹æ¡ˆï¼Ÿ

4. ä¼˜åŒ–è¿‡ç¨‹
   - åšäº†å“ªäº›ä¼˜åŒ–ï¼Ÿ
   - æ•ˆæœå¦‚ä½•ï¼Ÿ
   - æ€ä¹ˆéªŒè¯çš„ï¼Ÿ

5. ç»éªŒæ•™è®­
   - å¦‚æœé‡æ–°åšä¼šæ€ä¹ˆæ”¹è¿›ï¼Ÿ
   - å­¦åˆ°äº†ä»€ä¹ˆï¼Ÿ
```

### å¸¸è§æ·±æŒ–é—®é¢˜

```
Q: ä½ çš„ RAG ç³»ç»Ÿå‡†ç¡®ç‡æ˜¯æ€ä¹ˆè¯„ä¼°çš„ï¼Ÿ

å‡†å¤‡ï¼š
- è¯„ä¼°é›†æ€ä¹ˆæ„å»ºçš„
- ç”¨äº†ä»€ä¹ˆæŒ‡æ ‡ï¼ˆRecallã€Precisionã€F1ï¼‰
- äººå·¥è¯„ä¼°è¿˜æ˜¯è‡ªåŠ¨è¯„ä¼°
- è¯„ä¼°ç»“æœå…·ä½“æ•°å€¼

Q: æ£€ç´¢æ•ˆæœä¸å¥½çš„æ—¶å€™ä½ æ€ä¹ˆæ’æŸ¥ï¼Ÿ

å‡†å¤‡ï¼š
- å…ˆçœ‹æ˜¯æ£€ç´¢é—®é¢˜è¿˜æ˜¯ç”Ÿæˆé—®é¢˜
- æ£€ç´¢é—®é¢˜çœ‹ Embedding ç›¸ä¼¼åº¦
- åˆ†æ bad case
- å…·ä½“ä¼˜åŒ–æªæ–½

Q: ä½ ä»¬çš„ Agent æ˜¯æ€ä¹ˆå¤„ç†é”™è¯¯çš„ï¼Ÿ

å‡†å¤‡ï¼š
- é”™è¯¯ç±»å‹ï¼ˆå·¥å…·å¤±è´¥ã€LLM é”™è¯¯ã€è¶…æ—¶ï¼‰
- é‡è¯•æœºåˆ¶
- é™çº§ç­–ç•¥
- äººå·¥å¹²é¢„
```

---

## æ‰‹æ’•ä»£ç 

### Self-Attention å®ç°

```python
import torch
import torch.nn as nn
import math

class SelfAttention(nn.Module):
    def __init__(self, d_model, n_heads):
        super().__init__()
        self.d_model = d_model
        self.n_heads = n_heads
        self.d_k = d_model // n_heads

        self.W_q = nn.Linear(d_model, d_model)
        self.W_k = nn.Linear(d_model, d_model)
        self.W_v = nn.Linear(d_model, d_model)
        self.W_o = nn.Linear(d_model, d_model)

    def forward(self, x, mask=None):
        batch_size, seq_len, _ = x.shape

        # çº¿æ€§å˜æ¢
        Q = self.W_q(x)  # (B, L, D)
        K = self.W_k(x)
        V = self.W_v(x)

        # åˆ†å¤´
        Q = Q.view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)
        K = K.view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)
        V = V.view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)
        # (B, H, L, D_k)

        # è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°
        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)
        # (B, H, L, L)

        # æ©ç 
        if mask is not None:
            scores = scores.masked_fill(mask == 0, -1e9)

        # Softmax
        attn = torch.softmax(scores, dim=-1)

        # åŠ æƒæ±‚å’Œ
        out = torch.matmul(attn, V)  # (B, H, L, D_k)

        # åˆå¹¶å¤´
        out = out.transpose(1, 2).contiguous().view(batch_size, seq_len, self.d_model)

        # è¾“å‡ºæŠ•å½±
        out = self.W_o(out)

        return out
```

### å‘é‡æ£€ç´¢å®ç°

```python
import numpy as np

def cosine_similarity(a, b):
    """è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦"""
    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))

def vector_search(query_vec, doc_vecs, top_k=5):
    """å‘é‡æ£€ç´¢"""
    similarities = []
    for i, doc_vec in enumerate(doc_vecs):
        sim = cosine_similarity(query_vec, doc_vec)
        similarities.append((i, sim))

    # æŒ‰ç›¸ä¼¼åº¦æ’åº
    similarities.sort(key=lambda x: x[1], reverse=True)

    return similarities[:top_k]

# ä½¿ç”¨ numpy ä¼˜åŒ–ç‰ˆæœ¬
def vector_search_batch(query_vec, doc_vecs, top_k=5):
    """æ‰¹é‡å‘é‡æ£€ç´¢"""
    # å½’ä¸€åŒ–
    query_norm = query_vec / np.linalg.norm(query_vec)
    doc_norms = doc_vecs / np.linalg.norm(doc_vecs, axis=1, keepdims=True)

    # è®¡ç®—ç›¸ä¼¼åº¦
    similarities = np.dot(doc_norms, query_norm)

    # Top-K
    top_indices = np.argsort(similarities)[-top_k:][::-1]

    return [(i, similarities[i]) for i in top_indices]
```

### LoRA ç®€åŒ–å®ç°

```python
import torch
import torch.nn as nn

class LoRALayer(nn.Module):
    def __init__(self, in_features, out_features, rank=8, alpha=16):
        super().__init__()
        self.rank = rank
        self.alpha = alpha

        # åŸå§‹æƒé‡ï¼ˆå†»ç»“ï¼‰
        self.linear = nn.Linear(in_features, out_features, bias=False)
        self.linear.weight.requires_grad = False

        # LoRA å‚æ•°
        self.lora_A = nn.Parameter(torch.randn(in_features, rank) * 0.01)
        self.lora_B = nn.Parameter(torch.zeros(rank, out_features))

        self.scaling = alpha / rank

    def forward(self, x):
        # åŸå§‹è¾“å‡º
        original = self.linear(x)

        # LoRA å¢é‡
        lora_out = x @ self.lora_A @ self.lora_B * self.scaling

        return original + lora_out
```

---

## è‡ªæˆ‘ä»‹ç»æ¨¡æ¿

### 1 åˆ†é’Ÿç‰ˆ

```
é¢è¯•å®˜å¥½ï¼Œæˆ‘æ˜¯ [å§“å]ï¼Œ[X] å¹´å¼€å‘ç»éªŒã€‚

è¿‡å»ä¸€å¹´ä¸“æ³¨äº LLM åº”ç”¨å¼€å‘ï¼Œä¸»è¦åšè¿‡ä¸¤ä¸ªé¡¹ç›®ï¼š

ç¬¬ä¸€ä¸ªæ˜¯ä¼ä¸šçŸ¥è¯†åº“ RAG ç³»ç»Ÿï¼Œä»æŠ€æœ¯é€‰å‹åˆ°ä¸Šçº¿ç‹¬ç«‹è´Ÿè´£ï¼Œ
æ£€ç´¢å‡†ç¡®ç‡åšåˆ° 92%ï¼Œæ—¥å‡æœåŠ¡ 2000+ æ¬¡é—®ç­”ã€‚

ç¬¬äºŒä¸ªæ˜¯å¤š Agent è‡ªåŠ¨åŒ–è¿è¥ç³»ç»Ÿï¼Œè®¾è®¡äº†å¤š Agent åä½œæ¶æ„ï¼Œ
è‡ªåŠ¨åŒ–ç‡è¾¾åˆ° 95%ï¼Œä¸ºå›¢é˜ŸèŠ‚çœäº†å¤§é‡äººåŠ›ã€‚

æˆ‘å¯¹ RAG å’Œ Agent æŠ€æœ¯æœ‰æ·±å…¥ç†è§£ï¼Œ
å¸Œæœ›èƒ½åœ¨è´µå¸ç»§ç»­æ·±è€• LLM åº”ç”¨æ–¹å‘ã€‚
```

### 3 åˆ†é’Ÿç‰ˆ

```
é¢è¯•å®˜å¥½ï¼Œæˆ‘æ˜¯ [å§“å]ã€‚

å…ˆç®€å•ä»‹ç»ä¸‹æˆ‘çš„èƒŒæ™¯ï¼š
æˆ‘æœ‰ [X] å¹´å¼€å‘ç»éªŒï¼Œä¹‹å‰ä¸»è¦åšåç«¯å¼€å‘ã€‚
å»å¹´å¼€å§‹è½¬å‘ LLM åº”ç”¨å¼€å‘ï¼Œç³»ç»Ÿå­¦ä¹ äº†ä»åŸºç¡€åˆ°åº”ç”¨çš„æŠ€æœ¯æ ˆã€‚

æˆ‘åšè¿‡çš„æ ¸å¿ƒé¡¹ç›®æœ‰ä¸¤ä¸ªï¼š

ç¬¬ä¸€ä¸ªæ˜¯ä¼ä¸šçŸ¥è¯†åº“ RAG ç³»ç»Ÿï¼š
èƒŒæ™¯æ˜¯å…¬å¸å†…éƒ¨æ–‡æ¡£åˆ†æ•£ï¼Œå‘˜å·¥æ‰¾ä¿¡æ¯æ•ˆç‡ä½ã€‚
æˆ‘ç‹¬ç«‹è´Ÿè´£äº†æŠ€æœ¯é€‰å‹å’Œå¼€å‘ã€‚
æŠ€æœ¯ä¸Šç”¨ LangChain + ChromaDB æ„å»º RAG æµæ°´çº¿ï¼Œ
é‡ç‚¹ä¼˜åŒ–äº†æ£€ç´¢æ•ˆæœï¼Œé€šè¿‡ Hybrid Search å’Œ Rerankï¼Œ
æŠŠå‡†ç¡®ç‡ä»åŸºçº¿çš„ 77% æå‡åˆ° 92%ã€‚
ç°åœ¨æ—¥å‡å¤„ç† 2000+ æ¬¡é—®ç­”ï¼Œå‘˜å·¥æ»¡æ„åº¦å¾ˆé«˜ã€‚

ç¬¬äºŒä¸ªæ˜¯å¤š Agent è‡ªåŠ¨åŒ–ç³»ç»Ÿï¼š
ç”¨æ¥è‡ªåŠ¨åŒ–è¿è¥å›¢é˜Ÿçš„æ—¥å¸¸å·¥ä½œï¼ŒåŒ…æ‹¬æ•°æ®æ”¶é›†ã€æŠ¥å‘Šç”Ÿæˆã€‚
æˆ‘è®¾è®¡äº†åŸºäº LangGraph çš„å¤š Agent æ¶æ„ï¼Œ
å®ç°äº†ä»»åŠ¡åˆ†è§£ã€å·¥å…·è°ƒç”¨ã€ç»“æœæ•´åˆçš„å®Œæ•´æµç¨‹ã€‚
æœ€ç»ˆè‡ªåŠ¨åŒ–ç‡è¾¾åˆ° 95%ï¼Œæ¯å¤©èŠ‚çœ 3-4 å°æ—¶äººåŠ›ã€‚

æŠ€æœ¯ä¸Šï¼Œæˆ‘æ¯”è¾ƒæ“…é•¿ RAG ç³»ç»Ÿè®¾è®¡å’Œ Agent å¼€å‘ï¼Œ
å¯¹ Prompt Engineeringã€æ£€ç´¢ä¼˜åŒ–è¿™äº›æœ‰æ·±å…¥ç†è§£ã€‚
ä¹Ÿäº†è§£å¾®è°ƒå’Œéƒ¨ç½²ï¼Œèƒ½è¦†ç›– LLM åº”ç”¨çš„å®Œæ•´é“¾è·¯ã€‚

åŠ å…¥è´µå¸åï¼Œå¸Œæœ›èƒ½åœ¨ [ç›®æ ‡æ–¹å‘] ä¸Šç»§ç»­æ·±è€•ï¼Œ
ç”¨æŠ€æœ¯è§£å†³å®é™…ä¸šåŠ¡é—®é¢˜ã€‚
```

---

## â¡ï¸ ä¸‹ä¸€æ­¥

ç»§ç»­ [08-é¢è¯•é—®é¢˜é›†.md](./08-é¢è¯•é—®é¢˜é›†.md)

