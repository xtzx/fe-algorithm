# ✍️ 05 - 技术博客模板

> 项目复盘、技术总结、踩坑记录模板

---

## 项目复盘模板

```markdown
# [项目名称] 项目复盘：从零到一的 [技术方向] 实战

> 关键词：RAG、LangChain、向量检索、企业知识库

## 📌 项目背景

### 业务需求
[描述为什么要做这个项目，解决什么问题]

例如：
公司内部文档分散在多个系统，员工查找信息效率低。
希望构建一个智能问答系统，让员工可以用自然语言查询公司知识库。

### 技术挑战
- 挑战 1：多格式文档解析
- 挑战 2：检索准确率要求高
- 挑战 3：响应速度要求 < 2s

## 🎯 最终成果

| 指标 | 目标 | 实际 |
|------|------|------|
| 检索准确率 | 90% | 95.2% |
| 响应时间 | < 2s | 1.2s |
| 支持文档数 | 1万 | 5万 |

## 🏗️ 技术方案

### 整体架构

[放架构图]

### 技术选型

| 组件 | 选择 | 原因 |
|------|------|------|
| LLM | GPT-4o-mini | 性价比高，效果好 |
| 向量库 | ChromaDB | 轻量级，易部署 |
| 框架 | LangChain | 生态丰富，文档全 |

### 选型对比

[重要决策的对比分析]

例如向量库选型：
- Faiss：性能好但功能单一
- Milvus：功能强但部署复杂
- ChromaDB：平衡选择 ✅

## 💻 核心实现

### 1. 文档处理流程

```python
# 核心代码片段
def process_document(file_path):
    # 解析
    content = parse_document(file_path)
    # 切分
    chunks = split_text(content, chunk_size=500)
    # 向量化
    embeddings = embed(chunks)
    # 存储
    store(embeddings, chunks)
```

### 2. 检索优化

[描述检索优化的思路和实现]

### 3. 提示词设计

```
你是一个企业知识库助手...
[完整的 Prompt]
```

## 🔧 遇到的问题

### 问题 1：检索召回率低

**现象**：用户提问时，相关文档没有被检索到

**原因分析**：
- Embedding 模型对中文支持不好
- 文档切分粒度不合适

**解决方案**：
1. 更换为 BGE 中文 Embedding
2. 调整 chunk_size 从 1000 到 500
3. 增加 overlap

**效果**：召回率从 75% 提升到 93%

### 问题 2：...

## 📈 优化迭代

### V1.0 → V1.1

- 增加 Rerank 精排
- 效果：准确率 +5%

### V1.1 → V1.2

- 增加 Hybrid Search
- 效果：长尾问题覆盖率 +10%

## 📊 性能数据

[放性能测试图表]

## 💡 经验总结

### 做得好的

1. 早期做了充分的技术调研
2. 建立了完善的评估体系
3. ...

### 可以改进的

1. 应该更早引入 Rerank
2. 文档切分策略可以更智能
3. ...

### 关键经验

> RAG 系统的关键不在于 LLM，而在于检索质量。
> 80% 的问题都可以通过优化检索来解决。

## 🔗 相关资源

- [项目 GitHub](https://github.com/xxx)
- [在线 Demo](https://demo.xxx.com)
- [参考资料 1](https://xxx)

---

作者：xxx
日期：2024-xx-xx
```

---

## 技术踩坑总结模板

```markdown
# [问题关键词] 踩坑记录：[问题现象]

> 关键词：vLLM、显存、OOM、推理优化

## 🔴 问题现象

部署 vLLM 服务时，加载 7B 模型出现 OOM：

```
torch.cuda.OutOfMemoryError: CUDA out of memory.
Tried to allocate 2.00 GiB...
```

环境信息：
- GPU: RTX 3090 24GB
- 模型: Qwen2.5-7B
- vLLM: 0.4.0

## 🔍 排查过程

### 尝试 1：减少 GPU 内存占用

```bash
vllm serve --gpu-memory-utilization 0.8
```

结果：仍然 OOM

### 尝试 2：检查显存占用

```bash
nvidia-smi
```

发现：有其他进程占用了 4GB 显存

### 尝试 3：清理显存后重试

```bash
sudo fuser -v /dev/nvidia*
kill -9 [PID]
```

结果：成功启动！

## ✅ 解决方案

**根本原因**：有其他 Python 进程占用了显存

**解决步骤**：
1. 检查显存占用：`nvidia-smi`
2. 找到占用进程：`sudo fuser -v /dev/nvidia*`
3. 结束进程：`kill -9 [PID]`
4. 重新启动服务

**预防措施**：
```bash
# 启动前清理
pkill -f python
```

## 📝 经验教训

1. OOM 不一定是模型太大，先检查显存占用
2. 生产环境建议使用 Docker 隔离
3. 设置 `CUDA_VISIBLE_DEVICES` 避免冲突

## 🔗 参考资料

- [vLLM 官方文档](https://docs.vllm.ai/)
- [相关 Issue](https://github.com/vllm-project/vllm/issues/xxx)

---

作者：xxx | 日期：2024-xx-xx
```

---

## 技术教程模板

```markdown
# [技术名称] 入门指南：[目标描述]

> 本文将带你从零开始，用 [技术栈] 构建一个 [目标产物]

## 📖 前置知识

- Python 基础
- [其他前置知识]

## 🎯 目标

完成本教程后，你将：
- 理解 [概念1]
- 掌握 [技能1]
- 能够 [能力1]

## 📦 环境准备

```bash
pip install xxx
```

## 🚀 Step 1: [第一步标题]

[详细说明]

```python
# 代码示例
```

[代码解释]

## 🚀 Step 2: [第二步标题]

...

## 🚀 Step 3: [第三步标题]

...

## ✅ 完整代码

```python
# 完整可运行代码
```

## 🔧 常见问题

### Q1: [问题]
A: [解答]

### Q2: [问题]
A: [解答]

## 📈 进阶方向

- [进阶方向1]
- [进阶方向2]

## 🔗 参考资料

- [资料1](https://xxx)
- [资料2](https://xxx)

---

如果觉得有帮助，欢迎点赞收藏！
```

---

## 写作技巧

```
1. 标题要吸引人
   ❌ "RAG 实现"
   ✅ "从 0 到 1 构建企业级 RAG 系统：95% 准确率是怎么做到的"

2. 开头要有亮点
   - 先说结果/成果
   - 再说怎么做到的

3. 代码要可运行
   - 关键代码要完整
   - 最好提供 GitHub 链接

4. 要有量化数据
   - 性能指标
   - 前后对比
   - 具体数字

5. 结尾要有价值
   - 总结关键点
   - 给出下一步建议
```

---

## ➡️ 下一步

继续 [06-简历优化.md](./06-简历优化.md)

