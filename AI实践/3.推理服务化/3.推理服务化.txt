
这条路的核心是：让模型调用像一个稳定高吞吐服务，并能做成本/质量权衡。

与 Agent 平台、评估观测共用同一个项目（agent-platform），推理服务化为上层提供「稳定、可控、可计费」的模型调用能力。

⸻

3.1 你具体会做哪些事（非常具体）

A. 模型服务/网关（统一入口）

对内提供统一 API（OpenAI 兼容风格）：
	•	/v1/chat/completions（流式）
	•	/v1/embeddings
	•	支持多后端：
		•	闭源 API（OpenAI/Claude/Gemini）
		•	本地推理（vLLM/SGLang，云 GPU）
	•	支持多租户：API key、配额、权限

交付物：
	•	一个 LLM Gateway（统一鉴权、路由、日志、限流）

⸻

B. 性能与容量（压测 + 调参）

你会做：
	•	压测：并发、TTFT、p95/p99、tokens/s、显存
	•	调参：
		•	batch、max_tokens、上下文截断
		•	streaming buffer
		•	queue size、timeout
	•	资源规划：
		•	单模型能扛多少 QPS
		•	p95 目标下需要多少实例/并发

交付物：
	•	压测报告 + 容量规划表

⸻

C. 稳定性与保护（线上必备）
	•	限流：按用户/租户/接口
	•	超时/取消：避免请求堆积拖垮系统
	•	重试：区分可重试/不可重试（写工具不可乱重试）
	•	熔断：下游模型故障时快速失败并降级
	•	缓存：
		•	prompt 模板缓存
		•	RAG 检索缓存
		•	（谨慎）回答缓存（条件苛刻）

⸻

D. 路由与成本优化（高级但很值钱）
	•	任务分级路由：
		•	普通问答 → 快模型（gpt-4o-mini）
		•	复杂推理 → 强模型（claude-3.5-sonnet）
		•	代码生成 → 代码模型
	•	失败升级：
		•	先便宜模型
		•	置信度低/失败 → 升级更强模型
	•	成本核算：
		•	tokens_in/out
		•	每请求成本
		•	按团队/接口归因

⸻

E. 本地推理（云 GPU）

即使没有本地 GPU，也可以通过云 GPU 服务体验本地推理：
	•	云 GPU 平台选择：
		•	RunPod：按需付费，支持 vLLM 模板
		•	Vast.ai：便宜，社区 GPU
		•	Modal：Serverless GPU，按秒计费
		•	AWS/GCP/Azure：企业级，但较贵
	•	推理框架：
		•	vLLM：高性能，OpenAI 兼容 API
		•	SGLang：更快的 structured output
	•	模型选择（入门推荐）：
		•	Qwen2.5-7B-Instruct：中文友好
		•	Llama-3.1-8B-Instruct：通用能力强
		•	Mistral-7B-Instruct：速度快

交付物：
	•	云 GPU 上运行的 vLLM 服务
	•	压测报告（对比闭源 API）

⸻

F. 前端管理界面（简略 - 前端产出）

网关不只是后端，还需要管理界面：
	•	API Key 管理：
		•	创建/删除/禁用 API Key
		•	设置配额（tokens/请求数/费用上限）
	•	成本仪表盘：
		•	按租户/模型的成本趋势
		•	Top 消费排行
		•	成本告警配置
	•	路由策略配置：
		•	可视化配置路由规则
		•	A/B 测试配置
	•	模型健康状态：
		•	各后端模型的可用性
		•	延迟趋势

交付物：
	•	简单的网关管理界面

⸻

3.2 需要哪些能力（非常详细）

后端与系统能力
	•	高并发服务：Node 异步模型、并发控制与 backpressure
	•	SSE 流式：断线重连、心跳、取消
	•	队列/限流：token bucket/leaky bucket 思维
	•	超时/熔断：避免雪崩

性能工程能力
	•	指标：TTFT、p95/p99、吞吐、显存
	•	压测工具：k6/wrk/Locust（会用即可）
	•	资源测算：请求长度分布、KV cache 显存增长、长上下文成本

模型推理概念（不需要会写 CUDA）
	•	KV cache 为什么贵
	•	continuous batching 的收益与代价
	•	量化（INT4/INT8）对速度/质量/显存的影响（概念+选型）

前端能力（简略）
	•	表单与配置界面
	•	简单的图表展示
	•	状态管理

运维与交付能力
	•	Docker（GPU）
	•	日志与监控（至少能定位慢在哪里）
	•	灰度发布、版本管理

⸻

3.3 技术选型参考（新增）

网关框架
	•	Hono：轻量、快速、支持 SSE
	•	Fastify：高性能 Node 框架
	•	Express：简单，生态丰富

限流与队列
	•	自研：简单的 token bucket
	•	p-queue：Promise 并发控制
	•	bottleneck：限流库

本地推理
	•	vLLM：主流选择，OpenAI 兼容
	•	SGLang：structured output 更快
	•	Ollama：简单易用，适合本地开发

云 GPU
	•	RunPod：推荐入门，有 vLLM 模板
	•	Modal：Serverless，按秒计费
	•	Replicate：API 形式，最简单

前端
	•	Recharts：图表
	•	@tanstack/react-table：表格
	•	shadcn/ui：UI 组件

⸻

3.4 学习注意事项
	•	先把"网关 + 路由 + 限流 + 成本统计"做出来（即使后端全是闭源 API）
	•	没本地 GPU 也能学：网关能力不依赖本地推理；本地推理可用云 GPU
	•	先做正确性再做极致性能：否则你会在压测里调参调到"看起来快但结果不对"
	•	先学指标口径：TTFT、p95、队列长度、tokens/s —— 你会立刻理解为什么慢

⸻

3.5 实践注意事项（坑很多）
	•	只追吞吐不看 TTFT：用户体验崩
	•	没有取消/超时：队列堆积后全体变慢
	•	盲目缓存回答：可能导致"错答案被缓存扩散"
	•	路由策略不做回归评测：便宜模型路由后质量下降没人发现
	•	不做配额：成本无法控制
	•	无界队列：流量一上来必雪崩
	•	缓存没把权限放进 key：直接数据泄露

⸻

3.6 工作原理

像一个"LLM 反向代理 + 调度器"：

应用请求 → Gateway（鉴权/限流/排队/日志/trace）
→ Router（按策略选择后端：API 或 本地推理）
→ 后端生成（流式返回）
→ Gateway 记录 tokens/cost/latency/error
→ 监控告警 + 灰度发布 + 成本归因
→ 路由策略变更前后用回归集评测守门（衔接评估观测）

⸻

3.7 与其他方向的协作（新增）

推理服务化为 Agent 平台提供：
	•	稳定的模型调用：限流、熔断、降级保证可用性
	•	成本控制：配额、路由策略控制成本
	•	多模型支持：Agent 可以根据任务选择不同模型

推理服务化为评估观测提供：
	•	请求日志：每次调用的 tokens、延迟、成本
	•	trace 数据：集成到整体 trace 链路
	•	路由日志：为什么选择这个模型

共用的基础设施：
	•	packages/gateway：LLM Gateway 核心
	•	packages/shared：类型定义、错误码
	•	apps/web：网关管理界面

⸻

3.8 进阶方向（选修）

多区域部署
	•	就近路由：用户在哪个区域就用哪个后端
	•	故障转移：一个区域挂了切到另一个

Semantic Cache
	•	不是精确匹配，而是语义相似就命中
	•	需要 embedding + 相似度检索
	•	风险更高，需要仔细控制

模型蒸馏代理
	•	用大模型生成数据
	•	微调小模型
	•	在特定场景替代大模型

⸻

你在这里的稀缺性是"把 LLM 变成可运营的基础设施"。
