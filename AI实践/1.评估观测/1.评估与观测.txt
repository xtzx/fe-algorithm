
这条路的核心是：让 LLM 应用的每一次迭代都有数据支撑，不靠猜、不翻车。

与 Agent 平台方向共用同一个项目，评估观测为 Agent 提供「可定位、可度量、可迭代」的能力。

⸻

1.1 你具体会做哪些事（非常具体）

A. 可观测（Observability）全链路

你要把一次请求拆成多个阶段并记录：
	•	请求入口：用户/租户/会话、请求 id、模型选择
	•	RAG 检索：召回 k、耗时、命中文档 id、chunk id、embedding 版本
	•	Rerank（如果有）：候选数、耗时、TopN
	•	Prompt 构造：最终上下文长度、system prompt 版本、模板版本
	•	LLM 生成：TTFT、tokens_in/out、总耗时、模型版本、温度参数
	•	工具调用：工具名、参数校验结果、成功/失败、耗时、错误码
	•	后处理：结构化校验、内容安全、脱敏、引用对齐
	•	结果：是否完成任务、是否需要追问、是否转人工

交付物通常包括：
	•	trace（一次请求的"时间线"）
	•	metrics（可聚合指标：p95、成功率、成本）
	•	logs（可检索的结构化日志）

⸻

B. 离线评测（Eval Harness）+ 回归集

你要能一键跑：评测集 → 输出报告 → 对比版本。
	•	建一个 回归集（最开始 100~300 条即可）：来自真实业务问题、覆盖边界/失败类型
	•	定义 Rubric（评分规则）：正确性、引用正确性、工具正确性、安全性、格式正确性
	•	实现 评分：
		•	规则评分（schema 合法、工具参数正确、是否有引用等）
		•	Judge 评分（LLM-as-judge，需要 rubric + 抽样人工校准）
	•	输出报告：
		•	总体通过率/平均分
		•	按 tag 分桶（检索空、需追问、工具失败等）
		•	Top 失败样本列表（方便修）

⸻

C. 线上闭环（Badcase → 迭代）

你要做"能持续变好"的机制：
	•	用户点踩/失败/转人工 → 自动入库 badcase
	•	badcase 自动携带：输入、输出、引用、trace 指针、错误分类、模型版本
	•	每周固定流程：
		1.	聚类 badcase（按失败原因/业务域）
		2.	定根因：检索差？prompt 差？工具不稳？权限问题？
		3.	提修复：改 chunking、加 rerank、改工具 schema、调提示词
		4.	加入回归集，跑评测
		5.	小流量上线 A/B，观察指标

⸻

D. 前端可视化（新增 - 前端独特优势）

这是前端工程师的独占领域，也是让评估观测从"能用"变成"好用"的关键。

与 Agent 平台方向的差异化：
	•	Agent 平台：Trace 面板侧重于调试单次请求
	•	评估观测：侧重于聚合分析、趋势、对比版本

你会做：
	•	指标仪表盘（Dashboard）：
		•	关键指标卡片：请求量、成功率、平均延迟、token 成本
		•	趋势图：按小时/天/周展示指标变化
		•	分桶统计：按模型/工具/业务域/错误类型分组
	•	评测报告可视化：
		•	版本对比：baseline vs variant 的得分差异
		•	失败样本列表：可筛选、可排序、可跳转详情
		•	分 tag 柱状图：哪类问题最多
	•	Trace 聚合视图：
		•	阶段耗时分布图：哪个阶段最慢
		•	错误热力图：哪个时间段错误最多
		•	慢请求 Top-N 列表

交付物：
	•	React 仪表盘组件
	•	评测报告页面
	•	指标趋势图表

⸻

1.2 需要哪些能力（非常详细）

观测能力
	•	事件/Span 模型设计：trace_id/span_id/parent_span_id；事件字段规范
	•	指标体系：
		•	性能：TTFT、p50/p95/p99、阶段耗时拆分
		•	质量：任务成功率、引用命中率、工具成功率、拒答准确率
		•	成本：tokens、每请求成本、缓存命中率
	•	数据落地与查询：
		•	初期 JSONL/本地即可
		•	进阶：OpenTelemetry、Prometheus、Grafana、Jaeger/Tempo（会接入更强）
	•	隐私与脱敏：
		•	日志里不能直接存敏感字段
		•	需要 hash、mask、只存元数据

评测能力
	•	数据集设计：覆盖面、代表性、难度分层、去重
	•	Rubric 设计：可解释、可复现、能区分好坏
	•	Judge 可信性控制：
		•	同一 rubric
		•	抽样人工校准
		•	反作弊（避免把标准答案泄露给被测模型）
	•	版本管理：
		•	模型版本、prompt 版本、索引版本、数据集版本都要可追溯
		•	评测报告要能对比 baseline vs variant

工程与系统能力
	•	结构化日志（JSON log）、trace 传播（request context）
	•	脚本化（npm scripts / CI 定时跑 eval）
	•	基础数据分析能力（看分布、找相关性、做对比）

前端可视化能力（新增）
	•	图表库使用：Recharts / ECharts / Chart.js
	•	数据聚合与转换：前端处理 JSONL / 调用聚合 API
	•	表格组件：排序、筛选、分页、导出
	•	交互设计：下钻、联动、时间范围选择

协作能力（很关键）
	•	把"评测报告"变成团队共识：为什么要改、改了提升多少、有没有副作用
	•	跟 PM/QA 对齐指标：什么叫成功？什么叫可接受？

⸻

1.3 技术选型参考（新增）

Trace & Metrics
	•	开发阶段：JSONL 文件 + 自定义 tracer
	•	进阶：OpenTelemetry SDK + Jaeger/Tempo
	•	指标：Prometheus + Grafana（或自建简化版）

评测框架
	•	自研 eval harness（推荐先自研理解原理）
	•	参考：Langfuse、Braintrust、Promptfoo

前端可视化
	•	图表：Recharts（React 友好）、ECharts（功能强大）
	•	表格：@tanstack/react-table
	•	UI 组件：shadcn/ui（推荐）、Ant Design
	•	状态管理：zustand / jotai

LLM-as-Judge
	•	Claude / GPT-4o 作为 judge 模型
	•	需要固定模型版本，避免 judge 漂移

⸻

1.4 学习注意事项（怎么学不走弯路）
	•	先做"可定位"再做"漂亮仪表盘"：能看到哪一步慢、哪一步错最重要
	•	先规则评分，再上 judge：规则能定很多"硬正确"，judge 用来补"软正确"
	•	回归集宁少勿乱：低质量样本会让你"改错方向"
	•	把版本写死：不版本化就没法 A/B、没法回滚
	•	前端可视化要尽早做：能看到图表会大大提升学习动力

⸻

1.5 实践注意事项（上线会踩的坑）
	•	只看总体分数，不看分桶：上线后某一类问题崩盘你却不知道
	•	线上采样不稳：采样策略要固定，否则指标波动大
	•	日志泄密：最常见事故之一（尤其 RAG 把原文记进 log）
	•	judge 漂移：judge 模型升级会导致历史分数不可比（需要固定 judge 或换算策略）
	•	仪表盘数据不更新：没做定时刷新或 WebSocket 推送
	•	图表性能问题：数据量大时前端卡顿，需要分页/采样/后端聚合

⸻

1.6 工作原理（系统怎么跑）

可以理解为一个闭环：

请求处理链路
用户请求 →（生成 request_id）→ 检索/工具/生成 → 每一步写 span+metrics → 返回结果

数据流向
span/metrics → 落地存储（JSONL/DB）→ 聚合脚本 → 仪表盘展示

评测链路
回归集 → runner（模拟/真实调用）→ scorer（规则+judge）→ report（对比版本）→ 可视化报告

迭代链路
线上 badcase → 入库 → 聚类分析 → 改策略 → 加回归集 → eval 通过 → 灰度上线 → 指标验证

⸻

1.7 与 Agent 平台方向的协作（新增）

评估观测为 Agent 平台提供：
	•	Trace 收集：Agent Runtime 的每一步都写 span
	•	工具监控：工具成功率、耗时、错误分类
	•	质量门禁：新版本必须通过 eval 才能上线
	•	迭代反馈：badcase 驱动 prompt/工具/检索优化

共用的基础设施：
	•	packages/shared：trace 类型、错误码、指标定义
	•	packages/runtime：tracer 模块集成
	•	apps/web：仪表盘 + 评测报告 + Trace 面板

⸻

1.8 进阶方向（选修）

A/B 测试平台
	•	流量分桶：同一用户看到同一版本
	•	指标对比：转化率、满意度、成本
	•	显著性检验：判断差异是否真实

实时告警
	•	错误率突增 → 告警
	•	延迟突增 → 告警
	•	成本异常 → 告警
	•	实现：阈值规则 / 异常检测算法

多维度分析
	•	按租户/模型/业务域下钻
	•	相关性分析：哪些因素影响成功率
	•	归因分析：失败主要来自哪个环节

⸻

你在这里成为"能让系统迭代不翻车的人"，这是非常稀缺的。
